# -*- coding: utf-8 -*-
"""a2 (1).ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ghYdoIbH8NfyM3t2gKgDAUHK4bA4mBi9

Train a simple classifier, as a baseline. It could be a traditional classifier (SVM, Random Forest, NB, or other), or using some pre-trained models based on deep learning (pre-trained word embeddings or text embeddings or other models, fine-tuned or not). In fact, there are two baselines provided, based on transformers. You can run at least one of them and explain in your report what method was used and what was the accuracy you obtained.
"""

import pandas as pd
from sklearn import svm
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics import accuracy_score

# Import data
train_data = pd.read_json(path_or_buf='./train/subtaskA_train_monolingual.jsonl', lines=True)
test_data = pd.read_json(path_or_buf='./test/subtaskA_monolingual.jsonl', lines=True)
gold_data = pd.read_json(path_or_buf='./gold/subtaskA_monolingual.jsonl', lines=True)

# Vectorize the text
vectorizer = TfidfVectorizer(stop_words='english', max_features=59552)
train_vec = vectorizer.fit_transform(train_data['text'])
gold_vec = vectorizer.fit_transform(gold_data['text'])

y_train = train_data['label']
y_gold = gold_data['label']

# Train baseline and get accuracy based on gold standard
baseline = svm.SVC()
baseline.fit(train_vec, y_train)

# Calculate accuracy
y_pred = baseline.predict(gold_vec)
print(accuracy_score(y_pred, y_gold))

"""Train at least two advanced classifiers based on deep learning, such as fine-tuning a type of BERT model for the first method (though not the version from the baseline in part 1);  and using a recent type of generative LLM for the second method (such as Llama or something equivalent).  Use part of the training data for validation (or use the dev data for validation) when building your models and keep aside the test data for the final testing. (Alternatively, you can try prompt-based learning with LLMs for the second method)."""

import evaluate
import numpy as np
from transformers import AutoModelForSequenceClassification, Trainer, TrainingArguments

# Make JSON file to give to Cohere dashboard
train_jsonl = train_data.drop(['id', 'model', 'source'], axis=1)
train_jsonl.label = train_jsonl.label.astype(str)
train_jsonl.to_json('train.jsonl', orient='records', lines=True)

texts = test_data['text'].tolist()
# Make a list of lists where each inner list is of max size 96 (API limitations)
inputs = []
curr_input = []
for text in texts:
    curr_input.append(text)
    if len(curr_input) == 96:
        inputs.append(curr_input)
        curr_input = []

# Fine-tune Cohere
import cohere

co = cohere.Client('UyGhWFcqhjJGqRfXjRrh30DC2vjQoC94IszSiMuB') # This is your trial API key
responses = []
for input in inputs:
    response = co.classify(
      model='77a9f33a-4bba-4b4f-9fee-074373d8b122-ft',
      inputs=input)
    responses.append(response)

def print_results(pred, path):
    print_data = test_data.drop(['text'], axis=1)
    print_data['label'] = pd.Series(pred)
    print_data.to_json(path, orient='records', lines=True)

# Print results for SVM
print_results(y_pred, 'svm.jsonl')

# Print results for Cohere
cohere_pred = []
for response in responses:
    for pred in response:
        cohere_pred.append(pred.predictions[0])
print_results(cohere_pred, 'cohere.jsonl')