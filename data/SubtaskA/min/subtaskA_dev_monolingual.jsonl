{"text":"Giving gifts should always be enjoyable.  However, it may become stressful when trying to find that perfect present.   This wikiHow will help you figure out exactly what you'd love to receive this year!   If you're having trouble deciding between two different items (or more), try making lists of advantages\/disadvantages so you'll know which one would make the best choice.    Make sure it's appropriate - some people don't appreciate receiving certain types of presents from their friends and\/or family members.     Don't forget to include any special requests you've already made before now; these might have been forgotten by others who haven't seen them yet.        Write down all... Continue reading \u2192\n\nIf you can't think of anything specific right away but still feel like getting something nice,  consider giving yourself a treat instead!  Here are just a few suggestions:  Buying yourself flowers Gifting yourself tickets to see your favorite band Take time off work Treat yourself to lunch or dinner at your favorite restaurant Spend extra money on your wardrobe Have a manicure Give yourself a massage Book a hotel room for the night","label":1,"model":"bloomz","source":"wikihow","id":0}
{"text":"Yveltal (Japanese: \u30e6\u30d9\u30eb\u30bf\u30eb) is one of the main antagonists in the anime series Pok\u00c3\u00a9mon Black & White 2.  It was introduced as the final boss for Pok\u00c3\u00a9mon Ranger: Guardian Signs before being added to Pok\u00c3\u00a9mon Battle Revolution.   In this tutorial we are going to show how to draw Yveltal based off its appearance in Pok\u00c3\u00a9mon XD: Gale of Darkness.    Start with a new piece of paper or open a new file if you're already working on something else.     The image below shows what you'll have drawn so far after following these steps.    You can now start adding details like facial features etc..     If you'd like to see more tutorials about other Pok\u00c3\u00a9mons click here!    How To Draw Baron   How To Draw Gengar  How To Draw Nidoran\u2640   How To Draw Nidorina... \nYou may also like:\n\nPok\u00c3\u00a9mon Go - What Does Each Type Mean?  \n\nWhat Are Pok\u00e9mon Held Items Used For? \n \nHow Do I Get More Stardust?\n\n \n \nHow Can I Catch A Pikachu With My Friend On Pokemon GO? \n\n\n \n \nHow do i get my pokemons back","label":1,"model":"bloomz","source":"wikihow","id":1}
{"text":"If you'd rather not annoy others by being rude but still want to have fun during dinner, try these suggestions instead.  Don't forget to tip well!  You may also like:  See also:   References:   ... How to Talk About Yourself While Eating Out \n \n \n \n 1. Sit yourself down without asking permission from anyone else.   2. Make noise throughout the entire meal.    3. Take too long between bites.     4. Stand up suddenly and shout \u201ccode red\u201d whenever something goes wrong outside the restaurant. (This works especially well if you\u2019re sitting near a window.)     5. Pretend to know how much everything costs before ordering anything so you don\u2019t get ripped off later.   6. Complain about the service constantly until everyone knows what\u2019s going on.      7. Leave halfway through your meal because it\u2019s taking way too long.        8. Tell everyone who comes over that this place sucks unless they\u2019re willing to pay more than $25 per plate.       9. Go back home hungry just to make things interesting.","label":1,"model":"bloomz","source":"wikihow","id":2}
{"text":"If you're interested in visiting gravesite(s) of deceased loved ones and\/or ancestors who lived long ago, you'll want to plan carefully before making such visits.  This section provides some tips about how best to prepare yourself physically and mentally so that these trips can become more productive experiences than just emotional excursions into the past.   If possible, try to get permission from those living at present whose ancestors' graves you'd like to visit; however, it's always better to ask forgiveness rather...than asking permission!\nBefore going to any particular location, check first whether or not the site itself may still exist today!  For example, many historic sites were destroyed during World War II by Allied bombing raids over Germany.    In addition,...if you live near New York City, chances are good that most of its older cemeteries would already have been razed away due to urban development pressures since the early 20th century.     Even though you might feel disappointed when discovering that certain locations which once existed now lie under modern-day highways, shopping centers, etc., remember this:  The purpose behind researching one's own ancestry isn't necessarily to locate specific physical places anymore -- although doing so does provide tangible evidence of our shared heritage with others whom we never knew personally....","label":1,"model":"bloomz","source":"wikihow","id":3}
{"text":"The following are some tips for becoming successful salesperson.  You can also find more detailed advice from other sources such as:  http:\/\/www.rainmakersales.com\/sales\/advice\/selling-ideas-for-successful-businesses\/ The first step is to get out there and start networking!   Make sure you've got everything you'll need before going into any meeting.   This includes having a clear understanding of:  \u2022 What do you want? \u2022 Who will be present?  \u2022 Where does it take place?\n\u2022 When should they expect you to arrive? \u2022 How long have you known this person (if applicable)? \u2022 Why did you choose him\/her specifically? \u2022 What's important to know about their organization and\/or position? \u2022 What would make them happy\/buyers? \u2022 What makes you different than others who might offer similar products or services? If you'd like help getting started, try using LinkedIn's Sales Navigator tool - it's free!  It allows you to:  \u2022 Find people based... Continue reading \u2192\n\nIf someone asks if you can provide references, ask them how many they'd like to see....","label":1,"model":"bloomz","source":"wikihow","id":4}
{"text":"Summer is here! The weather's getting warmer by the day - and we're all looking forward to spending more time than ever before outdoors.  But how do we stay stylish while still being comfortable?  Here at How-To, we've got some tips for you:  If you're going somewhere warm where there's no need for layers (like Florida), then opt for something lightweight like linen pants paired with an oversized t-shirt.   Pair jeans with a crop top and sneakers for a relaxed yet chic vibe.    Wear loose fitting clothing over fitted ones; think about layering as well.     Keep accessories simple... Don't forget sunglasses!  Sunglasses aren't only great protection against UV rays, they're also super fashionable.  You could even consider wearing a pair of aviator glasses for extra style points.    Remember to keep your makeup light during the daytime....   Take care of yourself.  Eat healthy foods such as fruits and vegetables, drink lots of water, exercise regularly, sleep plenty etc..    Have fun!","label":1,"model":"bloomz","source":"wikihow","id":5}
{"text":"This section will show you how to install an aftermarket remote starter system in your 2005-2008 Dodge Avenger.  This method uses the same steps as installing a remote starter kit for Chrysler vehicles with the exception that it does not require any special tools or equipment.   The instructions are written assuming... Continue reading \u2192\n,Disconnect the positive battery terminal before beginning work under the dash area.. If there\u2019s no power source available when working underneath the dashboard,... Read more \u00bb\n. Make sure all wires have been disconnected properly by checking them again using a multimeter.... Read more \u00bb\n\nIf you\u2019re having trouble finding these parts, consult this guide instead. .  You can use either adhesive tape or zip ties if necessary but make sure they\u2019re secured tightly enough so that nothing falls off while driving..  Note that some kits come without a wiring harness; however, it\u2019s highly recommended since it makes installation easier and safer than doing everything manually.  Some kits also include additional components such as door\/window sensors which allow you to arm\/unarm the car remotely via keyless fob even though someone may be inside the vehicle.  These features aren\u2019t required at all but might prove useful depending on what kind of security concerns you want to address.    To prevent damage during shipping, most manufacturers wrap their products individually in bubble wrap and\/or cardboard boxes.  Unwrap each item carefully one-by-one until you reach the last piece then set aside somewhere safe where you won\u2019t lose track of them later on.  For example, place the mounting bracket near the front seat next to the driver's side window.  It\u2019s best to keep loose items like screws together into small piles rather than scattering around everywhere because it\u2019ll help reduce cluttering up the workspace.  Once unwrapped, take note of every component included along with its corresponding part number listed on the packaging label.  Write down important information including serial numbers, model names, etc.","label":1,"model":"bloomz","source":"wikihow","id":6}
{"text":"The f scale consists of:  The f scale starts at middle c.  You should play it from low to high.   Practice playing each note individually before trying them together.    To practice blowing harder or softer,  use different embouchures.     If you're having trouble getting any particular note out,   try using alternate fingerings until you've found what works best for you. For example,    instead of putting your ring finger over the fourth hole when playing d,     put it under the fifth hole so it's closer to where you'd normally place your pointer    finger. Alternate between scales and arpeggios while practicing. Once you've got   the basics down, start adding dynamics by alternating between forte and piano. When you're ready to move onto more advanced material, learn how to do vibrato and glissandos. Vibrato means \"trembling\";     glissando refers to sliding from one pitch to another without stopping. These are both very useful techniques which add expressiveness to music played on the flute. Don't forget to warm-up!","label":1,"model":"bloomz","source":"wikihow","id":7}
{"text":"This section will provide you with tips about how to talk to resistant parents when one of their children comes out as homosexual.  This may seem like an impossible task but it can actually work well.   It takes patience, understanding, compassion and knowledge.    You need to have a clear idea of why this person opposes same-sex attraction before trying to change their mind.  Ask yourself these questions::  What is the reason behind their opposition?   Is there any way I could address those concerns?  How would my approach differ depending upon whether it's a religiously conservative Christian opposed by biblical scriptures condemning homosexual behavior (and therefore... more likely to accept scientific evidence), or someone from another faith tradition where sexual orientation has been condemned historically because of its association with other sins such as adultery?\nIf you're dealing with a very traditionalist Catholic family member who's against homosexuality based solely on church teachings, try talking to him\/her about Pope Francis' comments regarding homosexuals.  He said he was \"deeply sorry\" over past mistakes made by the Church's teaching authority which had led people \"to feel rejected by God\":","label":1,"model":"bloomz","source":"wikihow","id":8}
{"text":"The... This article will give you tips about visiting the Magic Kingdom theme park located within Walt Disney World Resort.  You may also be interested in:  See also:  *Disclosure: Some links above are affiliate links meaning we earn a small commission if items are purchased after clicking them.... \nWalt Disney World Map*Note: If you have never been to Orlando before,... \n\nIf you've decided you'd rather stay home instead of traveling all this distance just to go to Disneyland Paris then perhaps you should consider taking advantage of what Florida offers! There really isn't anything else like it anywhere else in the world - especially since there aren't any plans currently being made to build another Disney resort outside Japan.   But first off let's start with getting here!  How do I get to Orlando? Well there's no shortage of options available depending on whether you live close enough to drive yourself or would prefer flying somewhere closer such as Miami International Airport.    Driving:   From North America:  US Route 1 Southbound: Follow Interstate 95 south until Exit 82A toward State Road 429\/Orlando\/Sanford. Continue following signs for FL-429 N\/Southern Expressway East\/Winter Garden Rd. Turn left onto Winter Garden Blvd. (SR-525), continue straight across Orange Blossom Trail Bridge, follow signs for SR-526 S\/Turnpike Rd. Merge onto...","label":1,"model":"bloomz","source":"wikihow","id":9}
{"text":"Lecture notes can be an invaluable tool when studying for exams or preparing for tests.  They allow students to keep track of all that they have learned throughout their courses while also helping them retain information better than if they were simply reading from textbooks.   However, taking good lecture notes requires practice and some effort; it may not come naturally right away.    This guide will provide tips and tricks designed specifically to help college students improve how effectively they take lecture notes so that they can study more efficiently and perform better academically overall.     Before we begin though,...   How do I make my notes look neat?   ... What should I write first?  Do I need to type out everything I\u2019m writing?   ... Should I use bullet points?   ... Where am I going to put my notes once I\u2019ve finished?    - These are only a few examples of the many questions that might arise about taking effective lecture notes....  The following steps outline our recommended approach towards taking effective lecture notes which has been developed by combining insights gained through years of research conducted by psychologists who specialize in learning theory along with practical experience teaching undergraduate psychology majors (the author) and graduate-level cognitive neuroscience courses (co-author).","label":1,"model":"bloomz","source":"wikihow","id":10}
{"text":"If you're using Windows XP Home Edition with no access to Windows Media Player 10, you'll need to download WMP10 from Microsoft's website before proceeding.  This method is not recommended if you've never used Movie Maker before because it's very difficult to get right without some experience.   You may also find it easier to create one soundtrack at a time instead of trying to combine them later; however, there are advantages to creating multiple soundtracks within a single project such as:  You'll probably end up having more than two soundtracks; therefore, by combining everything together first, it'll make editing much simpler when you finally decide which songs you'd like included in each separate soundtrack.    In order to add music to different parts of the video clip itself, you'll need to split the clips manually after adding the song(s) to the timeline so they line up correctly.... \nYou must save the project before continuing... Otherwise, changes made won't stick!   Save As Type = Video File (.wmv)\nSave Location = Choose location","label":1,"model":"bloomz","source":"wikihow","id":11}
{"text":"T-Shirt Blankets can be made from any number of different colors and\/or patterns.  You will need at least 10-12 t-shirts but more is better if possible.   The larger the size of your t-shirts, the bigger your finished product will be.    If you're using multiple colors\/types\/patterns make sure they're all washed before starting this project because you'll be cutting them apart later!   For example, if you've got 3 black tees, 1 white tee, 4 green tees, etc... You'll also need some basic sewing supplies like scissors, pins, thread, needle, ruler, rotary cutter, measuring tape, pencil, glue stick, selvedge foot attachment, bias tape maker, and a serger.     This tutorial uses 8-inch by 11-inch paper templates as well as graph paper.... \nIf you'd prefer not to sew yourself, consider hiring a professional quilter who specializes in hand-quilting.  They'll do most of the work while you relax!  Check local listings online such as:  https:\/\/www.facebook.com\/groups\/handquiltinginla\/?fref=ts","label":1,"model":"bloomz","source":"wikihow","id":12}
{"text":"SQL SERVER PASSWORD RECOVERY TO RESET THE PASSWORDS FOR SELECTED USERS WITHIN A DATABASE\nThe first thing which needs attention here is how do I know whether my problem lies within SQL or Windows authentication? If you're trying to connect via SSMS (Microsoft's SQL Management Studio) but getting errors like this: \nThen it's likely you've got problems with either your username\/password combination OR the permissions granted to your account.  In order to fix these issues you'll probably be better off contacting Microsoft Support directly as they tend... This method works only when you don't remember any passwords at all! You can't use this technique to recover forgotten passwords stored by third-party applications such as Outlook Express, Mozilla Firefox etc.. For more information about recovering lost passwords please read How to Recover Lost Passwords Using Keychain Access Utility . First things firs:  Make sure that you actually forgot your password(s). If you still remember something - even though it's wrong - there may be ways around it: Try typing the name of the computer into Google followed by \"sql server sa password recovery\"; chances are someone else ran across the same problem before you did!  There are several reasons why you'd want to change the password; perhaps because you think somebody knows what yours was, or maybe you simply made a mistake yourself.   Whatever the reason,...","label":1,"model":"bloomz","source":"wikihow","id":13}
{"text":"Valerian is a flowering herbaceous perennial native to Europe.  It has been used medicinally since ancient times as both a sedative and sleep aid.   The flowers are purple-blue and have five petals each.    In addition to its calming effects, some people believe that taking valerian may help treat anxiety disorders such as panic attacks,  obsessive compulsive disorder  and post-traumatic stress syndrome.  Some studies suggest that using valerian supplements regularly could also improve memory function by increasing blood flow to the brain.  However, there\u2019s no scientific evidence supporting these claims yet.  There aren\u2019t any known side-effects associated with short-term usage; however long-term use should be avoided because it might cause liver damage.  If you\u2019re pregnant,...   This article will teach... Purchase valerian seedlings from a local nursery if possible.  You\u2019ll need about two dozen healthy young plants to fill up a 12 x 16 foot plot of land.  Alternatively, you can purchase small starter plants online through websites like Amazon.com or eBay.com.  Look for varieties labeled as \"valeriana officinalis\" which means it\u2019s safe enough to eat.  Avoid buying plants grown under fluorescent lights unless they're specifically labelled as being suitable for growing outside.  Buy organic seedlings whenever possible.  They\u2019re less likely to contain pesticides than conventional ones....","label":1,"model":"bloomz","source":"wikihow","id":14}
{"text":"If you're interested in starting a rock band but aren't sure where to begin then read on! This guide will help you get started.  You may also be interested in:  Starting A Band In College How To Make Your Own Album Writing Songs Getting Into The Music Industry Making Money From Playing An Instrument Before we even think about putting together our own band there are several important steps we'll need to take before we actually hit the stage as professional musicians.   First off we're going to talk about what it's really like being part of a high school rock band - not everyone has been lucky enough to experience life inside such a group!  Then there's the matter of finding members who share similar interests...and musical abilities!\nNext comes the difficult task of coming up with original music ideas....then finally recording those tunes onto tape using whatever equipment we've managed to acquire over time.  Finally once everything else seems to fall into place we'd better find somewhere suitable to perform these new compositions live while still maintaining a reasonable amount of privacy during rehearsals.  We'd love to hear any comments you'd care to leave below regarding this topic.","label":1,"model":"bloomz","source":"wikihow","id":15}
{"text":"You may have been invited by one of your friends or maybe you've decided you'd like to invite someone else over.  Either way, it's important to make sure they don't mind having people stay over before asking your parents.   If this isn't possible, try talking to another family member who lives nearby so they'll let you spend the night there instead.    Once they've agreed, you'll need to find someplace where everyone will fit comfortably together (like sleeping bags) as well as food and drinks enough for everybody involved.  Sleepovers usually involve lots of fun activities such as:  Before making any arrangements, talk to your parents about what kind of things they'd enjoy doing during a sleepover.  This could include playing board games, watching movies, eating pizza, etc....   Remember - even though most kids love sleepovers because... It's always better when both sets of parents approve!    Talk to your parents again after you've talked to your friend's parents too.  They'll probably ask questions like:  When should we pick her up?","label":1,"model":"bloomz","source":"wikihow","id":16}
{"text":"If you've ever collected anything at all\u2014from baseball cards to stamps\u2014the chances are pretty high you'll eventually want to organize those items.  The same goes true when collecting trading cards; if you don't take steps now to keep things organized from day one, it'll only get worse over time.   This wikiHow teaches you some basic tips about organizing your own personal collection so that everything stays neat and tidy (and hopefully increases its overall value).   If you'd like more detailed information regarding specific methods used by professional collectors, see How To Collect Baseball Cards For Profit instead.    Get a binder or box. Depending on the type of trading... Continue reading \u2192\n\nSort by your chosen category. Before you put the cards into an album, it's a good idea  to sort them out first.... \nLabel your sections. Put the cards in their places. Organizing a collection shouldn't be stressful\u2014and doesn't need to consume every waking moment either. ...    A spreadsheet is another great option for keeping tabs on your collection.  It'll allow you easy access to any particular item while also giving you automatic cataloguing capabilities.  You could even use software designed specifically for this purpose such as:","label":1,"model":"bloomz","source":"wikihow","id":17}
{"text":"If you're having trouble getting along with people who frequent forums, it's important to remember how they work.  Forums can sometimes become very hostile places where users argue over topics they've been discussing.   This may happen because some users feel attacked by others' posts (even when they're just expressing their opinions);  this could also occur due to misunderstandings between different viewpoints.    It's always good practice to think carefully before writing anything online - especially since everything you've written stays forever once published!    Don't forget that all information shared through these sites becomes publicly available at any time.     Always keep things civilized as much...as possible....and try to avoid inflammatory language whenever possible.  Remember that even though you might be able to hide behind a computer screen,...you'll still need to treat each person equally regardless of whether he's anonymous or real life acquaintance.   Keep calm   Avoid being sarcastic or condescending toward anyone else's opinion.   Try to see both sides of every issue.   Be respectful of people's feelings.   Respectfully disagree without making personal attacks.   Know when to walk away.","label":1,"model":"bloomz","source":"wikihow","id":18}
{"text":"The next step will be creating an artistic pattern based on spherical shapes.  This time we are going to create a flower-like pattern using spheres as building blocks.   In order to do so, you need to know how many spheres there are per row.    You can find out about your computer's processor speed at http:\/\/www.cpubenchmark.net\/cpu-benchmarks\/fastest-computers-in-the-world-by-cpu-speed.html    If you're not familiar...   Click here if you'd like to see the complete instructions on making a flower pattern in MS Excel. Please note that these steps may take several hours depending upon your PC specifications. It would also help greatly if you had access to a powerful graphics card such as ATI HD2600XT or nVIDIA GTX260+. These cards provide better performance than integrated Intel GMA series video processors which come standard with most laptops. A good idea might be to save your work frequently while working towards completing each section below. Once you've completed all sections listed above you'll then proceed onto finishing off the project.","label":1,"model":"bloomz","source":"wikihow","id":19}
{"text":"Tempura is an authentic Japanese dish that consists of pieces of seafood and\/or vegetables coated in a crispy batter made using eggs, flour, and other seasonings.  The traditional way to serve this delicious treat is by placing several different types of cooked tempura onto a bed of steamed rice along with some sauce such as soy sauce.   This recipe will teach you how to create a simple yet tasty version of tempura at home! .\nTo begin preparing the batter, combine 1 cup of cold water with 1\/2 tablespoon of salt in a medium saucepan over high heat until boiling.  Once the water has reached its boil point, remove the pot from the stove immediately before adding any ingredients to prevent them from being scorched!  Pour the salted water into the ice bath right away after removing it from the burner.  Continue stirring occasionally until the batter reaches room temperature.    You may also use frozen peeled shrimp instead if you'd like; however, you'll need to thaw out the shrimp first prior to...","label":1,"model":"bloomz","source":"wikihow","id":20}
{"text":"This method is great if you'd like to try something different than regular box dyes.  It will give you bright colors without damaging your hair! This process takes about 24 hours but it's worth every minute!  You can also add food coloring instead of Kool-Aid powder.   The following steps should be followed when using this method:  1. Prepare yourself by putting on rubber gloves so you don't stain your hands while mixing everything together.    2. Place all three boxes of Kool-Aid inside one large bowl.     3. Pour enough warm\/hot tap water over each packet of Kool-Aid to dissolve the contents completely.    4. Once you've dissolved the Kool-Aid mix thoroughly with a spoon\/fork\/mixing stick.    5. Apply the mixture onto damp\/dry hair depending upon how dark\/tight\/saturated you would like your final result to look.    6. Cover your hair tightly with cling film\/baggy\/shower cap overnight.    7. Rinse off the mixture after 12-24 hrs....","label":1,"model":"bloomz","source":"wikihow","id":21}
{"text":"If you're looking forward to your next holiday by the sea - whether it's going away somewhere exotic such as Hawaii, Thailand or Australia; or simply spending some time relaxing along our own beautiful coastlines here in New Zealand - then this guide will help ensure that everything goes smoothly.  The following tips should be useful no matter where you've decided to spend your holidays.\n\nPlanning ahead\n\nThe best way to enjoy any kind of vacation is to plan well beforehand so that all aspects can run smoothly once you arrive.   Start thinking about what type of place you'd like to visit several weeks prior to departure.    Do some searching online using keywords related to beaches, travel destinations etc...   Then narrow down potential locations based upon things like:  cost price range accommodation availability food options transport costs distance from home safety\/crime risk local attractions available entertainment facilities family friendliness age appropriateness health & hygiene climate conditions language spoken security level accessibility special needs children\/family friendly swimming suitability budget restrictions This information may also come in handy when booking flights and\/or accommodations. \n\nOnce you've settled on a location(s)...","label":1,"model":"bloomz","source":"wikihow","id":22}
{"text":"Minecraft Forge allows users to play custom-made mods for their existing copy of Minecraft.  This method will allow players who have purchased an official copy of Minecraft to legally use third-party modifications without violating copyright laws.   You can also try downloading other popular mod loaders such as OptiFine HDU, MCPatcher, Twitch Plug-in, and more by following similar steps outlined below.   ...\"\nDownloading and installing Minecraft Forge requires that you be connected to the Internet while doing so. If you\u2019re not already logged into your Microsoft account when prompted during this process, do so now before continuing.     The installer should automatically detect which operating system you're running based upon your browser's settings; however,...   - Windows \u2013 Right-click on the downloaded file, choose \"Run as administrator\" \u2192 \"Yes\" \u2192 \"Save\" \u2192 \"Cancel\" \u2192 \"Done\"     - Mac OS X \u2013 Open the downloaded DMG file, drag \"Forge\" onto Applications\/Utilities\/Applications Folder, double-click on it to open    Once you've completed all three stages of the installation, you'll see a confirmation message appear stating \"FML has been successfully installed!\"  Now it's time to start playing around with some new mods!  To find out how to add a mod manually via command line, read How to Add Custom Mods Manually Using Command Line.","label":1,"model":"bloomz","source":"wikihow","id":23}
{"text":"This section will show how to fix an issue with your computer's display.  If you're having problems turning on or displaying anything on your monitor then read How to Troubleshoot Bad Display Problems before proceeding any further.   Before beginning make sure you've disconnected everything from the wall outlet except for what you'll need during repairs (monitor, laptop etc.).  Make sure nothing touches the front glass when removing it as it's very fragile.    1) First disconnect the cables attached to the monitor.     2) Take off the base plate which holds the monitor together.     3) Place the monitor face-down onto some soft material like cloths so there won't be scratches if something falls out while working.     4) Unscrew each lock nut along the edge of the monitor.     5) Carefully lift away the bezel covering the screen.     6) Continue lifting until only about 1\/4\"-1\/8\" remains between the bezel and the screen.     7) Be careful not to touch the screen itself!     8) Gently pull the bezel straight upward towards yourself.     9) Repeat steps 5-7 above for both sides of...","label":1,"model":"bloomz","source":"wikihow","id":24}
{"text":"Men may seem complicated, but they aren't! If you know what makes them tick -- their thoughts, feelings, wants, needs, fears...you'll find it easier to communicate effectively with them.  This guide will teach you:  How to make sure you're on the right track when trying to figure things out (and if not, where to go from there). What he likes best about being a man....and why he's so proud of his \"manhood\". Why he doesn't feel comfortable talking openly or honestly about certain issues. The reasons behind many of his actions - good ones AND bad ones. How to deal with his mood swings. How to handle his stubborn streak. How to keep up morale during difficult times. And SO MUCH MORE!  You won't believe how easy this is once you've read these pages!...or seen our video tutorials!. ...more\nYou might also like:\n\nThe Ultimate Guide To Understanding Women\nUnderstanding Your Boyfriend's Emotions & Behaviors\nWhat Does It Mean When A Man Falls In Love?","label":1,"model":"bloomz","source":"wikihow","id":25}
{"text":"The following steps can be taken by anyone who wants to compose original songs.  The first step towards becoming a composer would be learning how to play any kind of instruments such as:  Piano Guitar Violin Cello Flute Saxophone Trombone Clarinet Upright bass Drums Once you've learned some basic skills on these instruments you'll need to start listening closely to what they do when played individually.   This means paying attention to:   How does each note feel? What sort of moods does this particular instrument create?  Does it seem happy? Sad? Angry?\nOnce you're able to hear individual notes clearly it's time to move onto hearing patterns within those notes - which we call \"scales\".  Scales are simply groups of notes arranged into ascending order from lowest pitched tone to highest.  Learning different types... Continue reading \u2192\n\nYou should also know that there are many ways to write music depending upon whether you plan to perform live or record yourself singing along with instrumentalists.  There are several forms of notation including:","label":1,"model":"bloomz","source":"wikihow","id":26}
{"text":"Being assertive is an important part of communicating effectively, especially at work.  Being able to express your feelings and ideas without being rude can help you get along better with colleagues.   This will also allow you to have greater control over how people treat you because they\u2019ll know exactly where you\u2019re coming from.   ... How do I become more assertive?   What does it mean to communicate assertively?  Why should we learn to communicate assertively?\n \u30fb It helps us feel less stressed by expressing our true feelings and wants instead of bottling them all inside.  \u30fb We may find ourselves saying yes too often just so other people won\u2019t be upset (especially family members).  By learning to speak up for ourselves,... \nAssertive Communication Model\n\nThe following steps outline the process used during assertive communication:    1) Observe the facts surrounding any conflict or problem.  2) Express your feelings related to these facts.  3) Listen carefully to their responses.  4) Respectfully state your position again.  5) Agree upon next steps together.  6) Follow through on agreed-upon actions.  7)  Repeat Steps 3-6 until resolution has been reached.  8)...","label":1,"model":"bloomz","source":"wikihow","id":27}
{"text":"If you're interested in graphic design or just like creating artwork with computers then you'll probably find yourself wanting to know what it takes to become an expert user of one of the best programs available today - Adobe Illustrator CC (Creative Cloud). This program allows users to create vector images as well as raster images such as photographs. \n \n In order to get started we will take some time out to explain exactly what these terms mean so let's begin! \n \n \n \n Vector Images are created when lines are drawn between points instead of pixels being placed side-by-side. These types of images allow designers to scale their work without losing quality because they don't rely upon pixel density but rather line thickness.  \n \n Raster Images are created when dots are arranged into squares called \"pixels\". When viewed closely enough all computer screens display only individual pixels; therefore if you zoomed right in close enough you'd see each dot individually. Because of this fact, any image displayed on screen would appear jagged no matter how high its resolution was set. However, once printed onto paper there wouldn't really be much difference whether it's a low-resolution photo or a high-quality digital print since both would look smooth due to the large number of pixels used per square inch. \n \n Now that we've explained those two different ways of displaying images lets move on...","label":1,"model":"bloomz","source":"wikihow","id":28}
{"text":"A class action is when one person sues another group (the \"class\") over some common legal issues.  A class action suit allows many people with similar problems against someone else to sue together instead of individually.   This means they don't need their own lawyer;  only one representative needs to be appointed by the court to represent them.    The plaintiff(s), also known as the \"Class Representative\/Representatives\",  files this case on behalf of everyone involved.   If successful at trial, each member receives compensation based upon how much money was recovered from the defendant's insurance company and\/or other sources such as punitive damage awards.  In order...   See more \u00bb\n\nTo take part in a class action lawsuite, you'll first need to know which kinds exist.  There are four main types:    Consumer protection suits - These involve products sold directly to consumers like cars, appliances, computers, etc..     Business-to-business suits - These involve businesses selling goods or services to others.     Governmental liability suits - These involve government agencies providing public services.      Securities fraud suits - These involve companies offering securities to investors.   You should note that not every consumer product has been tested thoroughly enough to determine its safety prior to being released into the market....","label":1,"model":"bloomz","source":"wikihow","id":29}
{"text":"Stomachaches can occur due to many different causes.  Some common reasons include:  Heartburn Indigestion Irritable Bowel Syndrome Diarrhea Constipation Menstrual Cramps Food Poisoning Gastrointestinal Infections Stress Anxiety In some cases, stomach pains could also indicate something more serious such as:   Abdominal Cancer Gallbladder Disease Pancreatitis Appendicitis If you're suffering from chronic daily stomach ache, it's important to:  Talk to your parents about it so they know what's going on Ask them to take you to see your pediatrician Consider taking medication regularly Consulting a therapist might also be helpful in dealing with this problem. You should always seek immediate attention when experiencing any of these symptoms:  Vomiting Blood in vomit Nausea Fatigue Fever Chills Shaking... Continue reading at https:\/\/www.wikihow.com\/Stop-Daily-Stomach-Aches-for-Teens-27944#:~:text=If%20you%27ve%2Beaten,Try%20drinking%20a%20cup%20of%20herbal%20tea..","label":1,"model":"bloomz","source":"wikihow","id":30}
{"text":"This trick was created by David Copperfield.  It has been performed numerous times around the world.   The method used here involves three packets (A,B &C) which contain different amounts of cards each.    You can perform this trick using any number between 1-26 as well but we have chosen 13 because it\u2019s easy to remember and also happens to be my lucky number!   This version uses the same principle as the original except there aren\u2019t actually 27 cards involved \u2013 just like the real deal!  To begin I would ask if anyone wants their fortune told or perhaps they\u2019d prefer me... \nThe first packet contains 12 cards - 11 regular ones plus the 13th key card!\nAsk someone who knows how many cards are in the pack whether he thinks it's odd or even.  If he's not too bright then he'll probably say \"even\"  Now turn over the bottom card and show him\/her that there's another card hidden under it.... \n \n Next you'll need some practice performing the actual trick before trying out the full effect.  Start off by asking what colour his\/her favourite car is.  Then tell her\/him\/it that you're going to find its colour inside the pack.  Take the whole pack into your hands and hold it flat against the table while looking at both ends simultaneously.  Look carefully along the edge where the back meets the front of the pack.  You'll see tiny indentations made when the cards were pressed together during manufacturing.  These indentations form patterns known as \"fingerprints\".  Your job now is to look closely enough to spot these fingerprints and match them up correctly with those found on the backs of the cards themselves.  For example,...","label":1,"model":"bloomz","source":"wikihow","id":31}
{"text":"Once you've covered all surfaces you'd like to fake in stainless steel, you're done! You can now enjoy your new kitchen without worrying about scratching those expensive appliances.  If there are areas where the contact paper doesn't quite match perfectly, simply trim them off using scissors before moving onto another area.. For example, maybe there's an extra strip of contact paper sticking out above the refrigerator's handle. Simply remove these sections carefully so they don't get caught when someone opens the door next time. This is optional but recommended for best results. Once everything looks good,... Continue reading \u2192\n...you'll want to seal the entire piece of contact paper into place..  To do this:  Apply spray-on polyurethane acrylic primer to each appliance individually,  Allow the primer to completely dry according to manufacturer instructions.   Then apply two coats of spray paint to finish the job.  Make sure not to let the first coat fully dry between applications; wait until it's tacky enough to wipe clean with a damp cloth before applying additional layers.   ...","label":1,"model":"bloomz","source":"wikihow","id":32}
{"text":"If you're flying somewhere far away this summer vacation season (or even if it's only across town), you'll probably find yourself needing tips about what to bring along.  This wikiHow teaches you everything you should know about traveling by plane so that you don't forget something important!   If you'd rather read than watch videos, skip down to \"How To\" section below.   You may also like:  How to Get Tickets Cheaper How to Prepare for an Airport Security Screening What to Bring On Board A Flight How to Stay Calm During Long Flights How to Sleep While Travelling By Airplane How to Deal With Jet Lag Before Your Trip How to Choose Which Baggage Allowance is Right For You The first thing... Read more \u2192\n\nBefore you begin preparing your bags, make sure you've got tickets booked!  Once that's done, go over your itinerary one last time:  Are there multiple flights? Do they depart\/arrive at different times?  Is there a layover involved?  Will you arrive early enough to do sightseeing\/check into accommodations without rushing?  Does your destination require additional travel documents such as visas and\/or vaccinations?\nYou won't necessarily need every single piece of clothing listed here but having each type available gives you options depending upon where you're headed and weather conditions once you reach your destination.    Sweater - Wear sweaters under shirts and jackets; they're great because they'll help insulate against cold temperatures while still allowing airflow around your body.  Light jacket - Keep warm yet comfortable by layering a lightweight jacket underneath your shirt.  Shorts \/ pants - Depending on the climate,...","label":1,"model":"bloomz","source":"wikihow","id":33}
{"text":"iMovie is Apple's video creation software that allows users to easily edit their videos with music, text overlays, transitions between clips etc.  This tutorial will show how to use this feature in order to add still pictures taken using an iPhone 4S directly onto existing movie footage shot previously.   You can also download free third party applications such as PixTeller which allow for similar functionality but require more advanced knowledge than what we cover here.    Note:  The steps outlined below were performed while working within the iOS 6 operating system environment running on an iPad 2 device; however they should be applicable across all devices running iOS 5+ systems including iPod Touch models and iPhone\/iPad tablets\/phones. If you're not already logged... , click on one of them to open its details page..    - Tap the \"Downloads\" tab at top if you'd rather save the picture locally instead of uploading it to Facebook. - Tap \"Save Photo\" after selecting a photo to upload it immediately without posting it publicly. - Tap \"Post\" once you've selected a photo to post it instantly.","label":1,"model":"bloomz","source":"wikihow","id":34}
{"text":"The following steps should be followed by anyone who wishes to enter into competition.  The first thing one needs to know how to do after getting their horse ready is to practice on the ground.   Once you've practiced enough times, it's time to put all those skills together.    You can find more detailed information below under each section listed above.     Dressage classes  English Pleasure Classes  Hunter\/Jumper Classes  Reining\/Rodeo Classes  Barrel Racing\/Driving    Horse shows vary from state-to-state but generally follow similar guidelines across most states. In order to participate in any type of equine event there must always be:   1.) an entry fee; 2.) a registration form filled out; 3.) proof of ownership and\/or lease agreement; 4.) identification papers such as: 5.) vaccination records; 6.) health certificates 7.) microchip implant records 8.) vet certificate 9.) passport 10.) birth certificate 11.) breed registry 12.) pedigree 13.) blood test 14.) quarantine 15.) veterinary examination 16.) insurance 17.) liability waiver 18.) membership 19.) age 20.) height 21.) sex 22.) weight 23.) name 24.) number 25.) date 26.) signature 27.) owner 28.) trainer 29.) rider 30.) groom 31.) handler 32.) veterinarian 33.) farrier 34.) tack 35.) blanket 36.) saddle 37.) bridle 38.) helmet 39.) reins 40.) bit 41.) shoes 42.) clothing 43.) boots 44.) spurs 45.) safety vest 46.) riding crop 47.) lunge 48.) breeches 49.) jacket 50.) pants 51.) shirt 52.) coat 53.) hairnet 54.) glove 55.) booties 56.) bandages 57.) handkerchief 58.) earplugs 59.) eye protection 60.) mouthpiece 61.) leg wraps 62.) tail wrap 63.) body protector 64.) chaps 65.) western 66.) english 67.) hunter 68.) jumper 69.) reining 70.) barrel racing 71.) driving 72.) dressage 73.) pleasure 74.) jumping 75.) trail 76.) rodeo 77.) showing 78.) ranch 79.) hunter\/jumper 80.) cross country 81.) derby 82.) hunt seat 83.) polo 84.) fox hunting 85.) equestrian 86.) pony 87.) vault 88.) gymnastics 89.) hunter-jumper","label":1,"model":"bloomz","source":"wikihow","id":35}
{"text":"Bleaching shorts with hydrogen peroxide will give it a faded, worn-out look.  This technique works best when used over light-colored shorts like white ones.   Hydrogen peroxide (also known as water oxigen) comes from natural sources such as plants and animals.    It has been widely used since ancient times because it's cheap and readily available at most pharmacies.     The process takes about 30 minutes per pair of shorts so plan accordingly!     For more information see How to Bleach Clothes With Hydrogen Peroxide. Turn the shorts right-side-out after drying completely. Repeat until satisfied with results. Wash thoroughly using laundry detergent afterwards. Dye the shorts any color you'd like. Enjoy wearing your new shorts:).   Don't forget to share photos of how yours turn out:)! Happy fraying!.   ...more\nIf you don't know what kind of pattern\/effect you would like then try making dots along one side of each leg instead of separating everything evenly across both legs.","label":1,"model":"bloomz","source":"wikihow","id":36}
{"text":"Cauda Equina Syndrome (also known by its acronym \"CES\")...is a potentially life-threatening condition which occurs due...to damage occurring within the lumbar region of our spine....and results from pressure being exerted on the cauda equina,...the bundle of nerves located near the base of the spinal column. ...The most common cause of CES is trauma such as: ...A herniated disc; A fracture\/dislocation of vertebrae; Spinal stenosis; Other causes include: ...Pregnancy; Ageing; Degenerative conditions like scoliosis and spondylolisthesis; Diabetes mellitus; Immune disorders including multiple sclerosis and HIV infection; In some cases no obvious cause can be identified. \n \n Symptoms include but not limited to: \n \n \n \n Leg pain and\/or difficulty walking Bladder\/bowel problems Sexual difficulties Numbness\/sensory disturbances Loss of reflexes Triggering events Red Flag Signs If you suspect you might have CES then it's important to get immediate medical care so appropriate investigations can take place.  \n \n The diagnosis process includes:  Neurological examination Imaging studies Myelography Electrophysiology Studies Surgery Once you've been diagnosed with CES you'll need prompt intervention to prevent further deterioration. \n \n Treatment options include:   Medical management Surgical decompression Decompressive laminectomy Laminotomy Stabilization Support rehabilitation","label":1,"model":"bloomz","source":"wikihow","id":37}
{"text":"If you're administering fluids intravenously, it's important to know how to do so safely.  This guide provides step-by-step instructions as well as tips from nurses about how to give fluids via an Intravenous Line (IV).  It also includes information regarding complications associated with giving fluids by this method.   If you'd like more detailed information please see our full article here.    You can find out if someone has been admitted to hospital using their name and\/or date of birth at www.nhsinformatics.org.uk under 'Find A Patient'.     The following steps are based upon guidelines provided by NICE (National Institute For Health And Care Excellence) which were published in 2009 but remain current today.        1. Make sure you have an IV... 2. Wash your hands 3 times thoroughly with soap and water before starting 4. Double-check that you have the correct IV bags 5. Determine what sort of set you'll require 6. Get the appropriate sized needle 7. Gather your other supplies 8. Place everything onto a tray 9. Prepare the IV bag 10. Pipe\/insert the macroset\/Microset 11. Remove any air 12. Ensure the line doesn't touch the ground 13. Ask the patient to sit or lay down 14. Locate the best area 15. Tape 16. Sterilise 17. Insert 18. Check 19.","label":1,"model":"bloomz","source":"wikihow","id":38}
{"text":"If you've always been interested in fitness but don't know where to start when it comes to getting into coaching, then this guide can help! This section covers everything from what qualifications are required by employers to how to become a successful coach.  If you'd like more information on becoming a personal trainer instead, check out How To Be A Personal Trainer. You may want to consider taking some courses at community colleges if they offer them near you.   In addition, there are many online resources available such as:  The National Federation of State High School Associations (NFHS) offers free continuing education units (CEUs). These CEU's cover topics ranging from sports psychology to nutrition. There are several organizations which provide certification programs for coaches including USA Weightlifting, NSCA, NASM, ACE Fitness Professionals Association, ACSM, etc..  Each has its own requirements so it's important... Continue reading \u2192\n...and other related fields.You might find yourself teaching students about healthier lifestyles while earning a salary similar to those earned by teachers.","label":1,"model":"bloomz","source":"wikihow","id":39}
{"text":"If you've been invited to be a bridesmaid (or groomsman) at a medieval-themed wedding party, you'll want to dress appropriately.  This is especially true since many people don't know much about historical costumes; it's important not only to choose one that's historically accurate but also comfortable enough so that you won't feel embarrassed walking down the aisle with it on!   In this guide we'll cover some basic guidelines as well as tips on finding appropriate attire.   We hope these suggestions help you find just the right thing!  Remember though -- no matter which era's style you decide upon, always keep comfort first![1]  The most important factor here is probably knowing exactly where and when the event takes place.  For example, while there were certainly similarities between European countries during the Middle Ages, each country had its own unique fashions and styles.    Also consider things such as:[2]    What type of ceremony does the couple plan? Is it religious?  Or civil?\nWhat kind of reception is planned afterward? Will guests eat dinner before dancing?","label":1,"model":"bloomz","source":"wikihow","id":40}
{"text":"Second bases are usually played by girls who like speedy games.  The position requires quick thinking skills as well as good hand-eye coordination.   A player at this position must be able to react quickly when called upon during an emergency situation such as:  Being tagged out while running towards home plate;   Having to tag out a runner trying to steal second base after being caught stealing;   Tagging out a runner attempting to score but not having enough time to reach home plate before the umpire blows his\/her whistle.    To become proficient at playing second base one needs to develop both physical strength and mental agility.  This means that players should train their bodies regularly by:    Running long distance runs several times per week;    Jogging around the block once daily.     In addition,... ...read more...    ...more...  ...more...  ...more...  \n\nThe following tips may help improve one's performance at second base:\n\n1. Develop strong footwork Practice standing still without moving until you feel comfortable doing so.  Then practice walking forward slowly using only two feet.  Finally try jogging backwards and forwards across the infield.  Do these exercises repeatedly each day until you find yourself naturally shifting weight onto different parts of your body.  You want to make sure you're always ready to move swiftly if necessary.\n\n2. Improve reaction time Run full speed into a wall or fence and jump straight away again.  Repeat this exercise over and over again until you've mastered how much force you'll need to use to break through the barrier.  Once you've done this successfully many times it's important to keep practicing until you no longer notice any pain associated with the impact against the obstacle....","label":1,"model":"bloomz","source":"wikihow","id":41}
{"text":"Colds spread quickly in schools, offices, gyms... anywhere people spend time together.  The best way to avoid catching one yourself? Learn how to stop them before they even begin! Here\u2019s what experts recommend if you\u2019re feeling sick coming on:  Get lots of sleep. Rest helps fight off infection by allowing white blood cells called lymphocytes to work harder against viruses like rhinoviruses (the common cold).  Stay hydrated. Fluids prevent dehydration\u2014and keeping your throat moist makes it a less attractive spot for germs to congregate.... Continue reading\n\nDrinking enough water will also make sure you're getting all those other important nutrients\u2014like vitamins A, B12, B6, folic acid, calcium, magnesium, potassium, sodium, iron, copper, manganese, iodine, selenium, zinc, chromium, molybdenum, fluoride, choline chloride, carnitine, taurine, creatine phosphate, betaine anhydrous, alpha lipoic acid, glutathione, resveratrol, quercetin, luteolin, catechin gallate, epicatechin, epigallocatechin-3-gallate, hesperidin, rutin, naringenin chalcone, beta-carotene, lycopene, phytochemicals such as flavonoids,...","label":1,"model":"bloomz","source":"wikihow","id":42}
{"text":"This will make it easier for us to follow along with our instructions.  You can use any color origami paper you'd like! This method works best if you're using white paper because you'll see more clearly where we're folding.   If you've never done origami before, try making some simple shapes such as squares, rectangles, circles, etc. until you feel comfortable enough to start doing harder designs!\nFold the paper diagonally across its width starting at either end of the page.    The longer sides are called the base edges while the shorter ones are known as the tip edges.     Repeat these folds all around the square\/rectangle\/circle until they form one long straight line running through both ends of the paper.    Now we need to flip the entire thing 180 degrees!  We want to do exactly what we've just done - bring the corners together forming a triangle shape.  Remember how many times you folded? Well, we'll only go back three times since that's when we made those lines.  Let's say it's four... Don't worry about the other pocket yet; let's focus on getting this one perfect first!","label":1,"model":"bloomz","source":"wikihow","id":43}
{"text":"Gather all necessary supplies.  Determine how large you want your bag to be.   Create the base of the bag by cutting 4 rectangles (2 x 3) inches wide x 8\u00a0inches long. (The dimensions are approximate.)   Attach the sides together with glue.    Secure the corners using tape if needed.     Turn one of the bags inside-out.        Decorate the outside of the bag according to your preference.      Insert the handle straps.       Fill it with goodies!     Finished!    You can also use this method to make smaller sized gift boxes. This will help keep them organized until they need to go into their new homes. If you're making multiple baby shower gift... Continue reading \u2192\n, then you'll probably find yourself needing some extra storage space before too long. Fortunately there are plenty of ways to store items without taking away precious floor space - even when it's just an apartment bathroom!  Here we showcase five different ideas which could prove useful depending upon what kind of room you've got available....","label":1,"model":"bloomz","source":"wikihow","id":44}
{"text":"Cryptococcus gatti infections can occur anywhere around Australia where there has been an outbreak.  The most recent outbreaks have occurred on Vancouver Island off British Columbia Canada.   Other areas affected include:  South America Central Africa North America In some cases it takes up to 10 days after exposure before symptoms develop.    Symptoms usually begin with:    A cough Headache Nausea Vomiting Diarrhea Shortness of breath Difficulty swallowing Loss of appetite Weight loss Fatigue Muscle pain Skin rash Rash occurs when the body\u2019s white blood cells attack the fungi.  It looks like red bumps all over the skin.  This symptom appears between one week and two months following initial infection.  If left untreated these lesions will spread across large parts of the body including the face, neck, chest, arms and legs.     Once diagnosed early enough treatment should start within 24 hours so that the patient does not suffer any permanent damage such as:   Lung fibrosis Brain damage Kidney failure Vision problems Paralysis Death","label":1,"model":"bloomz","source":"wikihow","id":45}
{"text":"The following steps describe how to change your water pump if it's leaking.  If you're replacing the entire motor for any reason then skip down to \"How To Replace The Engine In A 1996 Mazda 6 M6\" at the bottom of this page.    This procedure applies only to vehicles equipped with an inline-four cylinder engine.   You should also replace the gasket between the water pump housing and the timing chain case before removing the old water pump assembly.    ... Continue reading below.... \nIf you've replaced the water pump recently you'll want to check the new unit first;  make sure there aren't any cracks or damage visible anywhere around the impeller blades.  Also inspect the mounting bracket for signs of rusting or corrosion.  Make note of whether the replacement part came with a seal kit included - some do while others don't so you'll either need to purchase additional seals yourself or use what was already installed previously.  Check all parts thoroughly against wear and tear prior to installation.  Be aware that many aftermarket pumps come without a seal kit attached although most modern cars require both upper and lower seals regardless of brand name.  Always double-check the manufacturer's recommendations regarding proper sealing procedures since every vehicle has slightly different specifications.","label":1,"model":"bloomz","source":"wikihow","id":46}
{"text":"The following instructions show how to build a wooden louvers that open outward.  This style of louver works best as a ventilation system because they provide maximum air flow while still allowing rainwater through.   The design shown here has been tested by our staff at How-To Geek HQ who have found these vents to perform very effectively.    Measure the gable to decide...   Continue reading \u2192\n\nCutting the louver boards:  Determine which direction you would like the slats to lay across the top of the gable.     Make sure there isn't any obstructions such as pipes running along the ceiling line.     Draw lines indicating where you'll place the slat pieces.     Using a circular saw set to its highest setting, rip the louver boards down according to their placement on the drawing.     Once you've completed cutting the entire row, flip the board over and repeat the process until you're left only with two long strips of louver board.     You should now have four rows of slats laid parallel to each other.     Flip the board over once again and make another series of cuts perpendicularly to those made previously.     Repeat steps 1-6 above until you reach the final dimensions of your louver.     If you'd prefer square slats instead of rectangular ones, simply draw straight vertical lines rather than diagonal lines.     When finished, remove the excess material around the edges of the louver board.     To create a pattern similar to the image below, start making horizontal cuts before moving onto vertical cuts.     After completing both sets of cuts,...","label":1,"model":"bloomz","source":"wikihow","id":47}
{"text":"The following is a list of things you should consider when creating your game show.  You can also use this as a checklist if you\u2019re having trouble coming up with ideas yourself.   The more time, effort, money, and energy that goes into making a successful game show, the better it\u2019ll turn out!   If possible, try to find someone who has experience producing shows or working behind-the-scenes at one before starting yours.    Choose a genre. Think about what kind of people would enjoy watching your show most....  For example,...    How many episodes do you want? Do you have any specific goals...  ...or just want to make something fun?  What type of questions do you think they\u2019d like to answer?...     ...and so forth.  Determine whether there are certain types of answers that you don\u2019t want them giving (e.g., \u201cyes\u201d).    Consider using multiple-choice questions instead of open-ended ones because it\u2019s easier to grade those later on. ...how much information does he need to know? ...what happens afterward?  Write down all these details beforehand so you won\u2019t forget anything important during filming.     Set aside some time every day to come up with new questions based off of existing topics and\/or themes from previous episodes.  Keep track of which questions were used previously so you don't repeat them twice.  Don't ask questions that require complicated explanations; keep everything simple!  Try not to give away too much information while still being able to get enough information back from the contestants.  Remember to include both easy and difficult questions throughout the entire series.  Be sure to write down the correct answers next to each question you're writing down.  This way you'll remember exactly what's going to happen once you've filmed the episode(s).  Once you've finished writing down all","label":1,"model":"bloomz","source":"wikihow","id":48}
{"text":"Henna has been used since ancient times to decorate hands and feet.  It can last anywhere from 3 days to 2 weeks depending upon where it's applied.   The designs range from simple lines to elaborate patterns.    This method uses a special technique called mehandi which involves painting intricate designs onto the body using henna paste made from grounded henna leaves.  You may also want to read How To Apply A Henna Tattoo  first.     If you're interested in learning more about this process click here .     Instructions    Mix all ingredients except turmeric in a large mixing bowl.      Turmeric should only be added at the end when you've finished making the paste because adding too much early could ruin the whole batch.        Once everything is well combined add some turmeric and mix again.       Pour the mixture through a fine sieve over another...   -Mixture-        Put the lid back on the jar and leave overnight.         -Application-         -Designs-             -Painting-              -Care-for-the-Tattoo-","label":1,"model":"bloomz","source":"wikihow","id":49}
{"text":"The pie charts are useful when you have multiple categories with different values that need to be represented on one graph.  This is especially true when there may not be enough space to show all the information using bar graphs.   The example below shows how we can use this feature by creating an interactive pie chart based off our zoo animals' population statistics.    We will create two separate worksheets within the same document - one containing raw data about each type of animal's population;  another showing only the final results as well as the corresponding pie chart.     In order to make sure both sheets work together seamlessly, we'll also add a few formulas into the second sheet so that users can easily change any value displayed on the first sheet without having to manually update anything else.  \n\nLet's start!     Create a New Workbook in Excel\n\nIn order to get started quickly, open up a new blank spreadsheet programmed specifically...    Click OK.\n\nNow let's enter the actual numbers we're going to base our calculations on:   Type \"Zoological Animals\" into Cell A1,    Type \"Population\" into Cell B1,    Type \"1\" into Cell C1,    Type \"Camels\" into Cell D1,    Type \"2\" into Cell E1,    Type \"Ellips\" into Cell F1,    Type \"3\" into Cell G1,    Type \"Hippopotami\" into Cell H1,    Type \"4\" into Cell I1,    Type \"Monkeys\" into Cell J1,    Type \"5\" into Cell K1,    Type \"Rhinoceroses\" into Cell L1,    Type \"6\" into Cell M1,    Type \"Spiders\" into Cell N1,    Type \"7\" into Cell O1,    Type \"Tigers\" into Cell P1,    Type \"Lions\" into Cell Q1,    Type \"0\" into Cell R1,    Type \"Total\" into Cell S1,    Type \"=\" into Cell T1,    Type \"A1\" & \"+\" & \"B\" & \"*\" & \"C\" & \"-\" & \"D\" & \"\/\" & \"E\" & \"+F\" & \"*G\" & \"-\"H & \"\/\"I\" & \"+J\" & \"*K\"L & \"-\"M & \"\/\"N\" & \"+O\" & \"*P\"Q & \"-\"R\" & \"\/\"S\"T\"","label":1,"model":"bloomz","source":"wikihow","id":50}
{"text":"Weaving is an easy way to add length, volume or color to natural hair.  It can be done at home using only some simple tools.   This method will teach you how to create a basic 3 strand twist weave that looks like it was professionally installed.    You may also want to try other types of weaves such as cornrows (cornrow), flat twists (flat-twist) or box braids (box-braids).    If you're new to weaving then it's best if you start by practicing first before attempting this technique.     Wash and condition your hair..     Before starting make sure you've washed and...   Make sure there are no tangles left behind from shampooing and\/or conditioning because they could get caught up while creating your weave.        To help prevent breakage use a moisturizing leave-in conditioner after washing your hair so that your strands stay hydrated throughout the process.      Choose which section you'd like to begin weaving..  The most common place people choose to do their weave starts near the nape of one's neck where one would normally wear a ponytail but any area along the scalp works well too.       Once you've decided what location you'll be doing your weave pick out two sections of hair about 1 inch wide each....","label":1,"model":"bloomz","source":"wikihow","id":51}
{"text":"Tips are generally considered polite gestures that show appreciation or gratitude towards someone who's done something nice for us.  In some cases they may be required by law (e.g. taxi drivers), while other times they're simply recommended as courtesy (e.g. bellhops).  Tips also vary widely depending upon where you're from; however, there are certain guidelines which apply worldwide.   This section covers all aspects... Continue reading \u2192\n\nThe amount you'll want to give depends largely on what kind of service you've received - it's always best to err on the side of generosity rather than stinginess.    If possible, try giving more money than you'd feel comfortable with so there's no question whether you've left enough.     The following table provides general guidance regarding common services and their corresponding tips:   Waiter\/waitress 15-20% Bartender 10-15% Taxi Driver 5-10% Bellhop 5-15% Hotel maid 3-10% Hair dresser 10-20% Barber 3-5% Airport\/skycap 2-5% Cruise ship crew 4-8%","label":1,"model":"bloomz","source":"wikihow","id":52}
{"text":"If you're looking for some extra bass but don't want to spend thousands on installing new speakers, consider adding a pair of powered subwoofers.  Powered subs use external amplifiers to boost their sound output so they can be heard over other audio sources like music players, radios, and even car stereo systems.   This guide shows how to install two 10-inch powered subwoofers with a 1000-watt Class D mono-channel digital amplifier under the front seats of any vehicle equipped with a standard radio\/CD system.    If you've never installed...   The following steps assume you'll be working underneath the dashboard area of your car; however, this method could also work well for trucks, SUVs, boats, motorcycles, RVs, ATVs, lawnmowers, snowmobiles, tractors, generators, computers, home theater setups, and virtually anything else with a 12 volt DC source!    You might need help from someone who knows what they're doing when removing parts such as the center console lid,...","label":1,"model":"bloomz","source":"wikihow","id":53}
{"text":"GIMP (GNU Image Manipulation Program) can be used to create animated images called \"Gifs\".  These are very simple animations that consist only of one repeating pattern or \"frame\" repeated many times.   In order to do so you'll need to:  Open your favorite text editor such as Notepad++ or TextEdit.    Copy\/Paste the following code into it.    ...    Run GIMP from Start Menu\/All Apps\/Gimp-2.8\/2.8\/Programs\/GIMP-2.8.exe     Click \"File\" -> \"New\" -> \"PNG\".\n    (Optional)  Set the dimensions of your gif animation here if you'd like. (e.g. 320x240)\n    (Optional)   Change the number of frames per second here if desired.  (e.g. 25fps)\n    Press OK.\n    Fill the entire canvas with Black\n    Go to Filter > Render > Pattern > Grid\n    ... (optional)    Hide all layers except the first one.\n    ...    (repeat steps 14 - 19 until you've created 15 more frames).\n    ...","label":1,"model":"bloomz","source":"wikihow","id":54}
{"text":"Integrating functions is often difficult because we must determine how much area under the curve lies between two points.  Integration by parts allows us to split up an integrand (the function being integrated), which makes it easier to integrate.   The process involves taking one part of the integrand as the new \u201cbase\u201d or \u201cnumerator\u201d in another fraction; this can be done repeatedly until you reach your final answer.    This method works best when integrating rational fractions that are not easily factored out.     In order to use integration by parts correctly, you'll need to know what the indefinite integral looks like for each term involved in the problem.... \nThe following examples will demonstrate several different ways to apply integration by parts to solve integrals. You may want to try solving them before reading further. Example 1 Find the value of:  \u222b3x2\u22122x+9 dx   Solution:    We begin by applying integration by parts twice more times to simplify the numerator so that it's just 3x2\u22123x+9.  First we'll set u=3x2\u22122x+9{\\displaystyle u=3x^{2}-2x +9} and dv=x+1{\\displaystyle dv=x}.  Then we take the partial derivatives of u and v to get du=2x+2dx{\\displaystyle {\\frac {2x+2}{dx}}} and dv=1dv=dx{\\displaystyle {\\frac {1}{dv}}={\\frac {d}{dx}}} respectively.  Finally, substituting all...","label":1,"model":"bloomz","source":"wikihow","id":55}
{"text":"The night before is always stressful when you're going back to school.  You might feel nervous because it's new territory --  maybe you've never been away from home before; perhaps this isn't exactly what you expected at all.   But there are ways to ease those nerves!  Here are ten things you should do right now!   1) Pick out your clothes the night... 2) Have everything packed and ready.... 3)... 4) ... 5)  6)   7)    8)     9)      10) Go to sleep early enough to wake up feeling refreshed (but try not to set an alarm that's too far into the future).  Don't worry - tomorrow won't seem as bad as today does!\nIf you want more tips on how to prepare yourself emotionally for starting high school, read How To Be Happy At High School. For advice on getting through middle school successfully see How to Deal with Middle School Stress. And finally, check out our guide to surviving elementary school here. Good luck! See also: Back-to-School","label":1,"model":"bloomz","source":"wikihow","id":56}
{"text":"This section needs expansion.  You can help by adding to it. (November 2015)\nRefrigeration\n\nFreezing\n\nRoom Temperature Storage\n\nGhee\n\nNotes\n\n\n\nButter is an emulsion made up of milk fat globules suspended in water.   The milk solids give butter its yellow color while the milk fats provide both flavor and texture.    When stored correctly, butter should remain edible indefinitely.     However, improper handling may cause spoilage more quickly than normal.      This guide covers proper ways to handle all varieties of butter including salted, unsalted, clarified, and cultured butter.\n\nThe following sections explain each type of butter separately along with their recommended storage methods. \n\nSalted butter\n\nSlicing into cold butter causes the milk proteins to contract which makes them less likely to absorb moisture from air when left out uncovered.... \nUnsalted butter \n\nUnlike saltier versions, unsalted butter does not have enough sodium chloride present to prevent bacterial growth during freezing.        Therefore, it's important...","label":1,"model":"bloomz","source":"wikihow","id":57}
{"text":"The following is an overview of how to remove your old starter motor.  This procedure should take about 2 hours if done correctly.   The replacement part number is:  Toyota Part Number: 1JZ-GE-700A   You may have to order this item directly from Toyota Parts Direct at http:\/\/www.toyotapartsdirect.com\/Toyota\/Engine\/Parts-Accessories\/1JZGE700A\/1JZGEEngine\/70007001    If you're replacing the starter because it's worn down beyond repair,  you'll need to purchase new bearings as well.    Bearing numbers are:  Front Left - 7200-2B10-010    ... Continue reading below ... \nIf you've decided to replace the starter rather than rebuild it:  Place the new starter into position using the same procedures outlined above.     Connect all wiring harnesses properly.     Install the new bearing assemblys by placing them onto their respective shafts.     Insert the splines of each gear into its corresponding hole in the flywheel.     Securely tighten any bolts or nuts needed to hold everything together.     Check the alignment of the crankshaft pulley before installing the belt.     Tighten the belt around both pulleys until snug but do NOT overtighten.     Repeat steps 3-8 for the other three wheels.","label":1,"model":"bloomz","source":"wikihow","id":58}
{"text":"If you're arrested on suspicion of drunk driving, you'll be charged with operating a motor vehicle while under the influence (OVI). This is also known as driving under the influence (DUI) in some states.  The penalties vary by jurisdiction but can include jail time and\/or fines.   In most cases, an OVI\/DUI conviction will result in points being added to your driver's license which may lead to suspension or revocation.    If you've been drinking before getting behind... Read more \u00bb\n\nThe legal limit varies between countries; however, it's generally 0.08% blood alcohol concentration (BAC), meaning 80 milligrams per 100 milliliters of breath air.  Some jurisdictions have different limits depending upon whether you're over 21 years old.  For example, New York City has separate BAC levels based on age ranging from .025% - .035% for drivers ages 18-20, .05%-.07% for those aged 20-30, and .10%-.12% for people 30-40.  Other states use weight-based measures instead such as grams of alcohol per deciliter of blood (g\/dL).  A person weighing 150 pounds should never drink more than three drinks within two hours because this could put them above their state's legal limit.  Drinking too many alcoholic beverages increases your risk of becoming intoxicated faster....","label":1,"model":"bloomz","source":"wikihow","id":59}
{"text":"Pimping out your car doesn't necessarily mean spending thousands of dollars at the dealership.  In this guide we cover many ways for you to modify your own car without breaking the bank.   We also discuss what sorts of modifications may not pass inspection if you ever need to get your car inspected again (e.g. after getting new brakes).   Finally, we'll talk about... How much money should I spend? What kind of changes would my insurance company notice?  Is there anything illegal I'm doing here?\nThe first step towards modifying your car involves deciding exactly which parts you'd like changed.  This section covers everything from simple cosmetic tweaks to complete overhauls of your engine control unit.    Make sure you've got enough cash saved before starting!  If you don't want to go through the hassle of finding someone else to pay for these expensive upgrades,... The next step toward transforming your car is painting its bodywork.  You could choose something subtle - perhaps just changing the color slightly so that it matches better with other colors around town -  Or maybe you fancy yourself a bit of a Picasso when it comes to choosing colors!    Modifying your lights isn't always easy because different states require different types of lighting equipment; however, upgrading them gives your car a completely new look while improving visibility during nighttime driving....","label":1,"model":"bloomz","source":"wikihow","id":60}
{"text":"Folding maps is an important skill if you're planning any kind of outdoor adventure.  The following instructions will teach you how to fold and store a topo map (a detailed roadmap) properly.   This method works best when used with large-format paper maps such as USGS 1:24000 scale maps; however, smaller-scale maps can be folded similarly but may require more folds than are described here.    If you'd like additional tips about caring for your map once you've finished folding it, see How To Store A Paper Map. Choose a clear, zip-closed plastic bag big enough to hold all four sides of the map together without stretching them out too much.  You should also choose a bag that's thick enough not to tear easily while being pulled across rough terrain.     For example, a standard-size 8 1\/2\" x 11\" sheet of printer paper would fit into a small sandwich baggie made of heavy-duty plastic. Place the map face-down onto a clean work surface.  Make sure there aren't any wrinkles or creases in...","label":1,"model":"bloomz","source":"wikihow","id":61}
{"text":"This method will teach you how to make Robert Sabuda's famous \"Reindeer\" pop-up Christmas card.  This project requires patience but it's worth every minute spent making this beautiful card! You can find more information about Robert Sabuda here http:\/\/www.robertsabudaworldwide.com\/artists\/index.html . If you're not sure if your printer supports printing double-sided or have trouble doing... Continue reading \u2192\n, then turn off \"Printing Double-Sided\" before continuing.) Print the template file onto white cardstock twice.   The first time should be printed front-side-out;  you'll need to flip the page around when you've finished printing.    Once again print the template file onto another sheet of white cardstock once more.     You'll now have four sheets of cardstock with the template printed on;   one for each color needed for the pop-up card. (You may want to use colored pencils to draw some guidelines where you'd like to place certain colors). Set aside three of these sheets while working only with the fourth sheet.       On the fourth sheet, trace the outline of the template by drawing lightly inside its edges with an erasable marker.        Trace the outlines of the pop-up pieces too.      Now erase any pencil marks made earlier on the other three sheets of cardstock.","label":1,"model":"bloomz","source":"wikihow","id":62}
{"text":"The first thing you'll want to do when getting behind the wheel of this vehicle will likely be:  1.  Get out of the passenger seat 2.   Put your foot onto the brake 3.   Pull down the lever next to it 4.   Push forward 5.   Release the clutch 6.   Move the gearshifts 7.   Go ahead 8.   Stop 9.   Back 10.   Take care 11.   Repeat 12.   Be patient 13.   Keep calm 14.   Relax 15.   Enjoy 16.   Continue 17.   Don't forget 18.   Remember 19.   Know 20.   Practice 21.   Learn 22.   Try 23.   Do 24.   Have fun 25.   Wait 26.   See 27.   Think 28.   Listen 29.   Watch 30.   Notice 31.   Check 32.   Consider 33.   Ask 34.   Answer 35.   Read 36.   Write 37.   Talk 38.   Hear 39.   Touch 40.   Smell 41.   Eat 42.   Drink 43.   Sleep 44.   Breathe 45.   Open 46.   Close 47.   Hold 48.   Lift 49.   Lower 50.   Stand 51.   Sit 52.   Walk 53.   Run 54.   Jump 55.   Clap 56.   Sing 57.   Dance 58.   Laugh 59.   Yawn 60.   Shhh 61.   Sigh 62.   Pause 63.   Rest 64.   Change 65.   Return 66.   Leave 67.   Enter 68.   Exit 69.   Reach 70.   Come 71.   Stay 72.   Pass 73.   Give 74.   Receive 75.   Choose 76.   Pick 77.   Set 78.   Drop 79.   Grab 80.   Let 81.","label":1,"model":"bloomz","source":"wikihow","id":63}
{"text":"HAMILTON'S LEGISLATIVE TEST FOR COMPLIANCE WITH THE CONSTITUTION (1788)\nA  No express powers granted by the constitution to the federal government over matters within the scope of these articles.  Yes    No    No    No    Yes    No    No   No    No    No     No    No    No      No    No    No       No    No    No        No    No    No         No    No    No          No    No    No            No    No    No             No    No    No              No    No    No               No    No    No                 No    No    No                   No    No    No                      No    No    No                       No    No    No                        No    No    No                         No    No    No                          No    No    No                            No    No    No                             No    No    No No    No    No  No    No    No                No    No    No                  No    No    No","label":1,"model":"bloomz","source":"wikihow","id":64}
{"text":"The following is an explanation about how to do the tutorials in Forge of Empires.  The first step involves meeting Ragu at his house near the river bank.   After that he gives you some tips such as:   Building a Hut  This is where you'll start out from when playing this game; it's very important because all buildings must be built around or inside the hut.    Accepting the Quests  You'll have three main quests:    Gathering Happiness  In order to gain happiness (the number above your city's name), you should make sure that:  Your population has enough food.     There aren't too many people living outside of your cities.      All citizens work full time.        When you're happy enough, you'll reach the bronze age!  Research Pottery  Now you've reached the bronze age, so go ahead and upgrade your technology by using your forging points; once done, you'll see the option 'Research Pottery' under 'Technology Tree'; after doing... Continue reading \u2192\n\nYou can also check out our wikiHow articles on:","label":1,"model":"bloomz","source":"wikihow","id":65}
{"text":"Teaching your young children math skills will help prepare them for school.  It also helps develop important thinking processes that are useful throughout life.   This section provides some suggestions you may find helpful when teaching basic math concepts to preschoolers.\n\nGeometry\n\nChildren love colors!  Put colorful cutouts of small, medium and big geometric shapes on the walls: circles, squares, triangles, rectangles...etc.    Ask questions like: \"What shape do we see here? What does this look like?  How many sides\/angles\/dimensions has each figure?\"  You could even ask what kind of food would fit into these different shapes!   For example, how much pizza dough would go inside a circle versus an oval?\n\nMake shapes from paper\/cardboard\/paint sticks....anything available at home!\nYou don't have to limit yourself only to plastic toys; there are lots of ways kids can explore shapes using everyday items around the house.  Cutting shapes out of cardstock works well because it's sturdy enough not to tear easily while still being flexible enough so little fingers won't rip through it.     If you're feeling crafty,...","label":1,"model":"bloomz","source":"wikihow","id":66}
{"text":"The goal here will be creating a pattern similar to the following: \n \n This pattern consists of: \n\n\n* A number of balls rotating around its center point at different speeds,\n* Each ball has a radius determined by its distance from the center point, \n* All balls are connected together via lines which form a ring shape. \n* There exists a line connecting all points between two adjacent balls. \n\nIn order to create such patterns we need to: \n\n* Determine how many balls there would be in total based upon user input parameters, \n\n* Calculate the coordinates of each individual ball,\n\n* Connect every pair of adjacent balls.\n\nNote: \nWe assume you already know how to calculate the coordinates of each individual... Continue reading \u2192\n, , , , \n'Set Defined Variables, Formates for Values and Formulas':=IFERROR(VLOOKUP(A7,TT,2),0)\n'Formulas that have changed since \"How to Create an Insectoide Pattern\":=A7+AVERAGE(B4:B499999)\n'Column Headers':=L5&\":\"&T5","label":1,"model":"bloomz","source":"wikihow","id":67}
{"text":"If you\u2019re using an iPhone with Touch ID enabled, tap Add fingerprint instead.  You may have to wait while... If you don\u2019t already use it,... .\nTap Done when finished.... \nYou should now be able to access all of your Google Photos by signing in to your Google Account.   To delete any unwanted items that take up too much space,  go to:  Settings > Storage & USB > Manage Storage  (iPhone\/iPad)  \/ Storage Management... (Android)   On Android devices only:   Go to https:\/\/photos.google.com\/settings\/storage-management?hl=en&tab=overview#quota_usage   (or press \u22ee\u22ee\u22ee\u22ee \u22ef\u22ef\u22ef\u22ef\u22ee\u22ee\u22ef\u22ef\u22efa\u22ef\u22ef\u22f1\u22ef\u22ef\u22f2\u22ef\u22f3\u22ef\u22f4\u22ef\u22f5\u22ef\u22f6\u22ef\u22f7\u22f8\u22f9\u22fa\u22fb\u22fc\u22fd\u22fe\u22ff\u22c1\u22c0\u22c2\u22c4\u22c6\u22c7\u22c8\u22c9\u22ca\u22cb\u22cc\u22cd\u22ce\u22cf\u22d0\u22d1\u22d2\u22d3\u22d4\u22d5\u22d6\u22d7\u22d8\u22d9\u22da\u22db\u22dc\u22dd\u22de\u22df\u22e0\u22e1\u22e2\u22e3\u22e4\u22e5\u22e6\u22e7\u22e8\u22e9\u22ea\u22eb\u22ec\u22ed\u22ee\u22ef\u22f0\u22f1\u22ef\u22f2\u22ee\u22ef\u22ee\u22ef\u22c3\u22c4\u22c5\u22c6\u22c7\u22c9\u22ca\u22ca\u22cb\u22cd\u22ce\u22cd\u22cf\u22d0\u22d0\u22d1\u22d3\u22d4\u22d4\u22d5\u22d4\u22d6\u22d4\u22d7\u22d4\u22d8\u22d4\u22d9\u22d4\u22da\u22d4\u22db\u22d4\u22dc\u22d4\u22dd\u22d4\u22de\u22d4\u22df\u22d4\u22e0\u22d4\u22e1\u22d4\u22e2\u22d4\u22e3\u22d4\u22e4\u22d4\u22e5\u22d4\u22e6\u22d4\u22e7\u22d4\u22e8\u22d4\u22e9\u22d4\u22ea\u22d4\u22eb\u22d4\u22ec\u22d4\u22ed\u22ee\u22ee\u22f0\u22ef\u22ef\u22f0\u22ef\u22ee\u22f0\u22ee\u22ef\u22c6\u22c7\u22c6\u22c8\u22c9\u22c9\u22ca\u22c9\u22ca\u22c8\u22c9\u22c8\u22c9","label":1,"model":"bloomz","source":"wikihow","id":68}
{"text":"Laptops are great tools that allow us to work from anywhere we have access to electricity.  However, they also drain batteries faster than desktop computers because they're always plugged in somewhere else (like our homes) while we're away.   This means that keeping them running longer between charges becomes even more important.    Here are some tips to help extend their life so they'll last as long as possible before needing replacement.  \n\n1.) Unplug the battery after each use:  The most common cause of prematurely dead laptops is leaving the charger connected all the time.  When... 2.) Use the hard drive more often than the CD\/DVD drives:   Hard disks spin constantly whether or not data is being read\/write; however, optical discs only spin up during reading\/writing operations which makes them far quieter but drains the battery quicker.  3.)  Clean battery contacts:    Over time dust builds up inside laptops causing problems such as poor connections and short circuits.  To clean these areas gently rub alcohol wipes over any exposed metal surfaces until no dirt remains visible.  4.)   Turn off WiFi:     Most modern laptops come equipped wi","label":1,"model":"bloomz","source":"wikihow","id":69}
{"text":"If you've been having trouble downloading files on Windows 10 lately\u2014or even just opening websites\u2014the problem may be that your internet is too slow.  This wikiHow will show you how to boost your download speeds so you'll have more time for other things like watching Netflix! If you're not sure what kind of speed you should expect with your plan, check out our guide here. You can also try these tips in order to improve your upload speed as well!\nDisconnect all unnecessary items before testing your download speed again. Make sure no one else has access to your account by signing out everywhere possible including email accounts, social media sites, etc.. Also make sure there are no programs running in the background which could potentially use up bandwidth such as:  iTunes; Spotify\/Apple Music\/VLC Player\/Plex Media Server; Skype; Google Chrome; Microsoft Edge; Adobe Reader; Dropbox; Box Syncing; BitTorrent clients; Any online games; Videos playing through YouTube or similar services. Some applications require constant communication between them and their servers when they run in the background. These include Facebook Messenger, WhatsApp, Instagram, Snapchat, Twitter, Slack, Discord, Tinder, Pinterest, Reddit News Feed, Amazon Shopping Assistant, eBay, PayPal, Uber Eats, Lyft, GrubHub, Postmates, Instacart, DoorDash, Foodtown, Walmart Grocery Pickup, Target Express Drive, Shopify POS, Venmo, Square Cash, Stripe Payments, AliExpress, Wishlist, Shazam, Pandora Radio, Stitcher SmartRadio, iHeartRadio, SoundCloud, Deezer, Napster, Rdio, SiriusXM, Sticky Notes, OneDrive, Outlook Mail, Office 365 Calendar, Cortana, Xbox Live Services, PlayStation Store, Steam Cloud Gaming, Candy Crush Saga, FarmVille 2, Clash of Clans, Minecraft PE Pocket Edition, Pok\u00c3\u00a9mon Go, Plants vs Zombies Garden Warfare, The Sims Freeplay, World of Warcraft Companion App, League of Legends Client...","label":1,"model":"bloomz","source":"wikihow","id":70}
{"text":"Cardiac arrest occurs when blood flow stops completely through the heart.  This can be caused by many things including trauma such as car accidents,  drowning  electrocution , drug overdose  , infections like meningitis   and viral diseases.   The most common cause however remains coronary artery disease which causes clogging up of arteries leading to the heart muscle itself causing it to stop pumping properly.    If cardiac arrest happens outside hospital setting then survival rate drops dramatically within minutes if no action taken at all.   In fact only about 5% survive this condition without any medical intervention whatsoever.     It becomes even worse once the person falls into water because chances...    To perform CPR effectively there should always be:  One rescuer who performs chest compressions while other prepares mouth-to-mouth resuscitation Two rescuers performing chest compressions simultaneously There must also be enough space around the victim where they will work together efficiently. You need to make sure that the victim does NOT have anything blocking their throat during the procedure. Remember that every minute counts!","label":1,"model":"bloomz","source":"wikihow","id":71}
{"text":"Refrigerators are one of those appliances that we use every day but rarely think twice about until something goes wrong.  Keeping our refrigerator running smoothly is important because it's where most of us store our perishables like meat, dairy products, produce, etc.   This article will provide you with some tips on keeping your refrigerator working at peak efficiency so you'll be able to enjoy delicious foods without worrying if they're going bad!   If you're interested in learning more ways to make life easier around... .\nKeep reading!\nBefore beginning this project there are several things you should know.\n\nFirst thing's first - take everything out!  Remove all shelves, drawers, racks, trays, lids, containers, bags, boxes, bottles, jars, cans, condiments, ice cubes, anything else that's currently sitting inside your refrigerator.    You can put these items somewhere cool outside while you work.     Don't forget to empty the freezer too!    The next step involves taking apart certain parts of the refrigerator itself.  Depending upon what type of refrigerator you've got, different components may need to come out.  Some common ones include:  Shelf brackets Drawers Crisper bins Ice cube tray Water dispensing system Door panels Interior light Fixture covers Freezer's door Handlebars Condenser Fan Coils Gaskets Sealant Be aware that not all refrigerators have each component listed above....","label":1,"model":"bloomz","source":"wikihow","id":72}
{"text":"Conures are very social, intelligent, active, playful parrots.  They have been known to be one of the best talking parrot species.   However they can also be stubborn at times so it is important you know how to train them properly.    This section contains tips about caring for these beautiful creatures from choosing their proper habitat (cage) to teaching them tricks like dancing or speaking.     You should always keep in mind that each bird has its own personality which may differ greatly from what you've read here but hopefully you'll find many useful suggestions!   If you're interested in learning more specifics regarding training techniques then please see our Training page instead..    How do I care for my conure? - Introduction\n\nThe first thing you want to consider when buying a cage for your conure is size.  The minimum recommended space per bird is 1 square foot (.09\u00a0m2), however if possible try to get 2 feet by 3 feet (61 cm x 91... Continue reading \u2192\n\n\n\n- Feeding Your Conure","label":1,"model":"bloomz","source":"wikihow","id":73}
{"text":"iMovie is Apple Inc. 's free software that allows users to create movies with their Mac computers.  It has many features such as importing pictures or video files directly from cameras connected via FireWire cables; creating slideshows using images stored on your computer; editing audio tracks for added effects; inserting subtitles; applying filters to enhance colors and contrasts within videos; and exporting finished projects onto DVD discs.   You can also share your completed films online instantly! This guide shows how to use some basic functions available in iMovie 9.0.1. If you're interested in learning more advanced techniques like making animated GIFS out of still images, see this tutorial instead. For detailed instructions about downloading and installing iMovie, please refer to How To Download & Install Any Software On A Mac OS X 10.6\/7\/8\/XP\/Vista . Launch iMovie after you've downloaded it from the App Store. The program comes bundled together with other apps made by Apple including GarageBand, Pages, Numbers, Keynote etc.. Once launched you'll be greeted with a welcome screen which gives you access to tutorials covering different aspects of iMovie. These are very useful if you're new to...","label":1,"model":"bloomz","source":"wikihow","id":74}
{"text":"Paper wallets store Bitcoins offline using printed information derived from an encrypted digital key stored elsewhere.  This is useful if you want to keep some or most of your Bitcoins safe without having access to the Internet at all times.   You will need to create this paper wallet once per device (computer and\/or mobile phone) where you'll be storing Bitcoins.    The process takes about five minutes and requires no special software other than what comes installed on every modern operating system.     If you're interested only in creating a paper wallet so...    To use BIP39, select \"RFC 3923 Base58Check Encoding\" instead when prompted during step 3 below.\n     2. Turn off Wi-Fi and Bluetooth connections\n     3. Open https:\/\/bitaddress.org\/index.html  in Chrome or Firefox.  Make sure that you don't have these browsers open anywhere else; otherwise, they may interfere with each other's ability to load properly.\n         4. Wait while the page finishes loading completely.  Do nothing more after the page has finished loading except move your cursor back-and-forth across the entirety of its surface.  Don't stop moving even though there won't appear to be anything happening initially - just continue doing so until something happens.  5. When the page finally begins to respond again,...","label":1,"model":"bloomz","source":"wikihow","id":75}
{"text":"Shirataki are Japanese-style rice noodles that can be found at most Asian grocery stores.  They should come packaged dry inside an airtight plastic bag.   The package will usually contain several different types of dried noodle shapes such as:   Shirataki - These long tubes look like spaghetti when they first arrive; however, once soaked in warm water (or even room temperature), these noodles expand up to 10 times longer than normal!  You may need to use scissors to trim off excess ends if necessary.    Nori - This type looks similar... , Beef fillet steaks,  2 tablespoons vegetable oil, 1\/2 cup sake, 1\/4 teaspoon salt, 1\/8 teaspoon pepper, 4 cloves garlic minced, 1\/3 pound shiitake mushroom stems diced, 2\/3 pound napa cabbage chopped into quarters lengthwise, 6 cups shirataki noodels drained, 8 ounces frozen grated daikon radish thawed out, 12 ounces fresh spinach leaves washed and torn apart, 3\/4 pound negi green onions peeled and quartered diagonally, 1 bunch shungiku flowers picked and ripped open, 1 small handful sesame seeds","label":1,"model":"bloomz","source":"wikihow","id":76}
{"text":"Altoids Tins can also serve many purposes beyond their original purpose.  Here is how you might reuse them:  You may find more ideas here. If you'd like to share yours please send us an email! . We'd love to hear about what you've done with your empty Altoids tin(s). Send pictures to: altoids@altoids.com. Please include your name if you're willing to do so. . The following suggestions were sent in:  Thank you all who have shared these great ideas!  Be sure to check back often because we add new tips regularly. . . .\nThe following comments came from: . . . .\nIf you would like to contribute your own idea feel free to e-mail us at: altoids@altoids-tin.com . . . . . .\nPlease note that this page contains user submitted content which has NOT been reviewed by Altoids-Tin.com staff members. This material is presented solely for entertainment value. For official information regarding Altoids products visit www.altoids.com\/en_us\/home\/index.html","label":1,"model":"bloomz","source":"wikihow","id":77}
{"text":"If you're receiving unwanted calls from someone who is harassing you, annoying you, or just making too many calls in general, there may come a time where you'll need to take action.  This wikiHow will show how to prevent this person from calling again by blocking his\/her telephone number.   You should know that it isn't possible to completely stop all incoming calls (even those coming from unlisted numbers); however, most people won't even realize they called a wrong number because they'll never get through once you've blocked him\/her.    On Android Phones  Open Settings > Apps & notifications> Phone app> Advanced call settings> Blocked contacts.  Tap New contact then enter the person's information including first... Continue reading \u2192\n, which allows users to report spam calls directly into its database without having to go through any third-party services such as Google Voice or Truecaller.  The company also offers a free online tool known as \"Call Filtering\" that lets customers filter out specific types of calls based on keywords entered manually before placing each outgoing call.  In addition, Apple has introduced a new feature within iOS 10 named \"No Caller IDs\" that blocks unknown numbers from displaying caller IDs so that spammers can't use these details against you.  However, both features require paid subscriptions to work properly....","label":1,"model":"bloomz","source":"wikihow","id":78}
{"text":"The following is intended to give readers some basic information about what... Continue reading \u2192\n...and why becoming a private banker might interest them:  What does being a private banker involve? How much do private bankers make?  Where would I go to school for private banking?...etc.  Who else works at a private bank besides private bankers themselves?  Are there any disadvantages associated with being a private banker? ...etc. The next section provides more detailed answers to these questions....etc.   A brief description of each type of private banker:   Listings of private banks around the globe:   An explanation of the difference between \"private\" and \"personal\":   Some examples of different types of private bankers:   Why should someone hire a private banker instead of doing all his\/her own investing and\/or portfolio management?  What makes a good private banker?  What kind of personality traits does a private banker need to have?  What skills does a private banker require?  What kinds of things does a private banker study\/learn\/know?  What qualifications does a private banker usually possess?  What training did he\/she receive prior to starting employment as a private banker?  What educational background is required to start a career as a private banker?\nWhat Does It Take To Be A Private Banker?\n\nEducation\n\nWhile most private bankers hold advanced degrees,...","label":1,"model":"bloomz","source":"wikihow","id":79}
{"text":"Photoshop is available as both a free web-based service called Photoshop Express  and as paid-for software installed onto computers.  This section covers how to:   Open images stored on your computer with Photoshop Express.   Save edited pictures back to your computer.    If you're not already signed up for Photoshop Express,  go here first before continuing.     To sign in:     Go to https:\/\/www.photoshopexpress.com\/sign-in\/ in any browser.\n    ...    Start by opening the website https:\/\/app.photoshop.com\/express\/ in a web browser. It's located near the top-right corner of the screen; clicking it prompts a drop-down menu.      You'll find this link below the \"Online\" heading that's just above the blue button labeled Launch Editor.       Doing so opens the Photoshop Express editor....      Once you've finished editing the photo(s), click File again, followed by Export Image.        Type a file name and\/or location where you'd like to save the image(s).        Select JPEG from the \"Save As Format\" drop-down list.","label":1,"model":"bloomz","source":"wikihow","id":80}
{"text":"People are often quick to call someone else \"racist\" but may not realize how offensive this word really is.  If you've been called racist by another individual (or even if you suspect that others might see you as racist), take some time before responding.   Think carefully about what happened -- was it truly racially motivated?  Did you intentionally offend?   Was it simply a mistake?\nOnce you've thought through exactly what's going on, you'll know better whether or not you should respond at all.    Before deciding how best to handle accusations against you personally, try to understand where they're coming from first.  People who accuse... 2) Recognizing Racism:  What Is It?, How Does It Work?, & Why Do We Have It? - YouTube Video 3)  Understanding Systemic Racism: The Role Of Institutions In Promoting And Maintaining Racial Inequality 4)... 5) Understand White Privilege 6) Know Your Rights 7) Learn About Affirmative Action 8) Educate Yourself On Other Types Of Discrimination 9)","label":1,"model":"bloomz","source":"wikihow","id":81}
{"text":"Conditional formatting allows users to highlight certain parts of their spreadsheet by applying different formats according to specific criteria.  This feature makes it easier for people who are viewing spreadsheets containing large amounts of information because they only need to look at those rows highlighted instead of reading through every single row.   In this tutorial we\u2019ll be using an example where each column represents a month from January 2005 until December 2009.    We\u2019re going to create three separate rules so that our final result looks something like:   Monthly Sales  Jan - $500    Feb - $1200    Mar - $1500    Apr - $2000    May - $2500    Jun - $3150    Jul - $3750    August - $4100    Sep - $4400    Oct - $5040    Nov - $5490    Dec - $6390     1. Input Data into Spreadsheet Open Microsoft Excel 2007 and enter some sample sales figures for 12 months into the following table layout:  Column A: Months Column B: Total Sales Column C: Average Daily Sale 2. Create First Rule Entering the formula directly into the rule field may cause errors when copying across multiple worksheets.     3. Set Second Condition Type =$B$2*2 into the Formula box next to \u201cApplies To\u201d then press \u21b5 Enter key.","label":1,"model":"bloomz","source":"wikihow","id":82}
{"text":"The following instructions are for reassembly only.  If you're looking for help with disassembly, please refer to How To Disassemble An EZ-Hold 2 Bar Clamp.   The pictures below show how everything should look when reassembled.    1) Remove all screws from the base by unscrewing them counterclockwise using any appropriate tool like pliers or screwdrivers.     2) Unscrew both nuts holding together the two halves of the body casing by turning each nut clockwise until they come loose enough to be removed without being dropped into the openings where you'll put back the springs later.\n     3) Pull out the metal rod attached to the bottom half of the body casing along with its rubber bushing.     4) Take off the black cap covering the top part of the body casing.     5) Unplug the motor shaft connector cable from the power supply unit.     6) Remove the white cover protecting the battery terminals.     7) Remove the four bolts securing the lower portion of...","label":1,"model":"bloomz","source":"wikihow","id":83}
{"text":"If you\u2019re looking to become faster as a player, there are many ways that you can do this.  The most common way players train their speed is by increasing their maximum speed over long periods.   However, if you're trying to play at an elite level where every second counts, it may not always matter how much distance you've covered - it's what you were able to cover during those seconds which matters.    This means that improving your acceleration could help you run further before fatigue sets in.     In addition,...   How to Improve Acceleration  Speed Ladder Training  Interval Training  Reaction Time Training  Joint Mobility Exercises  Ball Skills Exercise  Rest Days  Technique First  Strengthening Your Leg Muscles  Stretching Afterwards  Don't Overtrain!    If you'd like some advice from professional coaches who know exactly what's required to make yourself faster... check out our Ultimate Guide To Getting Faster For Football here!  You might find these articles useful too....  How to Run Sprints Like Usain Bolt  How to Jump Higher","label":1,"model":"bloomz","source":"wikihow","id":84}
{"text":"The following steps describe how to create an Excel spreadsheet containing breast cancer risk factors based upon age, race, family history, weight, height, menopausal status, hormone replacement therapy use, alcohol consumption, smoking habits, exercise level, and other lifestyle choices. \n \n This template was created specifically for women who have been diagnosed with ductal carcinoma in situ (DCIS). \n \n \n \n The purpose of creating this template is to: \n \n \u2022 Provide information about DCIS treatment options \n\u2022 Help patients understand their diagnosis and prognosis \n\u2022 Identify potential risks associated with different treatments \n\n\nThis template does NOT provide medical advice nor should any patient rely solely on its contents when making decisions regarding her health care needs. Consulting with... If you're looking for some help understanding breast cancer statistics\u2014or just want to see what they look like graphically\u2014the folks over at wikiHow have put together a nifty Excel document outlining everything you'll need to know. (Click here.) It includes charts showing survival rates broken down by stage, tumor type, and more\u2014and it's available free online! Click through the slideshow above to get a peek inside.","label":1,"model":"bloomz","source":"wikihow","id":85}
{"text":"Tobacco is an annual crop grown primarily by small farmers throughout the world.  It can be harvested from September through November depending on where you're growing it.   The most common types of tobacco used worldwide include Burley, Virginia flue-cured, Oriental, Turkish, and cigar varieties.    You may also find specialty tobaccos such as Perique, Cavendish, Latakia, Black cavendish, Connecticut shade-grown, and other rare blends available online or locally.     Tobacco has been cultivated since ancient times; however, its use was banned during many periods due to health concerns related to smoking cigarettes and chewing tobacco products.       ... Continue reading\n\nGrowing Conditions\nThe ideal temperature range for growing tobacco ranges between 65\u201375 degrees Fahrenheit (18-24 \u00b0C).    However, tobacco thrives under a wide variety of climates including mild winters and cool summers.       In general, tobacco grows well in areas with high humidity levels and plenty of rain.      If possible, choose a location away from direct sunlight so that the plants don't get scorched by the sun's rays.    Choose a spot near a source of fresh air because tobacco smoke tends to linger long after you've finished using it.    Planting Seeds","label":1,"model":"bloomz","source":"wikihow","id":86}
{"text":"Have a great day everyone!!!!!!\nGood luck!!!!!!! See ya soon!!!!!!!!!!! <3<3<3<3> How to Have A Great Day At School For Teens By wikiHow Staff Follow these tips from wikiHow's editors to help you start off each new school year feeling confident and excited.  The following is an excerpt from our full article available here . You can also find more articles like this one here:  http:\/\/wikihow.com\/Beautiful-Hair   http:\/\/wikihow.com\/2-Beauty-Tips-for-Girls\/   http:\/\/wikihow.... ...more... \nRelated Articles\n\nNext Article \n \nYou might be interested in: \n\nPlease consider supporting us by disabling Adblock Plus or whitelisting wikiHow on your ad blocker's list.  \n \nIf you'd rather support wikiHow without ads, please donate today. Thank you! This article has been viewed 1,082 times since its publication date May 16, 2013. To keep things running smoothly around here we sometimes place small data files called cookies on your device.","label":1,"model":"bloomz","source":"wikihow","id":87}
{"text":"Pheasants were introduced into North America from Europe over 200 years ago.  They can now be found throughout most parts of Canada and the United States.   The birds migrate south every winter but return north again when spring arrives.    In some states they will stay through summer months before migrating back down south once again.     Hunting seasons vary widely depending upon where you're located; however, many places allow year-round hunting under certain circumstances (e.g., youth hunts).   Some states require permits and\/or licenses...   This article provides information about how to find and hunt pheasants safely using traditional methods....  For additional tips related specifically to hunting pheasants,...    If you'd prefer to use modern technology instead, try downloading an app designed especially for finding and tracking wild game animals, including pheasants.  There are several available options which provide detailed maps showing locations of known wildlife habitats along with real-time updates regarding weather conditions and any potential hazards associated with specific regions.  These apps also include GPS-based tools allowing users to mark their own spots so others know exactly what they've been able to see themselves.  Apps include:","label":1,"model":"bloomz","source":"wikihow","id":88}
{"text":"If you're using an older version of InDesign (CS3), click here for instructions.  Open up your document file with InDesign.   You can also add page numbers manually from within this window as well.    The default setting should be set at 1.0.     This will allow you to adjust how many digits are displayed before the decimal point.        If you'd prefer to use Arabic numerals instead of Roman numerals, select \"Numbers - Arabic Numerals\" under the drop-down menu.      To make sure that your page numbers look good throughout your entire booklet\/book, repeat these steps until you've numbered every single page in your document.       Once done, save your changes!    For more information about adding page numbers in InDesign CS4, see this guide. For more information...   How do I add page numbers? Select File > Place Embedded Numbering....  Choose which type of numbering system you'll be using:  Type the starting value for your page number:  Set whether each page's number increments automatically after its predecessor has been reached:  Check the \"Automatically increment\" option so that when you insert new pages later on, they get their own unique page numbers:","label":1,"model":"bloomz","source":"wikihow","id":89}
{"text":"Make homemade cough drops by following this recipe.  You can also make cough drop candies from scratch if you prefer not to use store-bought cough drops as a base. If you'd like some additional ideas on how to prepare natural remedies that will help relieve cold symptoms such as sore throat pain, congestion, sneezing, runny nose, and feverishness, try making these other recipes instead.   Toothpaste  Gargle Mixture   Herbal Lozenges   ... How to Make Homemade Cigarette Papers ... Finished! Try out one of our many different ways to make cough drops below.   ... Making Your Own Natural Soaps: A Guide For Beginners    ... How to Make Sugar Scrub From Scratch  ... How to Make Lip Balm: Recipe & Tutorials    ... How to Make Bath Bombs From Scratch  ... How does this work? This is because they are made up mostly of simple sugars which dissolve easily during cooking but then recrystallize once cooled down again. The process of heating and cooling causes the sugar molecules to rearrange themselves so that crystals form inside the syrup. These crystals trap tiny pockets of air within their structure; hence why chewing gum has bubbles trapped inside its texture.    ...","label":1,"model":"bloomz","source":"wikihow","id":90}
{"text":"Mao (pronounced mah-oh) is played with 52 playing cards.  It can be enjoyed as either a single person game against oneself or it can be played between two people.   In this section we describe how to play Mao using the traditional method.    Recruit players. The minimum number of players for Mao is three; however there are usually four or more participants per round.   If you're just starting off learning about Mao then it's best if all of the players sit around a table so that they can easily see what everyone else is doing at any time during the course of their turn.     The dealer has a special... Continue reading\n\nThe dealer's job is to shuffle the deck before dealing them into individual hands and also to announce where the next player should go after taking his\/her turn.  This means that every time someone takes a turn he\/she must move clockwise from wherever he was sitting until reaching another seat occupied by another player whose turn comes up next.   For example, let's assume that Bob sits directly across from Alice while Charlie sits diagonally opposite her\/him.  After Bob's turn ends he'll need to move over to the chair that's currently being held by Carol who's seated right behind him\/it.  Then once Carol finishes her turn she'll move onto the chair that Bob previously sat down in and continue moving counterclockwise through the circle of chairs until she's back where she started again.   It's important to note here that the dealer does not get to choose whether or not s\/he wants to keep his\/her position throughout the entirety of the game but rather gets assigned randomly upon beginning the first round of play.   Once you've chosen a new dealer you'll want to reshuffle the deck and begin the process of distributing the cards among yourselves again....","label":1,"model":"bloomz","source":"wikihow","id":91}
{"text":"Progressive aphasia refers to a group of disorders characterized by difficulty speaking due to damage caused by neurological changes associated with aging.  The most common cause of this type of aphasia is Alzheimer's Disease.   Other conditions which may lead to... Continue reading \u2192\n\nThere are many different types of treatments available for individuals suffering from progressive aphasia.    These include medications designed specifically to treat symptoms related to dementia,...    There are several ways to help improve memory loss....    In some cases, cognitive behavioral therapy has been shown effective in helping patients cope better with their disorder....    It should be noted however that no medication currently exists to cure Alzheimer\u2019s Disease;  nor does any existing drug appear able to stop its progression once diagnosed....    You will need to consult with your physician before beginning any new medical regimen including supplements like vitamins B12 and B6, folic acid, melatonin, ginkgo biloba extract, etc. ...    If you live alone, consider hiring home health care workers to assist you during times when you experience increased difficulties communicating....","label":1,"model":"bloomz","source":"wikihow","id":92}
{"text":"If you are replacing an inner tube because it has been damaged beyond repair then follow this guide.  If you're just changing tires due to wear and tear see our other guides here.   This method will work best when removing front wheels but should also apply to rear wheels as well.    You don't need special tools in order to change a bicycle's tire - most people use their hands!   The only thing you'll really need besides some basic hand tools are:  A small flathead screwdriver (for loosening the nut holding the spoke end to the fork) An Allen wrench (or hex key) (to loosen the bolt securing the hub assembly)  Some good quality gloves (if working outside).  You'll probably want to remove the pedals before taking off... Continue reading \u2192\n\nThe following steps assume you've already removed the pedal(s), brake lever(s), seat post clamp\/screws\/nut, handlebars\/handlebar stem, etc. (see previous articles).   To begin take note of which direction each spoke points so they'll go back in the same place later.  Then loosen the nuts that hold each spoke-end to the fork using either a small flat-head screwdriver or allen wrench depending upon what type of spokes you have installed.     Once loose enough pull up gently on the spoke ends until they're straightened out against the fork.  Do this for every spoke attached to the fork.  Be careful pulling too hard since you could damage something like the derailleur hanger or bottom bracket shell.  Don't worry about getting everything perfectly lined up; once it's reattached it'll look fine anyway!  For more detailed instructions please refer to How to Change Front Wheel Spokes","label":1,"model":"bloomz","source":"wikihow","id":93}
{"text":"The state is located on Lake Superior's western shore.  It has been nicknamed \"The Land of 10,000 Lakes\" because there are over 1 million lakes within its borders.   The largest city is Minneapolis (pronounced \"Min-nee-sota\"), which was founded during the American Indian fur trading era.    Minnesota also boasts many other cities such as:    In addition to being home to numerous lakes, rivers, forests, prairies, wetlands, and wildlife habitats, Minnesota is also famous for having one of the most unique accents among Americans.   This accent can be described as \"soft-spoken but strong-voiced\".   People from this region tend to speak slowly yet clearly; they use long sentences and often pause between each phrase while speaking.     They will usually stress certain syllables in their...   You should try not to change too much about how you normally talk unless you're trying to imitate an actual Minnesotan dialect. If you want to learn what people actually say here, you'll need to listen carefully to native speakers talking amongst themselves.","label":1,"model":"bloomz","source":"wikihow","id":94}
{"text":"This is part 2 of the series on creating charts using Microsoft Excel 2007.  This section will focus on how you can make your own idea image by combining several different types of charting techniques into one single graph.   The final product looks like this:  You may find it useful to read Part 1: Creating Charts before continuing.   ...    Click here for more information about this topic. If you're interested in learning other ways to combine data visualization tools such as MS PowerPoint 2010, Adobe Illustrator CS6, Photoshop CC 2014 & CorelDRAW Graphics Suite 2015 together please see our book titled Visualizing Complexity: An Introduction To Interactive Infographic Design published by Packt Publishing Ltd.. For those who are looking forward to learn some advanced 3D modeling skills we recommend checking out Blender Foundation's online tutorials at https:\/\/www.blender.org\/resources\/tutorials\/index.html .     -   -   -   -  ...    -   -   - ...    -   -  ...  -   -   -  -","label":1,"model":"bloomz","source":"wikihow","id":95}
{"text":"The Internal Revenue Service (IRS) requires employees who receive wages to fill out Form W-4 so their employer can calculate federal tax withholding.  The purpose of filling out... This article provides detailed step-by-step instructions about completing Form W-4, Employee's Withholding Allowance Certificate.   If you're filing jointly,... , read the basic instructions provided by ... Continue reading \u2192\n, click here . \nYou must file Schedule A along with your return unless you qualify for certain exceptions....  To figure out what type of vehicle you'll drive during retirement, look back over past years' records. .   For help calculating your Social Security benefits,  see our guide .\nTo learn how to prepare your own tax returns using software such as TurboTax\u00ae, visit our guides . \n\nPlease note that these articles provide general guidance only; they do not constitute legal advice nor do we guarantee accuracy. Consult a professional accountant before making any decisions based upon the contents of these pages. We make no representations regarding third-party products mentioned herein.","label":1,"model":"bloomz","source":"wikihow","id":96}
{"text":"Winter in India starts from November end till February beginning.  The weather gets extremely chilly outside; however inside offices or houses there are no problems whatsoever because we have heating systems available.   But what if your office doesn\u2019t have any kind of heating system? What then?  Well here\u2019s how you could make sure everyone stays warm enough even without having access to air-conditioners.    Room Heaters  A good way would be by installing some portable heaters like oil filled radiators (which don\u2019t consume too many electricity) into every single cubicle\/work station within...   If this method seems expensive,...    Another option might be getting hold of a small space heater....    You may also try out infrared heaters but these tend to dry out the skin very quickly..     Make sure not to use anything electrical near flammable materials!    Furniture  Try arranging everything neatly together instead of leaving them scattered across different corners of the room.  This will ensure maximum coverage against the cold.  Also place carpets over hardwood floors wherever possible.  Remember - carpeted areas retain heat better than bare wood surfaces do!  Lighting  Darker colors absorb less light compared to lighter ones.  So go ahead and paint those walls black \u2013 it\u2019s actually quite trendy nowadays anyway!!  And remember to turn off unnecessary lights when you\u2019re done working.  It\u2019ll save energy and money both!!!    Electricity bills will shoot through the roof otherwise!...","label":1,"model":"bloomz","source":"wikihow","id":97}
{"text":"Teaching children how to use 911 may seem like something you don't need to worry about, but it's important for kids as young as three years old to know where to turn during emergencies.  This section contains tips on teaching your kid(s) how to use 911 properly.   You might also want to check out:  Tips for Teaching Kids About Safety at Home   Tips for Teaching Children...\" \/> Knowing When To Teach Your Child About 911:  It's never too early to start talking to your child about using 911 correctly; however, there are some situations (such as being lost outside home without parents nearby), where this topic isn't appropriate yet.    The best age range to begin discussing these topics depends upon each child's maturity level.     If you're unsure whether your child has reached the proper stage to learn more about 911,...    - See more at: http:\/\/www.wikihow.com\/Teach-Kid-Use-911#!\/tips\/3     - See also: http:\/\/en.wikipedia.org\/wiki\/911_emergency_number","label":1,"model":"bloomz","source":"wikihow","id":98}
{"text":"If you want to avoid having to go to school because it's boring (or whatever), then here is how you can easily fool your parents into thinking you've got an illness.  This method works best if you live alone with one parent who doesn't work outside the house.   You should also consider whether they would notice any signs of deception from you before trying out these methods.    The first thing you'll need to do is prepare ahead of time - preferably several days beforehand.  Make sure all your symptoms match each other perfectly; otherwise they'll catch onto what you're up to pretty quick.  For example, if you say... Continue reading \u2192\n, start acting as though you were really ill right away!   Your parents may ask questions such as:  \"What's wrong?\" \"What happened last night\/this morning?\" \"Why didn't you wake me up?\" \"If you'd been feeling unwell since yesterday why did you wait until now to tell us?\"  In order to answer their question truthfully without giving anything away, you might say:  \"I was afraid I'd forget my lines today if we talked about it earlier.\" \"You wouldn't believe me anyway even if I told you everything.\" \"It's nothing serious. I'm fine once I've rested.\" \"I'm sorry I woke you up; I'll sleep downstairs tonight.\"","label":1,"model":"bloomz","source":"wikihow","id":99}
{"text":"If you can't get your Samsung Galaxy S3 connected to your computer using its default connection method -- MTP or PTP -- you'll need to use an alternative way.  This guide will show you how to do that by enabling ADB debugging mode instead.   Note that this process may void... If you're still having trouble getting your S3 to work properly after following these steps, try contacting Samsung directly via their support page here. You can also visit our troubleshooting section if you'd like more help fixing other issues related to your Android smartphone. Restart both devices before continuing so they are ready to be reconnected.    The first thing we recommend doing is trying out another USB cord and\/or USB port since many users have reported success simply changing cables\/ports while attempting to fix problems between phones\/computers.     To check which ports are currently being used,...   -  Plug one end of the new USB cord into your computer's charging port..\n-  Plug the other end of the new USB cable into your S3's charging port..","label":1,"model":"bloomz","source":"wikihow","id":100}
{"text":"The automotive industry has been growing rapidly over recent years.  The demand for qualified mechanics will continue into the foreseeable future as new cars hit the road every year.   In fact, there were nearly 1 million jobs available across North America alone at the end of 2012 according to the U.S Department of Labor's Bureau of Labor Statistics (BLS).   This growth can be attributed primarily to two factors -  increased vehicle sales worldwide due largely to economic recovery from recessionary conditions;   and technological advancements which have made vehicles increasingly complex requiring highly trained professionals to maintain them properly.    There was also another factor contributing to this rapid expansion of the auto repair industry:    Many people who had previously worked in other industries such as manufacturing moved their skills to car maintenance because they realized how lucrative these positions could become compared to previous employment opportunities.    For example,  one mechanic told me he earned $70 per hour when his employer discovered... How do I get started? Research the automotive technician job market","label":1,"model":"bloomz","source":"wikihow","id":101}
{"text":"The following instructions were written specifically for students attending SUNY Buffalo State College.  These instructions may also apply to faculty members who teach courses that require them to use the buffalo state computer labs and\/or printers.   Students should contact their instructor if they do not know how to access course materials from the buffalo state computer system.    Note:  Instructions below assume that you already have a valid user name\/password associated with your buffalo state email account.     You must log onto the buffalo state website through one of the above mentioned browsers...If you're having trouble accessing the site via other browsers such as Edge, try clearing out all cookies stored within it....You cannot print directly from mobile devices like iPhones\/iPads etc..    - Go to https:\/\/mybcc.buffalostate .edu\/portal\/home\/sso\/logon?service=https%3A%2F%2Frpc3-wp-wsgi-script%3Ffile_path%3Dhome %26user_name%3DSUNY_Buffalo_State_Campus_System%40SUNY_Buffalo _State_Campus_System&no_cache=1#login   - Enter your BCC Student ID number followed by @buffalostate. edu. (e.g. 1234567890@buffalostate. ed u)  - Enter your Network Password Follow the prompts until you reach the \"Print\" page.","label":1,"model":"bloomz","source":"wikihow","id":102}
{"text":"This tutorial will show how to make a dinosaur head using Animator 8 or 9 (both are similar). This program can be found online for free at: http:\/\/www.anim8or.com\/downloads\/index.html . You may have trouble downloading if you're using Internet Explorer as it's known that IE doesn't work well with some websites.  If you don't know what version of Windows XP\/VISTA\/7\/8\/10 you currently run, go here https:\/\/www.microsoft.com\/en-us\/software-download\/windows10upgrade\/ .  The file size should be around 1 GB.   Download the installer from here: https:\/\/support.amd.com\/en-us\/kb\/313593  Install the drivers by double clicking them once they finish downloading. . After installing AniM8OR open it up. It takes about 10 minutes before it loads completely. When loading completes you'll see a window pop-up saying \"Anim8\". Now create a new project called \"Dino\". In order to do this:   Right-click anywhere inside the white space.    Create New Project    Name it: Dino    ...","label":1,"model":"bloomz","source":"wikihow","id":103}
{"text":"This section will teach you how to replace the brakes on an impala with disc brakes.  This procedure should take around 3 hours depending upon experience level.   You can find detailed information at How To Change Your Own Disc Brake Pads & Rotors In The Chevy Impala. Get your tools ready:  1x 8\u00a0in (20\u00a0cm) floor jack 2x 4x4 inch (10 x 10 cm)  2\u201d wrench 4x 5\/16\" socket Set the parking brake cable free before beginning this project. Make sure there\u2019s no one behind you when you\u2019re driving it down the road! If possible use a lift truck instead of a tow dolly because they have higher clearance than regular cars do which makes getting under the car easier.   ... Continue reading \u2192\n\nThe following steps show where each part goes during installation but may be different based on year model and\/or trim levels. Refer to your owner's manual for specific instructions regarding your particular vehicle. Before removing anything make sure you're using the correct replacement parts listed below.","label":1,"model":"bloomz","source":"wikihow","id":104}
{"text":"Being single mother isn't easy but it doesn't have to be impossible.  If you're struggling financially, emotionally and\/or physically then this guide will help you get through those tough times by giving you some tips on how to:  Find work Get money Apply for welfare benefits Make friends Be healthy Stay safe Build self-esteem Become independent Have fun! This section contains information about finding jobs as a single mom.   You may also want to check out How To Start A Business As A Mom which gives more detailed advice on starting up small businesses while raising kids at home.    There are several ways to find work:   Work online (e.g. data entry) Work part time at local stores Call around to see what positions might interest you Ask friends and relatives for job leads Go to career fairs Attend interviews Dress appropriately when going into interviews Keep track of all applications so they don't go astray Follow-up after each interview Don't give up!  Even though it's hard sometimes,  there... Read More ... The next step once you've found employment is getting paid income.  In most cases you'll need to apply for Social Security Benefits such as:  Supplemental Nutrition Assistance Program (SNAP), formerly known as Food Stamps Medicaid Child Care Services WIC Cash Aid Other government programs like Section 8 Housing Vouchers may provide additional resources depending upon where you live.  For example, in New York City families must meet certain eligibility criteria before applying for these vouchers.  See here for details.  Some states offer tax credits specifically designed for low-income working mothers.  These include Earned Income Tax Credit (EITC)  and Working Families Tax Relief Act (WFTRA).  Visit IRS website for specifics regarding taxes owed based on your situation.  Click here for more info....","label":1,"model":"bloomz","source":"wikihow","id":105}
{"text":"Varnishing raw wood will give any wooden object an elegant sheen.  It also protects the wood\u2019s surface from moisture damage.   This method works best for unpainted, untreated wood such as oak, mahogany, walnut, maple, cherry, birch, teakwood, pine, fir, cedar, poplar, alder, ash, sycamore, elm, hickory, pecan, bamboo, and other hard woods.    If you\u2019re working in a small space like inside your house, open up windows and\/or turn on fans to help circulate air around while you are painting.     You shouldn\u2019t apply this technique outside because paint fumes may cause health problems....   For example,...    Apply thin coats...     Let the first coat dry at least 8 hours before testing its tackiness with sandpaper..     Sand off the dried layer of varnish with 220-grit sandpaper..  Do NOT use steel wool! Steel wool scratches the finish instead of removing it..     The second coat dries faster than the first coat does..","label":1,"model":"bloomz","source":"wikihow","id":106}
{"text":"Converting audio files from .WAV format into MP3 format requires an advanced software called FFmpeg (Free Media Player). You will need this program in order to complete these instructions.  If you're not already familiar with how to download programs onto your computer then please refer to our guide titled \"How To Download Programs Onto Your Computer\" before proceeding any further.   The steps outlined here assume that you've downloaded the latest version of FFmpeg which supports both 32-bit as well 64-bit versions of... Converting Audio Files Using FFmpeg - Part 1  Converting Audio Files Using Ffmpeg - Part 2   Converting Video Files Into Different Formats With FFmpeg - Part 3    Note: For detailed information about each step listed above including screenshots click on the links provided.    Step 1: Install FFmpeg     Step 2: Create A Sample Folder Containing Both Wav And Mp3 File Types.     Step 3: Run Command Prompt\/Terminal From Start Menu Or Dash Board.     Step 4: Enter Commands To Convert Wav To Mp3 Format.     Step 5: Check That Newly Created mp3 File Can Be Played By Any Device.     Step 6: Exit Terminal\/Command Prompt Window.     Step 7: Delete Sample Folder After Completion Of Task.     Step 8:","label":1,"model":"bloomz","source":"wikihow","id":107}
{"text":"Vanillas have been grown since ancient times by indigenous peoples throughout Mexico.  The Aztecs used vanilla extensively both medicinally and cosmetically.   Today there is still much interest in this unique flavorful spice that can be found in many different foods including ice cream, cakes, cookies, candies, pastries, soups, sauces, teas, coffees, sodas, alcoholic drinks, perfumes, soaps, lotions, shampoos, toothpastes, cough syrups, medicines, etc.    There has also recently been an increase in demand for natural health products containing vanillin such as anti-depressants, pain relievers, sleep aids, antibiotics, anti-microbial agents, insecticides, fungicides, herbicides, and other agricultural chemicals.   In addition, vanilla extract is now being widely used in aromatherapy treatments because its scent promotes relaxation and helps relieve stress and anxiety.   This guide will teach... How to Plant Vanilla Orchids","label":1,"model":"bloomz","source":"wikihow","id":108}
{"text":"If you want to start up your own professional carnival company but don't have enough money or experience yourself, consider spending one season working alongside someone else who's already established their own professional carnival.  This will give you valuable insight into what it takes to run such a large operation successfully while also allowing you to make contacts within this industry.   You can find job opportunities by contacting existing carnivals directly through phone calls and\/or email messages asking about available positions.    If you're interested in opening... Continue reading \u2192\n\nOnce you've decided where you'd like to work during your internship (or if you haven't yet), it's time to begin planning out exactly which attractions you'll be adding to your new carnival.  You'll need to decide whether you would prefer to focus primarily on children's activities or adults' activities;  some people choose to specialize exclusively in either area depending upon market demand.  For example, if you live near a college town, then focusing solely on kid-friendly activities may not be very profitable because most students are adults anyway!    In addition to deciding which types of attractions you wish to include,...","label":1,"model":"bloomz","source":"wikihow","id":109}
{"text":"This method has been used for years by many people who suffer from foot-nail fungus.  It works well because both salt and vinegar are natural disinfectants that kill bacteria and other pathogens; they also help loosen the hardened layers around infected areas.   The warm water helps dissolve the salt crystals which then act like an abrasive cleansing agent against the affected toe.  Vinegar contains acetic acid which kills most types of mold spores including those responsible for athlete\u2019s foot.    This treatment will not work immediately but over time should remove the infection causing the nail to become detached naturally.  If there... is no improvement within three months consult a doctor.... \nTo prepare the solution mix together equal parts of baking soda and hydrogen peroxide using enough liquid soap to create bubbles.  Add some drops of tea tree oil  Mix everything together in a bowl creating thick paste-like consistency.  Apply directly onto the infected nail.  Cover with bandage overnight.  Remove next morning.  Rinse away with cool water.  Do twice each day for at least 2 weeks.  You may need longer depending upon severity of infection.  Keep doing this even though new growth appears underneath old nail.  Be patient!","label":1,"model":"bloomz","source":"wikihow","id":110}
{"text":"Puppies can become very attached to their owners when they're young.  They may even develop separation anxiety once they've been separated from them too long.   To prevent this problem, it's best to train your puppy early so she knows what her daily routines will look like while you're away at work.    The first step towards potty-training involves creating a solid schedule for yourself and your puppy.  This way you'll know exactly what's happening each hour throughout the day (or more frequently).  You also want to create a good relationship between... Once you've created a basic schedule for yourself and your new puppy, it's time to start implementing some rules around his sleeping habits.  A healthy amount of rest helps keep both humans and dogs happy!  Make sure your puppy gets plenty of exercise during the day but then goes straight back to bed immediately following one of these activities.  Sleeping times vary depending upon breed,... After establishing a basic schedule for yourself, your puppy, and any other pets living under your roof, it's now time to focus on making sure your puppy eats regularly.  Feedings usually occur three times a day - morning, afternoon, evening.  If possible, try not to feed your puppy just prior to leaving home for work....","label":1,"model":"bloomz","source":"wikihow","id":111}
{"text":"The following steps are not recommended by Sony or any other official PlayStation organization.  This guide does contain some potentially harmful information; if you're unsure whether it's safe enough for you to follow it please consult an adult before proceeding with these instructions.   If anything goes wrong while performing one of these procedures there may be permanent damage done to your PSP console and\/or its software (including games).  You can always try to find another way around whatever problem you're having instead of trying... .\nThis method involves hacking the firmware running on your PSP so as to allow you to run custom programs called \"homebrews\" - essentially free applications created by third parties using open-source code.  These include emulators such as NESticle, SNESticle, GBAticle etc. (which let you play old-school video game consoles right on your PSP), media players,... .1) Prepare your PSP:  Make sure you've updated all available patches for your current operating system. .2)  Unlock Bootloader:   Turn off your PSP completely.    Press R1+L2+R3+Start simultaneously.     Select \"Install Custom Firmware\".      Choose \"Download from Internet\".        Enter \"http:\/\/hacksonysp.googlecode.com\/files\/hotfix-2-0-3-hot-fix-update-v3.7.rar\"      In the next window select \"Custom Firmware Installer v6.60\"        Start downloading.       Once download has completed, install the CFW installer package.","label":1,"model":"bloomz","source":"wikihow","id":112}
{"text":"Parents want what's best for us.  They may try to control our lives in order to protect us from making bad decisions.   However,  sometimes we feel as though they're trying too hard to control everything we're doing.    Here is how teenagers can gain parental trust while still living an exciting life!   1) Listen to your parents' requests  2) Follow through on promises 3) Avoid lying 4) Respect yourself 5) Stay out late 6) Get into trouble 7) Stick up for yourself 8) Take responsibility 9) Accept criticism 10) Ask questions 11) Talk openly 12) Give honest answers 13) Work 14) Study 15) Eat healthy 16) Exercise 17) Sleep 18) Dress appropriately 19) Have fun 20) Go places 21) Drive 22) Travel 23) Hang out 24) Socialize 25) Watch TV 26) Play video games 27) Read 28) Write 29) Clean 30) Wash 31) Pick up after yourself 32) Pay attention 33) Stop using drugs 34) Don\u2019t go around kissing boys 35) Don\u2019t date until you\u2019re ready 36) Don\u2019t show off 37) Don\u2019t use profanity 38) Don\u2019t swear 39) Don\u2019t smoke 40) Don\u2019t drink 41) Don\u2019t drive 42) Don\u2019t text 43) Don\u2019t play violent computer games 44) Don\u2019t spend money 45) Don\u2019t shoplift 46) Don\u2019t skip school 47) Don\u2019t hang out with friends who don\u2019t care 48) 49)  50)   51)...","label":1,"model":"bloomz","source":"wikihow","id":113}
{"text":"Golf is played by 2-4 people using 52 standard playing cards.  It was invented during World War II when soldiers were bored waiting for mail from home.   There are several variations that allow more than three players but these will be covered later.    This version uses only 52 cards so it works best if there aren't too many players involved.     If you're teaching children how to play golf then it's better not...   For this variation you'll need an extra joker which should go into the center of the board before shuffling begins....     In this variation all players begin with eight cards instead of seven,...    To win golf quickly try replacing high-scoring cards such as aces and eights with low-scoring ones such as twos and fours.  It's also important to remember what other players have because they may take away some of those higher scores you've been hoping for.  Remember that you don't always get exactly what you ask for; sometimes you just get something else!    A good strategy would be:  Keep track of who plays last every round.  Don't let yourself become complacent about winning!  Be aware of the fact that you might lose even though you think you won.  Know where to look while counting your points:","label":1,"model":"bloomz","source":"wikihow","id":114}
{"text":"The following steps will show you how to remove all parts of your M16 service rifle except the stock.  This method can be used as an alternative way to clean or repair any part of this weapon system without having access to special tools.   The procedure described here should only be performed if you are familiar with basic firearm maintenance procedures such as:  Always keep safety precautions when working around weapons! Lock the bolt to the rear position before beginning.    1) Locate the rear dis-assembly pins (2), one on each side of the trigger guard area near where the backstrap meets the gun.     2) Insert a flat-blade screwdriver into the slot between the two pins until they pop open. (Do NOT use pliers.)     3) Pull both pins straight forward towards yourself using the screwdriver as leverage.        4) Continue pushing the pins until they reach their final resting place against the bottom of the receiver.     5) Once again insert...    6) Place the screwdriver behind the second pin and push it upwards toward the top of the receiver.     7) Do exactly what was done previously; however, do it now with the first pin instead of the second pin.      8) After removing these pins, separate the upper and lower receipts by pulling them apart gently but firmly.     9) You may need to apply some force to get everything separated completely.       10) Take note of which direction the screws go through the upper and lower receivers; make sure they're facing opposite directions....","label":1,"model":"bloomz","source":"wikihow","id":115}
{"text":"Grade 8 is an important step between elementary school and middle school.  You are no longer little kids but not quite adults yet either! This means there\u2019s lots going on during eighth grade!  Here are some tips to help you do well this year!\n1.   Get ready for all subjects - especially English Language Arts and Maths  Start studying early so you\u2019re prepared when tests come around or projects due..  If you're struggling with something ask teachers how they would like you to study more effectively.. Ask questions about things you don\u2019t understand clearly enough..\n2.   Stay organised Keep track of what needs doing by using a planner\/diary\/memo pad etc... Write down dates for exams\/tests\/projects and deadlines for handing work back..   Organise folders into which you'll put papers\/documents relating to different topics\/subjects....   3.   Be punctual Arrive at school every single day on time.    Don\u2019t be late for classrooms because it\u2019s embarrassing and disruptive to other students who might already know their lesson.  Don't leave lessons until they're over unless it's absolutely necessary.  4.   Study hard Work hard at whatever task has been assigned to you.  Try to find ways to learn faster than others.  Read ahead wherever possible.  Take notes while reading.  5.   Avoid distractions Turn off mobile phones\/iPads\/tv's\/computers\/etc...","label":1,"model":"bloomz","source":"wikihow","id":116}
{"text":"British English has many subtle but important differences from American English.  This section will help explain some of those differences.   If you're looking for more information than what can be found here, try using an online dictionary such as: http:\/\/www.dictionary.com\/browse\/british-english?s=t .\nFoods\n\nChips - Thick French Fries\nFrench Fries - Thin Potato Strips\nBread & Butter Pudding  - Bread soaked in milk then baked with butter and sugar\nCurry Sauce - Spicy Indian sauce made up of tomatoes, peppers, ginger etc.\nPork Chops\/Steak\/Pasta - Meat cooked according to recipe\nSoda Water\/Sparkling Water\/Cola - Carbonated Drink\nTea - Herbal Infusion\nBreakfast Cereals - Granola bars, muesli, oats, cornflakes, porridge, granola, cereal bars, cereals, toast, bagels, croissants, pastries Fruit Juice - Orange juice","label":1,"model":"bloomz","source":"wikihow","id":117}
{"text":"If you're playing The Sims 4: Seasons,  you'll need to wait at least one season before trying these steps.  This method works best when you've been living together as roommates for some time.   You can also use this method even after getting married by going through divorce first.    Make sure both Sims are unmarried!   Don't forget about career aspirations!  It may take several seasons depending upon their careers.     Keep doing things they enjoy so they'll continue having positive relationships.  Continue making friends with other Sims who might become potential partners. When two Sims live together long enough, eventually they will start dating each other automatically without any help from players. However, sometimes it's fun to make them fall in love yourself instead. Here are tips on how to speed up things:  Do activities such as: dancing, cooking meals, taking walks outside,... \nOnce they're officially dating, don't forget to spend quality time with them:  Spend lots of time talking to them; Complimenting them often; Giving gifts regularly; Having sex with them frequently;","label":1,"model":"bloomz","source":"wikihow","id":118}
{"text":"If someone who is on probation or parole commits an assault against you that causes serious injuries such as permanent scarring, disfigurement, loss of use of body parts, or even death, then he may be liable for monetary compensation.  You can file suit under California law by suing him directly through civil action.   If successful, this will allow you to recover money from his insurance company and\/or personal assets.    The statute of limitations... How do I know whether my claim has been settled? Answer not in context    In order to successfully sue a paroled person, it helps to have some idea about the amount of money you\u2019re seeking before hiring a lawyer.  This information should include all past and future medical expenses related to treatment received because of the crime; lost wages due to time missed from work while receiving care; pain and suffering caused by physical injury; emotional distress resulting from being attacked physically; mental anguish suffered after witnessing violence inflicted upon another individual; funeral costs associated with losing loved ones; and property damage sustained during the attack.  It also includes special damages like those listed above plus interest....","label":1,"model":"bloomz","source":"wikihow","id":119}
{"text":"Figs are one of my favorite fruits because they\u2019re sweet but not too sugary.  They also have an amazing texture that makes it hard to believe you\u2019re eating something so healthy! I love making fig jam every year when we get our first batch from the farmer\u2019s market.   It\u2019s easy enough even beginners should be able to make their own homemade preserves!  This recipe yields 3\u00bd cups which means there\u2019ll probably only be room left after all this work for two more batches before I\u2019ll need another jar!\nI\u2019ve made several different types of jam throughout the years including raspberry, strawberry, blueberry, blackberries, peaches, apricots, plums, cherries, mangoes, pineapple\u2026and now figs!!  Each time I\u2019ve learned some tips along the way that I\u2019m sharing below as well as how long each type takes me to prepare.   ... Read More ...Read Less\n\n1. Wash the figs under running water.\n\n2. Remove any stems. \n\n3. Put the washed figs in a large pot.  \n\n4. Stir together \u00bc cup (60 ml) of water and 2 tablespoons (30 ml) of freshly squeezed lemon juice until combined thoroughly.     The acidity helps prevent mold growth during storage.      You could substitute orange juice instead of lemon juice if desired.        If you're worried about preserving the color of your jam, consider adding a few drops of food-grade liquid stevia extract rather than sugar. Stevia has no effect on taste at all and won't affect the shelf life either.       Alternatively, add honey instead of sugar. Honey doesn't preserve like sugar does though, so you'll want to store your jam in the refrigerator immediately after you've finished cooking it.","label":1,"model":"bloomz","source":"wikihow","id":120}
{"text":"Swedish has the same numbering system for counting from 0-99 like most other European languages.  However there are some differences when it comes to constructing more complicated numbers.   In this section you will learn about these differences.   \n\nNumbers up to 19:  These can all be constructed by adding together different combinations of the digits 0-9 with each digit representing its own value.   For example, if we want to count to 20 then we would add an extra \"tio\" at the end which means \"ten\" or \"10\":   \"Tre + tio = treTTO\" equals 31.   \"Fem + tio = femTIO\" equals 51.   \"Sex + tio = sexTON\" equals 61.   \"Nitton + tio = nitTOno\" equals 71.   If instead...    How do I pronounce the letters A-E?     How do I pronounce G-J?  How do I pronounce K-Q?     How do I pronouce R-Z?     How do I write out my name using the Latin alphabet?","label":1,"model":"bloomz","source":"wikihow","id":121}
{"text":"Ankle boots go well with many different types of clothing - especially when it comes to dresses! Here is how to pair ankle boots with various styles of dresses:[1]  Pair booties (boots without laces) with midi skirts.  Booties also work great as shoes under long-sleeved shirts worn open over skinny jeans.   High-heeled boots should only be paired with mini-dresses; otherwise they may shorten your leg length unnecessarily.   ... How To Style Your Dress And Boots Together[2][3][4] ... You Can Make Any Outfit Look More Fashionable By Accessorizing It[5] [6] [7] [8] [9] [10] [11] [12] [13] [14] [15] [16] [17] [18] [19] [20] [21] [22] [23] [24] [25] [26] [27] [28] [29] [30] [31] [32] [33] [34] [35] [36] [37] [38] . For tips about choosing which color boots to buy see this wikiHow article. . .\nIf you're looking for ways to spice up an outfit that's already pretty basic then consider pairing it with jewelry; however it's important not to overdo it:   A necklace adds interest to any outfit.  Necklaces come in all shapes,...","label":1,"model":"bloomz","source":"wikihow","id":122}
{"text":"In France,...   Learn more... \n \n \n \n How to Say No in French: Apprenez comment dire non en fran\u00e7ais\u00a0! Utilisez des phrases courtes et directes pour refuser poliment une invitation ou un service.  Dites \u00ab\u00a0non\u00a0\u00a0\u00bb de mani\u00e8re directe lorsque cela est n\u00e9cessaire.   Soyez poli dans vos r\u00e9ponses n\u00e9gatives. \u00c9vitez les mots n\u00e9gatifs comme \u00ab\u00a0pas du tout\u00a0\u00bb (\u00ab\u00a0du tout\u00a0\u00bb) qui peuvent \u00eatre consid\u00e9r\u00e9s comme trop directs par certaines personnes. Essayez plut\u00f4t d'utiliser le mot n\u00e9gatif \u00ab\u00a0rien\u00a0\u00bb au lieu de \u00ab\u00a0pas du tout.\u00a0\u00bb  Si quelqu'un demande si quelque chose ne va pas avec vous, dites-lui que \u00e7a va tr\u00e8s bien. Vous pouvez aussi r\u00e9pondre simplement qu'il n'y a rien de mal quand on vous pose cette question. R\u00e9pondez aux questions sur votre sant\u00e9 d'une fa\u00e7on positive m\u00eame si ce n'est pas vrai. Par exemple, si l'on vous demande si vous \u00eates malade alors qu'en r\u00e9alit\u00e9 vous avez la grippe, r\u00e9pondez positivement que vous allez mieux maintenant....","label":1,"model":"bloomz","source":"wikihow","id":123}
{"text":"The Gregorian calendar was invented during the 16th century AD.  It uses weeks consisting of seven days; each month begins on Sunday except for February, whose first... This article will teach you about calculating dates using the Gregorian calendar system.   You\u2019ll learn what it means when someone says something happened \u201cin the middle ages\u201d; why there\u2019s no 13th month in our calendars; and more!   For example,... \nYou may be wondering where this information came from:  Didn\u2019t we just invent the Gregorian calendar? No, actually!  We inherited many aspects of the current calendar from ancient civilizations such as Egyptians, Babylonians, Greeks, Romans, Chinese, Japanese, Koreans, Hindus, Jews, Persians, and others.    These cultures used different types of calendars at various times throughout history....    There were also other systems besides these ones listed above\u2014for instance, the Maya civilization had their own unique calendar system based upon cycles of time called \"long counts\" and \"tun\" periods.  However, most people today only recognize the Gregorian calendar because it\u2019s been adopted worldwide since 1582 CE. (CE stands for Common Era.)","label":1,"model":"bloomz","source":"wikihow","id":124}
{"text":"The ship of 1000 cranes has been used as an offering by Japanese Buddhists since ancient times.  It symbolizes love, compassion, patience, endurance, courage, determination, hopefulness, friendship, joyful reunion after separation, and peace.   This origami crane pattern was created by Akira Yoshizawa who won first prize in the 1990 Origami USA competition.    Start with a square sheet of...   Step 1  Begin folding from corner to corner until all 4 edges meet each other forming a cross-like figure. You can use any color or size paper you'd like but if you're using colored paper then try not to mix colors too much because you'll need to see where you've folded before continuing on. If you don't know how to start off with a square piece of paper just cut some squares out of white cardstock instead! Once it's done unfolding turn it around 180 degrees and repeat Steps 2-5 above. Now flip the paper over once more and unfold it completely. Make another copy of the same thing starting with step 6 below.","label":1,"model":"bloomz","source":"wikihow","id":125}
{"text":"Dodgeball (also known as Dodge Ball) was invented in America by George Gipp during World War II.  It has since become popular around the world.   The object of this sport is simple;  hit out the opposing team members before they're able to tag you back.    This can only happen when you've successfully caught the ball being thrown at you by another member of the opposite team.     There are many ways to play this game but here we present our version which involves four teams playing against each other simultaneously using a regulation sized basketball court divided into three equal sections called end zones.   Each team consists of seven players including a captain whose job is to direct his\/her team mates throughout the match.   All matches consist of five periods lasting ten minutes per period except for the final where there'll just be three periods instead.   You may choose whichever number of players you'd like to include within these limits however it's recommended that you use between six and eight players per team so long as everyone plays equally well.   If possible try to find similar skill levels amongst your group otherwise you'll need to compensate accordingly.   For example, if there's no real difference in speed among your squad consider having more defenders than attackers while vice versa would require less defense and more offense....","label":1,"model":"bloomz","source":"wikihow","id":126}
{"text":"If you're interested in opening up an animal jam salon, here are things you'll need before doing so:  A lot of money.  You will also need to know how much each item costs because if you make too little or lose too many tips from bad service, you won't earn enough money to buy more stuff.   An employee.  This is optional but highly recommended as having one person working with you makes running this business easier than trying to do everything yourself.    Once you've got these three things together, it's time to open!  First thing...   Next step...  Now let's talk about making your own salon look good....    The next few steps cover getting ready for when clients walk through those doors..  Let's start by talking about the basics..  ...Next we move on to the fun part - actually giving treatments!...     Finally, once we've covered all the basic information needed to run our very own successful salon, we'll give you a list of other helpful hints which may help you along the way..... ...Now we're going to tell you exactly how to set up your shop and prepare for its grand opening day!!","label":1,"model":"bloomz","source":"wikihow","id":127}
{"text":"The first step is installing some of the required software packages that are needed by this tutorial.  This will download all available updates from repositories.   The package contains the header files used by many programs including GLEW which we need later when creating our own OpenGL application.    It also includes the Freeglut library which provides basic windowing functionality like resizing windows etc..    You can find out what version of OpenGL it supports using glxinfo | grep \"GLX\".     If there isn't any output then it means no OpenGL implementation was found.        Now let's compile the sample program provided with the distribution called \"Sample_OpenGL_Programs\".       Run the following command:   $ cd \/opt\/X11R6\/bin     $ gcc -Wall -I\/usr\/include\/SDL2 -lSDL2_gfx  -o test_SDL2_opengl main.cpp SDL_opengl.h -lGLEW -lGLU -lglut     Note: Replace \/usr\/include\/SDL2 with the path where you've installed SDL2.      Once compiled successfully you'll be able to execute the program by typing the following:      $ .\/test_SDL2_opengl","label":1,"model":"bloomz","source":"wikihow","id":128}
{"text":"Pyrography is the art of decorating objects made from wood (or other materials) through controlled burns.  Pyrographers use special pens that are heated up electrically until they produce enough heat to melt into the surface of the object being burned.   The result looks like etching but without all the effort involved.    This method works best if used as a way to add artistic flair rather than trying to create something realistic such as a photograph.     You will need several things to get started including:  A wooden item to work on; An electric wood-burning tool; Pens designed specifically for pyrography which have fine-tipped metal nibs at one end and a heating element inside them connected via wires to a power source; Graphite pencils; Sanding blocks; Rubber gloves; Safety goggles; Heat-resistant matt spray paint; Paintbrushes and\/or sponges; Paper towels; Scissors; Tape measure; Pencil sharpener; Glue gun;","label":1,"model":"bloomz","source":"wikihow","id":129}
{"text":"Dressing appropriately for an HR position requires more than just wearing nice clothes.  You need to make yourself presentable while still being able to perform all necessary tasks on behalf of your employer.   This section provides tips about how to choose appropriate attire based upon your specific role as well as other factors such as location (e.g., office vs. field) and client base.    Buy working basics  Stock your closet with items like button-downs,...   Choose colors carefully...    Wear pantsuits if possible....    Avoid revealing tops unless it's required of your position...  Keep accessories minimal but classy...    Don't forget shoes!  Pick one pair of heels and one pair of flats per season.  Heels should never go higher than 3 inches (7 cm).  Flip-flops aren't acceptable anywhere except home.  Consider getting a pedicure before going into meetings where you'll have to wear formal footwear.  Shoes must match your suit jacket color.  For example, don't wear brown leather pumps with a tan suit jacket; instead pick either brown suede pumps\/boots or nude patent leather pumps\/boots.  No matter what type of shoe you decide to get, make sure they fit properly!","label":1,"model":"bloomz","source":"wikihow","id":130}
{"text":"The following maintenance tips will help you keep your air-cooled Volkswagen on the road longer.  These suggestions apply only if you're using gasoline as opposed to diesel or other fuels.   If you have any questions about these procedures please contact us by email at: [email protected]  The information contained here was compiled with the assistance of many owners who contributed their knowledge over several years.    Before beginning work make sure that there isn't water inside the vehicle such as:   - Water may be trapped between the hood latch and its catch point.     - Water may also collect behind the windshield wipers.     To prevent damage caused by moisture use a hair drier set on low heat to defrost areas where ice has collected.   ...    Clean out the drain holes located near each tire well so they don't get clogged up with gravel and sand which could cause problems during rainy weather.\n    (This step applies to vehicles equipped with rubber bumpers.)     Replace the bumper cover gasket every two years. (If it looks like it's leaking replace immediately.)   This should take less than one hour per year depending upon how often you drive your car.   For more detailed instructions see our \"How-to\" section entitled \"Repair Your Car's Body Works\"\n     .","label":1,"model":"bloomz","source":"wikihow","id":131}
{"text":"A recipe binder can be an excellent way to store your favorite dishes so they are easy to find when you're ready to cook.  This method will show you exactly how to put together this type of binder using simple materials found around most homes.   You may also like:  How to Create Your Own Cookbook from Scratch How to Write a... Continue reading \u2192\n\nThis wikiHow teaches you how to use binders to hold your favorite recipes organized neatly while still being accessible whenever you'd like to prepare dinner. If you've ever tried looking for a particular recipe only to have no idea where it's stored\u2014either because there isn't any index or because everything's just thrown haphazardly inside\u2014the time has come to take action! A well-designed recipe binder makes finding anything quick and painless; you'll never again lose track of those precious family favorites!\nThe first step toward organizing your collection involves gathering necessary items such as:   1\u00a01\u20442 inch (3.81 cm) (0.6\u00a0in \u00d7\u00a015.24\u00a0cm) ring binder 5\u20448 inch (1.59\u00a0cm) (0.25\u00a0in \u00d7\u00a07.62\u00a0cm) wide white plastic page protectors Ring binder tabs\/labels Sticky notes Scissors Stapler Pencil\/pen Label tape","label":1,"model":"bloomz","source":"wikihow","id":132}
{"text":"If you're reading this page because you've been wondering which version of Pok\u00c3\u00a9mon X & Y should you buy - well... congratulations! You've made an excellent decision!\nPok\u00c3\u00a9mon are great fun for all ages, but especially children who love collecting them.  The best thing about buying these new versions over their predecessors is that they can play together!  This means that instead of having to split up after completing the story mode once again, you'll now be able to continue playing as friends wherever you go.   You don't need to worry too much though; even without trading cards there's still plenty of ways to keep track of what everyone has caught.    There are also some differences between the games themselves, such as:  * In Omega Ruby, players start off by choosing from three starter Pok\u00c3\u00a9mons, while in Alpha Sapphire it's only two.  * In Omega Ruby, the player's rival is Baron, whereas in Alpha Sapphire his name is Brock.  * In Omega Ruby,... Continue Reading \u2192\n,","label":1,"model":"bloomz","source":"wikihow","id":133}
{"text":"Giving horses medications via injection has several benefits over oral administration.  It allows precise dosage control; it bypasses the digestive system so that drugs are absorbed more quickly by the body; it's also less messy than administering pills or liquid medicine.   However, there are risks involved as well - including accidental injury to both humans and animals when giving shots.    This article will provide detailed instructions on how to safely give a shot to a horse using either the neck area or the rear end.  The information provided here comes directly from veterinarians who specialize in equine health care.  If you'd like additional tips regarding other types of veterinary procedures such as castration, teeth cleaning, etc... please visit our sister website How To Be A Good Friend For Horses.[1] X Research source .[2] X Trustworthy Source American Association of Equine Practitioners Professional organization providing resources related to animal welfare Go to source .\n[3] X Trustworthy Source American Veterinary Medical Association Organization devoted to improving public knowledge about pet health Go to source","label":1,"model":"bloomz","source":"wikihow","id":134}
{"text":"Chicken is one of those foods that can be prepared by boiling it.  This method works well if you want to prepare large amounts of food quickly without having to worry too much about cooking times.   The following are some tips regarding how long different types of chicken take to boil depending upon their size and whether they have bones or not.    If you're looking for other ways to cook chicken try our articles on:  How To Roast A Whole Chicken How To Poach A Chicken Breast How To Broil A Chicken Leg Bone-In Chicken Breasts - These should always be cooked at 325 degrees Fahrenheit (163 Celsius).   For this recipe you'll use 2 tablespoons olive oil per pound of meat....    Boneless Skinless Chicken Thighs - You may also choose to cut these into chunks before placing them in the pot.    ...     Braised Chicken Drumsticks - You'll first place the chicken legs in salted water then simmer them over medium heat for 45-50 minutes.   ...     Braise-A-Bone - In order to braise an entire chicken using this technique all you'll do is put the bird inside a crock pot along with enough water so it's completely submerged but no more than two inches above its top edge.   ...","label":1,"model":"bloomz","source":"wikihow","id":135}
{"text":"Gluten intolerance or sensitivity has become increasingly common over recent years.  Gluten is found primarily in wheat-based grains such as:  Barley Rye Wheat Oats Triticale Buckwheat The most obvious symptom of gluten intolerance\/intolerance is diarrhea.   Other less-obvious signs include:   Weight loss Fatigue Irritability Loss of appetite Nausea Vomiting Diarrhea Constipation Abdominal pain Headache Itchy skin Dry mouth Hair loss Weakness Muscle cramps In addition there may also be:  Anemia Fever Depression Low blood sugar Hypoglycemic reactions If you suspect yourself might suffer from gluten intolerance then it's important to see an expert who can confirm whether or not that's true.   ... How do I live with celiac? Try not to eat foods that contai ... n gluten, if any at al l . Do som e researc h t o f ind ou t wha t fo od s typicall y ha ve glu ten i n th em .  Readi ng ingredien t lab elin gs is th e onl y wa y t o tru ly kno w whe ther glute n is i n th e prod ucts yo ur buyi ng. . Looking for gluta ne-fre e item s on restau rant men us . . Seek th e adv ice of a nutri tionis t \/di etiti en specia li zing i n gluta ne -fre e d ie ts . . .","label":1,"model":"bloomz","source":"wikihow","id":136}
{"text":"If you're trying to lose some pounds by cutting back on carbs (carbohydrates), then you'll probably want to know how long it's going to take before you start seeing results.  The answer depends upon several factors including age, sex, current fitness level, diet plan you've chosen, etc.   In general though, most dieters will see noticeable results within two weeks if they're following their diet plans closely.    How Long Does It Take To Lose Weight?    If you're looking to drop five kilograms in one month,... Continue reading... \nYou may have heard about low-carb diets like Atkins Diet which limit carb intake so as to force your body to use its stored energy - i.e. fat reserves.     But what exactly does this mean?  What happens when we eat too few carbs?\n\nCarbs: Good or Bad?\nIn order to understand why low-carbing works, let's first look at how eating carbs affects us physically.\n\nWhen we eat food high in starch such as breads, pastas, rice, potatoes, cereals, fruits and vegetables containing starches, these foods break down inside our stomachs into simple sugars called glucose.      Glucose travels through blood vessels all around our bodies where it fuels every cell's need for energy.        When there isn't enough glucose available, cells turn to other sources of fuel known as ketones.  Ketones come from fatty acids found in adipocytes (fatty tissue) located beneath skin and between organs.       This process is known as lipolysis; breaking down of fat stores into free-floating fatty acid chains and glycerol molecules....","label":1,"model":"bloomz","source":"wikihow","id":137}
{"text":"French pronunciation may seem difficult because there aren't any easy rules.  However, once you've learned how to say each word correctly, you'll notice it's actually quite similar to English.   The most important thing about learning French pronunciation is listening carefully to what people around you are saying; then repeat after them!   Listen closely to the way they speak their sentences - especially the endings on certain words like 's', 't', 'r' etc.. These little details really matter!  Don't worry too much about grammar until you're more comfortable talking fluently..  If you'd rather learn from books instead of real life conversations, try using audio CDs which have been recorded by professional actors reading aloud entire chapters of children's stories...or even whole novels!\nIf possible, immerse yourself completely into France culture while studying French:  Watch films without subtitles,  Read newspapers,... Continue Reading \u2192\n\nThe best way to improve your vocabulary is simply to talk to other people whenever you get the chance.  Ask questions like: \"What does this mean?\"; \"How would you describe X?\"; \"Can you tell me where Y is located?\"","label":1,"model":"bloomz","source":"wikihow","id":138}
{"text":"In order to create templates using PhotoBie design studio you'll need to:  Open up Photoshop CS2 from Adobe.  Create an empty document by clicking File > New... > Blank Document.... Make sure that all three boxes under \"Resolution\" have been checked off so that when you print them they will come out exactly the same no matter what printer you're using.   Save your work!   Now open up another blank document again but instead change the dimensions to 8x10 inches which is standard 4x6 inch picture size plus white borders at each side.    Change the resolution settings back to 72 dpi because that's the default setting for most digital cameras these days..    If you'd rather not do anything else just yet,... To start creating our first template we can simply select the Template Tool from the toolbar located along the left hand side of the screen..  The Template Tool allows us to draw circles squares triangles rectangles ovals stars polygons hearts etc...  We can also adjust their sizes colors and positions within the canvas.  There are many different types of textures gradients patterns and fill effects available through the Tools panel found next to the Layers palette.     When we're done designing our template we'll press Ctrl+S to save our project before proceeding onto adding images to the template itself.      Let's say we'd like to insert two pictures taken during our recent trip to Hawaii.  First thing's first though; we must import those pictures into photoshop.  Right-click anywhere inside the workspace and select \"New Layer From Background\" option.  Name the newly created layer \"Hawaii\" and set its blending mode to Soft Light.  Next name the second layer \"Pacific Ocean\" and set it's blend mode to Overlay.  Finally rename the third layer \"Skyline\" and set it's blend mode...","label":1,"model":"bloomz","source":"wikihow","id":139}
{"text":"Variation One:  Foundation Blocks:   Variations Two & Three:    Finished Size:     .\n\nFoundation Blocks:\n\nCutting List:\n\n\n\n\n\n\nSewing Needles:      Thread:        Scissors:       Beading String:    Buttonhole Stitch:    Hemming Tape:         .               ...                 ...              ....             .....             ....          ......           ..........         .......                  .........                 ....................                     ................\nFabric Used:        Fabric Used:        Fabric Used;        Fabric Used:        Fabrics Used:        Fabric Uses:        Fabric Used:        Faux Leather:        Fabric Used:        Cotton Fabric:        Fabric Used:.        Fabric Used:        Fabric Used.         Fabric Used:        Fabric used:        Fabric Used:         Fabric Used:        Fabric uses:        Fabric Used:         Material Used:        Fabric Using:        Fabric Used:        Material Used:        Fabric Usa:        Fabric Used:       Material Used:        Fabric Using;        Fabric Used:        Materials Used:        Fabric Using;        Fabric Used:        Materails Used:        Fabric Usings:        Fabric Used:","label":1,"model":"bloomz","source":"wikihow","id":140}
{"text":"Pizza stones are used by professional pizzaiolos all over the world because they help cook pizzas evenly.  They also make cleanup easy since there isn't much grease left behind after cooking.   If you're interested in making homemade pizza but don't have access to a traditional brick oven, then this method will show how to bake them in your own kitchen.    This recipe makes enough dough for two medium sized pizzas; however if you'd prefer larger ones just double everything!   For best results try to find freshly made dough from local bakeries rather than buying pre-made frozen doughs which tend not to taste nearly as good.     The following instructions assume you've already prepared your desired crust according to another recipe such as:  How To Make A Basic Dough Recipe    1 cup warm tap water 2 tablespoons olive oil 3 cups flour 4 teaspoons salt 1\/2 tsp sugar 1\/4 tsp yeast Mix together the dry ingredients into a bowl before adding the wet ingredients. Once combined add the yeast mixture and knead well. Let rise covered overnight","label":1,"model":"bloomz","source":"wikihow","id":141}
{"text":"Betta fish can be very beautiful pets but they also have some health problems.  If you notice something wrong with your betta fish then it may need medical attention.   This article will help you identify common diseases so you know how best to care for him when sick.    Common symptoms include:  Fin rot \u2013 this disease causes the fin tips to turn black and fall off.     Ich (white spot) - these small white dots grow into larger patches covering most of the fish's body.     Swim bladder disorder - this problem makes the fish float upside down near the surface of its container.     Bubbles - this condition occurs because air bubbles get trapped inside... Continue reading \u2192\n\nIf you're looking for more information about caring for Bettas check out our full guide here!   You might find useful articles on:    How To Care For A Newborn Baby Betta Fish     What Is The Best Food For Your Betta?     How Long Can A Betta Live?  How Much Water Does My Betta Need?","label":1,"model":"bloomz","source":"wikihow","id":142}
{"text":"Graveler's evolution is one of many things that cannot be done without cheating.  This method involves using an emulator called VisualBoyAdvance Link (VBALink) which allows players to play their Pok\u00c3\u00a9mon Emerald roms simultaneously across multiple computers.   The first step will require downloading some software from other sites; however this should only take about five minutes at most.    Once you've downloaded everything you'll need open up both windows of VBLINK by double-clicking each icon.  You may have trouble playing the game if you're running Windows Vista\/7\/8\/10 64-bit operating systems so make sure... If you don't already own it then download the latest version of VisualBoyAdvance here before continuing.     Open the program's settings by clicking File \u2192 Settings....  In the \"System Language\" section change the language setting to Japanese. (You must do this because the English translation does not work properly.)    Next click OK twice to close out of the settings dialog box..     Now go ahead and load your Pok\u00c3\u00a9mon Emerald Rom file onto the emulator..  To find your Pok\u00c3\u00a9mon Emeral Rom file navigate to: C:\\Documents and Settings\\<Your User Name>\\My Documents\\VisualBoyAdvance\\games\\roms\\Pok\u00c3\u00a9mon Emerald.","label":1,"model":"bloomz","source":"wikihow","id":143}
{"text":"The following are some tips about how to have beautiful handwriting:  Trace an elegant font.  Write in loopy cursives.   Copy sections from your favourite book.    Practice writing letters of different thicknesses.     Emphasise elongated letters.      Learn calligraphy,    which is the art of making decorative handwriting.        Write with your shoulders and back.       Hold your arm out infront of you and practise writing big letters in the sky.         Practice your movements with apeninyournotebook.        Practice strokesandfigures.        Slowdownandwritemethodically.        Rememberto-stretchandmovearound.        Practisethatday-to-dayhandwritingeveryday.        Positionthepaperonthecoveredtabletopsurface.        Sittightandavoidslouchingbutnotstiffly.        Findingthepencorelighterthatworksforyou.        Gettinganewnotebook.        Findingalargeflatsurfaceformakingnotesonthego.        Boughtahandwritingcoursebook.","label":1,"model":"bloomz","source":"wikihow","id":144}
{"text":"If you've been issued with a traffic citation (also known as a \"ticket\"), there are several options available to you:  You may pay it immediately; however, this will result in points being added against your driver\u2019s license.  Alternatively, you could fight it; either on your own behalf through informal means such as requesting a jury trial before a magistrate court commissioner, or hiring an attorney who specializes in traffic law.   Finally, you might choose not to do anything about it until it's time to renew your registration.    If you'd like more detailed instructions regarding how...   The first step is to check your ticket carefully for any errors.  In most cases, you'll be able to correct these yourself without having to hire an attorney.  However, some mistakes cannot be fixed easily - especially those involving dates and\/or times.  For example,...    Once you've checked your ticket thoroughly, make sure you know exactly which parts of the statute(s) apply to your case.  This includes knowing where each violation occurred within the text of the statutes themselves.  It also involves understanding how they relate to one another.  Some violations occur simultaneously under different codes.  Others only exist when certain other violations take place....","label":1,"model":"bloomz","source":"wikihow","id":145}
{"text":"Thread a necchi sewing machine by following these easy steps.  Raise the presser foot.   Turn the balance knob until it's pointing towards yourself.    Find the spool pins.     Place one or two full spins of thread onto the spooler pins, with the thread facing away from the presser foot in counterclockwise directions.    Test if all the spoolers turn properly.    Note where the thread guides go.    Pull them down through the upper thread pressure dial.    Pull them under the check spring thread guider.    Pass the thread through the top loop atop the presser foot.    Pull the excess thread back through the bottom of the pressor foot.    Open the shuttlecock race cover beneath the presser foot, close to the presser foot itself.    Latch off the bobbin case and remove it from the machine.    Take hold of a new bobbin filled with thread.    Insert this bobbin into its own bobbin case.    Guide the excess thread through...   The next step will be to open the shuttlecock cover underneath the presser foot....","label":1,"model":"bloomz","source":"wikihow","id":146}
{"text":"If you've been trying unsuccessfully to lose some extra pounds on your own but feel like you'd be more successful in an environment where there are other people who share similar goals -- then hosting a weight-loss competition might just do the trick! This wikiHow teaches you everything you'll need to know about organizing such a competition. \n \n How to host a weight loss competition \n \n If this is not possible due to privacy concerns, ask each participant's permission to track them using only their first name instead of their full names.. You can also use nicknames that don't reveal any personal information. . Ask everyone participating whether they would prefer working alone or within groups. The latter option may make losing weight easier because it's less isolating than doing so by yourself. However, keep in mind that team members won't necessarily support each other equally throughout the entire process; therefore, consider setting up smaller sub-groups inside larger ones.  \n \n. Consider offering prizes ranging from gift cards to gym memberships depending upon the amount of money available. Make sure these rewards aren't too expensive though since most people tend to spend whatever they're awarded right away rather than saving it towards future purchases. For example,...","label":1,"model":"bloomz","source":"wikihow","id":147}
{"text":"I purchased these items for less than $30. \n \n 1) A headphone jack \n \n \n \n 2) Two pieces of speaker wire (about 6 feet each) \n \n \n \n 3) An fm transmitter \n \n \n \n 4) An external speaker \n \n \n \n 5) A pair of scissors \n \n \n \n 6) Wire strippers \/ crimpers \n \n \n \n 7) Plastic electrical tape \n \n \n \n 8) Soldering iron \n \n \n \n 9) Heat shrink tubing \n \n \n \n 10) Glue gun \n \n \n \n 11) Power cord for fm transmitter \n \n \n \n 12) Boom Box \n \n \n \n 13) Outside Speakers \n \n \n \n 14) Lighted outdoor christmas tree \n \n \n \n 15) Remote Controls \n \n \n \n 16) Battery pack for remote controls \n \n \n \n 17) Cables To connect everything together you'll need some cables.  \n \n You'll also want to purchase heat-shrink tubing so you don't fry anything when soldering.   \n\nYou'll probably be able to find most of what is needed locally - if not then Amazon should get them all shipped quickly. \n \n You may already own many of these things...","label":1,"model":"bloomz","source":"wikihow","id":148}
{"text":"This guide assumes you've successfully downloaded and extracted the latest version of uberSVN onto your server.  If you'd like more information about installing subversion please visit our wiki page at http:\/\/wiki.subversion.apache.org\/index.php\/Installing_Subversion_on_Linux .\nBefore we begin with the actual installation process it would help us greatly if you could answer some questions regarding your system specifications so we know what we're dealing with:  What operating system do you plan on installing? How much memory does your machine have available?  Do you prefer 32-bit or 64-bit installations?\nIf everything looks OK then it's time to get started!   Let's check your hardware compatibility before proceeding: \n \n # Check your CPU type against the list below: \ncat \/proc\/cpuinfo | grep \"model name\" > \/dev\/null 2>&1 && echo \"Core i7\/i5\/i3 processor\" || echo \"Other processors\"; \n\n# Check how many cores your processor supports. \ncat \/proc\/cpuinfo\" | grep \"cores\" >>\/dev\/null 2>&1;\n\n# Check how much physical ram your machine has available.  \ncat \"\/proc\/meminfo\" | grep \"Mem\" >>\/dev\/null 2>\/dev\/null;","label":1,"model":"bloomz","source":"wikihow","id":149}
{"text":"Breaking up can feel like an impossible task when you love someone so much.  But sometimes it's necessary for both people involved.   If you find yourself struggling between staying together and breaking up,  here are 10 signs that suggest  this might be what you should do:   1) You can't stop thinking about him\/her 24\/7 - This means that he\/she takes over every thought in your mind whenever you're at home,... 2) You're unable to focus on anything else... 3)... 4) ... 5)  6)   7)    8)     9)      10) 11) 12) 13) 14) 15) 16) 17) 18) 19) 20) 21) 22) 23) 24) 25) 26) 27) 28) 29) 30) 31) 32) 33) 34) 35) 36) 37) 38) 39) 40) 41) 42) 43) 44) 45) 46) 47) 48) 49) 50) 51) 52) 53) 54) 55) 56) 57) 58) 59) 60) 61) 62) 63) 64) 65) 66) 67) 68) 69) 70) 71) 72) 73) 74) 75) 76) 77) 78) 79) 80) 81) 82) 83) 84) 85) 86) 87) 88) 89) 90) 91) 92) 93) 94) 95) 96) 97) 98) 99) 100) 101) 102) 103) 104) 105) 106) 107) 108) 109) 110) 111) 112) 113) 114) 115) 116) 117) 118) 119) 120) 121) 122) 123) 124) 125) 126) 127) 128) 129) 130) 131) 132) 133) 134) 135) 136) 137) 138) 139) 140) 141) 142) 143) 144) 145) 146) 147) 148) 149) 150) 151) 152) 153) 154) 155) 156) 157) 158) 159) 160) 161) 162) 163) 164) 165) 166) 167) 168) 169) 170) 171) 172) 173) 174) 175) 176) 177) 178) 179) 180) 181) 182) 183) 184) 185) 186) 187) 188) 189) 190) 191) 192) 193) 194) 195) 196) 197) 198) 199) 200) 201) 202) 203) 204) 205) 206) 207) 208) 209) 210) 211) 212) 213) 214) 215) 216) 217) 218) 219) 220) 221) 222) 223) 224) 225) 226) 227) 228) 229) 230) 231) 232) 233) 234) 235) 236) 237) 238) 239) 240) 241) 242) 243) 244) 245) 246) 247) 248) 249) 250) 251) 252) 253) 254) 255) 256) 257) 258) 259) 260) 261) 262) 263) 264) 265) 266) 267) 268) 269) 270) 271) 272) 273) 274) 275) 276) 277) 278) 279) 280) 281) 282) 283) 284) 285) 286) 287) 288) 289) 290) 291) 292) 293) 294) 295) 296) 297) 298) 299) 300) 301) 302) 303) 304) 305) 306) 307) 308) 309) 310) 311) 312) 313) 314) 315) 316) 317) 318) 319) 320) 321) 322) 323) 324) 325) 326) 327) 328) 329) 330) 331) 332) 333) 334) 335) 336) 337) 338) 339) 340) 341) 342) 343) 344) 345) 346) 347) 348) 349) 350) 351) 352) 353) 354) 355) 356) 357) 358) 359) 360) 361) 362) 363) 364) 365) 366) 367) 368) 369) 370) 371) 372) 373) 374) 375) 376) 377) 378) 379) 380) 381) 382) 383) 384) 385) 386) 387) 388) 389) 390) 391) 392) 393) 394) 395) 396) 397) 398) 399) 400) 401) 402) 403) 404) 405) 406) 407) 408) 409) 410) 411) 412) 413) 414) 415) 416) 417) 418) 419) 420) 421) 422) 423) 424) 425) 426) 427) 428) 429) 430 Breaking up isn't easy but there may come times where you realize that you cannot stay with your significant other any longer....","label":1,"model":"bloomz","source":"wikihow","id":150}
{"text":"Choosing an appropriate mortgage lender is one of the most important decisions you'll make when buying or refinancing a home.  The right choice will save you money over the life of your loan.   This section provides information on finding qualified mortgage professionals as well as tips on choosing among them.    Putting Together Your Financial Portfolio  Before shopping for a new mortgage, it's best if you have assembled some basic documents so they may be reviewed with potential lenders.  These include things like:   A copy of your credit report (from Equifax Canada Inc.);  Proofs of income such as: pay stubs, bank statements showing deposits into savings accounts, investment accounts;  Bank statements reflecting balances owed on existing loans including car payments, student loans etc. (if any);  Tax returns for past two years;  If self-employed: business tax returns for last three years along... Withdrawal Authorization Form - this form authorizes us to obtain your credit reports through TransUnion Canada Limited..  You should also provide proof of employment status i.e. letterhead\/company ID card indicating position held within organization..    In addition to these items, it might help to bring other personal identification papers like birth certificates, marriage licenses, social insurance cards, driver's licences, passports, military discharge records, etc..\nRefreshing Your Memory Of Loan Terminology\n\nBefore starting out searching for a suitable mortgage provider, take some time to review key concepts related to mortgages.  Familiarizing yourself with common phrases used throughout the industry will allow you to better communicate with different types of people involved in the process....","label":1,"model":"bloomz","source":"wikihow","id":151}
{"text":"If you're being harassed at work because of who you are (e.g., race\/ethnicity), what you've done before (e.g., criminal history) and\/or how you look (e.g., gender identity), see this guide instead.  This wikiHow teaches you ways to defend yourself when your boss behaves inappropriately towards you on matters related solely to your performance at work.   If you feel like your... See more \u00bb\n\nThis wikiHow teaches you ways to:  Understand your rights Know where to report sexual harassment Learn about other forms of workplace discrimination Recognize bullying Look out for signs of emotional abuse Deal with verbal abuse Handle physical violence Find support Take care of yourself Consider taking legal action File a complaint Report your experience to HR Investigate your claim Follow up with HR Review your options Prepare for cross-examination Speak to witnesses Be honest Answer questions truthfully Provide evidence Document everything Don't blame others Accept responsibility Give examples Explain why it's important to change things Listen actively Acknowledge your feelings","label":1,"model":"bloomz","source":"wikihow","id":152}
{"text":"Interviewing someone face-to-face gives journalists access to valuable insights which cannot always be gained through phone interviews.  The following tips should help you conduct successful... [Read More]  How to Write a Press Release for Journalists \n \n \n \n How to Start Your Own Blog for Journalists \n \n \n... A journalist\u2019s job description includes gathering news stories by conducting research and writing articles based upon those findings. In order to gather these stories,... Read Article \u00bb ...more . This article was co-authored by our trained team of editors and researchers who validated it for accuracy and comprehensiveness. wikiHow marks an article as reader-approved once it receives enough positive feedback. Learn more .\n\n\n\n\n\n\n\n\n\nTo start off, find some good sources (people) whom you'd like to speak to regarding your story idea(s). Once you've found several potential subjects, contact each individual via email explaining briefly why you're interested in talking to him\/her\/them.... Find out where you'll be able to reach your source during business hours.","label":1,"model":"bloomz","source":"wikihow","id":153}
{"text":"The following is intended only as general information regarding becoming an oncologist.  It does not constitute legal advice nor should it substitute for professional counsel from qualified attorneys licensed in your jurisdiction who can advise you based on specific facts relating to your situation.   You may wish to consult such professionals before taking any action related to this topic.   ... This section needs expansion. You can help by adding to it. (November 2015)\nPlease help improve this guide by adding citations to reliable sources. Unsourced material may be challenged and removed.(October 2016) (Learn how and when to remove this template message)This article's lead section may need to be rewritten entirely.... If you're interested in pursuing medicine but don't know what field interests you most,... Learn More About Getting into Med School\n\nIf you've decided you'd like to become an oncologist,  you'll want to start planning now!   Start early!  You'll probably spend several years preparing yourself for med school if you plan to go straight after high school;  many people take time off between their bachelor's degree and entering med school.  \n\nYou might consider getting involved in some extracurricular activities during college which relate to health sciences and\/or medicine..  For example, you could volunteer at a local hospital, medical clinic, nursing home, etc..","label":1,"model":"bloomz","source":"wikihow","id":154}
{"text":"Buying a house requires careful planning.  This section will help guide you through each step along the way.   You may also find it helpful to read How to Sell Your Home before proceeding with this information.    If you've never bought or sold property before, it's important to understand some basic concepts related to buying a house so that you know exactly where you stand throughout the entire transaction.  The following sections provide detailed explanations of these concepts:  Mortgages - A mortgage is a long-term debt instrument secured by collateral such as land and\/or buildings used primarily for residential purposes.  In other words,...   Credit Scores - Lenders use credit scores to assess whether they should extend loans to borrowers based upon their ability to repay them over time.  There are several different types of credit scoring models available but most fall into one...    Loan-to-Value Ratio - Most people who purchase homes take out mortgages from banks which require monthly payments be made until all principal has been repaid plus interest charges accrued during the term of the loan.  To calculate the total cost associated with taking out a mortgage, lenders divide the outstanding balance owed on the loan by its original value....","label":1,"model":"bloomz","source":"wikihow","id":155}
{"text":"This guide will teach you how to build an autonomous mobile robot using only common household items.  The finished product should look like this:  This project requires some soldering skills as well as basic knowledge of electronics.   If this sounds too complicated,... ...more... \nThe parts list below includes all necessary materials needed to complete this project.\n\n1 x AA Battery Pack\n\n2 x Servo Motors \n\n3 x Red Jumper Wires \n\n3 x Black Jumper Wires \n\n4 x White Jumper Wires \n\n1 x Sensor Board \n\n1 x Push Button Switch \n\n1 x Resistor \n\n1 x LED Light Bulb \n\n1 x Arduino Uno R3 Micro Controller \n\n1 x Power Supply Cable \n\n1 x USB Cable \n\n1 x Kill Switches \n\n1 x Wire Cutters \n\n1 x Tape Measure \n\n1 x Glue Gun \n\n1 x Scissors \n\n1 x Pencil \n\n1 x Paper Clip \n\n1 x Boxcutter \n\n1 x Rubber Band \n\n1 x Small Hammer","label":1,"model":"bloomz","source":"wikihow","id":156}
{"text":"Making an area rug is easy once you've mastered making one yourself.  You can choose any color combination you'd like; however, it may take some time before you're able to find exactly what you want in stores.   If possible, try finding old carpets at thrift shops as they are often cheaper than new ones.    Choose the carpet from which you would like to make your rug.     The best way to do this is to measure where you'll put the rug first so you know how large...   How To Make A Carpet Into An Area Rug  - YouTube    Measure the space where you plan to hang the rug using measuring tape,    then add about 3 inches (7.6\u00a0cm).     This extra length allows room for fringe along both ends of the rug when finished.    For example, let's say we need our rug to cover a 4 foot x 8 foot (1.2 m \u00d7 2.4 m)  wall-to-wall living room.  We'd therefore purchase a 6 foot x 10 foot (180 cm \u00d7 330 cm), 1-inch thick (2.5-cm-thick) carpet.    It should also have a backing made of rubberized latex or polyester fabric.    These materials allow water vapor produced during normal household activities such as cooking,...","label":1,"model":"bloomz","source":"wikihow","id":157}
{"text":"The solution is based on the concept of neutral operations.  The idea behind neutral operation is that if you have two things equaling each other, they are also equal to their product; therefore, if you can solve any equation with three variables, you should be able to solve equations with four variables.   This method works best when all your numbers are whole numbers.    To begin solving our problem we need to set up some initial conditions so let's start off with:  We know that:   Now lets try to isolate b:  Plugging these values into the formula above gives us:  So now we have:  Let's see what happens if we plug those values:  And finally...we get this:  You might notice something strange about my answer here....I got a negative number! Well that's just how I like them!  But don't worry too much about that right now..let's keep going..  Lets take another look at our equation:  What do we want? A+B=C+D  Remember C=D-A-B?  Ok great but what's D?","label":1,"model":"bloomz","source":"wikihow","id":158}
{"text":"Religion is defined by Webster\u2019s dictionary as:  \u201cBelief in supernatural beings; worship offered them\u201d.  This definition does not take into account how these beliefs were formed nor what they mean for their followers.   In order to understand this topic better we need to look at its history.    History shows us that there was no single moment when man began believing in gods but rather that belief developed gradually throughout time with each culture developing its own unique way of expressing itself.   There seems little doubt however that the earliest known form of organized religion dates back some 10 000 years ago during the Neolithic period (the stone age) where hunter-gatherers living along the banks... Continue reading \u2192\n, which means \u201cthe knowledge of God\").   It is believed that he lived around 600 BC and founded his teachings based upon the Vedas - sacred texts written down thousands of years earlier.   He taught that every person could achieve liberation from suffering by following certain spiritual practices such as meditation and yoga.   His teaching spread quickly across India and influenced other cultures including Chinese Taoist philosophy and Japanese Zen Buddhism.   Today most Hindus consider him to be a god-like figure who represents love, compassion, forgiveness and wisdom.   Hindu scriptures state that after Gautama Buddha attained enlightenment, he became aware of the existence of another teacher named \u0100gama Metteyya whose teachings would eventually become Jainism.   According to legend, Gautama Buddha met \u0100gama Metteyyia before becoming fully awakened himself.   However, while both teachers preached similar philosophies, Gautama Buddha rejected \u0100gama Mettey's teachings because he felt that they did not go far enough.   For example, \u0100gama Mettey' s teachings stated that even animals should be treated kindly whereas Gautama Buddha said that only humans needed to be loved unconditionally.   Although Buddhism originated in India,...","label":1,"model":"bloomz","source":"wikihow","id":159}
{"text":"Greek yogurt can be made at home using only 3 ingredients - whole milk, active dry yeast, and plain nonfat yogurt.  This recipe will yield about 5 cups (.5 liter), which makes enough for one person per serving.   The process takes several steps but it's not difficult once you get started.    You may also want to try making sour cream by following this method instead!   For best results use pasteurized cow's milk because its fat content helps create thicker strands during fermentation.     If you're allergic to dairy products then consider trying coconut...    Instructions are based on an electric stove; adjust accordingly if cooking over gas flame.\n     Heat the milk slowly until it begins to simmer gently.  Do NOT boil!  Allowing the milk to reach 82\u00b0C\/180\u00b0F ensures that harmful bacteria have been killed while preserving beneficial enzymes such as lactase.  Milk should never exceed 90\u00b0 F\/205C since bacterial growth increases rapidly above these temperatures.  It does take some practice to determine how long it takes for the milk to reach 180\u00b0F so keep checking regularly throughout the entire boiling period.  Once you've mastered the technique you'll know exactly what temperature the milk hits every time without having to constantly monitor it.  When the milk starts bubbling vigorously turn down the heat immediately before it boils over.  Continue stirring frequently....","label":1,"model":"bloomz","source":"wikihow","id":160}
{"text":"If you're buying jeans online then read How To Buy Jeans Online instead.  This guide assumes you've already decided which style fits best based upon our previous guides.   You should also know how much money you'd like to spend beforehand as it helps narrow things down considerably.    The following tips apply whether you're buying from department stores such as Marks & Spencer's, Debenhams etc...or high street shops such as Topshop, River Island, New Look etc....    There are many different types of jean available but we won't go through all those now - we'll cover some key points below.     We recommend going up at least 1 full size above normal UK sizing because American brands tend to make pants smaller than European ones. (see notes about Levi Strauss).     For example, if normally wear a 10-12 in Europe, try 12-14 in America. Also note that styles vary between countries too!   Don't forget to consider colour options either!  Black skinny jeans might work great for a night out clubbing while blue skinny jeans would probably suit more formal occasions better.","label":1,"model":"bloomz","source":"wikihow","id":161}
{"text":"Laziness isn't always about doing absolutely nothing.  It means taking things slowly - even if you're not actually moving forward.   Here we give some tips on how to become more lazy around the home...and still get everything done!   Keep reading....  Don't forget to use our handy-dandy search bar above!  It's got loads of great articles on being lazy!\n1) Sit down when cleaning the dishes\n\n2) Wash the dishes while standing up (optional)\n\n3) Take advantage of any opportunity to avoid work.\n\n4) Make laziness seem like a virtue. \n\n5) Get into character.  \n\n6) Have a plan before tackling something big.  \n\n7) Ask people to help you finish jobs quickly.    8) Do one thing at once but don't complete anything.     9) Avoid using stairs whenever possible.        10) Never go outside unless it's necessary.      11) Sleep until noon every day.         12) Eat only healthy foods which require no preparation.       13) Go shopping less often.             14) Leave dirty clothes everywhere.           15)","label":1,"model":"bloomz","source":"wikihow","id":162}
{"text":"This section will teach how to make an adorable diaper cake for your friend or family member who just had their child.  This project can be made by anyone as long as they have basic sewing skills.   The finished product should look something similar to:  You may want to consider making this gift if you:   To begin creating the base of our diaper cake we need to line the bottom of a 9-inch round cake pan with three layers of folded diapers.    We then continue layering diapers around the outside edge of the pan until all four edges are covered completely.     For example, let's say there were 10 diapers used total; 3 at the bottom layer, 4 at the middle layer, 2 at... Once you've completed covering the entirety of the pan's outer rim you'll notice that some areas still remain uncovered inside the pan itself.  In order to cover these remaining spaces simply fold over any extra diapers that might exist within those openings before placing them down onto the next layer below....","label":1,"model":"bloomz","source":"wikihow","id":163}
{"text":"The Suspect Game is played with three players - two suspects (or more if desired) and an investigator\/police officer.  The goal of each player is to avoid being arrested while trying to arrest his\/her opponent\/s.   This version of \"The Suspect Game\" was created by John Hancock High School students from Boston Massachusetts as part of a school project.    You will need several different locations within which to play the game including a jail cell where all captured suspects are locked up until released during the trial phase of the game.   Each location must also include a bathroom so that the suspects may fake illnesses when necessary.   It would help... [Read More]  Make sure there isn't anything else around that could cause injury such as sharp objects etc..    Once everyone has arrived start playing!   When choosing who plays what role consider age gender and personality traits.   For example someone who is very talkative might not fit well into the role of a criminal because he\/she won't be able to hide her\/his identity easily enough.   Also think about whether anyone knows any martial arts skills or other special abilities that could prove useful throughout the course of the game.   Keep track of how many times each person escapes before finally getting caught.   After fifteen minutes return to the courtroom and announce the verdict!  Whoever escaped most often gets acquitted; however whoever had been caught escaping the most times receives life sentences without parole.   Everyone else goes free!","label":1,"model":"bloomz","source":"wikihow","id":164}
{"text":"Gout occurs most commonly in men over 40 years old.  It causes sudden swelling, redness, warmth, tenderness, stiffness, and severe pain around one or several joints.   The first step towards getting rid of these painful... Read More \u00bb\n.Gout is a common form of arthritis characterized by extremely painful episodes involving inflamed joints due to crystals formed inside them.  These crystals cause irritation and inflammation leading to extreme pain.  In fact,... Read More ... .The best way to treat gouty arthritis is through prevention - but what do you need to know? Here we look into all aspects of preventing gout....Read More... .Gout is a type of inflammatory arthritis associated with elevated levels of uric acid in the body's fluids such as urine and serum.  This condition affects mainly middle-aged males and is triggered by certain foods like seafood, alcohol, beer, wine, chocolate, nuts, beans, lentils, spinach, broccoli, cauliflower, mushrooms, yeast extract, organ meats, liver, kidney, brain, heart, spleen, pancreas, etc. .","label":1,"model":"bloomz","source":"wikihow","id":165}
{"text":"If you're diagnosed as having Type 2 Diabetes or pre-diabetes (high risk), then it's important to regularly check your blood sugar levels.  This is done using an electronic device called a glucometer which measures the amount of glucose present in your bloodstream.   The results from these tests help determine what treatment options may work best for you.    There are many companies who manufacture this equipment but there isn't one that's considered better than another because all devices use similar technology.  However, some people prefer specific models over others based upon their own personal experience.  If you've been prescribed... Continue reading \u2192\n, you'll need to get it filled before going out shopping for a new glucometer.  Your physician should give you information about where he would like you to obtain supplies such as lancets, test strips, alcohol swabs etc..    You might also want to ask him\/her why he\/she chose a particular brand\/model so that when you go searching for a glucometer, you know exactly what features you'd like to see included in yours.     Once you find a model that fits into your budget, make sure that you read reviews written by other users regarding its accuracy, ease-of-use, durability, reliability, and overall satisfaction level.  These reviews could save you time and money down the road since most manufacturers don't disclose product defects until after they've sold thousands of units.  In addition,...","label":1,"model":"bloomz","source":"wikihow","id":166}
{"text":"This page has been accessed 1,082 times. \n \n This page was last modified on 16 September 2020, at 17:55.  \n \n If you need further information about this topic, please feel free to contact us. \n \n(Please include the date of access in any citation.) \n \n You are currently reading \"How to write about disability\", which may also be found on these pages:  http:\/\/wikihow.com\/Write-About-Disability  \n  \n How To... The way we talk about disabilities matters\u2014to those who live them every single day! Here\u2019s how to avoid making mistakes when writing about disability so everyone feels included\u2014and respected. Read more \u00bb ... People living with disabilities experience barriers all around them: physical ones like stairs and inaccessible public transportation, but emotional ones too. They often face discrimination based on misconceptions they can\u2019t control themselves; it\u2019s up to society to change its ways instead. ... When you\u2019re talking about a specific type of disability, always capitalize it: \u201cDeaf\u201d refers to being hard-of-hearing, whereas \u201cdeafness\u201d means \u201clack of hearing\u201d; similarly, \u201cdyslexia\u201d describes difficulty learning to read, while \u201ca dyslexic child\u201d would mean one born without the ability to learn to read....","label":1,"model":"bloomz","source":"wikihow","id":167}
{"text":"Autism Spectrum Disorder (ASD) affects how people perceive information from the world around them; it also impacts behavior.  People with ASD have difficulty communicating and interacting socially; however, each individual has unique needs based upon his\/her own personal strengths and challenges.   The following tips will provide you with some basic strategies to support someone who has been diagnosed with ASD:  If you're supporting... Continue reading at https:\/\/www.wikihow.com\/Support-An-Autistic-Person .)\nIf you'd like more detailed advice tailored specifically towards helping your family member\/loved one cope with everyday life issues related to living with Asperger's Syndrome and\/or High Functioning Autism:   See our list of resources here .\nSee our guide to coping with sensory overload here .  See our guide to dealing with anxiety here .   See our guide to managing tantrums here .   See our guide on teaching kids about emotions here .   See our guide for parents raising teens with Aspergers syndrome here .   See our guide aimed at improving communication between adults with Asperger\u2019s syndrome and those close to them here .   See our guide focused on increasing independence among individuals with Asperger\u2019s disorder here .   See our guide dedicated to providing practical ways to deal with bullying directed toward people with Asperger\u2019s here .   See our guide devoted exclusively to women with Asperger\u2019s disorders here .   See our guide designed especially for men with Asperger\u2019s syndromes here .   See our guide intended solely for girls with Asperger\u2019s conditions here .   See our guide targeted directly at boys with Asperger\u2019s diagnoses here .   See our guide focusing primarily on teenagers with Asperger\u2019s diagnosis here .   See our guide geared entirely toward young adults with Asperger\u2019s: How To Deal With Life Challenges Here","label":1,"model":"bloomz","source":"wikihow","id":168}
{"text":"Blu-ray players are devices which allow users to watch movies stored on discs as well as streamed from online sources such as Netflix.  They can also be connected directly to computers via USB ports so they act like external hard drives.   The first generation of these products was released by Sony Corporation under its PlayStation brand name but has since been rebranded as PS3s.    There were two versions available at launch; the regular version (PS3) and the slim model (PS3 Slim).   Both models feature backward compatibility with all previous generations of PlayStations including original PS1 games.   However, only the PS3 Slim supports 3D gaming while both support digital video playback through their built-in DVD\/Blu-Ray disc drive.   This wikiHow will teach you how to set up any standard Blu-ray player regardless of what manufacturer made it.     Unpack your Blu-ray player. Before setting up your new Blu-ray player there...   You may need to adjust some settings before watching videos depending upon where you purchased them....","label":1,"model":"bloomz","source":"wikihow","id":169}
{"text":"You will need:  String Glue Pinch-n-squeeze bottle Scissors Wax paper Small bowl Large bowl A small amount of water Gently pull apart both ends of the ribbon until you have created an open-ended ring.  You can use any type of ribbon for this project; however, thicker ribbons tend to work better than thin ones because they hold their shape more easily when bent or twisted.   If you'd like to make multiple ornaments at once, cut several lengths of ribbon instead of just one long length. Cutting the ribbon before starting helps prevent tangling later on during the process. Place some hot glue onto the back side of the first 7.5cm (3\") piece of string that you've cut from the ribbon roll. Then carefully dip the entire length of the string into the glue using your finger as a guide.  Once it's covered evenly all around its surface, lift up the string by grabbing either end... Continue reading \u2192\n\nDip another 3\" piece of string into the same batch of glue that's still wet on the previous strand. Remove the excess glue from the string's surface again with your pinched fingers.    Repeat these steps twice more - coating 2 additional strands of 3\" string in glue then laying them down next to each other on top of the 2\" x 4\" piece of wax paper.     The final result should look something similar to this:     This step may take about 10 minutes depending upon how quickly the glue dries.","label":1,"model":"bloomz","source":"wikihow","id":170}
{"text":"If you're trying to hide or conceal something that doesn't belong there but can't get rid of it completely because it's too large - like old furniture that's taking up space in your garage - building a fence might be just what you need.  Building a fence isn't difficult; however, you'll want to make sure whatever materials you use won't rot quickly when exposed to moisture.   This method works best for small piles of trash such as broken appliances, scrap metal, etc.. If this sounds like exactly what you've been looking for, read on:  Clear a fence line around the area you'd like to cover by removing any grass and\/or weeds growing along its edges.    Measure the perimeter of the heap and calculate the length so... You may also consider adding some decorative touches here and there to help disguise what's inside even more effectively!   For example,... Once all necessary supplies arrive, start laying them down according to their intended locations.... The last thing left before finishing off would be installing gates!  Make sure they open easily enough without hitting anything else nearby while still being securely closed once they're shut.  Don't forget to add locks wherever applicable!\nOnce everything has dried thoroughly after construction, enjoy your newly built fence!","label":1,"model":"bloomz","source":"wikihow","id":171}
{"text":"Scrimshaw is the art of decorating bones using carving tools.  The word scrim means \u201cbone\u201d in English.   This technique was used extensively during the 19th century among whalers who would use their knives to create images that represented important events such as birthdays, weddings, deaths, etc.    It can be done on any type of bone but most people prefer to do this on whale ribs because they are large enough to hold many different designs.     You will need several items before beginning including:  A sharp knife (preferably one made out of carbon steel); Beeswax; Ink pads; Acetone Nail Polish Remover; Cotton swabs; Scissors; Paper tape; Glue stick; Small paint brushes; Ivory powder; Bone file; Buffing stone; Sanding sponge; Wooden block.     If you\u2019re not sure where to find these materials then check at local craft shops like Michael\u2019s Craft Store or Hobby Lobby.  There should also...","label":1,"model":"bloomz","source":"wikihow","id":172}
{"text":"A butterfly is a beautiful creature whose life cycle begins by hatching from its egg.  A butterfly's body consists of:  The front part of their bodies are called thorax which has three segments covered with scales.   They have four pairs of legs attached to this segment.    Their antennae look similar to small horns.     There are many different species of butterflies found all over the world ranging in size from 1 inch across to 4 inches wide!     You may choose any kind of butterfly you'd like but make sure it fits well within your budget!  If you're planning to go trick-or-treating then you'll need something more durable than if it's just going...   For example, monarchidae family includes monarchids such as: Monarchidae - Wikipedia    Papilionidae include papilions like: Papilionidae - Wikipedia    Nymphalidae include: Nymphalidae - Wikipedia    Hesperiidae includes: Hesperiidae - Wikipedia    Pieridae include: Pieridae - Wikipedia    Lycaenidae include: Lycaenidae - Wikipedia    Painted Lady butterfly - Wikipedia    Tiger Swallowtail butterfly - Wikipedia    Check local libraries and\/or online resources to find pictures of various kinds of butterflies so you know exactly how they look before purchasing materials....","label":1,"model":"bloomz","source":"wikihow","id":173}
{"text":"Laundry can be expensive at college.  If you're living off-campus it may cost more than $25 per week for laundering.   However, there are ways around this expense by doing some things yourself.    This guide shows you how to wash your own clothing using only coin operated machines found throughout campus.  It also includes tips on drying your laundry once it's washed.     You should always double check washing directions first but most shirts, pants, underwear etc...   Washing:    Drying::     Make sure everything fits into the dryer!     Don't forget to fold!  Folded Items:  For those who prefer to use their hands instead of machines,... \nIf you'd like help finding these locations please contact us via email at wiki@wikihow.com. Please include your name, location, phone number and\/or e-mail address along with what type of assistance you need. We look forward to hearing from you soon! Thank you very much! - The Wikihow Team \n \n Contact Us","label":1,"model":"bloomz","source":"wikihow","id":174}
{"text":"The first thing you'll notice when opening up the app will be how large it appears compared to its mobile counterpart.  The size difference may seem daunting but don't worry; once you've gotten used to using the app's interface you're going to feel comfortable navigating through all aspects of the program.   Once again there isn't much new about the layout itself so let's take some time now to go over exactly where everything is located within the application.    On the far-left-hand corner of the screen (or tab) we have our home feed from which we'll select games based upon their status - either Live or Postponed.   Below...   Click here for instructions on downloading the free version of the MLB GameDay app onto your smartphone device. You must download the paid version of the app before being able to access any features such as:  \u2022 Pitch by Pitcher Markup \u2022 Player Stats \u2022 Team Lineups \u2022 Infield Box Score \u2022 Quick View \u2022 Alternate Views \u2022 Stat Tracker \u2022 Scores & Standings \u2022 News Feeds \u2022 Social Media Integration \u2022 Video Highlights \u2022 Fantasy Baseball If you'd like more detailed instructions regarding installing the app please visit the following link: http:\/\/www.wikihow.com\/Download-the-MLB-Game-Day-Application-on-your-iPhone","label":1,"model":"bloomz","source":"wikihow","id":175}
{"text":"This recipe serves 4 people.  You will need the following ingredients:-  Prep time: 20 mins Cook time : 30 min Total Time: 50 Min.   Ingredients For The Pastry:   Ingredients For The Filling:   Preparation:   ... Finished!     ............................................................................................ .................. How To Make A Cauliflower And Broccoli Tart - Recipe Instructions . This recipe serves four people.    You will need the following ingredients:-    Prep time: 20minsCook time:30minTotalTime:50Min.        IngredientsForThePastry:        IngredientsForTheFilling:        Preparation:-      *You can use any type of vegetable you like but try not to include too many as they may make the filling runny.     *If you don't have an electric mixer then knead by hand instead....         .....................................................................................................                 ..               ...              ...             ....             .....          ...           ...         ...      ...  ...                  ...                    ...                     ...                      ...                        ...                       ...                         ...                          ...                           ...                            ...                             ...                              ...                               ...                                ...","label":1,"model":"bloomz","source":"wikihow","id":176}
{"text":"1.  In a pressure cooker, place all ingredients except potatoes:  2.   Cover the pan tightly and bring to boil over high heat..  3.   Reduce heat to medium-low or simmer until cooked through - about 20-25 minutes.. 4.   Remove the bay leaves before serving.  5.   Serve hot! 6.    Enjoy!  7.    Note:   8.    Tips: 9.    Variations: 10.    Nutrition Facts per serving: 11.    References: 12.    Related Articles: 13.    See also:: 14.    External Links : This recipe was contributed by user \"ahsan_ali\" . If you have any questions regarding this dish please contact ahsan_ali directly using the form below. You can find more information at http:\/\/www.wikihow.com\/Contact-Us .  How do I make aloo keema? Allo keema is an authentic Pakistani dish made out of minced mutton mixed with spices like coriander powder, cumin seeds etc...","label":1,"model":"bloomz","source":"wikihow","id":177}
{"text":"Pacman frogs love warm temperatures between 80\u201390\u00b0F (26-32\u00b0C), which they prefer year-round.  They also require very high humidity\u2014at least 50%\u2014and do best when it's 70-80%.   The ideal pH level for their habitat ranges from 6-6.5.    Make sure there isn't any direct sunlight hitting... .\nIf you're using a plastic container instead of glass, use UV-absorbing paint to cover the inside walls before placing anything else into the enclosure.... ...or even better yet, buy a terrarium kit specifically designed for reptiles like pacmans!   If you'd rather build your own enclosure yourself, follow our guide here. Keep in mind you'll probably need to purchase additional supplies such as:  Once you've set everything up according to your design plan(s) (and purchased whatever other items may be needed), it's time to cleanse the entire space thoroughly!  This includes rinsing every surface down with tap water until no traces remain visible; then allow each item to air-dry completely prior to adding it to the enclosure.  Cleaning the enclosure once per week helps keep things sanitary while preventing mold growth and\/or bacterial infections.","label":1,"model":"bloomz","source":"wikihow","id":178}
{"text":"Buttermilk biscuits are an American classic that can be made with just 6 simple ingredients! These soft-textured biscuits pair perfectly with soups like chicken noodle soup, stews, chili, and even casseroles.  This recipe makes enough batter for about 12 large biscuits or 24 smaller ones.   You may also like:  For tips on how to store leftover buttermilk biscuits see How To Store Leftovers . Preheat your oven to 450\u00b0F\/220\u00b0C.Cut six tablespoons butter... Continue reading at https:\/\/www.wikihow.com\/Make-Buttermilk-Biscuits-Without-a-Mixer#:~:text=You%20can%27t%20make%20biscuits%20with,butter%20and%20flour,. Cutting up frozen butter will help keep the texture consistent when mixing together all the other ingredients later. . If you're not able to find grated Parmesan cheese use another type instead such as Romano Cheese Grater.","label":1,"model":"bloomz","source":"wikihow","id":179}
{"text":"This method uses an old wine bottle stopper, which can be found at thrift stores.  You could also use any other round-shaped object like a plastic lid instead.   This recipe makes enough wax to fill three bottles.   ... How To Make A Candle In The Sand  - YouTube \n \n \n \n If you're looking for something different in candles then try making these unique candles made by casting hot wax inside a bowl full of wet sand! These are great gifts too!  They look fantastic lit up and smell amazing when burning. Watch our video tutorial below to see exactly what you'll need to do.... \nMaking candles isn't just fun; it's easy once you've got the hang of it\u2014and they don't have to cost much money. Here we show you how to create beautifully scented candles right in your own home\u2014with only a few simple ingredients. All you need is:   1 cup paraffin wax 1\/2 teaspoon vegetable oil 1\/4 teaspoon beeswax 1\/8 teaspoon essential oils such as: lavender chamomile orange cinnamon vanilla Once everything has been added together, place the mixture over low-heat until the wax begins to melt completely. ...","label":1,"model":"bloomz","source":"wikihow","id":180}
{"text":"This method will allow you to use any wireless headset as wired.  This is useful when playing games online because there are no lag issues like using Bluetooth.   You can also use this technique to turn a wired headset into wireless.    Buy a DC-DC step-  up\/down module at RadioShack or Amazon.com.     Get some extra length of USB cord so you'll have enough to reach between your computer\/laptop and wherever you'd be wearing the headphones. (You may need to get longer than what was originally used.)     Find scissors and\/or a razor blade.     Locate a pair of wire strippers.     Locate a pair wire cutters.     Locate a soldering iron.     Locate a set of needle nose pliers.     Locate a flathead screwdriver.     Locate a Phillips-head screwdriver.     Locate rubber gloves.     Locate a pair tweezers.     Locate a multimeter.     Locate a voltmeter.     Locate a test light.     Locate a piece of cardboard.     Locate a pencil.     Locate a ruler.     Locate a sharpie.     Locate a cutting mat.     Locate a sewing machine thread.     Locate a...","label":1,"model":"bloomz","source":"wikihow","id":181}
{"text":"If someone cancels all or part of what you owe them, they may send you a document called a \"cancelation statement\".  This will tell you how much money has been cancelled.   If there isn't one with this information included in it,  ask the person who gave up their claim against you about the amount owed before canceling.    You should receive a copy of these documents within 30 days after receiving cancellation statements from creditors..  The Internal Revenue Code allows people to exclude certain types of cancellations from gross income..    For example,   most student loans do not qualify for...   Some states allow residents to file state returns instead of federal ones....  File electronically using TurboTax Online,... \nReport Cancelled Student Loan Debt As An Exception To Gross Income\nYou cannot include loan payments made by parents towards private school tuitions paid by students under age 24 unless those payments exceed amounts allowed for other purposes such as scholarships, grants, work-study funds, etc..\nReport Cancelled Farm Loans As A Deduction From Gross Income","label":1,"model":"bloomz","source":"wikihow","id":182}
{"text":"If you've ever had an issue with your snowboard's binding not holding tight enough when riding downhill, this tutorial may be for you! This guide shows how to tune-up and repair your own snowboard using common household tools.  If you'd like more detailed instructions than can be found online, check out our book How To Fix A Broken Board by clicking HERE . You don't need much equipment beyond what's listed below; however, if you have access to some specialty items such as a heat gun and\/or a belt sander you'll find these helpful too!  For those who are interested in learning how to make their own boards instead of fixing existing ones, click HERE .\nTools Needed:  Torch Base Cleaner Metal Scraper Diamond Stone Edge Tool Plastic Scraper Heat Gun Belt Sander File Saw Horse\/Workbench Work Gloves Acetone Iron Wire Cutters Knife Scissors Paper Towel Wooden Block Sandpaper Small Boxes Rubber Bands\/Paper Clips Other Items You'll Need:","label":1,"model":"bloomz","source":"wikihow","id":183}
{"text":"Facebook is one of the most popular social media platforms on Earth.  Millions use it every day for personal reasons as well as professional ones.   This wikiHow will teach you how to navigate through its features so that you'll be able to make full use of this powerful tool!   If you're already familiar with using Facebook's mobile version but want more tips or tricks, see How to Get Better at Using Facebook Mobile instead.   ...Read Full Article\n\nIf you'd like some help getting started with Facebook before reading any further, check out:  Setting Up an Account Creating Your Profile Adding Friends Posting Photos Sharing Links Commenting on Other People's Content Tagging People Changing Settings Privacy Controlling Who Can See What You Share Managing Pages Deleting Your Facebook Account    To open Facebook, tap the blue \"f\" logo (or swipe down anywhere on the home screen) then enter your email address\/phone number and password when prompted. The first thing you'll probably do after opening Facebook is scroll through your news feed; it's where all new activity takes place.  It's also possible to view other people's profiles by tapping their names below the \"What's Happening?\" heading near the middle of the screen.     On Android devices, you'll need to tap \u2630 in the upper-left side of the screen if you don't have access to the News Feed yet.      You'll usually get notifications about things happening within your network here....","label":1,"model":"bloomz","source":"wikihow","id":184}
{"text":"The following is excerpts from The Art of Living Without Fear by Dr Wayne Dyer.  This book was published on September 1, 2006.   It has been translated into more than 40 languages worldwide including Chinese.    You can find it at most local libraries and online booksellers like Amazon.com. If you're interested in purchasing this book please visit www.waynedyerbooks.com\/art-of-living-without-fear\/ . \nDr Wayne Dyer wrote his best-selling self-help book called \"The Power of Intention\" back in 1997 where he shared how we could change our lives through intention setting -  using positive affirmations and visualizations.  In 2005,  he released his second book entitled \"Life Doesn't Have To Be A...\"   He also authored several other popular titles like:  \"You Can Change Your Life!\" , \"Free Yourself From Emotional Baggage\" ,  \"What Is Your Mission In Life?\" , \"Leadership Secrets\" .  His latest release includes two separate volumes titled \"Awakened\" & \"Unlimited Potential\" .   All these books have sold millions copies around the world and continue to inspire people all over the globe today....","label":1,"model":"bloomz","source":"wikihow","id":185}
{"text":"The sport of rugby has been around since 1823.  It was originally played without any set rules; however, over time many have developed into what we know today.   In this guide you\u2019ll learn how to play rugby from start to finish!   If you\u2019re interested in learning about other sports that are similar but different enough so they\u2019re worth trying out too, check out:  How To Play Lacrosse - A Guide For Beginners How To Play Baseball... [Read More]\n\nRugby is an intense contact sport where both sides try to score points against their opponents using physical force (tackling) and\/or skillful manipulation of the ball.    There\u2019s no doubt it\u2019s one of those games that\u2019s best enjoyed when everyone involved knows exactly what they're doing!  This guide aims to teach beginners everything there is to know about getting started with rugby safely and effectively.     You may also want to:    Learn About Other Similar Sports That Are Worth Trying Out     Get Into Shape Before Starting     Know What Equipment You'll Need","label":1,"model":"bloomz","source":"wikihow","id":186}
{"text":"Spit-shining boots can be done by yourself if you're willing to put some effort into it.  This method will take about an hour per pair but it's worth every minute because you'll have beautifully spit-shined shoes! If you'd rather pay someone else to do this job then check out our listings below where we provide links to local shoe repair shops who offer services like these.   You may also want to try spray-on lacquer instead of traditional polish which makes the process even easier!  Make sure both pairs of socks match before proceeding!\nStep 1: Apply a coat of polish onto each boot. Start off by putting on one thick coat of polish evenly across the entire surface of the boot starting from the bottom edge towards the top part of the boot.  The thicker the polish applied initially, the better results you'll get when you start rubbing away the excess later on.    Step 2: Allow the first coat of polish to set. Once you've finished spreading the initial coating... Continue reading","label":1,"model":"bloomz","source":"wikihow","id":187}
{"text":"Bartholin glands produce lubricating fluids which help protect against infection.  If these glands become blocked by an obstruction\u2014such as mucus build-up\u2014the result can lead to swelling called a \"Bartholin cyst.\"  While some women experience only mild symptoms associated...with their Bartholin cyst(s), others report severe pain when they urinate.   In addition,...many women feel embarrassed because this condition often causes visible bulges near the vagina.... This article provides information regarding how to treat a Bartholin cyst at home before seeking medical attention; however, it's important to know what tests need to take place so proper care can begin immediately afterwards:   Your physician should perform a physical exam including pelvic exams to check for lumps around the vulva and labia majora.    A laboratory test might also be performed to rule out other conditions like infections or tumors.     Ultrasound imaging could be used to confirm whether you're dealing with a Bartholin cyst rather than another type of mass.","label":1,"model":"bloomz","source":"wikihow","id":188}
{"text":"This project will take about 1 hour. \n \n Gather your tools and supplies.  \n \n \n \n You can find these items at most craft stores as well as online retailers: \n \n \u2022 Acrylic paints (white & black)\n\u2022 Silver Sharpie Markers\n\u2022 Plastic canvas 12\" x 18\"\n\u2022 Scissors\n\u2022 Pencil\n\u2022 Tape Measurement ruler\n\u2022 Metal wire 1\/8\u201d diameter\n\u2022 Jump Ring 2\u201d long\n\u2022 Ball Chain 16\u201d length\n\u2022 Cookie Sheet Oven-safe glass pyrex measuring 8 \u00bd \u201cx 11\u201d  Place this item into an electric convection oven set between 350 - 375 degrees Fahrenheit. (You may also use a regular oven if you do not own one that has a convection setting.) \n \nDownload and print out the pattern sheets below by clicking here.    The first page contains instructions for making the castle knight pendant; while the second page includes patterns for other types of Bakugan Knights pendants such as:  \n  \n \u2022 Black Knight","label":1,"model":"bloomz","source":"wikihow","id":189}
{"text":"A film junket is an organized trip where journalists go along with actors or directors at a particular set during production.  The purpose behind this type of travel is so that reporters can write articles based upon information they gather throughout the day.   This includes press conferences held after wrap parties (parties thrown by producers);  screenings of scenes shot over the past few weeks;  cast\/crew photo opportunities;  Q&A; sessions between stars\/directors\/producers;  and other events related to the making of movies.    A film junket usually lasts anywhere from three to ten days depending on how many locations there are scheduled to visit within one country.     You should always dress professionally no matter who you're meeting with.   If it's cold outside wear layers but make sure they're wrinkle free since you'll have to remove...   - See more at: http:\/\/www.wikihow.com\/Conduct-Yourself-Professionally-on-a-Movie-Junket#!step1\n\nCheck-in","label":1,"model":"bloomz","source":"wikihow","id":190}
{"text":"Bluffs are an important part of poker.  They can make up half of winning strategies.   A successful bluff will give you more chips than any other move except hitting a royal flush.    The best time to bluff is usually during the preflop stage; however there may also be times after the flop which call for a bluff.     You should never bluff too much because this will cause people to catch onto what type of player you really are.   If they think you've got a monster every single time you'll lose money over time.      There are many different ways to bluff depending upon how confidently you want to do so.       Here we go!    How To Bluff - Part 1  Look At Opponents Noses Not Into Their Eyes To Seem Less Shifty-Eyed     This one seems obvious enough...   Don't stare directly into someone's face while they're talking to you....it makes them uncomfortable!  Instead,...  ...make eye contact from across the table, like someone who's trying to figure out who said something funny before laughing hysterically yourself.  It'll work wonders for your image as a cool guy\/girl.        Also avoid crossing your arms around your chest--this gives off a closed-off vibe that's hard to overcome.         ...","label":1,"model":"bloomz","source":"wikihow","id":191}
{"text":"Sharpening knives can be done using either an electric sharpener or by hand.  Hand-sharpened knives are generally considered more durable than those which have been machine-ground because they retain their original shape better.   This method will work for both stainless steel blades (which do not require honing) and carbon steel blades (which need to be honed after being sharpened).   The process takes longer but produces much nicer results.    Gather these materials:  Two stones - coarse and fine Whichever type of kitchen knife you own A small bowl filled with warm tap water Wooden cutting boards Rubbing alcohol Clean cloths Small piece of sandpaper Stainless Steel wool   You should use two different types of stones when sharpening:    Coarse Stones - These stones tend... 2.5mm thick Fine Stones - These stones range from 0.3 mm-1.0 mm thick If you're working with a carbon steel knife you'll also want some oil such as:  Olive Oil Canola Oil Sunflower Seed Oil","label":1,"model":"bloomz","source":"wikihow","id":192}
{"text":"Pumpkin is one of those foods that can be used all year round\u2014from pies during Thanksgiving season to soups when it's cold outside.  But if you want something sweet with just enough fall flavors but without being too heavy, try these easy-to-make pumpkin bars! They are perfect as an after-dinner treat anytime you'd like!  You will need:  For the bars:  Cream Cheese Frosting:   Glazed Pumpkin Bars:   ... Finished.   \n\nYou may also like:  \n\nIf you're looking for more dessert recipes using pumpkins, check out: \n\nFor other ways to use up leftover pumpkin puree from pie-making, see How To Store Leftovers From A Pie . \nTo make this recipe gluten-free, replace the flour with cornstarch instead. . If you've made a lot of pumpkin pie filling recently, don't throw away the leftover pumpkin pulp\u2014the next day you'll have plenty of pumpkin puree ready-made for making delicious pumpkin bars.  \n \n The only thing left now is to wait patiently while the bars cool down before serving them.","label":1,"model":"bloomz","source":"wikihow","id":193}
{"text":"1.  Visit the customize page of your Wordpress site by clicking on the customize'it button on your Admin Bar. \n2.   Change the Skin. The skin option allows you to  change the color scheme of your website.    3.   Change Your Title And Tag Line . Your website's title and tag line are what appear in Search Results and in the Browser Tab.     4.   Upload A Logo & Favicon    5.   Customize Front Page     6.   Edit Your Website's Navigation   7.   Edit Post\/Page Layout 8.   Edit Comment Settings 9.   Add Social Links 10.   Edit Image Effect 11.   Add Custom CSS 12.   Create Slider 13.   Add More Images To Sliders 14.   Publish Changes 15.   View Site 16.   Go Back To Dashboard 17.   Delete An Existing Slide 18.   Update Slides 19.   Exit From Editor 20.   Download Free Themes 21.   Install New Plugins 22.   Manage Your Plugins 23.   Disable Plugin 24.   Enable Plugin 25.   Uninstall...","label":1,"model":"bloomz","source":"wikihow","id":194}
{"text":"The electric slide is one of many dance moves that originated from African-American culture.  It was popularized during the 1950s through 1970s when soul music became very popular.   The electric slide can be done at any time or place where there are people dancing to this type of music.    This wikiHow will teach you how to do an electric slide using basic movements for beginners who have never danced before but want to learn more about the electric slide.     You may also like...   How to Dance the Hustle  How to Learn the Macarena  How to Waltz  How to Tango  How to Foxtrot  How to Jive  How to Samba  How to Mambo  How to Disco  How to Breakdance  How to Lindy Hop  How to Charleston  How to Shag  How to Hip hop  How to Meringue  How to Boogie-Woogie  How to Swing Dancing  How to Tequila Sunrise  How to Paso doble","label":1,"model":"bloomz","source":"wikihow","id":195}
{"text":"Microwavable neck wraps are easy to make yourself using common household items.  They can be used as a warm compress when applied directly against the skin after being heated briefly in the microwave.   You may also choose to heat them indirectly by placing them between 2 towels before applying it to your body.    This method is especially useful if you're suffering from muscle pain that radiates down along one arm or leg.     The following instructions will help you create your own personalized neck wrap which fits snugly around your neck like a scarf but provides more coverage than most bandages do.      If you'd prefer not to sew,...    How To Make A Microwaveable Neck Warmer Using Fabric And Beans Or Grains\n\nMeasure Your Neck With A Fabric Tape Measure.\n\nCut The Fabric To Size. \n\nFold It In Half Lengthwise.  \n\nSew Both Ends Together Leaving An Opening On One Side.\n\nTurn Right-Side Out Through Opened Side Seam.\n\nFill Bag With Aromatic Grain Or Bean Mixture.\n\nSeal Remaining Side Seam Securely.\n\nHeat Up Bag For Approximately Three Minutes Per Time Period.\n\nWash Fabric Every 3-6 Months Depending Upon Usage Level","label":1,"model":"bloomz","source":"wikihow","id":196}
{"text":"Jacob\u2019s ladder is made by creating several layers of interlocking rings using only three pieces of string.  The pattern for this project can be found online or at most craft stores.   This method will create a vertical version of the traditional design.    You may need to adjust how much yarn you cut depending upon its thickness; however, if you're unsure about cutting too short, it's better to err on the side of caution than risk having no material left when you've finished making your tower.     For example, if you'd like to make a 12-inch (30 cm) tall tower, you'll want to cut four 6 foot (1.8 m) lengths of yarn instead of six 3 feet (0.9\u00a0m).    If possible, choose a colorful variety of colors as well as textures such as cotton, silk, nylon, etc..     It should also have enough length... 2. Cut 4 equal-length strands out of your chosen rope. Measure 1 inch (25 mm), tie knots into either end of these strands, and set aside until ready to use later.","label":1,"model":"bloomz","source":"wikihow","id":197}
{"text":"Plaid is a classic style that's been popular since the Victorian era.  It can be worn by men as well as women; it looks great on both sexes!  It's also versatile enough to work year-round!   This guide will show how to choose which plaid pieces are right for you based upon body type, skin tone, hair coloring, face shape, age range, height\/weight ratio, dress code (casual vs business), weather conditions, etc.   If you'd rather see some examples before reading this entire page,... continue here... \nIf you've never tried wearing plaid before but want to give it a shot anyway, start small -- just try out 1 item from each category below until you find what works best for you:  Shirts Tops Skirts Dresses Pants Jackets Accessories Once you've found 1-2 items that suit you perfectly, then move onto trying more complicated combinations using those same basic styles.    For example, if you love the way a particular top looks when paired with jeans, then keep experimenting with different bottoms while keeping the top constant.     You may even consider buying multiple versions of these staples so you'll always have something stylish to throw together quickly whenever inspiration strikes....","label":1,"model":"bloomz","source":"wikihow","id":198}
{"text":"Creating an Excel budget can help keep tabs on how much you're earning versus how much you're spending.  This will allow you to see where there are areas where you could cut back if necessary.   You may also find it helpful when making financial decisions such as whether it's time to buy something big like furniture or appliances.    Keep track of your income and...   Create a new sheet by clicking \"File \u2192 New from Template\u2026\"  Select \"Budget\" under \"Select Templates From:\" Click OK twice.     Enter your anticipated annual salary into cell A1 (e.g., $60,000).  Type \"=\" in Cell B1, then press \u21b5 Enter.  Type \"=\" in Cell C1, then press \u21b5Enter.  Type \"=\" in Cell D1, then press \u21b5 enter.  Type \"=\" in Cell E1, then press \u21b5enter.  Type \"=\" in Cell F1, then press \u21b5ENTER.  Type \"=\" in Cell G1, then press \u21b5 ENTER.  Type \"=\" in Cell H1, then press \u21b5ENTER.  Type \"=\" in Cells I1 through J1, then press \u21b5ENTR","label":1,"model":"bloomz","source":"wikihow","id":199}
{"text":"The following sections will provide more information on how to deal with some common side effects associated with opioid use.  Itching  Drowsiness  Sexual dysfunction   Addiction  If you're experiencing any serious side effects while using prescription painkillers containing opiates (such as morphine), call emergency services immediately at 911.   You should also contact emergency services if you've been prescribed narcotics by your doctor but they don't seem to work anymore - this could indicate overdose.\n\nItching\n\nApproximately 10 percent of patients experience itchy skin when starting treatment with opioids.  This condition usually occurs within one week after beginning therapy and lasts anywhere between two weeks and three months before gradually disappearing.  The most effective treatments include topical steroids like hydrocortisone 0.5\u00a0% cream applied twice daily until symptoms subside.    Other options include:    Antihistamines taken... 3 ways to reverse opioid side effects. Be aware of other potential side ef ... #PainManagement #painkiller","label":1,"model":"bloomz","source":"wikihow","id":200}
{"text":"HomeKit allows you to control various smart devices using Apple products like iPhones, iPods Touch, iPads, Mac computers, and Apple Watches.  You can use voice commands with Siri if you\u2019re running iOS 11 (or later) and\/or macOS High Sierra.   If you're not already signed into...   Upgrade your device(s).  Set up your first HomeKit-compatible accessory. Create scenes by adding multiple HomeKit accessories together. Manage your HomeKit accessories through the Home app. Activate Siri. View additional tips as needed. Troubleshoot problems that arise when trying to set up HomeKit items. See also How to Enable Siri on Your Device. See also How to Change Settings Using Voice Commands With Siri. See also How to Ask Siri Questions. See also How to Tell Siri To Do Something For Me. See also How to Send Messages From Siri. See also How do I Talk Back? See also How to Download Apps from the App Store?.","label":1,"model":"bloomz","source":"wikihow","id":201}
{"text":"You will find that there\u2019s no end to what you can do when it comes to transforming ordinary items around your home.  You don\u2019t even necessarily need to spend lots of money either \u2013 sometimes an idea like using plastic bags filled with sand instead of pumpkins works well too! .\nTo make sure everyone gets their share of spookiness during the party night here are some more suggestions from wikiHow readers who shared their own tips and tricks.\n\nMake a zombie attack window display\n\nThis one was created by reader Sarah Hicks (aka The Witching Hour) and her daughter Hannah. \n\nSarah says: \u201cWe made two sets - one for each side of my front door. We used white poster board, glue gun, scissors, paint pens, googly eyes, fake blood, cotton wool balls, ribbon, wire hangers and cardboard boxes\u201d.  \n\n\u201cMy husband painted them while I did the rest...\" \n\nTransform a dinner table into a haunted scene\n\nReader Amanda Jones shares her recipe for making a ghoulish feast setting which she calls \u2018The Uninvited\u2019.   She says \u201cI wanted to give people a little scare before they sat down to eat so I decided to turn my dining room table upside-down\u2026","label":1,"model":"bloomz","source":"wikihow","id":202}
{"text":"This is how I made my storm trooper helmet out of plastic bottles.  It took me about 3 hours total including drying time for each step.   The most important thing here was patience because this project requires many steps that need to be done one after another.    You can use any kind of plastic containers such as:  1) A large soda bottle (2 liter).    2) Two smaller sized milk jugs.    (You will have more work if you choose bigger size.)     .    ...   This is what you'll end up with:  .       ...  ...         ...      ...            ....          ...              ...                 ....             .....               ......           ...............         .........                  ..                     ..                     ....                 ....                   ....                      ....                         .....                       ....                         ....                          ....                           ....                            ....                             ....                              ....                               ....                                ....                                 ....                                  ....                                   ....                                    ....                                                                                                              ....                                               ....                                           ....                                          ....                                        ....                                      ....                                     ....                                    ....                                                                                                              ...      How do you make a Stormtrooper helmet?","label":1,"model":"bloomz","source":"wikihow","id":203}
{"text":"The first step towards planning healthy meals at home is creating an organized menu system.  This will help keep track of all the recipes you've tried before as well as those you're interested in trying.   It also helps ensure that everyone gets fed every single meal while keeping things interesting!  The best part?  You don't need to spend hours making this happen!   Here are just a few tips to get started:  Start by taking out a three ring binder and some loose leaf paper.    On one sheet write \"Master Menu\".     On another sheet write \"Master Weekly Plan\".     On still another sheet write \"Grocery Lists\".     Finally create a separate sheet labeled \"Daily Recipes\" if you'd like to include these too. (You may not.)     If you'd rather use computer software instead then try using Microsoft Excel or Google Sheets which allow you...    Now it's time to start filling up each section of your master menu.  First you'll need to determine how many people there'll be:  One person - 1 serving = 1\/2 cup cooked meat\/veggie per adult; 2 servings = 3\/4 cup cooked meat\/vegetable per child under age 10; Two persons - 1 serving = 2\/3 cup cooked meat\/vegatable; Three persons - 1 serving = 4\/5 cup cooked meat\/veganatable; Four persons - 1 serving = 5\/6 cup cooked meat\/vegantable; Five persons - 1 serving = 6\/7 cup cooked meat\/vegantable; Six persons - 1 serves = 7\/8 cup cooked meat\/vegentable; Seven persons - 1 serve = 8\/9 cup cooked meat\/vegenatable; Eight persons - 1 serve = 9\/10 cup cooked meat\/vegaentable; Nine persons - 1 serve = 10\/11 cup cooked meat\/vegantable; Ten persons - 1 serve = 11\/12 cup cooked meat\/veganotable; Once you've determined who'll eat along with you,...","label":1,"model":"bloomz","source":"wikihow","id":204}
{"text":"If you're not happy with something going on at school - whether it's a new policy being introduced by the government which doesn't suit you, or some other issue affecting students like bullying or poor behaviour from staff members - then perhaps you'd like to suggest changes yourself.  However, before doing so you'll need to know what makes a successful argument.   This guide should give you everything... How do we convince our parents?   What does \"negotiation\" actually involve?  Why can't we simply ask someone else to talk to our teacher instead?\nWhat happens after you've made your case?\n\nNegotiating isn't always easy, especially since many schools operate under strict rules set out by local authorities and\/or governments.    You won't necessarily succeed every single time,...    But remember this:  Don't ever take things personally!   Be prepared to compromise!  Remember that everyone wants what's best for children\/students.... \nDon't forget to:\n\nRead up on the subject beforehand!\nBe polite!\n\nRemember who you represent! \n\nHave confidence in yourself!  \n\nGet others involved!","label":1,"model":"bloomz","source":"wikihow","id":205}
{"text":"Cobra yoga poses can be done anywhere at any time.  You don't need special equipment; all you'll need is some open space where you're not likely to get knocked over while practicing this advanced posture.   If you'd like to practice outside of class but aren't quite ready yet, try performing cobra indoors first before moving outdoors (or vice versa). The most common place people perform cobra is during their warm-ups prior to beginning an actual workout routine.    This will help prepare them physically and mentally for what lies ahead.  It also helps improve flexibility throughout... Lie flat on your stomach facing upwards towards the ceiling.... Draw your shoulder blades gently apart,... Bend your neck backward until it's almost touching the floor. ...  Exhale deeply through pursed lips.  Inhale again when you've reached peak height.  Repeat 3-5 times per set.  Don't forget about breathing!   Do 2-3 sets total.  Rest between sets 1-2 minutes.  Practice regularly!  Be patient.  Always listen to your body's signals. .    To do this:  Stand straight with your arms raised above your head.  Turn one foot 45 degrees outward.  Rotate your torso 90 degrees counterclockwise around your hip joint.  Bring your arm opposite your turned leg behind your back.  Extend your free hand upward along side your bent knee.  Continue rotating your torso clockwise 180 degrees.  Return to center.  Repeat 10-12 times per set.  Do 2 sets total.  Rest 1 minute between sets.  Try these exercises once every 3-4 days after warming up.  They'll strengthen your core which makes cobra easier to hold longer without straining too much.  These movements should feel natural and easy--don't force anything.  Focus only on feeling good rather than trying to achieve perfection immediately.  For example,  if you find it difficult to keep your elbows tucked under your ribs, simply bring them closer instead of forcing them further inward.  As long as they're still pointing downwards, they won't interfere with the movement itself.  In fact, they'll actually make it look better because it'll give you something else to focus on besides just holding the pose correctly.  It's okay to cheat here since we're talking about improving our technique anyway.  Just remember to always return to correct positioning whenever possible.","label":1,"model":"bloomz","source":"wikihow","id":206}
{"text":"Survey Galaxy allows users to easily translate surveys into multiple languages.  This feature can help researchers reach out to respondents who speak different languages than those spoken by their own countrymen.   The following steps demonstrate how to:  - Set up multi-language translation settings; - Translate survey items using the built-in translator tool provided with Survey Galaxy; - Preview the results before publishing them online. Visit https:\/\/surveys.galaxyproject.org\/en\/login\/signup?source=welcome&utm_source=gmail&utm_medium=referral&utm_campaign=sign-up-links . Type in your name, e-mail address, date of birth, gender, phone number, and preferred username and\/or password when prompted. You may also choose whether you'd like to receive newsletters about upcoming events via email at this time as well. Once you've completed all required information fields, you'll need to verify that you're not a robot by clicking on the link sent to your e-mail account. If you don't see any verification message within 10 minutes after signing up, check your Spam folder first. After verifying your registration, return to the main page again and log-in now if necessary.","label":1,"model":"bloomz","source":"wikihow","id":207}
{"text":"Respecting your parents doesn't mean being afraid of them; it means respecting who they're as people.  It also does NOT mean agreeing with everything they say; it's perfectly fine if you're different!   You don't need to change yourself completely into someone else's image.   Instead, try to find ways to work together instead of against each other.    If there\u2019s something you really disagree with (like smoking), then talk openly about why you\u2019re upset without making accusations.  Try saying \u201cYou\u2019re right\u201d rather than \u201cI\u2019m right\u201d; this will help keep the conversation focused on finding solutions, which makes everyone happier!  For example,  if Mom wants you to go out with her friends tonight while Dad thinks it\u2019s too late,...    Don't argue with your parents over trivial matters such as:  Your appearance - whether you've got enough sleep, brushed your teeth properly etc....    Don\u2019t let your parent(s) control every aspect of your life \u2013 especially if they\u2019ve done so since childhood.  Letting go doesn\u2019t necessarily mean giving up entirely.  Sometimes letting go involves accepting certain aspects of your parent\u2019s behavior even though you might prefer otherwise.  Take time away from home occasionally.  This could involve spending weekends elsewhere, taking trips during school holidays, or moving back home temporarily after college graduation.  Keep in mind that no matter where you move, you'll still be able to visit your parents whenever you'd like.  Spend quality time with your siblings.  They'll probably miss you less than your parents would anyway.  Consider joining a support group online or meeting others who share similar experiences.  Find a therapist or counselor whom both parties trust.  A professional opinion can provide valuable insight into any potential problem areas between you and your parents.  The following steps should be taken before attempting to communicate directly with your parents:","label":1,"model":"bloomz","source":"wikihow","id":208}
{"text":"The Cessna 310 was introduced by Cessna Aircraft Company in 1956.  It has been produced continuously since that time.   This airplane can be flown solo or double seat configuration.    In this tutorial we will cover some basic information regarding flying the Cessna 310.  We have included links below where you can find additional information if needed.     If you're interested in learning how to fly other types of planes please visit our How To Fly page here.[1][2][3][4]  See also - Pilot License Requirements[5]   Flight Training[6]     General Information[7]     Flying Lessons[8]     Airports[9]     Instrument Rating[10]     Private Pilot License[11]     Commercial Pilot License[12]     Recreational Pilot[13]     Student Pilot[14]     Sport Pilot[15]     Night...[16]     Helicopter[17]     Be A Pilots Helper[18]     Become An Instructor[19]     Emergency Procedures[20]     Aeronautical Charts[21]     Weather Conditions[22]     Wind Shear[23]     Temporary Landing Permits[24]     Fuel Types[25]     Jet-A1 vs Avgas[26]     List Of All Airlines Worldwide[27]     List Of All Major Airports Around The Globe[28]","label":1,"model":"bloomz","source":"wikihow","id":209}
{"text":"If you're looking into playing sports at university then there are several things you'll want to consider before making this decision.  You may be wondering how much money you'd get paid if you were on a professional basketball team but it's important not to focus too heavily on these aspects when deciding whether or not to go pro as they can change over time.   It's also very important to remember that being a collegiate athlete isn't just about winning games; it takes hard work, dedication, determination, discipline, sacrifice, commitment, passion...and so many other qualities!   It'll take years (if ever) until you see yourself playing professionally - maybe never!  But that's okay because you've got plenty of time ahead of you!\nThe first step towards becoming a collegiate athlete involves getting noticed by scouts who will help you decide where you fit best academically and athletically.    Start by filming some game footagelike highlights of your favorite plays, rebounds,...etc. Send them along with a resume\/stat sheet to schools whose teams you think you'd enjoy joining.  Make sure they're sent directly to the appropriate person such as:  Athletic Director Coach(es)  Don't forget to include contact information including phone number and\/or email address.  This way they'll know exactly whom to reach out to regarding your application.  Remember though, sending video clips doesn't guarantee anything....","label":1,"model":"bloomz","source":"wikihow","id":210}
{"text":"If you're hosting people over for dinner who've just come back from a busy day's work then chances are they're looking forward to unwinding with a nice meal.  But how will you cope?  How will you ensure everything goes smoothly? Here we give you tips on being a great host - even though you've been working hard! .\n1. Calm Down:  You may find yourself panicking about having to cook a big meal for several people while also making sure everyone feels relaxed enough to enjoy themselves.   Don't worry!  It'll go well... eventually!\n2. Classify Your Guests Before Making Plans:   Think carefully about whether you'd rather spend quality time chatting away with old pals than getting stuck into preparing food for business associates.  Depending upon whom you'll be entertaining, different approaches should be taken.    3. Make A Plan:    Once you've decided who's coming round, it's important to decide exactly what's happening during the course of the evening.  For example,... 4. Prepare The Food:     It's best to start cooking the main dish(es), such as meat\/poultry\/fish and\/or vegetables, first because these take longest to cook.     Then move onto side dishes which don't require heating, followed by desserts\/dessert drinks.    5. Arrange The Table:      Place placemats under each plate\/bowl\/dessert container\/cutlery set-up.        Set napkins beside each place setting.        Put cutlery sets together according to type.        Add any decorations you wish to include (flowers, candles).        6. Greet Your Guests:       Introduce yourselves to each other.       Ask questions about how they've spent their day.      7. Begin Dinner:         Sit down around the table.         Enjoy your meal.","label":1,"model":"bloomz","source":"wikihow","id":211}
{"text":"This guide will help players who are having trouble with playing Block Story on their own.  This game has been developed by Minecraft developer Mojang AB.   The following steps should be followed when starting up this game.    1) Create an account  2)  Download the app from Google play store 3)   Launch the app 4)    Tap \"Play\" 5)     Tap \"New World\" 6)      Start creating your world 7) Wait till the end 8) Continue making more worlds 9) Finish 10) Return 11) Select \"Teddy Bears\" 12) Choose \"Make Teddy Bears! 13) Pick \"Wood Planks\" 14) Click \"Select Crafting\" 15) Place Wood Planks 16) Repeat 17) Move 18) Open 19) Make Sticks 20) Collect Wax 21) Gather Coal 22) Light Fire 23) Make Candles 24) Set Up Table 25) Sleep 26) Mine 27) Fight 28) Keep repeating 29) Complete Quest 30) Head Home 31) Talk To Ted 32) Ask For Help 33) Follow Instructions 34)","label":1,"model":"bloomz","source":"wikihow","id":212}
{"text":"This is an advanced spreadsheet with many functions used together.  It will take some time to understand how it all fits into place but once understood you'll be able to create your own designs using similar techniques.   This sheet has been designed so that each row represents a different design element (a single spiral).  Each cell contains information about its position on the pattern.    You can change any value here if you'd like to see what happens to other cells' calculations.     For example changing Adj from 1 to 2 would double the number of rows between adjacent spirals while keeping their distance constant;  increasing Divisor would make the spirals smaller overall without affecting their shape;  decreasing Factor would increase the angle at which they turn around themselves etc..    Adjusting these parameters may help you find better solutions for your particular problem.      ... None of the above choices . \nThe following formulae should work fine unless otherwise stated below:-   EVEN - returns TRUE if it's argument is even,    FALSE otherwise.    ROUND - rounds down to nearest integer.    PI - returns pi.    TOP - sets top limit for the range of numbers being calculated.    ADJ - adjusts the spacing between consecutive elements.    ADJR - adjusts the radius of individual elements.    FACTOR - controls the amount of rotation per unit length.    DIVISOR - controls the size of the whole pattern.    CC - control point co-ordinate.    DB - base diameter.    DEGREES - converts radians to degrees.    ROW - current row index.    AdjRows - total number of rows.    Designer - designer variable.    Adj - adjustment factor.    Top - maximum height.    AdjTop - adjusted maximum height.    AdjBottom - adjusted minimum height.    AdjLeft - adjusted leftmost x-coordinate.    AdjRight - adjusted rightmost y-coordinate.    AdjCenterX - adjusted center X coordinate.    AdjCenterY - adjusted center Y coordinate.    AdjWidth - adjusted width.    AdjHeight - adjusted height.    AdjAngle - adjusted angle.    AdjRadius - adjusted radius.    AdjDistance - adjusted distance.    AdjArea - adjusted area.    AdjVolume - adjusted volume.    AdjDiameter - adjusted diameter.    AdjCircumference - adjusted circumference.    AdjPerimeter - adjusted perimeter.    AdjArea2 - adjusted area squared.    AdjVolume2 - adjusted volume squared.    AdjDiameter2 - adjusted diameter squared.    AdjCircumference2 - adjusted circumference squared.    AdjPerimeter2 - adjusted perimeter squared.    AdjArea3 - adjusted area cubed.    AdjVolume3 - adjusted volume cubed.    AdjDiameter3 - adjusted diameter cubed.    AdjCircumference3 - adjusted circumference cubed.    AdjPerimeter3 - adjusted perimeter cubed.    AdjArea4 - adjusted area quartic.    AdjVolume4 - adjusted volume quartic.    AdjDiameter4 - adjusted diameter quartic.    AdjCircumference4 - adjusted circumference quartic.    AdjPerimeter4 - adjusted perimeter quartic.    AdjArea5 - adjusted area quintic.    AdjVolume5 - adjusted volume quintic.    AdjDiameter5 - adjusted diameter quintic.    AdjCircumference5 - adjusted circumference quintic.    AdjPerimeter5 - adjusted perimeter quintic.    AdjArea6 - adjusted area sextic.    AdjVolume6 - adjusted volume sextic.    AdjDiameter6 - adjusted diameter sextic.    AdjCircumferenc","label":1,"model":"bloomz","source":"wikihow","id":213}
{"text":"Cherry turnovers are delicious treats that can make any occasion special.  They have a sweet filling made up of freshly picked cherries mixed with brown sugar and cinnamon.   The crust is flaky buttery goodness that's easy on the stomach.    This recipe will yield about 12 cherry turnovers - enough to share or eat all by yourself!   If you don't like cherries, try making lemon turnovers instead!  You could also use blueberries if you'd prefer something other than red fruit inside your turnover. These turnovers taste best when served warm so plan accordingly before serving this dish. Enjoy!    For tips on how to store leftover turnovers see below. To prepare the filling:  In a large bowl combine 1 pound of sliced tart cherries along with 2 tablespoonfuls each of brown sugar and ground cinnamon.  Mix well until evenly combined.     Set aside while preparing the pie crust. Preheat oven to 350 degrees Fahrenheit (180 Celsius).   Pour 1\/4 cup melted butter over top of the dry ingredients.  Using a fork mix thoroughly until crumbly.  Add 1\/3 cup cold water and continue mixing using a fork until just moistened.  Stir in 3 ounces grated Parmesan cheese.  Continue...","label":1,"model":"bloomz","source":"wikihow","id":214}
{"text":"Make sure that your cupcake pan has 12 cups.  If not, use an ice cube tray instead! You can also make these cupcakes using a standard muffin tin if desired. Preheat the oven to 350 degrees Fahrenheit (180 Celsius), then grease each cup of the cupcake pan by spraying cooking oil inside.   The amount of frosting needed depends upon how much you like to frost your cakes; however, one batch should cover 24 cupcakes. For this recipe, you'll need 1\u00bd sticks (150 g\/ 5 oz.) unsalted butter, 4 large egg whites, 3\u20444 cup (100 ml) whole milk, 1\u20443 cup (80 ml) freshly squeezed lemon juice, 1\u20442 tsp. (2.5 ml) vanilla extract, \u215b tsp. (0.6 ml) ground cinnamon,  \u215b tsp.  (0.6 ml)  grated nutmeg ,1\u20444 cup (60 ml) granulated white sugar, \u00bc cup (30 ml) brown sugar, 1 tbsp. (15 ml) cornstarch, 1 Tbsp. (15ml) all-purpose flour","label":1,"model":"bloomz","source":"wikihow","id":215}
{"text":"Finding an apartment can take some time.  If you've been offered one by someone else or if it's already available before you arrive,  congratulations!   However, many people find themselves searching through listings on their own after arriving at their destination.   This section provides tips about how best to do that.    The first step towards finding an apartment is knowing where exactly you'd like it located.  You may want something close enough to work so as not to waste too much money commuting every day;  however, this might mean sacrificing proximity to entertainment venues such as bars and restaurants.  Alternatively, you could choose somewhere closer to nightlife but farther away from your workplace (or vice versa).   Decide what matters more to you: convenience over location?  Location over convenience?    Once you've decided upon a general area within which... In addition to deciding where you would like to live, you should also decide whether you prefer renting privately owned property or public housing.  Public housing tends to offer lower rents than private properties while providing similar amenities including security cameras, elevators, laundry rooms, etc.  Private properties tend to provide better views and higher quality facilities although they often cost significantly more per month.  Rent prices vary widely depending on region,...","label":1,"model":"bloomz","source":"wikihow","id":216}
{"text":"This section will guide through installing Windows NT 3.5 onto a PC running DOS 6.22.  This version of Windows can be used alongside MS-DOS 7 without causing conflict between them.   The following steps assume that you've already downloaded the latest copy of Windows NT 3.5 from Microsoft's website at http:\/\/www.microsoft.com\/nt\/windows\/download\/default.asp  You should also ensure that you're connected to the Internet before proceeding.    1. Insert the first floppy disc containing the Windows NT 3.5 installer program into the computer's floppy disk drive.     2. Start the computer.     3. Insert the second floppy disc containing the drivers needed during the installation process.     4. Wait until the screen displays the text \"Welcome to Setup Wizard for Windows NT Workstation\" followed by two options -   A: Standard Installation A     B Advanced Installation B    5. Select \"A\" from the list displayed above and press \"Enter\".      6. Follow the instructions provided by the wizard to proceed with the installation procedure.        Note: During the installation process you'll need to provide some basic information such as:        Your full name;Your company;The location you'd like to install Windows NT on;A username and password;The type of operating system currently being run on your machine(e.g. DOS).      Once completed, you'll see a confirmation page stating that the installation has successfully finished.       7. Insert the third floppy disc containing additional software packages and\/or drivers necessary for certain types of hardware devices....","label":1,"model":"bloomz","source":"wikihow","id":217}
{"text":"Whirlpools can be installed by anyone with basic carpentry skills.  However, it may take several days depending on how much help you're able to get.   If you'd like more detailed information about installing a whirlpool bath yourself,  see How To Build A Bathtub Step-by-Step  This guide will give you step-by-step directions as well as tips along the way.    The first thing you'll need to decide before beginning installation is whether you want to build a custom whirlpool tub using concrete blocks (see Building Your Own Custom Whirlpool Bath) or purchase one pre-made from a company such as Jacuzzi\u00ae, Kohler\u00ae, American Standard\u00ae, etc. (see Buying An Existing Whirlpool Bath).  Either option has its advantages and disadvantages so it's important... Continue reading \u2192\n\nIf you've decided to buy a pre-manufactured whirlpool tub then all that's left now is to choose exactly what type of tub you would like based upon style, features, price range, etc..    There are many different types available including freestanding models, corner units, alcove models, built-in models, portable models, jetted whirlpools, air jets only whirlpools, combination steam\/air jets whirlpools, and even some \"sauna\" whirlpools!   You should also consider other factors when choosing between these options like:","label":1,"model":"bloomz","source":"wikihow","id":218}
{"text":"The following is an overview of how to replace brakes.  This process should take about 2 hours per wheel.   You may want to have someone help you lift heavy objects while replacing the brakes so it doesn't strain your neck muscles too much.    The steps below are listed by order but some can occur simultaneously depending upon what tools you're using.     1. Prepare yourself before beginning this project.  Make sure all necessary supplies such as replacement brake pads, drums, etc...are available prior to starting work.  If not, make arrangements now!     2. Begin removing the front left wheel.  3. Continue removing each wheel until you've removed both rear wheels.  4. Repeat Steps 5 through 12 for every other wheel except the right front one which you'll start with next time around.  5. Start rebuilding the braking system again after installing the last wheel!  6. Finish tightening down the lugs when finished.  7. Check the alignment of the tires once they're installed properly.  8. Drive away safely!","label":1,"model":"bloomz","source":"wikihow","id":219}
{"text":"If you're concerned... You should know how to keep your blood pressure under control while pregnant.  This information could save your life!   If you've never had high blood pressure but think it's something you'll experience once you're pregnant,...  How does being overweight affect my chances of developing high blood pressure?  What happens if I don't lose enough weight?    Why am I advised to stop drinking alcoholic drinks like beer, wine, etc.?    Is caffeine bad for me?    Do I need to change anything else besides my diet?    Are certain foods better than others?    Can eating too much salt cause high blood pressure?    Does exercising help lower my blood pressure?    Should I try meditation techniques?    Am I likely to get sick again after giving birth?    Will taking medication make me feel ill?    Where would I go for treatment?    Who might want to see their doctor immediately?    What's going to happen next?    What questions did we miss?    References.","label":1,"model":"bloomz","source":"wikihow","id":220}
{"text":"If you're feeling bored because it's taking too much time to get through one day, then this article may give some ideas on what to do.  If you'd like more information on how to make each hour pass faster than it actually does (or if you feel like every minute goes by so slowly), read below! You can also check out our list of ways to stop wasting time here. This section contains tips specifically designed to keep you occupied during boring days when nothing seems interesting enough to hold your attention. These suggestions should not only provide entertainment but they might even teach something useful along the way!\n1. Sleep in Don't wake up until noon!  It's hard to believe right? But sleeping longer has many health benefits including improved moods,  better memory  and increased energy levels.   It'll take all afternoon to recover after waking up late though...so don't overdo it!   2. Have a long shower The water running down your body feels great doesn't it?  Soaking under hot water relaxes muscles,...","label":1,"model":"bloomz","source":"wikihow","id":221}
{"text":"The following steps will help you discover your true vocation.  This list was created from my own personal experience over many years.   It has been tested on myself personally and also used successfully with others who were looking for their life\u2019s work.    The first step towards realizing our dreams is knowing them!  We all know we should follow our heart but sometimes it's hard to figure out exactly... What does your heart desire?   How would you feel about spending every waking moment pursuing an interest?  Wouldn't you love to wake up each morning excitedly anticipating going to work?\nWhat kind of things excite you most when you're young?  Do they still hold any appeal today?  If you've never explored those passions before now - don't worry;  there's plenty of time left in your life to start!\nDo you dream of traveling around the globe?  Or perhaps you'd prefer to settle down somewhere warm and sunny?  Maybe you just want to live close enough to family members to visit regularly.  Whatever your ideal lifestyle looks like,...","label":1,"model":"bloomz","source":"wikihow","id":222}
{"text":"Running can be an exhilarating experience for many people.  It has been shown by science to have numerous health benefits including weight loss, increased energy levels, improved mood, reduced stress, better sleep habits, lower blood pressure, stronger bones, decreased risk of heart disease, stroke prevention, diabetes management, cancer reduction, and more.   Running also helps improve mental focus and concentration as well as memory retention.    If you're new to... Read More ... How To Run Longer And Faster - Part 1 Of 2 (Video) By wikiHow Staff  Last updated at 07:40 PM on May 31,  2016 . This image may not be used by other entities without the express written consent of wikiHow, Inc. You must log in to continue. Log In   or Sign Up Now! \n \n \n \n Have questions? Get answers from our community members. Start here .\n \nYou will need to sign in before continuing. Please enter your email address below then click \"Sign in\" button again. Your account was successfully created!  Continue Here","label":1,"model":"bloomz","source":"wikihow","id":223}
{"text":"This section is divided into several subsections.  Each subsection contains detailed instructions as well as tips and tricks along the way.   You may find yourself skipping ahead if you're familiar with some techniques but not others; however, we recommend reading through each step before moving onto the next so you'll know exactly what needs to happen.    If you've never trained... Continue Reading \u2192\n\nThe following steps assume that you have already taught your dog basic commands such as:  Sit Stay Come Here Heel Off Leash Walk Around The Room (or other area) Turn Left\/Right Stop Eat This task should only take about 10 minutes per session until your dog learns which end opens up..  Start by placing something like a small toy inside the refrigerator near its bottom edge.  Then place another item outside the refrigerator somewhere within reach of your dog.  Have someone hold both items while you stand behind them.  Tell your dog \"Open\" and wait patiently.  When your dog grabs either object, praise her immediately!  Repeat these training sessions every day until she begins grabbing objects without being told.  Once she's able to do this consistently, try putting food inside instead of toys.  Keep practicing until your dog has mastered grabbing things from the refrigerator.  Now it's time to teach your dog to bring those things over to you!   Don't forget to use positive reinforcement during this phase too - don't scold your dog if she doesn't pick up right away because she'll associate negative feedback with getting treats later....","label":1,"model":"bloomz","source":"wikihow","id":224}
{"text":"Stamped concrete can be used for driveways, patios, pool decks, walk ways etc.  This method will show how easy it is to create an attractive pattern using texture paver from Elite Crete Systems.   You may use other brands but make sure they are compatible.    Pressure Wash The Surface  Prepare Your Work Area  Apply Base Coat Finish   Create A Bond Coat    Add Textural Paving  Allow To Cure  Mark Out Cutting Lines  Blast Off Dust  Color Stain  Seal It Up!    ... Continue reading here.... \nYou should now see the imprint of the texture paver on top of the thin finish coating.     If there were areas where the texture did not stick properly, spray some more thin-finish over those spots until the entire surface looks even.      Remove masking tape carefully so no paint gets removed.        Let cure for 24 hours.         Do not add too much water when mixing the colors because if it's too diluted you'll end up having to redo the whole thing later on.","label":1,"model":"bloomz","source":"wikihow","id":225}
{"text":"Instagram has become one of the most popular ways for young adults (and older ones too) to showcase themselves.  With more than 500 million active monthly users sharing 80 billion posts per month,  there are plenty of opportunities out there if you\u2019re willing to put some work into it.   If you're looking to get famous quickly or make money off of your newfound celebrity status, keep reading this guide.    The first step towards becoming Instagram famous is choosing a good username.  Your username should be something that's unique yet descriptive so it's easily recognizable by others.  You can use numbers at the end of your username like @jessica1234567  because they won't affect how many likes you'll receive.  For example,...   Once you've chosen a username, upload a high-quality profile picture.  This will appear next to every single thing you post, which means it\u2019s important to choose wisely.  Make sure...    Next up? Pick a theme! Think about what interests you; maybe travel, fashion, food?  Whatever it may be, stick to it!  Having consistency throughout all of your pictures makes them look much better and gives viewers a reason to follow you instead of someone else\u2019s feed full of random images.  A great way to find inspiration is to check out other people's feeds.  Look through profiles similar to yours and see what's working well for them.  Try not to copy anyone's style exactly though - remember, everyone looks different and have different tastes.  Just take ideas away from each other....","label":1,"model":"bloomz","source":"wikihow","id":226}
{"text":"If you'd like help making an animated video using Microsoft PowerPoint instead, see How To Create An Animated Presentation With Pictures And Text In Powerpoint 2016.  You can also use this method as a guide to make videos without pictures or music by following these instructions instead:  If you want more detailed information about how...   This will create a new folder called \"MovieMakerProjects\" inside which you'll find another subfolder named after today's date (e.g., \"July 18, 2017\"). Your computer may ask whether you trust the program before continuing \u2014 do so! You'll be able to add images from here; however,... \nYou should now see a list of files located within the newly created folder. The file names are listed alphabetically under their respective folders. For example, if you wanted to include three different songs into your project, place each song in its own separate folder labeled \"Audio 1\" through \"Audio 3\" respectively.   Once you've added everything you need, close out of the File Explorer window.","label":1,"model":"bloomz","source":"wikihow","id":227}
{"text":"If you're like most people who struggle with keeping their homes neatly organized because they don't enjoy spending hours scrubbing floors, vacuuming carpets, mopping baths, etc.  - then this article will help change your life forever!\nThe following tips were developed by professional organizer Marie Kondo (author of The Life-Changing Magic of Tidying Up) over many years of helping her clients transform cluttered living spaces into peaceful havens.   If you'd like to learn more about Marie... continue reading here ... . \nMarie Kondo has been featured in:  Oprah Winfrey Show Wall Street Journal New York Times People Magazine Forbes CNN Good Morning America Today ABC News CBS News NBC News Fox Business Channel CNBC NPR WNYC PBS This American Life BBC World News Marie Kondo was born in Tokyo Japan but moved to Los Angeles California USA at age three. She graduated magna cum laude from Santa Clara University with degrees in Japanese Language & Literature and Political Science. After graduating she worked briefly as an interpreter before becoming interested in organizing. In 2002 Ms. Kondo began studying organizational psychology under Dr. Richard Yates Ph.D. (founder of the Institute for Challenging Disorganization). While pursuing her studies she also became certified through the National Association of Productivity Professionals (NAPO), which led to her founding KonMari Media Inc. .","label":1,"model":"bloomz","source":"wikihow","id":228}
{"text":"Skateboarders usually dress very casually but still have an edge about them.  They tend not to follow fashion trends too closely because they prefer comfort above all else.   The most important thing is to choose clothes which allow freedom of movement so as not to get in the way when you're skating.    You don't need expensive brands - just make sure they're comfortable!   If possible try skate clothing before buying anything online.     T-shirts     Pants\/leggings     Shorts     Jackets\/Sweaters     Socks     Boots\/Bottines     Glasses     Belts     Headbands     Hats     Beanies     Sunglasses     Jewelry     Makeup    For example, wearing a pair of skinny jeans would probably restrict... Continue reading at Wikipedia.org. This section needs expansion. You can help by adding to it. (November 2015) \n \n Please expand this section to include information on how to style yourself into a skater look. See also How to Style Your Hair","label":1,"model":"bloomz","source":"wikihow","id":229}
{"text":"You can use either fresh or frozen cultures.  Fresh cultures will take about 24-48 hours longer than frozen ones before they become active but have more flavorful results.   You may also purchase commercial starter culture packets online if you'd prefer not to grow your own bacteria colonies.    If you're growing your own bacterial colony you'll need to start off with some good quality raw cow's milk that has been pasteurized at high temperatures (135-145\u00b0F) for 30 seconds followed immediately by rapid cooling so as to kill any harmful pathogens present.     The best way is to buy raw whole milk directly from local farmers who sell their products locally because it's fresher and higher... This method uses natural ingredients instead of chemicals like most other methods do which makes it healthier too!   It takes time though - anywhere between 12-24 days depending upon how much effort you want to put into it!  But once you've made your first batch successfully it'll be easier every time afterward!\nThe following instructions assume that you already know what to expect when culturing dairy products such as yoghurt etc. (see How to Grow Your Own Yoghurt Culture).","label":1,"model":"bloomz","source":"wikihow","id":230}
{"text":"Pok\u00e9mon is one of the most popular video games ever created by Japanese developer Niantic Inc.. Millions of people around the world play this game every day.  However, some players may find it difficult to trade with other players because they cannot get their hands on certain rare Pok\u00e9mon characters that others have.   Fortunately for these gamers, there\u2019s an easy way to obtain any kind of Pok\u00e9mon through editing software called PokEdit.    This method allows users to change the appearance of existing Pok\u00e9mon so that they look like different ones without changing its stats (evolution points) or abilities.     In order to do this, all you need to know how to set up your system settings properly before starting the process.  Here\u2019s what you\u2019ll do:     Power on your Nintendo handheld gaming console. Access Network Settings from within the main menu. Set up your internet connections manually instead of automatically obtaining them. Enter the following IP addresses:  Primary DNS:   Secondary DNS:    Save changes made to your System Settings. Open the PokEdit website. Create a new account.","label":1,"model":"bloomz","source":"wikihow","id":231}
{"text":"Alternate Titles: How To Teach Without A Teaching Certificate; How Do You Become An Uncertified Teacher?; How Can I Be Hired As A Teacher If I'm Not Qualified?\nTeaching without a degree can be done through various routes.  Some states allow people who do not hold traditional degrees from colleges with majors related... Continue reading \u2192\n...but there are many ways that one might go about it.   The most common way would probably involve getting into a program like Teach For America which places recent graduates directly into schools across the country.    Another option involves working towards earning credentials over years instead of doing so all at once.     In this case,...    There are also programs available specifically designed for those interested in teaching English Language Learners....     This page lists resources regarding how to obtain certification as a teacher outside of formal educational settings.  \n\nThe following pages provide additional information:\n\nRelated topics: Education , Educational psychology  , Education law  ,  Education policy  & planning ,  History of education","label":1,"model":"bloomz","source":"wikihow","id":232}
{"text":"Consigning autographs can be lucrative but it requires some effort.  The first step involves preparing your items so they will sell at their highest value.   You should also choose which auction house (or online auction site like eBay), where you'll consign them.    Once you've decided what you're going to consign, it's time to prepare yourself mentally before submitting your materials to the auction house.  This means knowing how to price your items correctly and negotiating the best deals available.   If all goes well, then hopefully you'll get paid handsomely when your signed memorabilia sells!    How do I find out who owns my signature?   What does \"auction\" mean?  Who's responsible for shipping costs?\n --------------\n\nThe next section covers steps 1 through 3.\n\nStep 1: Prepare Your Items For Sale\nBefore you begin selling anything, make sure you know exactly what's up for grabs.  Start by making a list of every single item you'd like to put under consideration.  Make this list comprehensive - include everything that's valuable enough...","label":1,"model":"bloomz","source":"wikihow","id":233}
{"text":"If you're considering getting divorced from your spouse but don't know what steps you'll take next, this guide can help.  This section covers all aspects of the divorce process including eligibility criteria, forms that must be filed, fees associated with each step along the way, as well as information regarding alimony (spousal maintenance), child custody\/visitation rights, division of marital assets, etc.   If you'd like more detailed information... Read More \u00bb\n\nDelaware has one of the most liberal no-fault divorce laws in America; however there are some restrictions which apply.    The state requires couples who wish to get divorced to meet certain minimum residency standards before they can begin their divorce proceedings.    In order to qualify for a divorce under Delaware law, both parties must reside within the State of Delaware for six consecutive months prior to initiating the proceeding....    There are two ways by which you could satisfy these residency requirements:  Either party may establish domicile in Delaware by:   Living continuously in Delaware for three years immediately preceding commencement of the action for dissolution of marriage.   Having lived in Delaware for five years immediately preceding commencement of such action, provided he or she was domiciled outside of Delaware during those first four years.   Or either party may maintain residence in:  A hotel,...","label":1,"model":"bloomz","source":"wikihow","id":234}
{"text":"1.  Heat oven to 350 degrees Farenheit.   Salt the meat and rub it all over using your fingers or tongs until you can taste the salty flavor throughout the turkey breast.    Add pepper to the turkey breast by sprinkling it generously across its surface so that each piece has an even amount of spice covering it.     Coat the turkey breasts evenly with flour coating them lightly but completely.    Place one pound of ossobucco turkey breast cutlets onto a large baking sheet lined with parchment paper and bake uncovered for about 20 minutes turning halfway through the cooking time.    Remove the turkey breasts when they're golden-brown and cooked thoroughly inside.    Allow the turkey breasts to cool slightly before cutting them open lengthwise along their center seam allowing the juices to flow out freely.    Cutting the turkey breasts allows more of the flavors to seep...    Prepare the vegetables:  Fry the vegetables:   Cook the pasta:     Finish off the meal:","label":1,"model":"bloomz","source":"wikihow","id":235}
{"text":"This will make sure that they are all even.  You should be able to see the marks clearly when you're done.   This way you'll have four equal rectangles.    If this doesn't work for you then try using duct tape instead.     Press down gently so as not to rip or damage any parts of the paper.  Do this several times across each side of the box.  Once you've made them stick well use some more Blu-Tac if necessary.  Don't worry if there isn't much left behind; it's okay to go back later and add extra Blu-Tac wherever needed.  It may take a few minutes before the Blu-Tac cools completely after applying it.  Be very gentle while doing this; don't pull away the sides yet!   It's important that both ends overlap slightly because that's how they'll stay connected once they're folded together.  Now do exactly what we did earlier - apply Blu-Tac to the inside corners of these tabs (the ones facing towards the center).  Then repeat steps 12-15 above for the remaining two tabs.  Repeat step 16-18 twice more for the rest of the tabs.  Continue wrapping the Blu-Tac around the outside of the hull until every inch has been covered.  There shouldn't...","label":1,"model":"bloomz","source":"wikihow","id":236}
{"text":"Discover how to avoid many common disappointments.  Walk by faith, not only by sight.   Get training and education that you're seeking and needing.   Serve needs--focus on true needs of others--and create what's needed for your own situation\/need.    When serving...you've got deserving to think about too....so your efforts aren't wasted on people who won't try to improve themselves\/help others (passing it along).   Don't keep track of disappointment:  People to \"get even with\"; arguments to remember; failed jobs\/work\/careers, etc. ...forget 'em!  Set backs & hurt feelings in relationships; forgotten!  Failed jobs & work; careers; businesses closing down.....when one door closes......another one's open!!   Change and support changes coming into your sphere of influence through hard work AND supporting other's important events\/occasions where it's appropriate.   Realize that success IS making progress---everyday and every hour!!!    Forgive and forget(if you can!)--or forgive without forgetting, IF THE FAULT HAS BEEN SCULPTED IN YOUR MIND!!!!     Be a better friend,...","label":1,"model":"bloomz","source":"wikihow","id":237}
{"text":"Meditation has been shown to help reduce stress levels, improve focus, increase happiness, and even lower blood pressure.  If you\u2019re interested in these benefits too, then it\u2019s important to find ways to motivate yourself to practice regularly.   Here are some tips to get started.    Know what\u2019s holding you back. Before you can figure out how to motivate yourself to meditate every day, take stock of all the obstacles standing between you and regular meditation sessions.  Write down any thoughts about why you haven\u2019t yet made meditation part of your daily routine.  For example, maybe you're worried you'll feel uncomfortable sitting still for long periods of time, or perhaps you've never found an appropriate place where you won't be distracted while trying to meditate.  Or... You might say something like this:  \u201cI'm really excited about starting my new meditation habit!\u201d \u201cI know I will enjoy this activity because I'm going to learn more about myself through it.\" \"I am committed to practicing meditation once per week until I've built enough momentum to make it happen each morning before work.\"","label":1,"model":"bloomz","source":"wikihow","id":238}
{"text":"The following are instructions on studying medical conditions associated with the hormone vasopressin:  1) Read through all information provided below.  2) Review your notes from lecture or class, including any handouts you received during lectures\/classes related to hormones (including vasopressin).  3) Go back over your textbook chapters regarding hormonal regulation\/dysregulation.   4) Look at online resources available via your school's library system and\/or Google Scholar search engine using keywords relevant to the topic you're researching.    5) Make sure you've understood what each resource has said before moving onto the next one!    6) Write down questions you may still have after reading these materials.  7) Consult an endocrine specialist who will help answer those remaining questions. 8) Continue reviewing material until you feel comfortable enough to take the test. 9) Take good care of yourself while preparing for exams - eat healthy foods, get plenty of sleep, exercise regularly... This section provides detailed descriptions of various types of disorders involving the hormone vasopressin; however, it does NOT provide answers to specific diagnostic tests nor treatments for patients suffering from these disorders....","label":1,"model":"bloomz","source":"wikihow","id":239}
{"text":"If you want to customize how your electric guitar looks, painting is one way that can be done quickly.  This method will allow you to change colors completely as well as add designs like flames.   You may also use this technique if you're repairing scratches from normal wear and tear.    To begin customizing your guitar by painting it:  Turn off all power sources connected to the guitar before beginning work. Make sure there are no loose wires attached to the guitar when working around its neck area. Remove the tuning pegs at each end of the guitar\u2019s fretboard (the part where you play). Unscrew the nut screw located between the first two frets near the top of the guitar. Unplug the jack plug which connects the guitar to the amplifier. Take out the pickup covers along both sides of the guitar. Remove the metal plate covering the hole underneath the bridge. Remove the screws holding the bridge together. Remove the screws securing the pick guard coverings surrounding the pick ups. Remove the screws attaching the tuners to the guitar. Remove the screws inside the soundhole. Remove the screws fixing the truss rod into place. Remove the screws keeping the tremolo arm in position. Remove the screws fastening the tailpiece to the guitar. Remove...","label":1,"model":"bloomz","source":"wikihow","id":240}
{"text":"Dogs communicate with humans in many ways.  They use facial expressions such as eye contact (or lack thereof), ear position, lip curl\/bared-teeth display;  posture changes like leaning towards something\/someone, rolling over, sitting down, lying down;  vocalizations including growls,... This article will teach you how dogs express themselves through body language.   Pay close attention to your own dog\u2019s behavior so... You can also watch videos online showing examples of various types of body language used by dogs.... If you're interested in learning about this topic further, check out these articles from wikiHow:   How To Read A Dog's Mind - Part 1 - Basic Emotions & Communication  How To Read A Dog\u2019s Mind \u2013 Part 2 - More Advanced Emotion Recognition  How Do I Know When My Dog Is Afraid? The first step toward understanding your dog\u2019s communication style involves simply observing him\/her closely for any clues he\/she may be giving off.    Look for things like:    Eye Contact: Does he look directly at you?  Or does his gaze wander around the room?  Does it seem focused on someone else?  Head Positioning: Does she tilt her head sideways while watching TV?  Or do her ears go up and down whenever she's excited?  Tail Movement: Does he wave his tail high above his back?  Or does he hold it stiffly behind him?  Posture Changes: Does he stand tall and proud?  Or crouch low and hunched-over?  These are all important questions!","label":1,"model":"bloomz","source":"wikihow","id":241}
{"text":"A transcript is written word-for-word from audio recordings.  It may be recorded by radio stations, television networks,  medical offices, law firms, government agencies  or private individuals who want their conversations documented.   There will always be a need for transcribers because there is no way to record everything we say.   \n\nTranscribing requires listening carefully to spoken words while simultaneously writing them down exactly how they were said.  This means not only hearing every word but understanding its meaning so that one knows which punctuation mark should go where (e.g., periods at end of sentences).   Some people think it's easier than speaking English!    It's important to know when to use capital letters correctly - especially if you're working under time constraints.    \n\nThe most common form of transcription used today is dictation done by doctors during patient visits.  However, many different types exist including legal, business meetings, interviews, etc....   Transcriptionists usually work alone in home office settings using computers equipped... \nThere are several ways to find jobs as a transcriptionist.  Many companies post positions online through websites such as:","label":1,"model":"bloomz","source":"wikihow","id":242}
{"text":"Puppies are adorable bundles of energy who require lots of attention.  They can also be very destructive if not trained properly.   This guide will help you train your new puppy so he or she grows into an obedient dog with good manners.    You\u2019ll find tips on:  \u2022 House training \u2022 Biting prevention \u2022 Basic commands such as sitting, lying down, staying, coming, etc. \u2022 How to teach your puppy tricks If you\u2019re adopting a rescue pet, read this first!   Training is important no matter what age your puppy is; however, it\u2019s especially crucial during his early years (the \u201ccritical socialization period\u201d).   The earlier you begin teaching him basic skills like walking on leash,... \nHouse training\n\nThe most common reason people give their dogs away is because they can\u2019t housetrain them.  A well-trained puppy won\u2019t soil its crate while sleeping nor defecate where it eats.  To prevent accidents before they happen, start by setting aside one room that\u2019s off limits until your puppy learns how to use the bathroom outside.  In addition, keep all food out of reach except mealtimes.  When you take your puppy outside every two hours after waking up, eating, drinking water, playing, going to bed, and any time it's been inside too long without being let...","label":1,"model":"bloomz","source":"wikihow","id":243}
{"text":"This article was written by Travis Boylls. \n \n It has been translated and adapted by Alexander... \n \n This wikiHow teaches you how to build an RC (remote-controlled) version of the classic yellow dump truck using LEGOs. The finished product measures about 12 inches long when assembled! If you're interested in building other types of vehicles instead, check out our guides on making a police car , tank , race car , motorcycle , train  , plane  or spaceship . \nBefore we get started assembling the actual bodywork of the truck itself, let's start by creating some basic bricks needed as supports later on:   Open the Brick Separator app if necessary.   Select \"LEGO\" under \"Sort By:\" at the top-right corner.    Tap each colorful square individually until all are selected.     Tap \"Done\" once you've created one hundred squares.      Repeat steps 1-5 above but select only white squares instead. (If you don't want to use the Brick Separator tool, simply tap every tenth colored square.)     Now we'll create two more sets of 100 squares - one set consisting entirely of black squares,...","label":1,"model":"bloomz","source":"wikihow","id":244}
{"text":"If you're looking for love online, you'll want to make sure you've got all bases covered when it comes to getting together face-to-face.  This wikiHow will teach you how to:  Make an impression Start off small Talk about common interests Ask questions about each other's lives Find ways to get closer Be yourself Don't forget to flirt! Keep things exciting Set aside time for romance Remember to listen well Plan dates ahead of time Know when to call it quits Learn to deal with rejection Stay strong Move forward Look past physical appearance Accept differences Respect boundaries Communicate clearly Express feelings Openly discuss sexuality Practice safe sex Enjoy being single Have fun! Read more... .\n\nReaders who found this page through search engines may also be interested in:\n\nGetting over someone Getting back into dating Making new friends Dating while traveling See also: \n\nRelated articles How do we know we're ready for another relationship? What does it mean to commit? Why am I still single? Is there something wrong with me? Shouldn't everyone else already be married?","label":1,"model":"bloomz","source":"wikihow","id":245}
{"text":"The United Kingdom uses miles per hour instead of kilometers per hour.  The maximum speed limit outside built-up areas varies between 60 mph (97 km\/h), 70mph(112km\/h)  80mph  (128km\/h). In built up areas such as towns and villages  50mph (80kmh).  On roads where no specific speed limits have been set 30mph (48kph).   Speeding tickets start at \u00a360 plus court costs which could amount to several hundred pounds.   Driving under influence of alcohol carries a minimum fine of \u00a3500 and\/or imprisonment for 6 months.    There are many differences when compared to American traffic laws; here they are:   When turning into another lane always signal using both indicators unless you're approaching a blind crest or hilltop.    ...    You must carry insurance cover for all passengers including children aged over 3 years old who travel in the vehicle even though they're sitting in their own seats.    Children below 12 years cannot sit next to a front seat passenger without wearing a restraint system approved by the Department for Transport. (See Child Restraint Systems.)     For further information see http:\/\/www.dft.gov.uk\/cars\/car-seat-checklist","label":1,"model":"bloomz","source":"wikihow","id":246}
{"text":"Before buying any mobile phone there are several things you'll need to take into consideration.  The following list outlines these considerations along with their importance.   You should also read our guide How To Choose A Cellphone Plan before making this decision.    Price - This may seem obvious but it's always best to set aside enough money so you're able to get exactly what you want without having to compromise too much.  Make sure you know how much data plans cost because they vary widely between carriers.     Screen Size & Resolution - Most people prefer larger screens when browsing websites...   Power Source - Some phones require special chargers which might limit where you can charge them up....     Memory Storage Capacity - While many smartphones come equipped with large amounts of internal memory, others only offer limited storage space.  It's usually possible to expand external memory using SD Cards though, although this does increase costs slightly.\n    ...    Operating System - Android has become very popular recently thanks to its wide range of applications available from Google Play Store while iOS devices tend to focus more on media consumption than productivity tasks like emailing and texting.  Windows Phones run Microsoft software and provide access to all Office 365 apps including Word,...","label":1,"model":"bloomz","source":"wikihow","id":247}
{"text":"Joint custody, also known as shared or split custody, means both parents share legal decision-making over children after divorce.  In most cases involving joint custody, each parent has primary residential responsibility (meaning they live with the child more than 50% of the time) but must allow frequent visitations by the other parent.   The goal of joint custody arrangements is to give both parents equal access to their children while still allowing them some degree of independence from one another.    If you're interested in getting joint custody during... Continue reading \u2192\n\nIf you\u2019re looking into how to get joint custody, there may already be a custody arrangement in place between yourself and your spouse....  Start a case when you are divorced., Begin the process when you\u2019re unmarried., Petition the court if you\u2019ve begun your case., Consider hiring a lawyer., Find the right courthouse., Fill out all required paperwork., Check off your forms before filing them., Serve notice upon the other person involved in the case., Wait for an answer..","label":1,"model":"bloomz","source":"wikihow","id":248}
{"text":"Replacing or repairing your wireless driver can fix problems with connecting to certain networks.  This method should only be used if other methods have failed.   You may need administrator access on this computer before proceeding.    If you're not already connected to an internet source (e.g., Ethernet), connect it now so that you'll know how to proceed when installing new drivers. .  You'll find this option near the bottom of the window. Doing so opens the Devices manager page. It's listed below \"Hardware resources and devices\" heading toward the middle of the page. The Network Adapter entry here lists all available adapters installed by default on your PC. Scroll through until you see the one that's currently being used as your primary wireless adapter; its name usually ends with \"Wi-Fi\" followed by another string like \"Broadcom\" or \"Atheros\". Make sure it's selected first!   For example, if your current adapter's name is Intel(R) Dual Band Wireless-AC 3160, look for Broadcom 802.11ac Wi-FI Adapter Driver.","label":1,"model":"bloomz","source":"wikihow","id":249}
{"text":"Craigslist can be an excellent place to find used cars at great prices.  However,  it's also notorious for scammers who will try their best to get you to buy something that's not quite right.   This guide will help you avoid getting ripped-off by giving you some tips before buying a used car online through Craigslit.  It should... Continue Reading \u2192\n\nIf you're looking for a new car but don't want to pay retail price, consider purchasing one directly from its manufacturer instead.  Manufacturers often offer incentives like rebates,... How do I know how much my car is worth?   What does \"Blue Book\" mean?  Why would someone sell me their car for less than Blue Book value?    Is it safe to trade-in my old car without going through a dealer?     Are there ways to save money during the inspection process?      Do all mechanics charge the same amount for repairs?       Can I fix my own car?        Does insurance cover damage caused by hail storms?","label":1,"model":"bloomz","source":"wikihow","id":250}
{"text":"The FBI MoneyPak is one of many viruses that infects computers through email attachments or downloads, then displays pop-up advertisements asking you to call a number so they can collect payment.  If you\u2019re not sure if your computer has been infected by this particular virus, look out for:  A message saying something like \u201cThe FBI wants to know about illegal activity taking place on [your] computer! Call 1-800-TELL-FBI (844-353-4243) immediately!. An error message stating that there\u2019s no connection between your PC and their server when trying to access certain websites such as Facebook, Google, Yahoo!, etc.. Pop-ups appearing randomly while browsing online. You may also see messages... .\nIf you're unsure whether you've got the FBI MoneyPak virus installed on your system yet, try running some basic checks first.   Check your web browsers - are any pages opening automatically? Are popups showing up every now and again?  Try accessing sites blocked by default \u2013 do these work correctly? Do you get errors when attempting to connect to them?\nCheck your hard drive space usage \u2013 does anything seem off? Is your disk filling up quickly without reason?","label":1,"model":"bloomz","source":"wikihow","id":251}
{"text":"Cortana is Microsoft's virtual assistant developed exclusively for Windows Phone 8.1 devices running on ARM architecture.  It was first introduced with Windows 10 Mobile but now it's also available for Windows Phone 8.1\/Windows RT\/PCs.   Cortana allows users to perform various tasks such as:  \u2022 Searching information from web services like Bing or Wikipedia;   \u2022 Setting reminders;    \u2022 Getting directions via GPS navigation systems;   \u2022 Playing music stored locally or streaming online through Xbox Music app;   \u2022 Launching apps.    To activate Cortana:     1. Tap Start button at bottom right corner.     2. Type cortana into search bar.      3. Tap Cortana icon in top left side of results page.     4. Enter your account details if prompted.     5. Follow any additional setup instructions displayed on-screen.     6. Restart device when asked.     7. Wait until Cortana finishes activating.     Note: Cortana may take some time before she becomes active... This step only applies to those who live outside United States and\/or Canada.","label":1,"model":"bloomz","source":"wikihow","id":252}
{"text":"Tankless water heaters are very popular because they provide endless supply of hot water, but like any other appliance or device that uses electricity it is important to maintain its efficiency.  This section will show you some basic maintenance tips so as not only to keep your unit running smoothly but also extend it's life span.   If you're unsure about what type of water heating system you've got installed then refer back to this guide where we discussed different types of water heaters.    Power Source    The first thing you'll need to do before starting with cleaning procedures is to shut down the electrical power source connected to... Continue reading \u2192\n\nFlushing Your Unit With Vinegar Instead Of Chemicals   Flushing out sediment build-up can be done using either chemicals such as muriatic acid or vinegar which does an excellent job without damaging pipes or plumbing fixtures.  To start the flushing process open both the cold and hot water faucets until they're fully opened.     Then close one of the two faucets completely while leaving the other halfway open.  Make sure there isn't enough pressure coming into the closed faucet to cause water damage if there's no water flowing out of the open faucet.     After closing 1 of the 2 faucets attach a garden hose to the remaining open faucet and run it towards the floor drain.  Keep doing this until the entire amount of water needed to fill up the tank has passed through the drainage line.    When the tank is full disconnect the garden hose from the open faucet and leave the water inside the tank to sit overnight....","label":1,"model":"bloomz","source":"wikihow","id":253}
{"text":"Photoshop CS4 offers an array of new features.  These include some very powerful editing capabilities along with several new effects.   Photoshop CS4 also includes Adobe Camera Raw 3 (ACR), giving photographers more control over their images before they begin processing them.    Here we will take a look at how these functions work together so that you may get the best results possible while creating your artwork.     We will start off with learning basic concepts like Layers, Adjustments, Filters, Blending Options etc..     Then move onto advanced topics like:   How to create custom presets  How to save time during repetitive tasks  How to combine multiple photos into one seamless composite photo  How to retouch portraits  How to create realistic waterfalls  How to create 3D objects  And much more!    If you're interested in taking classes but don't live near me,... see my website www.photobee.com . I teach online courses too!  See my blog http:\/\/blog.photobee.com\/","label":1,"model":"bloomz","source":"wikihow","id":254}
{"text":"Double Dare is an American television game show created by Mark Burnett & Dave Brooker which debuted September 16, 1997 until it ended its run February 18, 2002.  The series ran for eight seasons with over 200 episodes produced.   It featured two teams of three children who competed against one another through various physical challenges along with answering questions.    Each episode had four rounds - Introduction, Question Rounds, Physical Challenge, and Obstacle Course.   At the end of every episode there are prizes awarded based upon how much money they earned throughout their performance at the beginning of the episode.   In addition, if both teams won all four rounds then the winner gets double the amount of money than usual.   If only one team wins all four rounds but not both teams, then the other team will get half the normal amount of money.    ... This page contains information regarding the popular children's game show \"Double Dare\" hosted by David Spade and his co-host Kara DioGuardi.   You may also find helpful links below related to the show itself such as:   How To Play Double Dare Quizzes  Videos","label":1,"model":"bloomz","source":"wikihow","id":255}
{"text":"You may be able to save money by teaching your children at home instead of sending them off to daycare.  You\u2019ll also get more time spent with each individual child than is possible when enrolling him\/her into an educational institution such as preschool.   However, there are costs associated with this type of education \u2014 namely textbooks, supplies, and curriculum guides (or curriculums).  Here are some ways to cut down those expenses while still providing quality instructional materials:  Set a budget. Before you begin educating your... Continue reading \u2192\n\nShare classes. If you're a member of a homeschoolsing network, collaborate wiht other parents to teach your chilren together in a small class.. Sharing lessons plans among several families allows everyone involved to share resources like books, games, toys, computers, etc..  This way you'll not only reduce the amount of money you'd spend purchasing these items individually but you'll also increase the number of people who know what they're doing!  For example, if you've decided to focus on science experiments over the next few weeks, ask around about which experiment would best suit your child's age range; then all three households could purchase their own version of said experiment!","label":1,"model":"bloomz","source":"wikihow","id":256}
{"text":"Ghusl (Arabic: \u063a\u064f\u0633\u0652\u0644\u200e) refers to complete cleansing of one's body as part of Islamic practice.  It can be done at any time during the day but most Muslims prefer doing so before dawn.   The word \"Ghus\"l comes from an Arabic root meaning \"washing\" which also means \"cleanse thoroughly.\"  In Islam there are certain circumstances that require a person to do Ghusl; these include sexual intercourse, menstrual periods, childbirth, death, sickness, travel, etc..   If you're not sure when it's necessary then consult a Muslim scholar.    This article will explain how to perform Ghusl according to Sunni orthodoxy.[1][2][3][4] Before beginning make sure that:    You don't need to wash your hair unless you've had sex; however, even without having had sex you'll want to rinse out shampoo residue because it may cause irritation while bathing.     Make sure there's no dirt stuck between... \nThe purpose behind Ghusl is to purify oneself physically and spiritually.","label":1,"model":"bloomz","source":"wikihow","id":257}
{"text":"The following steps will show how to use blender's physics engine.  This tutorial was created using version 2.5 but should work for any version above 1.49b.   The first step is to create an object or scene from which we want to animate our physics system.    To do this go into Object Mode  and select Create > Cube.     You may also choose other shapes like cylinders etc..     If you're not familiar with Blender yet I suggest starting off with something simple like cubes before moving onto more complicated things.      Once you've made one shape you'll need to give them some properties so they behave correctly when animating.       Go back to Edit mode    and set the object's name to whatever you'd like.        Next we'll change its mass property,    which determines it's weight.    Set Mass = 0.1        Now let's adjust the density property.    Density sets what happens when two objects collide.    We don't...      Blending between multiple images requires creating several frames at once....         ...and finally save all changes!","label":1,"model":"bloomz","source":"wikihow","id":258}
{"text":"Sports can help people stay fit physically while also improving their mental health.  They teach children valuable lessons such as discipline, dedication, perseverance, self-confidence, respect for authority (coaches), fair play, cooperation, leadership, and many more.   The following tips will give you some ideas that may improve your performance or enjoyment of playing any kind of sports:  Setting Goals - If you really want to excel at anything, then setting specific targets is essential....   How To Improve Your Performance In Any Sport...  ...And Enjoy It! ...and enjoy it!\nSetting Aim High But Realistic\n\nBe A Good Athlete\n\nBe Patient.\n\nAccept Criticism.\n\nNurture Friendships With Other Players.\n\nLet Yourself Have Fun!\n\nJoin A Team Or Club.\n\nGet An Excellent Coach.\n\nSet Wide Sights And Keep Them There.\n\nTend To Basics First.\n\nAllow Room For Flexibility.\n\nAdd New Levels Of Challenge As You Build Skills.\n\nPractice Until Your Skills Become Second Nature.\n\nApply For A Gym Membership.\n\nGet Enough Sleep.\n\nEat Healthy Food.\n\nDrink Lots Of Water.\n\nStaying Clear Of Intoxicants.\n\nGetting Lots Of Rest The Night Before Gameday.\n\nCarbohydrate Loading Prior To Games...\n\nThe day before games\/competitions eat plenty of carbs like pasta\/rice\/potatoes etc. \n\nWarming Up\n\nKnowing Your Competition\n\nBeing Focused On The Game","label":1,"model":"bloomz","source":"wikihow","id":259}
{"text":"This is an easy way to make a realistic looking stuffed animal using simple materials.  The finished product will be about 12\" tall x 10\" wide at its widest point.   This method can also work well when making other animals such as:  A baby elephant; A zebra; An ostrich; A lioness; A tiger; A leopard; A jaguar; A cheetah; A puma; A bear cub; A monkey; A gorilla; A chimpanzee; A baby; You may want to use different colors depending upon what type of animal you're making.   ... How To Make A Realistic Looking Stuffed Animal Using Simple Materials - Part 1 ... How To Make A Realistcally Looking Stuffed Animal Usin... How To Make A Baby Elephant From Fabric And Cotton Balls - Part 2 ... How To Make A Zebra Out Of Fabric & Cotton Balls - Part 3 ... How To Make A Lion Cub From Fabric & Cotton Balls- Part 4","label":1,"model":"bloomz","source":"wikihow","id":260}
{"text":"Driving around town or even just driving through a city may seem intimidating to drivers who've never done it before.  However, once you've gotten over the initial shock (and maybe learned something about yourself), you'll find it's not as bad as you'd thought!   This section will help newcomers adjust their driving habits so they'll feel comfortable navigating urban areas.   It also includes tips for dealing with other drivers' behavior which might surprise those unfamiliar with metropolitan driving conditions.    Driving in a big city isn't all fun and games; however, most cities offer plenty of opportunities for entertainment outside of cars!  Read this guide first then check out our guides on getting around without a vehicle and finding parking spots near popular attractions.[1][2][3][4]  You should always follow local laws regarding speed limits, seatbelt usage, etc..[5]  For example, many states require children under age 13 to sit in booster seats while riding in vehicles[6] - regardless of whether you're traveling across country roads or highways within a state.","label":1,"model":"bloomz","source":"wikihow","id":261}
{"text":"Putting on compression stockings can be difficult, especially when you're just starting treatment.  However, once you've mastered how to put them on correctly you'll find that they are easy to wear each time.   This section will teach you everything you need to know about putting on compression stockings so that you feel comfortable wearing them throughout the day.    If you'd like more information before purchasing compression stockings please see our buying guide here..    You may also want to read this article on choosing the right type of compression stockings based on what condition they're being used for..  How do I choose my... The most important step is getting yourself ready by gathering all necessary supplies needed to apply compression hose properly.  Make sure you have:  1 pair of compression socks (or hosiery); 2 pairs of clean cotton socks; Talc-free baby powder; Gloves; A small stool\/chair; An open space where there won't be any distractions.     Once you get home from work it's best to take some time to prepare yourself mentally too!  Set aside 10-15 minutes per application to ensure proper fitting....","label":1,"model":"bloomz","source":"wikihow","id":262}
{"text":"If you're looking at someone but aren't sure he's \"the one\" yet, here is some advice on how to tell.  If you've been dating this person for quite sometime now (or even just started), then chances are you'll have learned more about each other's personalities by now.   You may find yourself thinking things like \"I can't imagine my life without him\/her!\"  Or maybe you'd think \"He's\/she's so perfect I don't need anyone else!\"   But what does all of these mean?  Does he\/she really make you happy? Are they truly right for you?\nHere are 10 signs which will help you figure out if they're \"The One\":    1) Do you enjoy spending time with them?  2) Can you picture yourselves growing old together?  3)... 4)  5)  6)  7)  8)  9)  10)  ... \n10 Signs He's The One For You\n\n1 - Do you enjoy spending time together?\n\nDoes seeing\/being with this person give you butterflies inside?  When you're apart do you miss their company?  These feelings might seem silly when you're younger; however as we grow older our relationships become deeper and it's important to spend quality time with people whom bring us joy.   \n\n2 - Can you picture yourselves growing up together? \n\nDo you envision having children and\/or grandchildren someday?  Would you live near enough to visit each other regularly?  What kind of career would you both pursue?  How much money should you save before getting married?  All of these questions are very serious ones; however,...","label":1,"model":"bloomz","source":"wikihow","id":263}
{"text":"Kibbeh can take up to an hour to prepare if done correctly.  If you're looking forward to eating authentic Lebanese food soon, here we provide you with instructions on how to make one complete meal using only Kibbeh! This recipe serves four people.   You must use frozen meat when making Kibbeh; however, it's recommended that you defrost the meat overnight prior to preparing the dish.   ... Finished. Enjoy!   For more information about Lebanon's cuisine visit www.lebanesefoods.com . Please note that there are many variations within the same family of dishes across Lebanon itself - these differences depend upon where they were originally made and\/or what type of meat was used.... \nFor example, the famous \"Kebba\" which means \"balls\" varies greatly throughout Lebanon ranging from:  \u2022 Kebba Al-Nahdawiya (Al-Nahdawi balls). These are round balls stuffed with rice and cooked in tomato sauce served during Ramadan. They also come in various shapes such as:  \u2022 Kofta kebba (meatballs).  \u2022 Kafta kebba (ground meatballs).   \u2022 Falafel kebba (fried chickpeas)...","label":1,"model":"bloomz","source":"wikihow","id":264}
{"text":"This method will teach you how to create a stained-glass window effect that looks like something you'd find at a cathedral.  This technique works best for designs featuring large areas of solid color separated by thin bands of lightly colored paint.   You may also choose to add some texture if desired.    The finished product should look similar to this example (click here).    In order to achieve realistic results you'll need to:  Gather your supplies from around... Continue reading \u2192\n\n1) Acrylic paints - preferably thickened acrylics which have been thinned down slightly but not runny enough to drip.     2) Black Magic Marker     3) Waterbrushes 4) Paper Towels 5) White Kitchen Erasers 6) Clear tape 7) Scissors 8) Glue 9) Large sheets of watercolor paper 10) Small pieces of cardboard 11) Tape Measure 12) Pencil 13) Sharpie Markers 14) Sponge 15) Stencils 16) Other small items 17) Painter's Palette 18) Canvas","label":1,"model":"bloomz","source":"wikihow","id":265}
{"text":"A concussion is an extremely serious brain injury that can cause lasting damage.  It happens because something hits your head hard enough to knock out consciousness briefly (a brief loss of consciousness), but doesn\u2019t break any bones.   A concussion may also be caused by hitting your head against another object without losing consciousness.    The most common causes include car accidents, falls, sports injuries such as footballs, hockey pucks, bats, balls, etc..    Symptoms usually appear quickly after being hit on the head; however they could take up to 72 hours to show themselves fully.     If you're experiencing one or more of these symptoms, seek immediate treatment at the nearest Emergency Department or call 911.      You should always get checked out even if you've had previous...   -  -  -  - \n \n \n \n Instructions for Children \n \n \u2022 Stop playing and tell someone else what's happened.\n \n \n \n \u2022 Lie down flat on your stomach so nothing touches your neck or face.  Don't lie down flat on your back unless told otherwise by a healthcare provider.  Never sleep lying flat on your back following a suspected concussion.  This position increases pressure inside the skull which can worsen bleeding around the brain.  Keep your eyes closed during this time period.  Try to relax and breathe deeply.  Stay calm and quiet.  Talk only to those who ask questions about how you're feeling.  Let people know where you live and work information like phone numbers and addresses.  Tell them whether anyone needs transportation home.  Make sure there\u2019s someone available to drive you home safely.  Remember \u2013 stay calm!  Don\u2019t try to talk too much!","label":1,"model":"bloomz","source":"wikihow","id":266}
{"text":"LINQ stands for Language-Integrated Query (query language integrated into the programming language). It allows you to query databases without having to write any SQL statements yourself.  You do not need an existing database before creating one using LINQ:  Instead,... \nIn order to connect to SQL server through Linq-to-SQL, you'll first need to:\n\nOpen up Visual Studio Express 2013 Community Edition if it isn't currently opened. \nRight-click on \"My Projects\" in the left-hand pane under Solution Explorer. \n\nSelect \"New \u2192 Class Library\".\n\nGive your project a meaningful name such as \"LinqToSQLExample1\".  \n\nNext, right-click again on \"My Projects\" in Solution Explorer.    Select \"Add New Item\u2026\".     Choose \"C# Windows Form Application\".      Give your form a meaningful name like \"UserForm1\".        Double-click on the UserForm1 designer window.       Drag out two text boxes named txtFirstname and txtLastname.   Place another button called cmdSave onto the form.         Double-click on the Command Button's properties tab.        Set its Text property value to \"Submit Changes\".        Press F5 to run the application.","label":1,"model":"bloomz","source":"wikihow","id":267}
{"text":"The following steps are intended to be used with Microsoft Office 2011 Professional Plus.  If you're using other software such as OpenOffice\/LibreOffice\/Google Docs etc... , you'll need to adjust accordingly.   This method can also be applied to any 3D curve whose coordinates are known along its length.    Note:  It should take about 20 minutes per spheroid if done manually; however, there exists a macro written by user jmharrison that does all calculations automatically within seconds!    To download his macro click here. You'll find it useful once you've mastered how to calculate arc lengths yourself!\nThis example uses the same spreadsheet file as the previous examples found here: Revised-Garthwaite Curve.zip .\nYou must first open up the original file before continuing!  Once opened, go to: File > Save As....   Click \"Save As Type\":  Then choose \"Comma Separated Value (.csv).\"  Name the file appropriately so it's easy to recognize later. (e.g. \"Garthwaite\").  Choose where you'd like to store the file.     Now close out the original file.       Next,...","label":1,"model":"bloomz","source":"wikihow","id":268}
{"text":"Gothic rock has been around since the 1980s but only recently have people started taking notice of its popularity.  It was originally created from heavy metal fans who wanted something different; they were tired of all the same old songs about demons and vampires.   They loved heavy metal so much that they decided to create their own genre which incorporated elements of classical music along with other influences.    This new type of music became known as \"black metal\" because many of these musicians wore black clothing while performing on stage.   In fact, some of them even dyed themselves black!    Eventually this movement spread across Europe where there were already underground groups creating alternative forms of music including death metal, doom metal, thrash metal, grindcore, hardcore punk, etc...   These genres eventually merged together forming what we now know today as \"gothic\" music.   There are several subgenres within the realm of gothic music ranging from symphonic black metal through melodic death metal to atmospheric folk metal.   Some examples include symphonic black metal,...","label":1,"model":"bloomz","source":"wikihow","id":269}
{"text":"If you're over 40 years old (or even younger), then chances are that you've heard some version of this advice before.  But how many people actually follow through with these tips?  If they do work as well as advertised,  why don't we see more women who appear decades younger than their actual age?\nThe truth is that there really aren't any miracle cures out there.   The best thing anyone can do in order to maintain her youthful appearance  is simply to take good care of herself throughout life - from childhood right up until death.    This means eating healthy food,    exercising regularly,   drinking plenty of water    and avoiding stressors like cigarettes   and excessive amounts of alcohol.   It also includes taking steps now to prevent wrinkles by applying sunscreen every day and moisturizing often.    ...more information here....please click link below..... \n \n \n \n How To Look Years Younger: A Guide For Women Over Forty By Dr Wayne Jonas Reviews: 4 stars \n \n \n \n How To Keep Looking Young At Any Age By Suzanne Somers Reviews: 3 stars","label":1,"model":"bloomz","source":"wikihow","id":270}
{"text":"The following information should provide some basic guidelines regarding diagnosing meningitis.  If you're interested... Read More \u00bb\n, How Do You Treat Bacterial Meningitis?\nBacterial meningitis can only be cured by antibiotic therapy.   The type of medication used depends upon which bacteria are causing the disease.    Antibiotics must reach high levels within the cerebrospinal fluid surrounding the brain and spinal cord to effectively treat this condition.   This requires intravenous administration of drugs because oral forms cannot penetrate the protective membranes covering these areas.   It usually takes several days after starting an antibiotic regimen until its effectiveness becomes apparent.... \nInfants younger than three months old require special attention due to their increased risk of developing severe complications associated with meningitis.   They need prompt diagnosis and aggressive management including hospitalization at specialized centers where intensive care facilities exist.   Children between 3 months and 2 years old present similar challenges but generally respond better to treatment than infants.  Treatment options include both parenteral and oral antibiotics depending on the severity of illness,...","label":1,"model":"bloomz","source":"wikihow","id":271}
{"text":"If you're going to prison, it's important that you let your employer know before it happens.  This will give them time to prepare for what is coming next.   If they don't hear until after you've been sent away, they'll probably be very surprised by the news.    It's also helpful to inform your employer so he can start looking into alternative arrangements for work during your incarceration (if applicable).  For example, some employers may allow their employees who go to jail to continue working remotely via computer.     In addition, letting him\/her know ahead of time gives both parties more time to discuss whether you'll still receive health insurance benefits once... Continue reading at https:\/\/www.wikihow.com\/Tell-an-Employer-You-Are-Going-To-Jail#.Wp-BKZqJbE.3    Once you find yourself facing criminal charges, gather up everything related to those charges including court dates, bail amounts, etc..     Make sure you understand exactly where these charges come from and why they're being filed against you.  Knowing this information beforehand helps you decide which details you'd like to share with your employer.  It might even save you money on legal fees down the road!   Don't assume anything!  Even though you think something won't affect your ability to keep your job, ask someone knowledgeable such as a lawyer or paralegal to confirm this fact for you.  Remember - ignorance isn't bliss here!    The most common reason people end up behind bars is because they've violated probation terms set forth by judges and\/or courts.  These violations include things like:  missing scheduled meetings with your probation officer; failing drug tests; breaking curfew rules; committing new crimes; leaving home without permission; using alcohol\/drugs\/other substances inappropriately; having unapproved contact with certain individuals....","label":1,"model":"bloomz","source":"wikihow","id":272}
{"text":"The following procedure describes how to change your timing belt if it's broken.  If you're replacing the belt for other reasons such as routine maintenance then skip down to Step 5 below.   This guide assumes you've already removed all belts connected to... The timing belt should last between 60k miles - 100k miles depending upon driving conditions so don't panic if yours has lasted longer!   You might want to consider getting an extended warranty before doing this job though!  There are many videos available online showing exactly what needs to happen here.    It takes some practice to get used to working underneath the vehicle like this; however once done you'll find yourself able to do most jobs without too much trouble!\nIf you'd rather take advantage of our expertise we offer a full service at competitive prices including labour & parts. Call us now on 01625 7340007\/7350005 or email [email protected] . \nYou could also use a 'timing light' instead; however there isn't really any point unless you plan to sell the car again soon after changing the belt anyway since the mileage reading would still be wrong even if everything was perfect.     It's best to check both sides of the belt while removing it; however only replace the belt itself....","label":1,"model":"bloomz","source":"wikihow","id":273}
{"text":"The Graduate Record Examination is an entrance examination administered by Educational Testing Service (ETS). The purpose of this standardized test is to assess your knowledge base across several academic disciplines.  You can choose which subjects you'd like to take based upon what programs you're applying into.   In order to be considered for graduate school admission within most fields related to literature, you'll likely want to score well enough on both sections of the English Language Arts portion of the General GRE.    This includes reading comprehension questions, vocabulary questions, writing sample essays, and analytical reasoning skills questions.  For more detailed information about how to apply for the GRE, please see our \"How To Apply for the GRE\" guide here.. , http:\/\/www.ets.org\/gre\/subjects\/english-literature\/literary-arts-in-general-gre-subject-test\/overview\/index.html]  Go to the Educational Testing...   - Take advantage of online tools such as: [http:\/\/www.gmatprep.com\/resources\/general_gre_subjec t_tests.aspx?id=2][http:\/\/testmasters.com]","label":1,"model":"bloomz","source":"wikihow","id":274}
{"text":"Windows Vista introduced several advanced power saving features such as:  Idle Time Limitation: The operating system limits how long applications can be left running without user interaction.  Standby Mode: When you are not using your PC or laptop, it goes into standby mode where only essential components like hard disk drive spin up while other devices remain powered down.   Hybrid Boot: If there was no activity from the last boot cycle (e.g. mouse movement), then the next boot cycle starts with minimal services loaded so less energy is consumed during this period.    In addition, some hardware manufacturers have also implemented their own proprietary power-saving solutions through software drivers provided along with their products. These include:    AMD Cool'n'Quiet Technology: It allows users to adjust processor clock speed according to workload requirements.     ATI Catalyst Control Center: Users may choose different levels of performance depending upon application needs.\n    ... \nIn order to save battery life when working remotely over wireless networks:   Turn Wi-Fi radio OFF if you're not connected to anything. (This includes Bluetooth.)     Set screen resolution lower than native resolution.      Reduce brightness level.        Lower CPU frequency settings.       Disable all unnecessary visual effects.        Uninstall unused programs.        Avoid playing games or watching movies online.","label":1,"model":"bloomz","source":"wikihow","id":275}
{"text":"Bullying in schools occurs every day.  There are many different kinds of bullying but there are only two categories of people who engage in it:  Bullies: People who intentionally cause harm to another person through physical contact such as hitting, kicking, pushing etc.   Victims: People who get harmed physically because someone else wants it to happen.    The first type of bully will be obvious - you'll know right away whether it's happened to... Continue reading at wikiHow.com.... \nThe most common form of bullying happens between children themselves rather than from one child towards an adult.  This kind of bullying usually involves name calling, teasing, rumours spread about victims,  threats made over social media sites like Facebook, Instagram, Snapchat, Twitter, YouTube, Tumblr, Vine, WhatsApp, Kik Messenger, Line, WeChat, Skype, Viber, iMessage, Google Hangouts, emailing pictures\/videos\/links containing sexual images\/messages\/songs, spreading rumors online\/in real life, making fun of someone's appearance, stealing things belonging to the victim(s), threatening to hit\/kick\/punch\/hurt the victim(s).   In some cases this may also involve cyberbullying using technology including cell phones, computers, tablets, iPods\/iPhones\/iPads, video games consoles, smart watches, GPS devices, digital cameras, MP3 players, PDAs, electronic readers, portable music players, televisions, radios, satellite receivers, DVD\/Blu-ray players, set-top boxes, gaming systems, home theatre equipment, networked appliances, wearable tech gadgets, internet connected toys\/gadgets\/devices\/technologies, Bluetooth enabled products, Wi-Fi routers\/wireless access points, webcams, surveillance cameras, security alarms, remote controls, thermostats, smoke detectors, baby monitors, fitness trackers, heart rate monitors, blood pressure cuffs, glucose meters, insulin pumps, pacemakers,...","label":1,"model":"bloomz","source":"wikihow","id":276}
{"text":"If you've been wanting to see one particular rock group perform live but haven't gotten permission from your parents yet, this guide should help.  The first step towards convincing your parents to let you attend an upcoming concert (or any event for that matter) is planning ahead.   This means knowing exactly what kind of concert you'd like to see before approaching your parents; it's also important to know where the concert is being held and when.    Once you've done these two things, make sure you ask your parents whether they would mind paying for tickets themselves instead of having you do it; they'll probably feel more comfortable letting...   - Ask your parents why they're against allowing you to go to the concert alone....  - Show your parents examples of other kids who've gone to similar events without parental supervision,...    - Explain to your parents that there'll only be a limited number of people attending the concert anyway,...  ...and don't forget to mention that there's no guarantee that you'll actually meet anyone interesting while waiting outside the concert hall! ...","label":1,"model":"bloomz","source":"wikihow","id":277}
{"text":"Ashley Tisdale has been known as one of pop music's most fashionable stars since her days on Disney Channel.  She often wears clothes from popular brands like Tommy Hilfiger,  American Eagle Outfitters  and Forever 21.   Her signature hairstyle consists of long layers cut into an angled bob; she also frequently changes up her color by dying it blonde, brown, red, black, purple, pink, blue, green, orange, yellow, white, gray, silver, gold, platinum, ombre, balayage, streaks, highlights, lowlights, spikes, waves, braids, twists, cornrows, Bantu knots, box braids, French braids, Senegalese twists, rope braids, fishtails, ponytails, buns, half-up\/half-down styles, side-bangs, bangs, extensions, and more.    Here are tips on how to dress just like Ashle:   Style 1: Long Layered Bob Cut - This is probably the easiest way to get Ashley's look because all you'll need...","label":1,"model":"bloomz","source":"wikihow","id":278}
{"text":"Painting chairs is an easy way to change their look without spending too much money.  You don't need any special skills - just some basic tools like paintbrushes, rollers, sponges etc.. This guide will show how to do this step by step.   If you're not sure what kind of paint would be best suited for your project, check out our other guides on How To Pick The Right Paint For Your Project  and How To Mix A Color Swatch In Paint .\nBefore starting make sure that:   1. You are working outside or somewhere well-ventilated because most paints contain volatile organic compounds which may cause health problems when inhaled.    2. You wear old clothes so they won't get dirty from paint splattering.     3. You protect yourself against harmful substances such as solvents used during cleaning process. 4. You cover all surfaces nearby where you work with newspaper or plastic sheets. 5. You prepare everything you'll need according to the list below. 6. You choose appropriate furniture depending on its material.","label":1,"model":"bloomz","source":"wikihow","id":279}
{"text":"Tiropita is an authentic Greek dish made from layers of thin phyllo dough filled with creamy goat's milk cheese.  The name means \"filled triangles\" because each triangle-shaped piece has been folded diagonally then straight across like a book page before being fried.   This recipe makes enough fillings for 12-15 pieces depending upon how large they are when finished.    Crumble the feta cheese by placing it inside a plastic baggie and using your hands to break apart any lumps.     Mix the beaten eggs into... Continue reading ... .     Cut the filo sheets into 10 cm (4 inch) wide strips....    Filling:  Butter:   Assembling the tiropitai:  Cooking:  Finished!:  Enjoy!:  Variations:  .         .\n\nIf you'd prefer not to use all-purpose flour as instructed above, try this variation instead.  In place of 1 cup (240 ml), mix together 1\/2 cup (120ml) plain yogurt and 1\/4 teaspoon salt.  Add 2 cups (480 ml) whole-wheat bread crumbs slowly while stirring constantly.  Keep mixing until everything comes together smoothly.  You can also add some chopped parsley if desired.  If you're looking for something sweet rather than savory, replace the feta with cream cheese mixed with sugar and cinnamon.  For another twist, sprinkle paprika powder along both sides of the filo after cutting but prior to rolling out.  To make mini-tropitai, roll the filo thinner; fold each triangle in half horizontally first followed by vertically.  Alternatively, bake the rolls flat-side down on parchment-lined cookie sheets covered lightly with olive oil spray.  They will take less time to cook since they're smaller.  Serve these appetizers alongside tzatziki sauce and\/or hummus dipping sauces. .","label":1,"model":"bloomz","source":"wikihow","id":280}
{"text":"Giving a thoughtful present is one way we show our love towards others.  It can be difficult finding just the right thing when it comes time to buy a birthday gift.   This wikiHow will teach you some ways to find the best possible gift for anyone from family members to coworkers.    Start by thinking about why you're giving this particular person a gift.  Is there a special reason?  Are you celebrating his or her birthday?   If so, you'll need to pick up a few more items than usual because it's customary to get people several presents during their birthday celebrations.  For example, if they're turning 21 years old,... ...more\n...more...    You may also consider making a homemade gift instead of buying one.  Making things yourself shows dedication and effort which makes it even better as a gift.  Try baking cookies or cupcakes decorated especially for the recipient's favorite color or flavor.  Make sure not to make too many mistakes while cooking!  Otherwise, they'll think you've forgotten all about their tastebuds!","label":1,"model":"bloomz","source":"wikihow","id":281}
{"text":"This section will provide detailed instructions about installing your own baseboard heaters.  This method can be used with either gas-fired or electrically powered baseboards.   If you're unsure whether you'll need permits and\/or inspections before starting work,...   You may also want to consider hiring someone who specializes in these types of installations instead....  For safety reasons it would probably best to hire... A typical home's basement area might have concrete floors but most other areas should have wood flooring unless otherwise specified.    In some cases, such as when using natural gas, it's possible to use existing piping systems rather than having to run new lines.     However, depending upon local regulations, you could still require a permit even though you've already got pipes running through the ceiling or elsewhere in the house.    It'll take longer to get the job done because you'll likely have to wait until after business hours to do so; however, it'll save money since you'll only pay labor costs once per hour regardless of what time they start working.       Depending upon the complexity of the installation project, expect to spend anywhere from $150 - $1200+ dollars just getting everything hooked up properly.        It's important to note here that many electricians charge extra fees based on their experience levels; therefore, don't assume that all electricians charging less than others necessarily aren't qualified enough to complete the task safely and correctly.         Electrical contractors typically charge higher rates due to the fact that they're licensed professionals whose services include things like design\/planning, permitting, inspection\/review, testing\/evaluation, troubleshooting\/support, maintenance\/care plans, warranty service agreements, emergency response support, etc..","label":1,"model":"bloomz","source":"wikihow","id":282}
{"text":"The following is an excerpt from How To Do It Yourself Car Maintenance & Repair Guide published by McGraw-Hill Book Company.  This guide was written specifically for DIY car enthusiasts who want to save money over taking their cars to mechanics or dealerships.   You may find this book useful if you are looking for more information beyond what we have provided here.   ... If you're going to do it yourself, you'll need some basic tools like sockets, screwdrivers, clamps etc.. You'll also need something called a \"spark plug wrench\" which looks sort of like this:  A good quality set will cost around $25 - $30 USD but they last forever!   There should only ever be one spark plug per cylinder; there shouldn't even be two!  Each spark plug has its own number stamped onto it; these numbers correspond directly to where they're supposed to go when reinstalled.  For example, if there's a \"1\" stamping next to the tip of the plug then it's meant to go into the first hole down below the distributor cap.     Make sure you've got all four new ones installed before turning the key....","label":1,"model":"bloomz","source":"wikihow","id":283}
{"text":"Tigers are one of the most popular animals used as cartoon characters because they have such an interesting appearance.  This tutorial will show how easy it can be to make your own cartoon tiger using basic shapes.   Start off by making two circles joined by three short vertical lines (see Figure 1).   The first circle should represent the forehead while the second represents the chin.    You may also want to add some horizontal lines between these two circles so that there appears more depth when viewed from behind.     Make sure all four ends meet up neatly before continuing onto step 3.    To create the neck area, connect both circles with another pair of diagonal lines running parallel to the ones already drawn. (See figure 2.)   These new lines need not go through either... Continue reading \u2192\n\nNext, draw a large \"U\"-shape extending downwards from the middle point of this cross.  (See figure 4.)     Now we start adding details to our tiger face!   First, draw a small triangle pointing upward next to the tip of the lower half of the \"U\"-shape you've created earlier.   (This forms the mouth.) (See figure 5.)   Then, draw a smaller triangle pointing downward below the larger triangle you made previously. (This creates the snout.)  Finally, draw a tiny oval inside the snout to give him\/her a cute nose! (See figures 6 & 7.)   For the ears, simply extend the uppermost parts of the \"U\"-shapes into pointed triangles or cones. (See figure 8.)   If you'd like to add eyebrows, do so now!  Simply draw a couple of dots along the tops of the ears. (See figure 10.)   Don't forget to include the pupils too - they're very important if you're trying to get the look of a real tiger!  (You could even use a black marker pen here!)    (If desired,...","label":1,"model":"bloomz","source":"wikihow","id":284}
{"text":"This wikiHow teaches how to set up an entire town with buildings, roads, parks, stores, schools, police stations, firehouses, hospitals, libraries, churches, factories, farms, waterways, bridges, airports, train tracks, highways, subway lines, trains, buses, cars, trucks, boats, planes, helicopters, tanks, soldiers, policemen, firefighters, doctors, nurses, teachers, students, parents, children, pets, animals, trees, grasslands, forests, mountains, rivers, lakes, oceans, islands, deserts, jungles, swamps, caves, tunnels, docks, lighthouses, forts, castles, palaces, mansions, towers, skyscrapers, swimming pools, beaches, piers, harbors, marinas, yacht clubs, golf courses, tennis courts, bowling alleys, amusement arcades, rollercoasters, theme parks, aquariums, zoos, museums, concert halls, arenas, stadiums, gyms, fitness centers, laundries, dry cleaners, grocery stores, supermarkets, convenience stores, gasoline stations, car dealerships, auto repair shops, banks, credit unions, insurance companies, stock exchanges, post offices, courthouses, jails\/prisons, government agencies, military bases, national guard units, emergency services, volunteer organizations, community groups, fraternal orders, social service organizations, youth groups, senior citizens associations, retirement communities, nursing home facilities, daycare centers, animal shelters, homeless shelter, soup kitchens, food pantry, recycling plants, waste management plants, cemeteries, graveyards, crematoriums, funeral parlors, mortuaries, mausoleums\/tombstones, gardens, orchards, vineyards\/wineries, greenhouses, farm fields, livestock barns, poultry sheds, dairy farms, horse stables, dog runs, catteries, rabbit hutches, chicken coops, pig pens, sheep pastures, goat corrals, cowsheds; along with their respective inhabitants.  This guide also covers creating towns from scratch using Microsoft Paint,...","label":1,"model":"bloomz","source":"wikihow","id":285}
{"text":"Chili con carne (Spanish pronunciation:[\u02c8t\u0283\u026ali k\u0254n \u02c8k\u0251rn\u025b]) or simply \"chili\" refers to stewed red chilies cooked together with ground beef and\/or pork.  It originated from Spain but has since become popular throughout Latin America.   Chile con carne can be made mildly spiced by using green bell pepper instead of jalape\u00f1o; however, most recipes call for one or two hotter varieties such as serrano or habanero.    This recipe uses both types of peppers along with tomato paste which gives an authentic Mexican taste.     If you're looking for something different try our Thai Red Curry Chicken!    For those who don't eat meat there are plenty of vegetarian options available including our Vegetarian Green Bean Casserole!  You may also like:  How To Make A Pot Of Rice   How To Prepare An Authentic Paella   How To Make Jambalaya... Preheat oven to 350 degrees Fahrenheit(180 Celsius). Cut the tops off of four large roma tomatoes then slice each half lengthwise. Place sliced tomatoes onto baking sheet lined with parchment paper. Bake until tender, 30-40 minutes. Remove from oven once done. Meanwhile prepare the remaining ingredients. In small saucepan combine olive oil, minced garlic cloves, diced onion, salt, black pepper, cayenne pepper, paprika, sugar, vinegar, water, and crushed red pepper flakes. Bring mixture to boil over high heat. Once boiling reduce heat to low-medium setting and allow to cook uncovered for 10-12 minutes stirring occasionally. While vegetables are cooking saut\u00e9 1 pound lean ground beef in another skillet over medium-high heat. When browned remove from heat and set aside. After 12 minutes take out the roasted tomatoes and place them directly into the vegetable mix. Return the entire contents back to stove-top and bring to boil again. Allow to simmer covered for 15-20 minutes while stirring frequently. Pour through strainer after finished. Season with additional salt and pepper according to preference. Enjoy immediately.","label":1,"model":"bloomz","source":"wikihow","id":286}
{"text":"This method is not suitable for delicate leaves like orchid leaves.  This technique will preserve the texture and color but may cause some distortion due to shrinkage during drying process.   You should be able to find this type of resin online or local craft stores that sell arts & crafts supplies.    Clear Polyester Resin - The most common kind used by artists who cast their own sculptures are made out of two components; liquid resin (resin) and hardener\/catalyst(catalytic).    Make sure both parts come separately packaged so they don't mix together until you're ready to use them.     Hardener\/Catalyst - Catalyst works best if it has been stored under cool temperatures since it's more active when cold. If possible store it in refrigerated conditions prior to opening the package.    It also helps to keep the container tightly closed once opened to prevent evaporation which would make... How do I prepare my materials? What does the mixture look like? How long must I wait between coats?  When am I done painting?","label":1,"model":"bloomz","source":"wikihow","id":287}
{"text":"Incorporating is one way that small businesses can separate themselves from their owners.  This process allows them to limit personal liability by establishing limited-liability companies.   In addition, corporations may enjoy tax advantages over sole proprietorships.    The following steps outline how to incorporate as a general partnership with shareholders in Georgia:  If you're interested in incorporating without any shareholders, see How to Start Your Own Corporation Without Shareholders instead..    You must file articles of organization with the Secretary of State's Office before starting operations under your corporate... Continue reading \u2192\n\nThe first step towards forming a company involves choosing its legal form.  There are several options available including partnerships, LLCs, S-corporations, C-corps, and other forms such as trusts and estates.  Each has different benefits depending on what type of entity it represents.  For example, if you want to start a restaurant chain, then you'd probably choose a C-Corp because this would allow you to raise capital more easily than most other types of entities.  However, if you just wanted to open up shop selling handmade jewelry online, then perhaps a simple LLC might work better since there wouldn't need to be much investment involved.     Partnerships: A partnership refers to two or more people working together to run a business.  Partners typically contribute money and\/or labor into the venture but they also assume joint responsibility for all debts incurred during operation.  They cannot operate independently unless they agree otherwise.  Unlike corporations, partners do not own stock; rather, they hold equal ownership interests known as \"partnership interests\" which give rise to unlimited personal liability.  An operating agreement should always accompany a partnership so that everyone knows exactly what's expected....","label":1,"model":"bloomz","source":"wikihow","id":288}
{"text":"Autumn is one of my favorite seasons.  I love everything about it:  The colors; the smell of burning leaves underfoot after a rainstorm; the crunchy sound they make when walking through them; the way sunlight streams through bare branches at dusk.   Autumn inspires me creatively every year - this time around I've decided to celebrate its beauty using miniature watercolors.    Here are some tips to get you started creating your own mini-autumn masterpiece!    1) Make a list of words that describe what makes autumn special to you:   Reds?  Yellows? Pumpkins?\n2) Gather inspiration:     Go online.     Read books.     Take pictures.     3) Begin cutting your canvas:     Choose an appropriate sized sheet of watercolor paper (or several sheets).  You will need enough room to create multiple small paintings within a single large square\/rectangle frame.  4) Create shapes cut-outs:     Using scissors, draw free hand lines across the page forming squares and\/or rectangles.  These will serve as frames for your miniature paintings.      5) Paint your first rectangle\/square:     Pick...","label":1,"model":"bloomz","source":"wikihow","id":289}
{"text":"Rabbit owners often find themselves wanting their pets to be more tame so they can carry them around without being bitten.  This guide shows several ways to safely handle any size rabbit.   The most important thing about picking up an animal like this one is to make sure you're gentle but firm at all times; if you are unsure whether something feels wrong, stop doing what you\u2019re doing immediately!   If you've never picked up your rabbit before, start small - just try touching him\/her lightly until he\/she gets used to having his\/her paws touched.    Once... Continue reading \u2192\n\nIf you'd rather read our tips instead of watching videos, here are some things you'll need to know:  Never grab a rabbit from below it's belly as this could cause serious injuries such as broken ribs and\/or internal organs damage. Do NOT pull a rabbit away from anything unless absolutely necessary \u2013 even if he doesn't want to go there now!  Rabbits have very delicate bones which break easily, especially those near joints where muscles attach.     Always keep your rabbit calm during transporting. Make sure he's relaxed enough to fall asleep.","label":1,"model":"bloomz","source":"wikihow","id":290}
{"text":"This is an excellent recipe because it's very simple but tastes great.  You don't have to do anything special except save everything from scraps when you're preparing food.   This includes saving all chicken parts that are leftover... Continue reading \u2192\n, which include bones and skin;  Save all vegetable parts such as:   Combine these ingredients together as follows:    Add seasonings like herbs, etc.    Strain out vegetables before storing broth.     Freeze any extra liquid in small containers so you'll always know how much you've got available.      Enjoy!     If you'd rather just keep things simpler than making this delicious soup\/broth combination, try my other recipes:      How to Make Homemade Bone Broth - A Simple Recipe For Healthy Living        How to Make Your Own Beef Stock       How to Make Your Own Fish Stock         ...more... \nYou might want to consider adding more veggies depending upon what type of flavor profile you prefer. The longer you leave the bone-in pieces of chicken in the slow-cooker, the richer the flavors become.","label":1,"model":"bloomz","source":"wikihow","id":291}
{"text":"You can make these at home using common household items.  These are great for target practice or just shooting fun! You will be able to shoot them outdoors if there is no wind because they don't travel very far.   This video shows how easy this project really is:  https:\/\/www.youtube.com\/watch?v=hvWwLZq7yYM   The following materials should work well enough but feel free to experiment until you're satisfied.    1 piece of paper (any size)1 pencilRubber bandsBullet(s) (paper works best)  How to:    Cut off one end of an old pen roll so that it's about 2 inches long.     Fold over both ends of the pen roll by half inch each time. (This makes sure all sides have equal length.)     Put some glue on top of the folded edges where you've made creases.     Glue down the bottom edge too.     Once dry, cut along the fold lines.     Open up the pen roll carefully.     Wrap the rubber band tightly around the center part of the pen roll.     Fit the bullet into the hole created when cutting open the pen roll.     If necessary use tape to hold everything together.     Set aside while drying.     When done remove the tape gently.     Remove the...","label":1,"model":"bloomz","source":"wikihow","id":292}
{"text":"Veruca Salt was played by actress Margaret O'Hara as one of the main characters in the 1971 film adaptation of Roald Dahl's novel \"Charlie and the Chocolate Factory\"; she also appeared briefly in its 2005 sequel.  She is described as \"the most spoiled child ever born into this world!\"  Her character has been portrayed many times since her debut; however, none have captured the essence quite like O'Hara.   In fact, it can still be argued today whether anyone could play the role better than O'Hara did back when she first took part in the movie.  Here we will give some tips about how to act like Veruca Salt based off of O'Hara's performance.    First impressions matter!  When meeting someone new, make eye contact before greeting them so they know... You may think it's impossible to look exactly like Veruca Salt because she's such an iconic figure -- but that's just not true!   The biggest thing you'll need to do is find out where to buy all those fancy clothes and accessories that Veruca always wore throughout the entirety of the movie....","label":1,"model":"bloomz","source":"wikihow","id":293}
{"text":"The first step towards understanding how computations can help us understand chemistry better involves learning about some of the various computational techniques that have been developed over time.  These include quantum mechanical methods like density functional theory (DFT), semiempirical methods like AM1, PM3 etc. , force fields , Monte Carlo simulations  and molecular dynamics .\nQuantum Mechanical Methods\n\nDensity Functional Theory(DFT)\nIn DFT one starts from the Schr\u00f6dinger equation:\n$$ H |\\psi\\rangle = E |\\psi \\rangle $$ \nwhere $|\\psi\\rangle$ represents wave function of the system under consideration . This equation has no closed form analytic solution except when dealing with very small molecules e.g Hydrogen Atom. \nHowever it does provide useful information regarding ground state properties of many... Quantum mechanics describes matter at atomic level using concepts such as particles called fermions - neutrons,... How do you learn more? Click here! \n \n \n \n Semi Empirical Methods:\nThese methods use experimental data alongwith ab initio results to predict physical parameters of large number of molecules accurately within reasonable amount of computer resources available today.   They are based upon fitting calculated values against experimentally determined ones so they tend to work best if there exists sufficient experimental data for comparison purposes.    Examples include:    *AM1: Absolute Molecular Modeling   *PM3: Parametric Model 3","label":1,"model":"bloomz","source":"wikihow","id":294}
{"text":"Personal watercraft have become increasingly popular over recent decades.  They offer recreational opportunities like kayaking but without having to paddle.   The popularity of personal watercraft continues to grow worldwide.    This section will provide you information regarding safety precautions related to riding a personal watercraft including basic operation procedures and common sense tips from experienced riders who want others to enjoy their experience safely too!   You may also find it helpful to read our articles on:  How To Kayak Safety Tips For Boaters    Before getting into... more \u00bb\n\nThe following sections contain detailed instructions on how to:     Securely attach the PWC lifeline to your lifejacket.     Start up an electric-powered PWC.     Run gasoline powered PWCs.     Navigate safely.     Stay safe during inclement weather.     Avoid collisions with other vessels.     Respect private property rights.     Enjoy yourself responsibly!     Read the owner\u2019s manual carefully.     Keep track of important maintenance items.     Take care after use.     Get help quickly.","label":1,"model":"bloomz","source":"wikihow","id":295}
{"text":"This method works best for people who already know how to bleach their own hair.  This technique does require some patience as well because of all the waiting times involved before applying each new colour.   You may need more than one box of markers depending upon what shade(s) you choose.    If this sounds like too much work, try using an ombre effect instead!   The following are the steps you'll follow when doing this:  Assemble your supplies:    1 pack of permanent markers (red, orange, yellow, green, blue & purple)  A comb Hair ties Bleaching cream Styling gel Blow dryer Combine equal parts hydrogen peroxide and developer into bowl until mixed thoroughly. (Mixture should resemble thick paste.) Mix together 2 tablespoons of bleaching cream per every 10ml of mixture from previous step. Apply bleaching cream evenly over entire length of hair starting at scalp. Leave in 30 minutes under plastic wrap while mixing next part. Rinse off bleaching cream completely after 30 minute wait period. Dry hair completely by blow drying.","label":1,"model":"bloomz","source":"wikihow","id":296}
{"text":"The following recipe will make enough pastry for four people.  If you'd like more information about how to prepare your own Greek sweet breads from scratch using traditional recipes passed down through generations, please see our full guide here! .\n1 tablespoon olive oil 2 cups warm water 3 1\/2 tsp honey 4 tbsp white vinegar 1\/4 cup salt 3\/4 cup all-purpose flour 1 packet active dry yeast 1 cup granulated sugar 1 cup whole milk 1\/3 cup melted unsalted butter 6 oz. (180 g.) grated Parmesan cheese 8 oz. (230 g.) shredded mozzarella cheese 12 oz. (340 g.) ricotta cheese 2\/3 cup finely chopped parsley Salt & pepper to taste . Preheat the oven to 375\u00b0F\/190\u00b0C. . In a small mixing bowl combine the first five ingredients; stir well to dissolve the honey completely.   Pour over the baking sheet lined with parchment paper and spread evenly across surface.... ...more\u00bb\n.","label":1,"model":"bloomz","source":"wikihow","id":297}
{"text":"Do not allow anyone to influence how you provide patient care.  Be honest with yourself.   Know your rights.   Keep track of interactions with medical professionals.   Avoid conflicts of interest.   Don't give up!   Get help. If you're feeling overwhelmed...\"  Read more at: http:\/\/www.wikihow.com\/Be-a-Health-Care-professional-in-the-U-S#Patient-care . This wikiHow teaches you how to become an effective healthcare professional in the United States (US). It covers topics ranging from dealing with difficult coworkers to avoiding conflict-of-interest issues.... \" How to Become A Healthcare Professional In The US - Part 1 ... http:\/\/bit.ly\/1qJZxjF #healthcareprofession #nursing\" \"How To Become An Effective Nurse In America | Nurses Need You\" https:\/\/youtu.be\/yxKmLhvWwYM?t=1110 \"This video was made possible through funding provided by: https:\/\/www.youtube.com\/watch?v=z7bGk9V5Hr4&feature=youtu.be","label":1,"model":"bloomz","source":"wikihow","id":298}
{"text":"Potstickers, also known as guo tie \u9903\u5b50, are steamed Chinese dumplings filled with savory fillings such as pork mince mixed with various aromatics.  They can be made using either freshly-made wrappers that have been left out overnight so they become soft enough to fold over without breaking, or by making them yourself.   This recipe will show you how to make both types of wrappers.    If you're not sure where to find these ingredients, check an Asian grocery store near you!   You may want to add some extra spices if you'd like; however, this is completely up to personal preference!  The exact amounts used here should give you a flavorful mixture that's perfect for...    For example, try minced garlic instead of peeled cloves, or use green beans instead of carrots!\nIf you've never tried making your own wrappers before, it's best to start off with just half a cup of cold water first - then slowly work your way towards 1 full cup once you get the hang of things!","label":1,"model":"bloomz","source":"wikihow","id":299}
{"text":"If you're interested in taking on more responsibility within your political party, consider running for an official position with your local party organization.  This may seem intimidating but it's actually quite easy once you've got some experience under your belt.   The first step towards becoming involved locally is to become active in your neighborhood - start attending meetings regularly and volunteer wherever possible.    Once you've gained enough... Continue reading \u2192\n\nThe following steps outline how to take control of your local party precinct.  You should already have taken care of all the necessary paperwork needed to register yourself as a member of your chosen political party.  If not, see How To Register As A Political Partisan In Your State  Before proceeding any further.  \n\n1)  Run For Office At The Precinct Level   There are many different positions available depending upon which part of the country you live in:    Captain\/Committee: These people represent their party's interests during primaries\/caucuses and help elect delegates who go forward into higher levels of government.  They also serve as liaisons between the party members they represent and other parties' representatives in order to keep everyone informed about what's going on in the community.     Treasurer: Responsible for keeping track of funds raised throughout the year so that money isn't wasted unnecessarily.\n    2)    Start Getting Signatures On Petitions     It might sound like a lot of work now, but you'll thank us later!","label":1,"model":"bloomz","source":"wikihow","id":300}
{"text":"If you're using Windows XP Professional edition instead of home edition then skip ahead to step 5.  If you've installed an alternate version of IE such as Microsoft's beta testers release you'll need to uninstall it before proceeding with disabling Internet Explorer.   You can download Firefox free of charge at:  http:\/\/www.mozilla-europe.org\/projects\/firefox\/index_en.html   Mozilla Thunderbird - A Free Email Client:  http:\/\/downloads.mozillamessaging.com\/thunderbird\/releases\/thunderbird-3.0.6en-us-win32-x86-official-release.exe    Opera:  https:\/\/www.opera.com\/en\/download\/desktop\/... This will allow you to change which programs are allowed to run when windows starts up.    The next screen should show two options; one labeled \"Programs\" and the second \"User Accounts\".     In order to do so open \"My Computer\" in File Explorer and select \"Computer\" from the top toolbar....     For example, if you want to set Google Chrome as the default internet explorer replacement but don't see it listed there,  go back into Control Panel > System> Advanced system settings > Environment Variables...","label":1,"model":"bloomz","source":"wikihow","id":301}
{"text":"Sewing a simple vest requires very little material but will make an excellent addition to any wardrobe.  This tutorial shows how to create a basic vest using only three pieces:  The first step involves creating patterns based off of a shirt that fits well.   You may also use other types of clothing such as sweaters if they are loose enough to fit over your chest comfortably.    If you're not sure what size to cut each piece you'll need to try them on before cutting!   For this project we used cotton jersey knit which was purchased at our local craft store.     We chose to add some decoration... How do I trace my pattern? Trace a tank top or a T-Shirt (with sleeves tucks into place so you can get the armpits) onto pieces of newsprint or a brown paper bag that\u2019s been opened up.. Add about \u00bd\u201d (13mm) around the whole outline for seam allowances. . Make the Front Piece In Two Halves . Make the Back Piece By Laying The Tshirt Flat And Tracing Along It . . Cut Out Your Pattern Pieces And Inspect . . .","label":1,"model":"bloomz","source":"wikihow","id":302}
{"text":"The following steps may help you:  Have an \"Ah Ha!\" moment.  Decide some areas of our abilities and things that are interested for us to study.   Keep up our stamina and enthusiasm in those areas where we have strengths and interests; this way we'll discover our best abilities as well as great opportunities which suit our personalities.   Focus primarily on just one thing -one at once- but don't focus too much; instead put all efforts towards making something really strong-not confusing.   Let everything flow naturally from there.   ...and more....   Be consistent!  Don't give up; never lose sight of your goals because you'll get lost along the way.   Build your confidence and ability to learn-and to earn-with faith in your own ability to develop new abilities and skills-by being persistent and reliable-believing that you're blessed.   If you've followed these guidelines correctly,...you'll see results soon enough!...    You won't feel confused anymore!     ...And that's how it's done!     Enjoy life!     Thank you very much!","label":1,"model":"bloomz","source":"wikihow","id":303}
{"text":"JavaScript supports several data types for its variables.  These include:  Variables can hold different kinds of information depending on their type.   In general, there are four basic types:   Strings - these store text strings such as \"Hello World\" numbers - these stores integers like \"25\" booleans - these store true\/false values functions - these store references to other scripts Arrays - these store collections of objects of various types Data Types    To create a string variable, simply enter the word \"string\" followed by the desired name into the script tag.    For example:     <script type=\"text\/javascript\">string myStringVariableName = \"My First String\";<\/script>    This will create a new string object named \"myStringVariableName\" which contains the text \"My First...\".     Note that if you're not assigning anything else to it besides the default \"\" empty string then you'll need to include quotes around it:     <script type=\"text\/javascript\">string myFirstString = \"\";<\/script>    If you'd rather have a number stored inside a variable instead of a string, just add the word \"int\" before the variable's name within the script tag.    For example,    <script type=\"text\/javascript>int myIntVariableName = 25;<\/script>","label":1,"model":"bloomz","source":"wikihow","id":304}
{"text":"Personalizing your car with an individualized license plate is fun way to express yourself while driving around town.  In addition, it can be used to commemorate special events like birthdays, anniversaries, weddings, graduations, retirements, etc.   You may also want to get one that reflects your interests, hobbies and\/or occupation.    The following steps will help guide you through this process:  Apply online Go to: http:\/\/www.dmv.maryland.gov\/plates\/apply-online.asp Click \"Register\" then enter all requested information into the appropriate fields.  Once you've entered everything correctly click \"Submit\".  If you're applying via email instead go to: dmv-plates@maryland.gov Send them off!   Make sure they are postmarked no later than 60 calendar days from when... Continue reading \u2192\n\nMaryland offers several different types of personalized license plates which include:    Birthday - This type features the month and day of birth along with the first initials of the person's last name followed by their birthday.  For example, \"JULY 15TH JENNIFER SMITH\" would appear as: 0715JSM.  Anniversary - These feature the years married separated by hyphens between each letter of the spouses' names.  For example: \"MARIE-CHRISTINE BROWN 25 YEARS MARRIED\" appears as: MCB-25-YMM.  Graduation - These include the graduate's school district, high school, college\/university, major field(s) of study, degree earned, and year graduated.  For example: \"SOUTH EASTERN COLLEGE OF ARTS AND SCIENCES 2000 AGRICULTURE ENGINEERING BS\" appears as: SECOA-2000-AES-BS-EngineeringAgriculture.  Retirement - These have the same format as anniversary plates but replace the word \"marriage\" with \"retired\".  For example: \"RICHARD LEE RETIRED 20 YEARS\" appears as: RL-20-RR.  Hobby & Interest - These allow you to select any hobby or interest you'd like displayed on your plate.  They must contain only letters, spaces, periods, apostrophes, commas, slashes, underscores, tildes, exclamation points, question marks, dollar signs, percentages, brackets (), braces {}, parentheses [], plus sign +, minus sign -, asterisk *, ampersand &&&, colon :, semicolon ;, equal sign =, bracketless hash mark ~, caret ^, backslash \\, forward slash \/, dash \u2013, underscore _, period ., and tilde ~.  No more than four characters per line should be selected....","label":1,"model":"bloomz","source":"wikihow","id":305}
{"text":"Alimony, also known as spousal maintenance (in some states), refers to money paid by one former spouse to another after their divorce.  In most cases, it will continue until death unless there are certain circumstances that would cause an end date.   It can take many forms including lump sum payment(s) and\/or periodic payments such as monthly installments.    If you have been divorced recently, chances are you may still receive alimony even though you were not entitled to it before because the rules regarding how much should be awarded has changed over time.     You might find yourself paying alimony...or receiving it!    This guide provides detailed instructions about calculating alimony based upon several factors outlined below.... \nThe Purpose Of Alimony\n\nIn general terms, alimony exists so both parties involved in a divorce can maintain the same lifestyle they enjoyed while married.  However, this varies greatly depending on where you live since every state sets its own guidelines governing what qualifies as reasonable alimony and how long it must last.  For example, California requires that any award of alimony include a provision stating that the recipient cannot use the funds received to purchase alcoholic drinks or drugs.  On the other hand,...","label":1,"model":"bloomz","source":"wikihow","id":306}
{"text":"In this method, we\u2019ll show you how to add unlimited amounts of power... , Open the official mobile version of Injustice (free-to-play) by navigating to https:\/\/appstools2.ea.gamespotting.com\/app\/7511-injustexe?id=com.ea.gamespotter&l=en-US in Safari for Mac OS X.  If you\u2019re running Windows 10, navigate to https:\/\/www.ea.com\/games\/in-justice-mobile\/download in Edge instead.   Tap Install app.    Start playing with your newly acquired powers!   You can also try other online tools such as:  This is an alternative way that allows players who don\u2019t have access to computers to get free resources.... \nIf you're having trouble getting past the first stage of the tutorial after installing the modded files,... \n\nYou may need to restart your console before continuing. Press A twice during the cutscene following the end of the match against Black Canary's boss fight. Go back into the menu screen once you've finished the tutorial.","label":1,"model":"bloomz","source":"wikihow","id":307}
{"text":"If you\u2019re planning on hiring professionals to help you relocate from A to B (or even C), then this section is not relevant.  However if you're going to do most of the work yourself - whether it's just loading into trucks and\/or driving across town, or doing both - you'll want to read these tips carefully.   The following sections provide detailed information about what should go where during... Continue reading \u2192\n\nThe best way to prepare for relocation is to start early enough; ideally months ahead of time.    This gives everyone involved plenty of time to plan their schedules accordingly.     It also allows you ample opportunity to find reliable movers who fit within your budget and meet your needs.    You may choose to hire professional movers because you don't feel confident handling such a big project alone,    but there\u2019s no reason why you can\u2019t handle much of the heavy lifting yourself.    In fact, it\u2019s often cheaper than paying someone else to do it!    Moving companies charge based upon volume rather than weight, which means that they\u2019ll bill more money simply because something takes up space.    For example, a dresser that\u2019s only half-full might cost twice as much to ship compared to another identical dresser that's completely full.    To save money when shipping bulky items, consider renting a storage unit until after you've moved.    Then fill it with books, clothing, toys etc. instead of sending those items along with your household goods.    Another option would be to ask friends or relatives living nearby to hold onto certain items while you move.    When choosing a mover, check online reviews thoroughly.    Ask around among people whose recent moves were handled by various local firms.    Check references too.    Never pay anything upfront unless you know exactly whom you\u2019re dealing with.    Keep in mind that reputable movers won\u2019t require payment until they've completed the job.    There are numerous websites dedicated solely to helping consumers locate qualified movers; here are two examples:  http:\/\/www.movingreviews.com\/find-a-moving-company\/   https:\/\/www.trustpilot.com Once you\u2019ve found a few potential candidates,...","label":1,"model":"bloomz","source":"wikihow","id":308}
{"text":"The following list contains all creatures mentioned above with their common names.  This guide was written by wikiHow's community members based on research from various sources including books, websites, videos, etc.. If you'd like to add anything else please use this page! \nPlease note - these animals may change color depending upon season and\/or environment so don't worry if what you're looking at doesn't match exactly!  Also keep in mind that many species have different life cycles; i.e. they're born one way then later molt before changing again... For example, dragon flies hatch from eggs laid near bodies of water while most damselflies lay their eggs directly onto vegetation.   Some animals can regenerate lost body parts such as snakes who regrow tails after losing them during fights.    Be aware that even though it looks dead sometimes it's just sleeping.... Don't touch living things unless you know how to handle them; e.g. do NOT pick off antlers from deer fawns because you'll hurt its mother who's nearby!\nIf you want help identifying an animal you've seen click here","label":1,"model":"bloomz","source":"wikihow","id":309}
{"text":"Coaches play such an important role that it would be impossible to run a successful sports program without one.  A coach's job includes planning practices, motivating athletes during games, keeping track of scores\/standings, recruiting new members, etc.   This section will help coaches plan their sessions so as to maximize efficiency while minimizing time spent away from family\/friends.    The following steps assume you're coaching youth soccer; however many concepts apply equally well to other age groups and sports programs too!   If you'd like more information about becoming a professional coach please see our guide here. Before you begin organizing anything else it's essential... Read More \u00bb\n\nThe most important thing you'll want to do before starting each season is make copies of all relevant documents including insurance certificates, emergency contact forms signed by parents, medical release forms if applicable, league rules & regulations, registration forms\/schedules, payment schedules, etc..    You should also keep these documents somewhere safe but accessible - don't leave them lying around loose!  It's always good idea to scan everything onto computer files which makes sharing info easier between coaches and reduces paperwork considerably.     Once you've got this done then you can start thinking about scheduling matches against local teams and tournaments outside the area....","label":1,"model":"bloomz","source":"wikihow","id":310}
{"text":"Installing new siding can be an easy way to add value to your home while also improving its appearance.  This guide covers how to install both horizontal and vertical siding.   If you're interested only in learning about one type of siding, skip down to the section below that best suits what you'd like to do.    Horizontal Siding  Vertical Siding    Before beginning any work outside, make sure you've got permission from your local building inspector if necessary.     You should have all tools needed before starting; however, it's important...   For more information see our full list of articles related to Home Improvement. See also: How to Remove Existing Siding     The most common types of horizontal siding include aluminum and vinyl.  Both materials come in many colors and styles, including shakes, clapboards, board-and-batten, and stucco finishes.  Aluminum siding comes pre-painted; therefore, you'll need primer paint to cover bare spots where there may not already be color present.  Vinyl siding does require painting after being installed.      To begin,...","label":1,"model":"bloomz","source":"wikihow","id":311}
{"text":"This is an incomplete list.  See also Category:Terraria's weapons and armor for more information on how to get each item listed below.\n\nThe following items are not available at launch but will become obtainable through updates.   The release date may change without notice from publisher Mojang AB.   \n\nWings can only be equipped by flying creatures such as bats, dragons, wyverns, harpies, etc..    This section lists every wing currently known about.    \n\nNote:   Some wings require special conditions before they drop; see their individual pages for details.  \n\nWyvern Wing - Drops after killing any type of wyvern including the boss, Behemoth, and Arch-Wyverns.     \nHarpy Wing - Drops when killed by a harpie in hard mode worlds; requires raining weather.       \nMosswing - Drops when killed... None! \nBatswing:  None! \nDragonfly: None! \nFairy: None! \nGiant Bat: None! \nHellbat: None! \nNightmare Bats: None! \nPigmy Hellbat: None! \nRavenscale: None! \nScarab Beetle: None! \nSpider: None! \nSpitfire: None! \nTiny Devilfish: None! \n\nAngel: None! \nArchangel: None! \nBat Angelic: None! \nDevilish: None! \nDemonian: None! \nEagle: None! \nFirebird: None! \nFrost Bird: None! \nGriffon: None! \nHumanoid: None! \nLava Serpent: None! \nMoonlight Butterflies: None! \nPhoenix: None! \nRed Death: None! \nSeraphim: None! \nSwarmer: None! \nVengeful Spirit: None! \nWhite Devils: None! \nZeppelin: None!","label":1,"model":"bloomz","source":"wikihow","id":312}
{"text":"Feather pillows can be washed by hand as well.  If you're not sure whether it's safe to wash them yourself, check their care label before proceeding. You should also consider getting professional cleaning done on your feather pillows every three years so that they're kept fresh and hygienic. This will help remove dirt from the feathers while preventing damage caused by rubbing against each other during the wash process.   For example, you could use a soft-bristled toothbrush to scrub away stains around seams. Make sure there is no loose fabric sticking up at... Place 2 pillows inside the washing machine along with 1\u20442\u00a0\u00a0c (120\u00a0ml) (2\u20443\u00a0\u00a0tsp) liquid dish soap. The amount of soap you'll need depends on how dirty your pillows are; start small and add more later if necessary. Avoid using regular dishwashing soap because its high suds may cause the feathers to clump together when wet. A mild detergent like Woolite Laundry Stain Remover works best since it won't strip color off of the pillows' covers. To avoid damaging the feathers, set your washing machine's temperature setting to \"Delicate\" instead of \"Hot\". Then select \"Drain & Spin Cycle Only\" rather than \"Washing\". Doing this helps prevent excess moisture from building up within the pillows after they've been cleaned. It'll only take 10 minutes longer but it'll make all the difference! Once you've finished running through these additional steps,...","label":1,"model":"bloomz","source":"wikihow","id":313}
{"text":"Locking in or floating your interest rate on a home loan has many advantages.  It allows you to know exactly what your monthly payment will be during the life of your loan.   You also have peace of mind knowing that no matter where interest rates go up from here, you\u2019ll still pay less than someone who did not lock theirs in at this current low level.    However there are some disadvantages too.  If interest rates drop before closing day then you\u2019re stuck paying more money over the course of your loan term because you locked them in higher than they were currently available.  Also by locking in now, you're giving yourself little flexibility should you need to move quickly due to job loss etc...   For example let's say you get pre-approved today for $300k at 4 percent fixed 30 year amortization....  Let's assume that you decide you'd like to lock into today's 3.5 % interest rate instead of waiting until tomorrow's possible 4% increase..  Your total cost would be:  $315,000 x .035 = $10,650","label":1,"model":"bloomz","source":"wikihow","id":314}
{"text":"The following are tips that may help players who are struggling or having problems playing the PSP: \nDrum 1: Taro  Start the game from the beginning again.  Rebirth any dead Patapons.   Get ready for battle!   Beat the boss!  Continue beating bosses till you reach Level 50.    After reaching Level 50 continue exploring the island and collecting more materials so you'll be prepared when you face off against the next boss!\nPatapon 2 - How to Play Guide\n\nMission 2: Gather wood.  Return back to the village and talk to the chieftain about building houses.     Build two houses near the water source.     Talk to the chieftain once again and build another house just behind him.     Once you've built three houses return to the forest area and find some trees.     Cut down these trees using the saw machine.     Bring the logs over to... If you're stuck somewhere during the story mode try restarting the game completely before continuing. Don't worry too much about dying; it's part of the experience. When starting out don't focus solely on making strong soldiers but instead create as many different types as possible.","label":1,"model":"bloomz","source":"wikihow","id":315}
{"text":"Lap dances are one of those things every woman wants her man to do once...or twice....or maybe even more! But how exactly does this sexy dance look? How much effort goes into performing such a stunning show?\nWell don't worry ladies - we've got just the right tips and tricks to make sure you're able to wow your man when he gets home from work tonight!\n1) Establish some basic rules.  Make sure it's consensual!  You needn't go full stripper mode if that's not something you'd really fancy doing (and we know most women wouldn't).  Remember though; men love watching their girlfriends get naked.   So why not give them a treat?  Just remember to keep everything classy and elegant.  It's important to establish boundaries before starting any type of sexual activity.  This means telling your boyfriend\/partner beforehand whether you'll allow oral sex,...   Keep reading \u00bb\n\n2) Set up the mood  The first step to creating the perfect environment is lighting candles around the room.  Candles create a romantic ambiance which helps build anticipation and excitement.  Light scented candles to add another dimension to the experience.  Play softly seductive music while preparing for your performance.  A good playlist includes slow ballads sung by soulful singers who have powerful voices.  It also includes R&B tracks featuring sultry lyrics and beats.  Some examples include Beyonce Knowles' \"Crazy in Love\",  Janet Jackson's \"When I Think About You\",   Mariah Carey's \"All I Want Is You\",    Usher's \"Can't Get Away From Me\",    Rihanna's \"S&M\";    Lady Gaga's \"Paparazzi\".    Once you've chosen your playlist, choose two or three songs that you would particularly like to use during your performance.  These could either be songs that remind you of special moments shared together or simply songs that you find very erotic.  Be prepared to change these songs mid-way through your performance depending upon how well they fit within your overall theme.  Alternatively, you might consider using a CD\/DVD containing pre-recorded videos of strippers...","label":1,"model":"bloomz","source":"wikihow","id":316}
{"text":"Minnie Mouse is an iconic character from Disney's Mickey Mouse franchise.  Her signature look includes red polka dots, white gloves, black shoes, and pink hair bows.   These easy-to-make DIY minnie mouse ears are perfect for dressing up as her during Halloween costume parties, birthday celebrations, or just because you want to be cute! .\nTo make these adorable ears you'll need:  * A pair of scissors . * Felt squares that measure 2 1\/2\" x 2\" each. . * Cardstock circles measuring 1 1\/4\" across.. * Fabric or ribbon that's 25cm\/10in wide...and 6m\/2ft long....or longer than you'd like your finished bow to be..  * Gathering string that's 7.5cm\/3in wide.....and 30m\/100ft long......or longer than you'd like it to be...  - You can use any color combination of materials you'd like but we chose purple\/red\/pink\/magenta colors here so they would match our daughter's favorite princess outfit perfectly!  If you're using a different size headband then adjust accordingly by cutting larger or smaller sized circles depending upon how large\/small you wish them to be.    To begin creating this template simply draw a small dot where both corners meet.  Then connect all four points together forming a square.  Repeat this process twice more drawing another set of three connected dots within each original drawn square.  This will create six total squares surrounding the first central point you've created.  Draw lines connecting every corner between each of the newly formed squares.  Finally,...","label":1,"model":"bloomz","source":"wikihow","id":317}
{"text":"This is an in-depth guide about building a subwoofer cabinet.  This method uses computer generated designs which are then printed onto paper using special printers called plotters.   The plans can also be downloaded directly from the internet by searching online for \"how-to-build-a-subwoofer-cabinet\".    If you're interested in making one without any help whatsoever, see How To Make A Subwoofer Box Without Computer Software below instead.     You may find it useful to use a piece of cardboard cut down to fit over each earpiece when testing different sizes of boxes - this way you'll know exactly what sound quality you'd get before committing to cutting away actual MDF!     For those who don't have access to such equipment there...   There are many ways to build a subwoofer cabinet but most require expensive tools and\/or materials like laser-cutting machines and high density fibreboards.  However, these methods tend to produce better results than DIY alternatives because they allow precise control over every aspect of the construction process including panel thicknesses, internal bracing etc..  In contrast, the following instructions describe a simple alternative based around inexpensive off-the-shelf materials available through local hardware stores....","label":1,"model":"bloomz","source":"wikihow","id":318}
{"text":"If you're serious about becoming famous then you'll need to start working towards that goal right now! This guide has been written with beginners who want to become rockstars in mind.  If you'd rather just be a normal person than a rock star (or maybe already are one), skip down below where we talk about being a \"normal\" rockstar instead!\nThe first thing any aspiring rockstar needs to learn is playing their chosen musical instrument(s). You can either choose guitar, bass, drums, piano etc... Or all four!  The most important things are:  - A good sense of rhythm - An ear for melody - Knowing which notes go with each chord Once you've mastered these basics it's time to move onto learning songs from bands such as:   Nirvana U2 Coldplay Oasis Green Day Queen Guns N' Roses Take That Bon Jovi After you've learned enough songs to fill a whole album, it's time to put them into action.   Start practicing every day until they come naturally to you so they'll sound great live. When you're ready to perform those songs \"live\" before anyone else,...","label":1,"model":"bloomz","source":"wikihow","id":319}
{"text":"The following describes how to complete the \"An Unexpected Journey\" quest line.  This quest line starts after completing \"The Waywards\" quest.   The first step involves sleeping aboard an abandoned boat near the city of Dawnstar.    Sleeping aboard the boat  You can find this boat docked off shore just south-east of Dawnstar along the coast between two small islands.  It looks like it belongs to some fishermen but no-one seems...   On the second day, go into town and talk to any citizen to get the news story about the missing captain.... \nOn the third night,  return to the boat again and sleep inside its cabins.  When you awake, you'll see that the boat has sailed away without anyone aboard!    You'll be greeted by a bandit's leader named Lynch who's trying to take over the boat.  He'll give you a key if you're willing to help him free the rest of the crew members.  If not, he'll attack you immediately.  Either way, fight him!  After killing him,...","label":1,"model":"bloomz","source":"wikihow","id":320}
{"text":"This wikiHow will teach girls about looking great during their middle-school years.  It covers topics such as:  How to stay clean; including hygiene tips,  fashion advice, beauty tricks, and more. This section includes information on:   Looking stylish Dressing appropriately Wearing accessories Staying healthy Eating healthily Exercising Avoiding unhealthy habits Sleeping... Continue reading \u2192 ...more details below.... \nIf you'd rather read this page's text than watch videos,... \n\nThe following sections include detailed instructions on how to:\n\nBeautiful Hair: The Ultimate Guide To Gorgeous Long Healthy Locks Beauty Tricks: 10 Easy Tips That Will Change Your Looks Forever Fashion Advice: What Goes With What? Style Tips: How To Rock Any Outfit From Head-to-Tail Skin Care: 5 Secrets Of Beautiful Skin Body Beauty: 8 Steps To Perfect Pout Smell Good: 7 Natural Perfumes You'll Love Getting Ready Fast: 15 Quick-Fix Morning Routines Stylish: 20 Cool Accessory Ideas","label":1,"model":"bloomz","source":"wikihow","id":321}
{"text":"The following information provides general guidelines... If you're reading this page, chances are good you've been suffering from some type of coughing fit recently.  The purpose of this guide isn't necessarily to provide specific advice tailored specifically to your particular case,... but rather to give you a broad overview of what might be going wrong.... and offer suggestions regarding possible treatments.   In order to get more detailed information related directly to your own personal circumstances, it's important to consult with your physician and\/or pharmacist.... who should both be able to recommend appropriate remedies suited especially to your needs.[1] X Research source This section covers basic strategies aimed primarily towards treating mild-to-moderate cases of acute coughs caused by viral infections like colds and flu's[2][3][4][5]:  Rest. Drink lots of liquids. Take OTC medicines. Suck on throat lozenges. See a doctor immediately if necessary. Discuss your condition thoroughly with your doctor. Follow up with additional tests recommended by your doctor. Consider prescription medications only after discussing them with your doctor.","label":1,"model":"bloomz","source":"wikihow","id":322}
{"text":"If you're looking for some ideas about how to have fun at home during quarantine, then this is just what you've been searching for! This wikiHow will teach you exactly how to do a teen beauty session.  You can also try doing other activities like baking cookies together (or eating them), playing board games, watching movies\/TV shows\/movies online, etc... If you'd rather not read through an entire guide that has lots of steps involved, feel free to skip ahead by clicking one of these links:  How To Do A Teen Makeup Look How To Get Your Nails Dipped In Gel Polish How To Give Yourself Pedicures How To Shower Like An Adult 1) Pick out a pair of comf y, f uzzy PJ 's . 2) T ake yo ur cl othes , s it th em fold ed neatl y i n t he bathro om , aw ay fr om wh ere t hey c ou ld g et w as hed or damag ed. . 3 ) I ts ti me t o sh ow er .  4 ) R un t h e wat er i nt h e show er t o t he te mpera tu re ot herwise u sefu l , b ut y ou cou d go highe r or lowe r dep endi ng ont hat 'si sts saf e' and co mfo rt able . . 5 ) Ch ose t he sampo of yo ur cho ice , and us e yo ur fi ngeri ty t o thorough ly mas sa ge t he sampo ov er yo ur ha ir , goin g fro m yo ur sc al pi nd t o t he tip s of yo ur ha ir . . . 6 ) Repeat t his ste p wit h condit ioner . . . . 7 ) Tak e a bod y so ap of yo ur ch oi ce and la turate yo ur ent ire bo dy wi th it. . . . 8 ) N ow woul d be a gr eat tim e t o sh ave yo ur leggs and underrams. . . 9 ) Ri nk yo ur bodi n wi th cold wa ter . . . 10 ) Ste pt ou t of t he ba tt umba nnd wrape yo ursel f i n a tow el . . . 11 ) Ri nk yo ur fa ces wi th cold wate r . . . 12 ) Co mb yo ur ha ir ou t , and le ft it ai r drie . . . 13 ) Po nt ou t yo ur pa jama s . . . . 14 ) Str ip yo ur nail s off of eve rything , and ad di ct a cle ar coa t . . . 15 ) Moti vated ? 16 ) Go t hu gher !","label":1,"model":"bloomz","source":"wikihow","id":323}
{"text":"If you\u2019re looking into purchasing a new bicycle for your toddler but aren\u2019t quite sure what size they\u2019ll need, there are several things that will help determine which one would be best.  The first thing to measure when choosing a bike is how tall your child is.   You should also take note of whether your child prefers riding standing up (with their knees bent) or sitting down while cycling.    Once you've determined these two factors you'll know where to start shopping for bikes.     To find out more information regarding sizing bicycles for children please continue reading this wikiHow guide. Before you begin measuring your child's body parts, gather all necessary supplies such as measuring tape, pencil, paper, etc.. This way you're ready once it's time to get started!   Take some measurements around your home beforehand just to see what's available already!  For example, use a piece of string instead of measuring tape because it\u2019s easier to move around and doesn\u2019t tangle as much....   ... \nOnce you\u2019ve gathered everything together, ask your child to sit still next to a wall.  Then mark off his\/her height using chalk or paint pen.  Be careful not to let him\/her lean over too far since this could affect the accuracy of the results.  Keep track of any marks made along the way by writing them down somewhere visible.  It may seem silly now, but doing this later will come in handy during the process of finding the right sized bike for your child.  After marking each step taken, repeat the same procedure again after waiting 30 minutes.  Your child might grow half an inch within those thirty minutes; therefore, repeating the whole process ensures accurate results.  In order to ensure proper growth,...","label":1,"model":"bloomz","source":"wikihow","id":324}
{"text":"The most common cause of vaginal... See More... How do I get pregnant? What causes vaginal bleeding in pregnancy?\nWhat does labor feel like? \n \n What should my diet look like before giving birth? \n \n... \n \n Read our full list of articles here! ... \n\n\nReaders also asked questions about: \nPregnancy & Childbirth\n\nDo not use douches if you're experiencing bacterial vaginosis.  Douching removes healthy bacteria along with harmful ones, which could make BV worse.   If you want to try using an alternative product instead of plain water when washing out your vagina, talk to your healthcare provider about products she recommends.... The main symptom of vaginal prolapse is feeling something protruding through the opening of your vagina. You might notice some bulging tissue near the entrance of your vagina; however, it's possible you'll experience no symptoms until you've developed advanced stages of vaginal prolapse. In addition to feeling something coming down inside your vagina, there are several other signs and symptoms associated with vaginal prolapse. These include:   Difficulty emptying your bladder or bowels Experiencing urinary incontinence Leaking urine or stool around your urethra or anal area Painful intercourse Feeling discomfort or pain in your back Low self-esteem due to changes in body image A sensation of heaviness in your lower abdomen or pelvis Swelling or redness around your vulva","label":1,"model":"bloomz","source":"wikihow","id":325}
{"text":" This game really requires more than about 10 people to play per team, that's why its commonly played at schools.;\n, A level floor playing surface is adequate enough, however, more often this game is played in a large gymnasium or other large open area where it is deemed acceptable to play a game similar to kickball.\n\n, Although you can use whatever \"base\" you'd like to use,as long as it's large enough for all the players to stand on the bases at the same time. The bases themselves tend to be gymnasium mats (thereby the reason why this game is called Matball).\n\n, Crossing that imaginary line makes the kicking player automatically safe. Most often times, this imaginary line is centered at the direct center of the area of the gymnasium floor - all so that the kicking player can have room to take action on the ball when it arrives.\n\n,, The kicker can either kick the ball and begin running the bases, or they let it slide past and into whatever is behind them. However, the outcome of missing this ball can be deemed to be either an out or just a miss, but must be based solely on the umpire or other officials officiating the game.\n\n, They can either run to the right (much like the current game of baseball) or towards the left and run from base to base until they reach home.\n\n, You can have many players on one single mat.\n\n\nSometimes, game officials give the teams an ultimatum when they reach base, in hopes that they are not called \"out\" at any time (outs are discussed a little later). They can continue running in hopes not to get stopped by any one baseball fielder near the mat, or they can stop and safely rest on the mat.\nRecognize what should happen in the case the player oversteps a base (even by just one foot). Most often times the game official gives players the opportunity to lead with their one lead foot, however, they must not come off the base or they can be thrown out to be considered \"out\". Most often times, once you leave a base, you can't return again to that same base.\n\n, Games vary from location to location and include (but are not limited to) these two universally-accepted rules.\n\n\nIf the ball is kicked behind home plate, it is considered to be a foul ball.\nIf the ball hits the ceiling of the location traveling forward, it is either a foul or out (dependent on location rules).\n\n, An out for a player is recorded when either the player has swung their leg and missed on several attempted kicks or when they swing and kick but the fielder catches with their hands, when the running player gets tagged while running the bases or during an altercation in the game that causes the out.\n\n, If the location's doors are open in front of home plate and the ball heads out the doors, some places consider the ball to be live at all times until the fielding team retrieves it, or it may be considered a home run. Sometimes if the game is played indoors inside a gymnasium and there are basketball hoops that have been raised and the ball travels into or through the baskets, it's an automatic home run, but that's only a general advisement and not a required idea to determine.\n\n\nRecognize that there are more scenarios than what are posed here, but vary widely upon each individual situation the location encounters.\n\n,, Sometimes, interference can be called in the field when some members of the fielding team are playing their position in or near the basepath and\/or to a certain extent so that the running player must collide with them to get to their next base. If this error is recorded, the player can safely be marked as safe on the next base, and the player who committed the error must sit out until the next inning or till the end of the game and can't be up to bat unless these players are elementary school age and are just learning the game.\n\n,, Sometimes, as long as theirs a ball in play and their is a runner on a base (or is running the bases) these teammates can continue running the bases. However, if you want, you can tell the teams that they have to stop after (x) amount of runs.\n\n, After several outs have been recorded by the kicking team, the side is out. If all players reach base without leaving any other players behind, you may end up waiting for someone to make a mistake and have all players leave their bases, but that won't happen very often.\n\n, Repeat this play after the two teams switch places (the kicking teams becomes the fielding team and the fielding team becomes the kicking team).\n\n, The team at the end of a few rounds or the one to record the most runs will win and the game will be over.\n\n,","label":0,"model":"human","source":"wikihow","id":942}
{"text":" Such physicians are known as haematologists.\n\n\nTheir routine work mainly includes the care and treatment of patients with haematological diseases, although some may also work at the haematology laboratory viewing blood films and bone marrow slides under the microscope, interpreting various haematological test results.\nIn some institutions, haematologists also manage the haematology laboratory.\nPhysicians who work in haematology laboratories, and most commonly manage them, are pathologists specialized in the diagnosis of haematological diseases, referred to as haematopathologists.\nHaematologists and haematopathologists generally work in conjunction to formulate a diagnosis and deliver the most appropriate therapy if needed.;\n, Haematologists may specialise further or have special interests, for example in:\n\n\nbleeding disorders such as haemophilia and idiopathic thrombocytopenic purpura\nhaematological malignacies such as lymphoma and leukemia (onco haematology)\nhaemoglobinopathies\nthe science of blood transfusion and the work of a blood bank\nbone marrow and stem cell transplantation\n\n, Only some blood disorders can be cured.\n\n, Haematological analysis involves the determination of different blood parameters, which can be done using either the electronic quantification or the manual quantification.\n\n\nThe electronic quantification can be done with the use of the auto counter and these displace about 15 parameters, while the manual quantification of PCV for example, is by the use of the Micro haematocrit centrifuge. This Micro haematocrit centrifuge is used to determine the Packed Cell Volume (PCV), from which many other parameters can be obtained.\nThe best way to determine blood parameters is through the electronic device called auto counter because it gives accurate values and manual red blood cell counting is obsolete and inaccurate Although, the manual quantification is also used to confirm whether the values obtained from the auto counter correlate with it for PCV.\nThe haematocrit (Ht or HCT) or packed cell volume (PCV) or erythrocyte volume fraction (EVF) is the proportion of blood volume that is occupied by red blood cells. It is normally about 46% for men and 38% for women. It is considered an integral part of a person's complete blood count results, along with hemoglobin concentration, white blood cell count, and platelet count.\n\n, The term \"haematocrit\" was coined in 1903. Its roots stem from the Greek words hema- blood, and krites, judge - meaning to gauge or judge the blood.In mammals, haematocrit is independent of body size. The PCV of animals can also be determined, to know their anaemic state.\n\n\nElevated haematocrit - In cases of dengue fever a high haematocrit is a danger sign of an increased risk of dengue shock syndrome.\n\nPolycythemia vera (PV), a myeloproliferative disorder in which the bone marrow produces excessive numbers of red cells, is associated with elevated haematocrit.\nChronic obstructive pulmonary disease (COPD) and other pulmonary conditions associated with hypoxia may elicit an increased production of red blood cells. This increase is mediated by the increased levels of erythropoietin by the kidneys in response to hypoxia.\nProfessional athletes' haematocrit levels are measured as part of tests for blood doping or Erythropoietin (EPO) use; the level of haematocrit in a blood sample is compared with the long-term level for that athlete (to allow for individual variations in haematocrit level), and against an absolute permitted maximum (which is based on maximum expected levels within the population, and the haematocrit level which causes increased risk of blood clots resulting in strokes or heart attacks).\nIf a patient is dehydrated, the haematocrit may be elevated. Repeat testing after adequate hydration therapy will usually result in a more reliable result.\n\n\nLowered haematocrit - Lowered haematocrit can imply significant hemorrhage (for example, in an ectopic pregnancy).\n\nThe mean corpuscular volume (MCV) and the red cell distribution width (RDW) can be quite helpful in evaluating a lower-than-normal haematocrit, because it can help the clinician determine whether blood loss is chronic or acute. The MCV is the size of the red cells and the RDW is a relative measure of the variation in size of the red cell population. A low haematocrit with a low MCV with a normal RDW suggests a chronic iron-deficient erythropoiesis, but a high RDW suggests a blood loss that is more acute, such as a hemorrhage.\n\n\n\n, They include:\n\n\nInfants who may not have adequate iron intake\nChildren going through a rapid growth spurt, during which the iron available cannot keep up with the demands for a growing red cell mass\nWomen in childbearing years who have an excessive need for iron because of blood loss during menstruation\nPregnant women, in whom the growing fetus creates a high demand for iron.\nPatients with chronic kidney disease, as their kidneys no longer secrete sufficient levels of the hormone erythropoietin, which stimulates red blood cell production by the bone marrow.\n\n, This is the main oxygen transport protein in the fetus during the last seven months of development in the uterus and in the newborn until the neonate (newborn child) is roughly 6 months old. Functionally, fetal haemoglobin differs most from adult haemoglobin in that it is able to bind oxygen with greater affinity than the adult form, giving the developing fetus better access to oxygen from the mother's bloodstream.\n\n\nIn newborns, fetal hemoglobin is nearly completely replaced by adult haemoglobin by approximately the twelfth week of postnatal life. In adults, fetal haemoglobin production can be reactivated pharmacologically, which is useful in the treatment of such diseases as sickle-cell disease.\nFoetal haemoglobin, (also hemoglobin F or HbF) is the main oxygen transport protein in the fetus during the last seven months of development in the uterus and in the newborn until the neonate (newborn child) is roughly 6 months old. Functionally, fetal haemoglobin differs most from adult haemoglobin in that it is able to bind oxygen with greater affinity than the adult form, giving the developing fetus better access to oxygen from the mother's bloodstream.\nIn newborns, fetal hemoglobin is nearly completely replaced by adult haemoglobin by approximately the twelfth week of postnatal life. In adults, fetal haemoglobin production can be reactivated pharmacologically, which is useful in the treatment of such diseases as sickle-cell disease.\n\n, When fetal haemoglobin production is switched off after birth, normal children begin producing adult haemoglobin (HbA). Children with sickle-cell disease instead begin producing a defective form of haemoglobin called haemoglobin S. This variety of haemoglobin aggregates, forming filaments that cause red blood cells to change their shape from round to sickle-shaped, which have a greater tendency to stack on top of one another and crowd blood vessels. These invariably lead to so-called painful vaso-occlusive episodes, which are a hallmark of the disease.\n\nIf fetal haemoglobin remains the predominant form of haemoglobin after birth, however, the number of painful episodes decreases in patients with sickle-cell disease. Hydroxyurea promotes the production of fetal hemoglobin and can be used to treat individuals with sickle-cell disease. Combination therapy with hydroxyurea and recombinant erythropoietin\u2014rather than treatment with hydroxyurea alone have been shown to further elevate haemoglobin F levels and to promote the development of HbF-containing F-cells.\n\n, This refers to the process of determining the genotype of an individual by the use of electrophoresis. Current methods of doing this include PCR, DNA sequencing, ASO probes, and hybridization to DNA micro-arrays or beads. The technology is important in clinical research for the investigation of disease-associated genes.\n\n\nDue to current technological limitations, almost all genotyping is partial. That is, only a small fraction of an individual\u2019s genotype is determined. This experiment actually only determines the phenotype, not genotype.\n\n, Experiments with blood transfusions, the transfer of blood or blood components into a person's blood stream, have been carried out for hundreds of years. Many patients have died and it was not until 1901, when the Austrian Karl Landsteiner discovered human blood groups, that blood transfusions became safer.\n\n\nMixing blood from two individuals can lead to blood clumping or agglutination. The clumped red cells can crack and cause toxic reactions. This can have fatal consequences. Karl Landsteiner discovered that blood clumping was an immunological reaction which occurs when the receiver of a blood transfusion has antibodies against the donor blood cells.\nKarl Landsteiner's work made it possible to determine blood groups and thus paved the way for blood transfusions to be carried out safely. For this discovery he was awarded the Nobel Prize in Physiology or Medicine in 1930.\n\n, An adult human has about 4\u20136 liters (1.6\u00a0US\u00a0gal) of blood circulating in the body. Among other things, blood transports oxygen to various parts of the body. Blood consists of several types of cells floating around in a fluid called plasma. The red blood cells contain hemoglobin, a protein that binds oxygen. Red blood cells transport oxygen to, and remove carbon dioxide from, the body tissues. The white blood cells fight infection. The platelets help the blood to clot, if you get a wound for example. The plasma contains salts and various kinds of proteins.\n\n, The differences in human blood are due to the presence or absence of certain protein molecules called antigens and antibodies. The antigens are located on the surface of the red blood cells and the antibodies are in the blood plasma. Individuals have different types and combinations of these molecules. The blood group you belong to depends on what you have inherited from your parents.\n\n\nThere are more than 20 genetically determined blood group systems known today, but the AB0 and Rh systems are the most important ones used for blood transfusions. Not all blood groups are compatible with each other. Mixing incompatible blood groups leads to blood clumping or agglutination, which is dangerous for individuals. Nobel Laureate Karl Landsteiner was involved in the discovery of both the AB0 and Rh blood groups:\n\nABO blood grouping system - According to the AB0 blood group system there are four different kinds of blood groups: A, B, AB or 0 (null).\nBlood group A - If you belong to the blood group A, you have A antigens on the surface of your red blood cells and B antibodies in your blood plasma.\nBlood group B - If you belong to the blood group B, you have B antigens on the surface of your red blood cells and A antibodies in your blood plasma.\nBlood group AB - If you belong to the blood group AB, you have both A and B antigens on the surface of your red blood cells and no A or B antibodies at all in your blood plasma.\nBlood group 0 - If you belong to the blood group 0 (null), you have neither A or B antigens on the surface of your red blood cells but you have both A and B antibodies in your blood plasma.\nRh factor blood grouping system - Many people also have a so called Rh factor on the red blood cell's surface. This is also an antigen and those who have it are called Rh+. Those who haven't are called Rh-. A person with Rh- blood does not have Rh antibodies naturally in the blood plasma (as one can have A or B antibodies, for instance). But a person with Rh- blood can develop Rh antibodies in the blood plasma if he or she receives blood from a person with Rh+ blood, whose Rh antigens can trigger the production of Rh antibodies. A person with Rh+ blood can receive blood from a person with Rh- blood without any problems.\n\n\n\n, According to above blood grouping systems, you can belong to either of following 8 blood groups:\n\n\nA Rh+\nB Rh+\nAB Rh+\n0 Rh+\nA Rh-\nB Rh-\nAB Rh-\n0 Rh-\n\n, You mix the blood with three different reagents including either of the three different antibodies, A, B or Rh antibodies. Then you take a look at what has happened.\n\n\nIn which mixtures has agglutination occurred? The agglutination indicates that the blood has reacted with a certain antibody and therefore is not compatible with blood containing that kind of antibody.\nIf the blood does not agglutinate, it indicates that the blood does not have the antigens binding the special antibody in the reagent.\nIf you know which antigens are in the person's blood, it's easy to figure out which blood group he or she belongs to! Application:\n\nA person with A+ blood receives B+ blood. The B antibodies (yellow) in the A+ blood attack the foreign red blood cells by binding to them. The B antibodies in the A+ blood bind the antigens in the B+ blood and agglutination occurs. This is dangerous because the agglutinated red blood cells break after a while and their contents leak out and become toxic.\nA person with A+ blood receives B+ blood. The B antibodies (yellow) in the A+ blood attack the foreign red blood cells by binding to them. The B antibodies in the A+ blood bind the antigens in the B+ blood and agglutination occurs. This is dangerous because the agglutinated red blood cells break after a while and their contents leak out and become toxic.\n\n\n\n, For a blood transfusion to be successful, AB0 and Rh blood groups must be compatible between the donor blood and the patient blood. If they are not, the red blood cells from the donated blood will clump or agglutinate. The agglutinated red cells can clog blood vessels and stop the circulation of the blood to various parts of the body. The agglutinated red blood cells also crack and its contents leak out in the body. The red blood cells contain hemoglobin which becomes toxic when outside the cell. This can have fatal consequences for the patient.\n\n\nThe A antigen and the A antibodies can bind to each other in the same way that the B antigens can bind to the B antibodies. This is what would happen if, for instance, a B blood person receives blood from an A blood person. The red blood cells will be linked together, like bunches of grapes, by the antibodies. As mentioned earlier, this clumping could lead to death.\n\n, Of course, you can always give A blood to persons with blood group A, B blood to a person with blood group B and so on. But in some cases you can receive blood with another type of blood group, or donate blood to a person with another kind of blood group.\n\n\nThe transfusion will work if a person who is going to receive blood has a blood group that doesn't have any antibodies against the donor blood's antigens. But if a person who is going to receive blood has antibodies matching the donor blood's antigens, the red blood cells in the donated blood will clump.\n\n, This is the localization of antigens or proteins in tissue sections by the use of labeled antibodies as specific reagents through antigen-antibody interactions that are visualized by a marker such as fluorescent dye, enzyme, or colloidal gold. There are numerous immunohistochemistry methods that may be used to localize antigens. The selection of a suitable method should be based on parameters such as the type of specimen under investigation and the degree of sensitivity required.\n\n, Cancer occurs as a result of mutations, or abnormal changes, in the genes responsible for regulating the growth of cells and keeping them healthy. The genes are in each cell\u2019s nucleus, which acts as the \u201ccontrol room\u201d of each cell. Normally, the cells in our bodies replace themselves through an orderly process of cell growth: healthy new cells take over as old ones die out. But over time, mutations can \u201cturn on\u201d certain genes and \u201cturn off\u201d others in a cell. That changed cell gains the ability to keep dividing without control or order, producing more cells just like it and forming a tumor.\n\n\nA tumor can be benign (not dangerous to health) or malignant (has the potential to be dangerous). Benign tumors are not considered cancerous: their cells are close to normal in appearance, they grow slowly, and they do not invade nearby tissues or spread to other parts of the body. Malignant tumors are cancerous. Left unchecked, malignant cells eventually can spread beyond the original tumor to other parts of the body.\nDuring the immunohistochemical staining of a tissue, antigens are usually expressed if the tissue stains positive. These antigens can either be breast cancer antigens or non-breast cancer antigens\n\nBreast Cancer Antigens under investigation\n\nA. Estrogen Receptor (ER)\nB. Progesterone Receptor (PR)\nC. Her \u2013 2 neu\n\n\nNon Breast antigens\n\nA. Leucocytes common antibody (LCA) for Lymphoma\nB. AE1 and AE2 for epithelial tumor and other carcinoma\nC. Vimentin for messenchyma tumor\nD. NSE\nE. CD5\nF. CD20\nG. CD30\nH. Desmin\nI. Myogenin\n\n\n\n\n\n, Breast cancer is an uncontrolled growth of breast cells. To better understand breast cancer, it helps to understand how any cancer can develop. The term \u201cbreast cancer\u201d refers to a malignant tumor that has developed from cells in the breast. Usually breast cancer either begins in the cells of the lobules, which are the milk-producing glands, or the ducts, the passages that drain milk from the lobules to the nipple. Less commonly, breast cancer can begin in the stromal tissues, which include the fatty and fibrous connective tissues of the breast.\n\n","label":0,"model":"human","source":"wikihow","id":943}
{"text":" It\u2019s important to distinguish between these two terms and to use them properly in your assessment.\n\n\nA hazard is anything that may cause harm. For example: chemicals, electricity, working from heights like a ladder, or an open drawer.A risk is the chance that these hazards could cause harm to others. For example: a chemical burn or an electric shock, a fall from heights, or an injury from hitting an open drawer.;\n, Think about any hazards you notice as you walk around. Ask yourself, what activities, processes, or substances could injure your employees or harm their health?Look at any objects, office furniture or pieces of machinery that could be hazards. Examine any substances in the workplace, from chemicals to hot coffee. Think about how these substances could injury your employees.If you work in an office, look for any long wires in walkways or under desks, as well as any broken drawers, cupboards, or counter tops. Examine the chairs at your employees' workstations, the windows, and the doors. Look for any hazards in the common areas, such as a faulty microwave or an uncovered section on the coffee machine.\nIf you work in a big box store or a warehouse, look for any machinery that could be hazards. Note any spare materials, like hangers or safety clips, that could spill or land on an employee. Look for any hazards in the aisles of the store, like narrow shelving or cracked sections of the floors.\n\n, Your employees will be able to help you identify any hazards they encounter on the job. Send out an email or have an in person discussion asking for feedback on any possible hazards in the workplace.Ask specifically about hazards that your employees think could result in significant harm, such as slips and trips, fire hazards, and falling from heights.\n\n, They can help to explain the hazards and put them in perspective in terms of how the equipment was made to be used and how misuse can lead to hazards.The manufacturer's instructions will likely be on the labels of any equipment or substances. You can also check the manufacturer's manual for more information on possible hazards associated with the equipment or substances.\n\n, These documents will help you identify less obvious hazards, and any hazards that have occurred before in the workplace.If you are in a management position, you can likely access these records for your company online or in the company files.\n\n, Long term hazards are hazards that will have an impact on workers when they exposed to the hazards for a long period of time.\n\n\nThese could be exposure to high levels of noise, or exposure to harmful substances over a sustained period of time. This could also be safety hazards through the repetitive use of equipment, from a lever on a work site to a keyboard at a desk., Depending on your country, you can access practical guidance on hazards in the workplace via a government health and safety guidelines website. These sites have a list of hazards and possible ways to control them, including recognized hazards such as working at height, working with chemicals, and working with machinery.\n\n\nIn the U.S., you can access the government health and safety guidelines website here: https:\/\/www.osha.gov\/.\nIn the UK, you can access the government health and safety guidelines website here: http:\/\/www.hse.gov.uk\/.\n\n, You are creating an overview of all potential individuals at risk, so avoid listing every worker by name. Instead, make a list of groups of people in an environment.\n\n\nFor example, \u201cpeople working in the storeroom\u201d or \u201cpassers-by on the street.\u201d\n\n, You then need to identify what type of injury or ill health might occur for each group.\n\n\nFor example: \u201cshelf stackers may suffer back injuries from repeated lifting of boxes\u201d. Or, \u201cmachinery workers may suffer joint pain from repeated use of a lever.\u201d\nThis could also be more specific injuries like \"workers may be burned by the printing press\" or \"cleaners may trip over the wires under the desks\".\nKeep in mind some workers may have particular requirements, such as new and young workers, new or expectant mothers, and people with disabilities.\nYou will also need to account for cleaners, visitors, contractors, and maintenance workers who may not be in the workplace all the time. It\u2019s important to also identify any possible hazards to the general public, or \u201cpassers-by\u201d.\n\n, If the workplace is a shared space among several workers or several hundred workers, it\u2019s important to reach out to your employees and ask them who they think is at risk. Think about how your work affects others present and how their work affects your staff.\n\n\nAsk your staff if they can think of any group you may have missed when identifying who is affected by certain hazards. For example, you may not realize that the cleaning staff also has to deal with lifting boxes at your employee\u2019s desks, or you may not be aware that a certain piece of machinery is a noise hazard for pedestrians on the street.\n\n, Risk is a part of everyday life and through you might be the boss or the person in charge, you are not expected to eliminate all the risks. But you need to make sure you are aware of the main risks and you know how to address and manage these risks. So, you need to do everything \u201creasonably practicable\u201d to protect people from harm. This means balancing the level of risk against the measures needed to control the real risk in terms of money, time, or trouble.Keep in mind you do not need to take action that would be considered disproportionate to the level of risk. Don\u2019t go overboard on your risk assessment. You should only include what you could be expected to know, within reason. You are not expected to anticipate unforeseen risks.\nFor example, a risk of a chemical spill should be taken seriously and noted as a major hazard. But smaller risks, like a stapler harming someone using it or the lid of a jar hitting someone, are not considered \"reasonably practicable\". Do your best to identify major and minor hazards, but do not try to account for every possible hazard in the workplace.\n\n, For example, maybe you provide shelf stockers with back protectors and safety gear (also known as PPE, or Personal Protective Equipment). But ask yourself: Can I get rid of the hazard altogether? Is there a way to rearrange the storeroom so shelf stockers do not have to lift boxes from the ground? If this is not possible, ask: how can I control the risks so that harm is unlikely? Practical solutions include:Trying a less risky option. Such as having the boxes on a raised platform or ledge to reduce the distance the shelf stockers will have to lift from.\nPreventing access to the hazards, or organizing the workplace to reduce exposure to the hazard. Such as rearranging the storeroom so the boxes are placed at a level that do not require lifting by the shelf stockers.\nIssuing protective equipment or protective practices to your workers. Such as back guards, PPE, and information on how to complete an action safely. For example, you could educate shelf stockers on how to properly lift a box from the ground, bending at the knees, with a straight back.\nProviding welfare facilities, such as first aid and washing facilities. If your workers deal with chemicals in the workplace, for example, you should provide washing facilities and first aid close to their workstations.\n\n, Improving health and safety does not necessarily mean spending a lot of company money. Simple adjustments like placing a mirror on a blind corner to prevent vehicle accidents, or holding a brief training session on how to lift objects properly are all low-cost precautions.In fact, failing to take simple precautions can cost you a lot more if an accident does happen. The safety of your workers should mean more than the bottom line. So if possible, spring for higher-cost solutions if they are your only option. Spending money on precautions is a better choice than having to take care of an injured worker.\n\n, Many of these groups come up with risk assessments for particular activities, like working with heights or working with chemicals. Look at the National Institute for Occupational Safetyand websites that focus on a certain sector, like miningor administration.\n\n\nTry to apply these model assessments to your workplace and adapt them as necessary. For example, a model assessment may have suggestions on how to prevent a fall from a ladder in the workplace. Or a suggestion on how to make loose wires in the office more safe for employees. You can then apply these suggestions in your own risk assessment, based on the specifics of your workplace.\n\n, It\u2019s important to involve your workers in the process of evaluating the risks and coming up with possible precautions. This will ensure that what you propose will work in practice and won\u2019t introduce any new hazards into the workplace., The assessment should cover the hazards, how people might be harmed by them, and what you have in place to control the risks.\n\n\nIf you have fewer than five employees, by law you do not have to write down a risk assessment. But it\u2019s useful to do this so you can review it at a later date and update it.If you have five or more employees, you are required by law to write down the assessment.\n\n, There are several templates available online, based on the type of workplace you work in.A basic risk assessment should show that:\n\n\nA proper check of the hazard was made.\nYou asked who might be affected.\nYou dealt with the obvious, major hazards, and took into account the number of people who could be involved.\nThe precautions taken are reasonable and practical.\nThe remaining risk is low and\/or manageable.\nYou involved your employees in the process.\nIf the nature of your work changes often or the workplace changes and develops, such as a construction site, your risk assessment may have to concentrate on a broad range of risks that can be anticipated. This could mean the possible state of the site your workers will be building on that day, or the possible physical hazards in the area, like fallen trees or rocks.\n\n, If your risk assessment identifies a number of hazards, you need to rank them in order of importance. For example, a chemical spill in a chemical plant will likely be the most serious risk, and a back injury from lifting a barrel in the chemical plant may be a less serious risk.\n\n\nThe rankings of the hazards are usually based on common sense. Consider the hazards that could lead to serious injuries like death, the loss of a limb, or a serious burn or cut. Then, work down from the most serious to the least serious.\n\n, This could mean better spill prevention for the chemical plant, or a clear evacuation procedure in the event of a spill. You could also provide high quality PPE for workers to prevent exposure to the chemicals.\n\n\nNote if these improvements or solutions can be implemented quickly, or even with temporary fixes, until more reliable controls can be put in place.\nRemember that the greater the hazard, the more robust and reliable the control measures will need to be.\n\n, Your risk assessment may include the need for employee training on a safety practice, like picking up a box from the ground properly, or for training on how to deal with a chemical spill.\n\n, Another approach is to use a risk assessment matrix, which helps you determine how likely or unlikely a risk may occur in your workplace. The matrix will have a column for \u201cConsequence and Likelihood\u201d, which is divided into:\n\n\nRare: May occur only in exceptional circumstances.\nUnlikely: Could occur at some time.\nPossible: Might occur at some time.\nLikely: Will probably occur in most circumstances.\nAlmost Certain: Is expected to occur in most circumstances.\nThe top column will then be divided into sections for:\nInsignificant: Low financial loss, no disruption to capability, no impact on community standing.\nMinor: Medium financial loss, minor disruption to capability, minor impact on community standing.\nSerious: High financial loss, some ongoing disruption to capability, modest impact on community standing.\nDisastrous: Major financial loss, ongoing disruption to capability, major impact of community standing.\nCatastrophic: Mission critical financial loss, permanent disruption to capability, and ruinous impact on community standing.\nA copy of an example risk matrix can be found here: https:\/\/intranet.ecu.edu.au\/__data\/assets\/pdf_file\/0017\/207080\/how-to-write-a-risk-assessment-and-management-plan.pdf.\n\n, You are not required by law to share the risk assessment with your workers but it may be a good practice to share the completed document with them.File a hard copy of the risk assessment and keep an electronic copy on the company's shared drive. You want to have easy access to the document so you can update it or adjust it accordingly.\n\n, Few workplaces stay the same, and sooner or later, you will bring in new equipment, substances, and procedures that could lead to new hazards. Review your employee\u2019s work practices on a daily basis, and update the risk assessment. Ask yourself:Have there been any changes?\nHave you learnt anything from accidents or near misses?\nSet up a review date for the risk assessment in a year\u2019s time. If there is a significant change at your workplace during the year, update the risk assessment as soon as possible.\n\n","label":0,"model":"human","source":"wikihow","id":944}
{"text":" Most people don't know what cheerleading really is. If your parents think that cheerleaders are mean people that do nothing but wave pom-poms, explain to them what you really want to do, how much you love cheerleading, and that all-star cheerleading is not the same thing as cheering for your school or football. Make sure you aren't putting them on the spot. Make sure they agree with you and you aren't pressuring them. They will more likely help you if you stay calm and show them your mature.;\n, Be happy, smile, make friends, don't be mean, laugh a lot, and be sporty! Remember that being mean and rude to other teammates could get you kicked off the cheerleading team, so remember to be nice to everyone in your team and outside our team. Also make sure that you pick a good team.\n\n, Before you become an all-star cheerleader, you need to know what all-star cheerleading is. Look up videos on Youtube or visit the USASF (US All-Star Federation) website to see how it's like being an all-star cheerleader. Know all the rules and levels so you'll have an idea what level you'll be on and what you'll be doing. And be sure to know the difference between all-star cheerleading and football\/basketball cheerleading before you sign up!\n\n, If you're tall and strong, you'll probably be a base or backspot. If you're short, flexible, and agile you'll probably be a flyer. If you're a combination of both, maybe you could be two things at a time!\n\n, Look on the website for pictures, look on youtube for videos, and see if on the website they have requirements for the team you want to try out for. For previews on competitions go to websites like Jamfest or U.S. Nationals. Remember to find a good team. If the team only has seven girls or goes in only two competitions a year, you might not want to join if cheerleading is your passion. Make sure you join a good sized teams with plenty of levels and that goes on competitions! And if possible, try joining a team that goes to Worlds.\n\n, Don't be lazy! You should practice a little everyday. Stretch and exercise.\n\n, Believe it or not, the more flexible you are, the less you'll get hurt, because your body will be so used to bending. Stretch every day and do a couple of strength exercises like crunches, lemon squeezes, and push-ups about three times a week.\n\n, Run at least once a week. Being strong can help you get better at a lot of cheerleading things, like jumping, tumbling, stunting, or maybe even dancing if your team's dance is fast and difficult.\n\n, Do tumbling, private lessons, or a be in a recreational team for about two months before you actually do the real thing, and remember that you must know the basics. Those classes will teach you all the basics. If you already took something like gymnastics or tumbling before, you don't really have to. The better you are at tumbling, the more likely you'll be able to make a higher level in your team.\n\n, Practice in front of the mirror and remember to squeeze your body the whole time. While doing motions, keep your fists straight up and tight, not bent! The better your basics are, the better your tumbling will be.\n\n, Practice every day and when you're sitting down, be in the position of the jump you want to learn\/improve. When you jump, whip your legs and arms as hard as possible and keep your arms tight (that's why basic motions are important in cheerleading).\n\n, It may sound silly, but dance is important in all-star cheer. If you don't know how to dance, you'll be all messed up while your teammates are doing great in a competition, and all all-star cheer routines have at least one dance part in them. Your dance moves should be clean and tight, and remember to do facials\/smile while you dance.\n\n, They are very important cheerleading skills. Remember that in most tumbling passes, a cheerleader will do a roundoff\/roundoff back handspring before she does the final trick. Even thought a back handspring is essential, don't attempt one unless your coach already gave you permission to, because if you did one by yourself and fell, that could lead to serious injury.\n\n, If you fall backwards, just do a candle rock with your arms straight above your head, and stand back up. If you fall forwards, tuck your chin in and roll.\n\n, If you have a fear of going backwards, just practice jumping backwards on a mattress and doing bridges. Remember that tumbling can sometimes be scary\/dangerous, so be prepared to fall, because it's impossible to never fall!\n\n\nDon't be scared in any position you take. If you're a base, make sure you get under your flyer and don't be scared. If you're scared then the flyer will be scared as well. Be confident. If your a back-spot, don't be scared as well. Make sure you're catching your flyer if you do cradles or baskets. You need to grab the flyer's ankles or the base's wrists and pull up to relieve some of the weight. When doing cradles or baskets, help push then step aside to catch.\n\n, Facials give you more points in competitions and just make your team look better while performing. You need to look like you're having fun in a competition, because if you look bored, you're probably gonna lose.\n\n, Once you feel prepared, sign up for try-outs. Don't worry, you always make the team when you're an all-star!\n\n, Smile all the time and don't attempt anything you can't do or anything too dangerous. Be confident and always stretch first. Wear comfortable clothes. Look at the judges\/coaches in the eye and pretend you're just at home and no one is watching!\n\n, Don't make cliques in your team and don't try to mess anyone up! Also, don't laugh at someone if they fell or they can't do a tumbling trick everyone else in the team can do.\n\n\nDon't cry, be upset, or be too hard on yourself. Always be positive, because all cheerleaders are supposed to have a positive attitude.\n\n, Granola bars or energy drinks can be good, but don't drink too much caffeine or you'll get too hyper and will not be able to tumble!\n\n, Even if you already have a back handspring, using a barrel\/roller will make your back handsprings even better than they already are.\n\n,,\n\n\nDon't be afraid. The more you're afraid, the worse it may be. Stay clam throughout your routine, keep your composure, and work on facials. Facials is what wins the judges over.\n\n","label":0,"model":"human","source":"wikihow","id":945}
{"text":" a plastic bottle. You can use any you like, but 3 liter (0.8\u00a0US\u00a0gal) cider bottles would be better if you wanted a bigger biodome.;\n, The heavy, black plastic bottom has groves which add strength to the bottom shape. The oval dome lid snaps tightly onto this bottom.\n\n\nAfter enjoying the chicken, wash the container lid and bottom completely. (The big label sticker on the outside can be removed with a little Gumout or WD40). In the bottom fill the deeper grooves with coarse sand or fine, washed pea gravel.\nAdd real soil scooped out of your backyard with a spoon, building up about 1 inch (2.5\u00a0cm), patted down. Now, in your backyard or a city waste lot, find some small, low lying plants that are only a few inches high. Wet the area around each such wild plant with a cup of water. Then use a larger tablespoon to dig down and under the roots of the little plant you have watered, and gently lift it out.\nMany common weeds that are mowed, burned over or stepped on are fine. 2 examples would be wild strawberry, ordinary grass, or plantain. Even newly emergent seedlings, if they are small enough, will work.\nTaking care to keep as much of the roots and soil around them as possible when you lift them out, place them into the layer of soil you have put into the bottom. Now gather a few more such plants.\n\n, Even shingles on a roof often have moss on them that can be carefully scraped off. So after adding some compost soil or topsoil put some moss on the top and arranged around the little plants as a top ground cover. If the plants wilt a little at first, they will be fine when you water them if you have gotten most of the fine roots into the soil. Now sprinkle water gently until the soil, moss and plants are well watered. Your bottom plastic base will let excess water, and eventually the plant roots drain down into the deeper grooves.\n\n, Here you have to be careful no soil or plant parts are in the area you have to snap together. When you have snapped the lid. on correctly it will be tight and snug.\n\n,,, So, with a heated sewing needle (from a candle flame or match) you are now going to punch the hot needle through the upper sides of the dome roof in clusters of about 6 - 10 needle sized holes. #*Do this at each upper end, and the middle of each upper side. This will allow some moisture to escape so that the entire dome doesn't cloud or \"fog\" up. (The needle holes will be too small for bugs to escape.)\n\n, Once sealed up, these little biodomes will not need more water added for many months, even most of a year.\n\n, Cut 3 sides of the door only, so that the door can be opened and closed as needed, and use a tab of mending tape to keep the lid closed. For bugs, try to avoid earthworms. Small beetles or crickets or sowbugs are fine. Lacewings are good, and fruit flies will work too, if you can catch them.\n\n, Remember, you can also use a larger, fancy cake tray and dome that many grocery stores sell in the deli or bakery. The main thing with a larger such deli dome, is that you need the black plastic bottom tray to have enough rigidity and grooves for the sand and excess water.\n\n, But, what the heck - even if you have to eat the whole cake, it only weighs 3 - 4 pounds. Invite someone to help you eat it.\n\n, Firm it down to help stop bacteria.\n\n, Consider putting in these plants:\n\n\nFruit producing plants\nPerennial plants, plants that don't need much water\nSelf fertile plants\nPlants that produce a large amount of seeds.\n\n,,\n\n, Then, leave the seedlings (you should have about 3 or 4 evenly spaced) until they have covered most of the compost.\n\n,, If you want to be extra sure there are no gaps, put something like superglue around the joint.\n\n\nThe crickets or beetles will produce carbon dioxide that the plants will take in and give off oxygen as a by-product. The crickets will breathe the oxygen and the cycle goes on. The plants will be eaten slightly by the crickets, but, making sure they are fast growing, they will give the crickets or beetles the supply they need.\nOf course, the plants will take in the water you gave them, and they will respire and the water will just evaporate and condense, and will give the give the crickets something to drink and the plant something to drink. The crickets or beetles will get rid of the water one way or another, so it goes on.\nThe plants will get energy from the soil and light, the leaves will die off through time (replaced by new ones) and will rot into the soil, recycling nutrients and giving back the water that was retained in the leaves.\n\n","label":0,"model":"human","source":"wikihow","id":946}
{"text":",,,,\n\n\nThere are 4 sets of goals and each set contains 5 goals, so you can create a maximum of 20 goals.\n\n,,\n\n\nThe \"On\" radio button will be selected by default, but you can select the \"Off\" radio button if you want to activate your goal at a later time.\n\n,\n\n\nThe Goal Position field can be useful if you have multiple goals and want them to appear in a specific order in your Analytics reports.\n\n,\n\n\nChoose \"URL Destination\" if your goal is to have visitors land on a specific page on your website. This goal option will allow you to set up a funnel, which is useful if you want to track the progress of visitors as they click through pages to reach the goal destination.\nChoose \"Time on Site\" if you want to measure the amount of time a visitor spends on your website.\nChoose \"Pages per Visit\" if you want to view how many pages visitors browsed while they were on your website.\n\n,,\n\n\nThe Match Type field determines how you want the URL used by the visitor to match with the URL you identify for this goal type. In some cases, a URL can change slightly depending on where the visitor is coming from. An Exact Match is when all your URLs remain the same and never change. A Head Match allows you to designate a specific string of characters in your URL that you want matched and can be useful if visitors have an identification number when they are logged into your website. Regular Expression enables wild card matching of certain characters in the URL and is helpful if visitors land on the URL from a sub-domain.\n\n,\n\n\nInclude the part of your URL that follows after your domain; do not provide your domain. For example, if your website ends in \".com,\" include all the characters following \".com,\" including the forward slash.\n\n,\n\n\nIf your domain or URL requires certain upper-case or lower-case letters for it to work, place a check mark in the field next to \"Case Sensitive.\"\n\n,\n\n\nFor example, if your URL Destination is a checkout page for visitors that buy a product from you, enter the amount of total transactions you want to occur.\n\n,\n\n\nClick on \"Yes, create a funnel for this goal\" to open your funnel options.\nProvide each URL in the series of URLs for the funnel by typing in the characters following your website's domain.\n\n,","label":0,"model":"human","source":"wikihow","id":947}
{"text":" Don't treat yourself the way you wouldn't treat a friend. This means no more insulting yourself when you look in the mirror, setting harsh dieting rules, or beating yourself up when you make a mistake. If it would be mean to treat a friend that way, then stop treating yourself that way.;\n, Don't worry about trends or fashion \"rules\"\u2014choose clothes that make you look good and feel confident and comfortable. It's usually good to wear something you can move around in.\n\n\nYou don't need to wear makeup. If you decide to try it, start small and work your way up as desired.\nWear clothes that make you feel happy. Whether you love lace, rainbows, overalls, or poofy skirts, you should choose clothes that work for you.\nSkimpy outfits can move around when you wear them, and it's no fun to be constantly monitoring your neckline or hem. Err on the side of modesty so that you can have fun without worrying about whether your clothes are in place.\n\n, Treat it with respect, and look after your health. Work fruits, vegetables, whole grains, and fruits into your diet. Get plenty of sleep and relaxation. Find ways to make exercise fun, like dancing, swimming, walks with loved ones, and backyard sports.\n\n\nFocus less on what your body looks like, and more on what it can do.\nDrinking, smoking, and drugs are very bad for you! Avoiding or quitting them will help you feel more energetic, positive, and alert.\nTry to go to bed and wake up at around the same time each day. This will help your body get into a good rhythm. Ask your parents to remind you to go to bed if it helps.\n\n, Your free time is precious, so use it for what matters most to you. Engage in your favorite hobbies, read good books, and hang out with people you love. To shake things up, make a list of all the things you'd like to try someday. Then start trying them.\n\n\nTV is great for relaxing, but too much TV can leave you drained and tired. Balance your free time to include hobbies, family, and friends.\n\n, Just as you're growing physically and emotionally, you're also growing intellectually. Try reading books from different genres, from classics to science fiction. Try hobbies from arts and sciences. What is fun to you?\n\n\nLook for articles online about topics that you enjoy.\n\n, The media can have discouraging messages, such as hypersexuality, mean behavior, and rude and incorrect stereotypes. This is not healthy. Surround yourself with media that celebrates your values, and the type of person you want to be.\n\n\nLook for media that supports your self-esteem, especially in fighting negative stereotypes. A girl who loves robots could watch TV with women in STEM, a girl who uses a wheelchair could read books about people with disabilities, and plus-size girls can look at pictures of stunning women of the same size. Remind yourself that people like you exist and have worth.\n\n, Even the kindest, most cheerful people have bad days sometimes. It's okay to feel sad, get a bad grade on a test, or lose your cool every once in a while. Being good does not mean you have to be perfect, and people will understand that.\n\n\nIf you feel down, try talking to someone about how you feel. You'll feel better, and they'll be glad they could help.\nIf you make a big mistake, talk about it. Apologize to anyone you hurt, and ask how you can make up for it. The mistake is less important than how you handle it.\n\n, Maybe this means talking to a loved one who inspires you, going to a religious or volunteering group, reading a book that exemplifies your values, or even re-reading this article. When you feel lost or uncertain, it can remind you of your purpose and the way you want to live your life.\n\n, They have more life experience than you do, so they may be wiser than you realize. Ask them for advice, and value the things they tell you (even if you don't always agree). Think about how many good things they have done for you. Chances are, you'll never able to stop counting.\n\n\nParents aren't perfect, and sometimes they give bad advice and make bad decisions. Hear them out, and then explain your point of view as calmly as you can. Work together to figure out a good approach.\n\n, When you see your parents or siblings looking like they need help (struggling with packages, doing chores, etc.) say \"Is there anything I can do to help you?\" Open doors for them, help carry things... little things can make a big difference.\n\n, The next time you see your parents or older siblings doing chores, try asking them to show you how. The two of you can do it together. You'll practice and help out at home, and then you'll be good at it when you're an independent adult. Learn how to cook, do laundry, do housework, and other basic tasks. Help out your parents and anyone you see who could use a hand.\n\n\nTry yard work like mowing your lawn, taking care of the plants, and weeding.\nTry your hand at basic engineering tasks, such as auto repair, changing a flat tire, and fixing a leaky tap. Girls are better at engineering than most people realize!\nSome parents are willing to set up an allowance in exchange for chores, or pay you for doing chores (e.g. $10 for mowing the lawn).\n\n, Clean your room and any messes that you make around the house. If you see messes that are not yours, you can still help clean them up. Sweep, vacuum, put away clutter and dirty clothes, and clean things like windows and mirrors.\n\n\nCleaning up other people's messes is nice, and optional. Don't feel obligated if you don't have the energy.\n\n, Look for the things that you have in common, and have fun together. Speak politely to them (even when they annoy you!), and respect their boundaries\u2014they'll probably do the same to you in return.\n\n\nLet them have space when they need it.\nIf you can't handle your siblings right now, then don't. Say that you need some quiet time, and retreat to a quiet space. Your siblings should respect that, and if they don't, ask an adult for help.\n\n, Be respectful, kind, and helpful to everyone at school. This will make people like and respect you more.\n\n, Study for all tests and quizzes, because they all count. Turn in your homework on time and pay attention in class. Participate in class for even more points towards your grade.\n\n\nTeachers love it when you raise your hand in class, whether it's to give an answer or ask a question. If you give a wrong answer, it's okay.\n\n, Do your homework promptly every night and do the best job that you can on it. This will give you better grades, and it will also help you become more disciplined and responsible.\n\n, Cramming for three hours straight isn't good for your health, and you won't learn as much. Start working early to make it easy on yourself. Then you can take lots of study breaks and feel less pressured.\n\n\nDon't stay up late studying before a test. You need your brain to be fresh and alert, so get plenty of sleep in order to prepare.\n\n, Look at the teacher, face front, and focus on note-taking and listening.\n\n\nDon't chat, text, pass notes, or use your phone in class. These are disrespectful to your teacher and your peers who are trying to learn.\nIf you have a disability that causes you to fidget or appear inattentive to non-disabled people, let your teacher know. Find ways to accommodate your needs so you can focus.\n\n, Learn to ask politely for help if you are struggling in your classes. Most people love to give advice and be helpful, and your teachers want to see you succeed. Explain that you're struggling and ask for help. Their advice can help you, and they'll respect you more for it.\n\n, Remember the platinum rule: treat others the way they would like to be treated. This means being respectful and considerate.\n\n\nAs a general rule, don't say something behind someone's back that you wouldn't feel comfortable saying in their presence.\n\n, It's normal to have wants and needs, so be assertive and ask for things. Use \"I\" statements like \"I would like...\", \"I think...\", or \"When you ____, I...\"\n\n\nBeing passive might feel kinder, but it isn't. Hinting or beating around the bush will only confuse or frustrate people.\n\n, Listening and validating people's feelings are tremendously useful skills. This means treating them like what they say and how they feel is important (even if you don't agree with them). People will feel much better and start opening up to you if you do this.\n\n, Whether it's small like holding a door for someone, or big like creating a picture book for your little brother, random acts of kindness are a great way to be and feel good. Look for opportunities to make people smile.\n\n, If you think your friend's shirt is awesome, let him know. If your sister made a really cool science project, tell her how much you like it. Even a random compliment from a stranger can brighten someone's day.\n\n\nSometimes it's a bad time\u2014for example, you wouldn't stand up in the middle of a concert and shout \"I love your singing!\" You can wait until a good moment (e.g. after the concert), or tell other people about your thought. People always feel happy when they learn you've been saying good things about them behind their back.\n\n, Everybody has positive traits, so look for them in each new person you meet. Treat them with the assumption that they are well-meaning and intelligent in their own way. They may just rise to meet your expectations.\n\n\nKeep doing this, and you will be a ray of sunshine that inspires others to be their best.\nA few people are mean and rude no matter what you do. Keep a safe distance from these people, and continue being positive. It may rub off on them, or it may not.\n\n, It can be tempting to write people off as \"weird,\" \"stupid,\" or \"stuck up.\" Don't. Everyone has a story, and is struggling in a way that you might not notice. Treat everyone like they are good at heart.\n\n\nBe respectful to everyone, including the people you don't like. They may warm up to you.\n\n, Courtesy will show others that you are considerate and mature. Use the phrases \"please\", \"thank you\", and \"may I\". Let other people go first. Get a book on everyday etiquette, or ask someone who appears to be particularly knowledgeable on it.\n\n, People may say things that don't make sense to you\u2014but they probably have a good reason. Instead of giving up or getting mad at them, ask questions. This works in a variety of perplexing situations, from a person who is acting strange to someone who is being mean.\n\n\n\"That's interesting. Why would you say\/ask\/do that?\"\n\"I don't understand. Could you explain it?\"\n\"I'm surprised that an educated and considerate person like you would stereotype transgender people like that.\" (Compliments can defuse rude behavior.)\n\n\n, Nobody can truly \"win\" an argument. Calm yourself down, or excuse yourself if you don't think you can handle it calmly. You can always continue the discussion later when you have a cooler head.\n\n\n\"I need to take a break.\"\n\"I don't know how to respond to that.\"\n\"I'm so upset, I don't know how to handle this. I'm going to take some quiet time.\"\n\n, You won't get along perfectly with everyone, and that's okay. Spend your time and energy on the people who build you up and make you feel good. They can help you feel happy and remind you of the person you want to be.\n\n\nYou're allowed not to be best friends with everyone. If you feel upset when you're around someone, be polite to them, and focus your attention elsewhere.\n\n, A good girl doesn't rush into romance, and makes sure that she is ready before trying something new. Communicate with your partner, and talk about kissing and intimate touching before you do them.\n\n\nLearn how to say no. An \"I don't want to,\" \"Not tonight,\" \"I'm not ready for that,\" or just plain \"no\" makes it clear to your partner how you feel.\nLearn to recognize danger signals. Some people are disrespectful: they push your boundaries, laugh off your discomfort or worries, or trash-talk other people. Steer clear of them.\n\n, When someone does something kind for you, let them know you're grateful with a simple smile or \"Thanks!\" Give them a hug or some kind words sometimes, so they know how much you care.\n\n","label":0,"model":"human","source":"wikihow","id":948}
{"text":", Using the socket ended crossbar, loosen the lug nuts on the left or right front tire (the side you start on does not mater). Put a block of wood behind the opposite side tire to make sure the car does not move when jacking or working.\n\n, When jacking up your car, make sure to use the dealer specified jacking points first; if you feel the\u00a0dealer specified jacking points are not safe simply use the\u00a0pinch weld, as close as you feel is safe to the dealer jacking points\n\n, Make sure the jack stands and car do not move.\n\n,, Remove the caliper mounting bolts using the socket set. After the bolts have been removed, tie the caliper to the coil spring making sure the hydraulic line stays attached\u00a0to the caliper. Take notice of where the bolts are removed from.\n\n\n\n,, Take notice of which holes the bolts come out of.\n\n\n, The rotor may be difficult to get off due to rust and\/or friction, but the rotor simply pulls off\n\n, Most professional mechanics prefer to clean the braking surface with brake clean fluid before installation. This is not mandatory step but it is preferable to remove manufacturing compounds that may be left over., Rotors, brake pads, and brake pad clips should all be replaced together as a set, as\u00a0they affect how the rotors and pads wear while in operation.\n\n\n, Remember to put the bolts and mount back in the original position they were removed from. There is no torque spec for this component but it should not move at all. Tighten until the component is stable on the car.\n\n\n,\n\n\nA second person will be needed for the step. Untie the caliper from the coil spring and set aside the wire. It will not be needed further for the current side. Using the C-clamp, compress the piston all the way down.\n\n\n\n\n\n\n\n\n\n\n\n\n, There is no torque spec for this step but the caliper should be secured to the mounting bracket.\n\n\n\n,, Torque specs can be found in your vehicle owner's manual.\n\n, Lower the car back down to the ground.\n\n","label":0,"model":"human","source":"wikihow","id":949}
{"text":" Thrush, or oral candidiasis, is caused by an overgrowth of a type of fungus, or yeast, called candida albicans. Candida is a normal part of the body.Candida albicans is naturally found throughout the gastrointestinal tract, including the mouth. Candida is also a normal resident of the skin.A thrush infection happens when the candida albicans yeast cells find a source of nutrients they like, and grow beyond what is considered normal.;\n, White, patchy areas on the tongue, and other parts of the mouth are the most common symptom.Additional symptoms include inflamed, or reddened, areas in the mouth, accompanied by soreness. This can cause a sore throat, difficulty swallowing, and a loss of taste.Some areas might bleed slightly if they are scraped.Cracking, itching and pain at the corners of the mouth is common with oral thrush., Understand the risks of untreated thrush infections. Candidiasis infection, like thrush, left untreated can result in serious health problems.Candida is normally present on the skin, and throughout the gut, and causes no health problems.However, when an overgrowth occurs, it is important to treat the condition before it spreads further, and gains access to the systemic circulation. A systemic candidiasis infections is called invasive candidiasis.Realize the seriousness of an invasive candidiasis infection. Invasive candidiasis is an infection that occurs when the candida infection spreads into the systemic circulation, called candidemia.This type of infection is a serious condition, and can affect the blood, heart, brain, eyes, bones, as well as other parts of the body.People with weaker immune systems are the ones primarily at risk of developing invasive candidiasis. This type of infection requires hospitalization, involves added expense, and, in some cases, results in less than desirable outcomes.Invasive candidiasis is a major type of infection acquired by patients being treated in a hospital or facility setting for other reasons.See a doctor early. Seeking medical attention early and taking a prescription medication are the best ways to effectively manage the initial symptoms of a candida infection.This is the most effective approach to prevent invasive candidiasis and candidemia., It is somewhat rare to see oral thrush in healthy children, adolescents, and adults. But anyone can develop thrush, and the infection is easily treated.Since this condition is considered unusual in healthy people, there may be an underlying cause for the thrush to have developed.In addition, some conditions may look like candida, such as oral cancers or precancerous conditions, so make sure to see you doctor if you are haven't had thrush before or if it does not go away with treatment.\nIt is recommended that cases of thrush be evaluated by a doctor to provide both effective treatment, and to be sure there is no change in the person\u2019s overall immune system., According to the Centers for Disease Control and Prevention, thrush, or candida, infections that develop in the mouth or throat require the use of prescription medications for effective treatment.The exact medications and the length of treatment will vary depending on the person\u2019s age, general health status, currently prescribed medications, and immune system strength.Make sure that you complete the full prescribed course of medication, or else the thrush may return.\n\n, Treating oral candidiasis is often done by using a product that can be applied topically. This is especially true for infants and young children.Liquid products, such as nystatin oral suspension, are applied topically by either swabbing, or \u201cpainting\u201d, the suspension onto the surfaces infected. Nystatin is effective at treating the infection, and is safe if swallowed.In addition to using liquid forms, antifungal creams, ointments, and oral doses such as troches, deliver the medication in a topical manner.Use dissolvable prescription products. Some products are made in dissolvable forms, called troches, that are placed in the mouth to dissolve.This method of administration allows the medication to come into direct contact with the infected areas., In some cases, the medication may be prescribed in a tablet, capsule, or liquid form, that is intended to be swallowed.The antifungal medication works through systemic absorption, just like taking an antibiotic.Some examples of medications prescribed to treat thrush include fluconazole, nystatin, itraconazole, clotrimazole, ketoconazole, posacanazole, and miconazole.These medications can interact with other medications, so be sure to tell your doctor what medications you conditions you have. They can also have side effects, so call your doctor if you develop any new symptoms while taking these medications.\n\n, Infants that develop a thrush infections typically have the white, patchy, lesions, in their mouth. They may have trouble feeding, and be more fussy and irritable.The infant can pass the infection to the mother, then they continue to pass it back and forth, until the infection is effectively treated.The mother\u2019s breasts can become unusually sensitive and reddened, with cracked and itchy nipples. The circular area around the nipple, called the areola, can turn shiny, with flaky skin areas.The mother may experience pain during nursing or notice pain in the nipple area between feedings. The discomfort may also feel like stabbing pains deeper within the breast., Tell your doctor if your baby has a diaper rash also, as candida may also cause a diaper rash, and your baby may need different treatment for this. If your doctor considers the case to be mild, he or she may recommend just using good hygiene measures, and watching the areas in both baby and mom for a few days.Treat the baby. If treatment is warranted, the medications can be easily and safely applied.In many cases, an antifungal product called nystatin suspension, can be prescribed. This is a liquid medication that can be applied directly into the baby\u2019s mouth to the areas involved.\nThe applications are often recommended several times a day for about a week.Treat mom. To allow the mother to continue breast feeding and disrupt the cycle of passing the infection back and forth, the same medication, or a similar one, may be prescribed.\nTopical antifungal creams or ointments are applied to the affected areas of the nipple on mom\u2019s breast. Applications are usually recommended several times a day, for about a week, until both baby and mom are symptom free.You may also want to consider using disposable nursing pads to avoid passing the infection to your clothing.Talk to your doctor about cleaning or boiling items such as bottles and nipples, pacifiers and any detachable parts of a breast pump to reduce the chance of the infection coming back.\n\n, People that are diabetic, are prescribed inhaled corticosteroids, take certain types of antibiotics, and people that wear dentures, develop oral thrush infections more often than otherwise healthy adults.Some people with serious illnesses that involve weakened immune systems more commonly experience oral candidiasis.These groups include people with HIV or AIDS, people receiving cancer treatments, and people that have had organ transplants., Make an appointment as soon as possible to have the thrush infection evaluated and treated.The doctor will choose the appropriate prescription medications based on the overall health and existing medications of the person with thrush.People that are elderly, have asthma or COPD, and people with weakened immune systems require quick intervention to prevent the thrush from spreading into their bloodstream.\n\n, The at-risk groups are more challenging to treat since most are already taking a combination of medications that can sometimes interact with antifungal drugs.\n\n\nThe doctors know how to properly combine the antifungal medications needed, with their current prescribed drugs, to quickly and effectively treat the thrush infection.In some cases, intravenous therapy and hospitalization may be the safest course of action., One scientific research study looked for evidence of efficacy for natural and herbal products claiming efficacy in treating oral candidiasis. Unfortunately, the researchers were not able to find any evidence to support the claims.This does not mean the natural and herbal products do not work. The study results suggest that further studies, following proper scientific research methods, are warranted in order to show efficacy for these treatment approaches., When you have a thrush infection, warm saltwater rinses may provide some relief.Ask your dentist or doctor about using saltwater rinses to be sure it is the best option for you.To make a saltwater rinse, dissolve \u00bd teaspoon of salt in 1 cup of warm water.\nSwish the rinse around in your mouth. Be sure to spit out the rinse and avoid swallowing it. Repeat this several times a day., Scientific research has shown that probiotics containing the lactobacilli species can help to control the overgrowth of candida albicans in some situations.The study authors recommend additional research in this area, but the initial work done in a controlled research setting was promising., The scientific literature suggests this to be possibly helpful in treating thrush. However, products available are not adequately regulated and exact dosing recommendations are not available.Talk to your doctor about specific products or sources, recommended by him or her, to treat thrush in this manner.Yogurt containing live or active cultures of the lactobacilli species are difficult to find. Yogurt products are now required to undergo processes like pasteurization that kills the live cultures.\n\n, If you choose to try gentian violet, talk with your doctor first, then proceed carefully. Since safer and easier products are available, using gentian violet is often considered to be unfavorable option.\n\n\nGentian violet is effective in the topical treatment of fungal infections, including oral candidiasis, but the product is difficult to use. The product should not be swallowed, and will stain the skin and clothing.Side effects of gentian violet include redness, and irritation at the site where it is applied. Gentian violet should not be swallowed. Diarrhea, nausea, and vomiting can result. If gentian violet is swallowed, call a poison control center.One study found that gentian violet applied topically using a 0.00165% product was somewhat effective at treating oral thrush and did not stain the affected areas., See your dentist regularly, and follow recommendations provided by your dentist or your regular doctor.General recommendations to prevent thrush infections include brushing your teeth at least twice each day, floss once a day, and never share your toothbrush., Some people have trouble reaching all areas of their mouth with a regular toothbrush.Talk with your dentist about switching to an electric toothbrush if this will help you to more effectively clean your teeth., If you have recently had a thrush infection, you may want to replace your toothbrush several times.Use new toothbrushes, and discard any contaminated ones, until you are comfortable that the infection has been effectively treated, and your new toothbrush has not been contaminated., Some mouthwashes and rinses can alter the normal flora in your mouth, and allow for the candida infection to get started more easily.Talk to your dentist to be sure. Many dentists recommend the use of mouthwashes and mouth rinses., Sugary foods, and foods and beverages that contain yeast, can encourage the growth of candida.\n\n\nLimit the amounts of these foods that you eat or drink, and be sure to brush your teeth after eating., People that wear dentures at a higher risk of developing oral thrush infections.Your dentist can suggest different products and equipment to use in cleaning your dentures if this is happening to you., By maintaining tight control of your blood sugar levels, you can reduce the amounts of excess sugars that are found in your saliva.This helps to limit the sugary food source in your mouth area that helps the candida yeast to grow., Research has provided good evidence that this can help reduce the risk of oral candidiasis infections in people receiving cancer treatments.The most commonly prescribed prescription strength mouth rinse contains a solution of 0.12% chlorhexidine gluconate., Some people with lung conditions, like asthma and COPD, regularly use inhaled corticosteroids.If you can, use a spacer (or aerochamber) attached to your inhaler. This greatly reduces oral thrush from using inhaled corticosteroids. Both children and adults should use spacers. In addition, they help the medication go deeper into the lungs instead of to the back of the mouth.\n\n\nPeople that use these products can reduce their risk for getting thrush by rinsing their mouth with water, or a doctor recommended mouth rinse, after each use of their inhaler.","label":0,"model":"human","source":"wikihow","id":950}
{"text":" Then, try to make the dog put his head into the collar: don't push him if he is scared, but bring the harness very slowly closer and closer to the dog head, till he will have worn it; if he gets scared, freeze where you are, let him calm down, and then continue from where you stopped;\n,, You will do this by praising him when he runs, recalling him and encouraging him to resume running when he stops.\n\n,, Also add 2 more words ( like \"SLOW\" and \"RUN\" ), when you decelerate and when you increase your speed. At the beginning, you will start on straight paths: later go where there are some crossroads too: you will repeat \"LEFT\" or \"RIGHT\" any time you turn one or another direction.\n\n, If you are not sure, ask your vet about any doubt! Then, over weeks, slowly add speed and duration. Again, if in doubt, ask your vet, and anyway, better do less than the dog could do, than doing more and hurting him.\n\n,, Keep on praising and encouraging him when he runs in front if you, pulling you. If you notice he is unsure about what to do, increase your speed to make him understand he does have to run in front of you, faster than you, even if you are slowing down.\n\n, At the beginning, do not ride the bike, but run by its side: once the dog will fully understand how to behave, you can start actually riding\n\n, This step is not much different than passing from jogging to biking, but sled for sled dog are difficult to handle if you are not used to. Also, to pull one, you need at least three dogs, of the same size and weight.\n\n, Also, this is a demanding activity so it is important your dog is fit and well muscled to avoid injuries.\n\n, Also follow his\/her warnings on how to proceed.\nwhen you start, start with something that is not more demanding than your dog's usual activity: if he is used to walking for 5 minutes a day, start with a 5 minutes walk.\nadd difficulties gradually and slowly: a general rule is that when the dog, once home, stops being as tired as usual but only appears satisfied and still ready to go, it is time to add duration\/speed. For example: you start off with a 20 minutes walk, and once home your dog is not willing to do anything more for the whole day; let's say that after 2 weeks, he comes home tired but after, say, 6 hours he is ready to play and run again: then, you can start adding difficulties, for example from 20 to 25\/30 minutes walk\ntrain your dog for many activities: only doing one kind of activity\/training and nothing else is unhealthy and can lead to injuries ( it happens with humans too ). Instead, alternate sled dog\/running training with at least 2 other completely different sports, like disc dog ( lots of jumping ), agility ( jumping and sprinting ), swimming, etc. Also, teach your dog \"tricks\" ( things like: roll on the floor, give paw, etc. You can find many examples if you search for \"dog tricks\" on youtube ): they develop strength, agility and flexibility in dogs, which lead to less injuries, better performance, longer life and better health for the dog.\ndo not overtrain your dog: learn and respect his limit, never force him to do something he is scared of, uncomfortable with. Always bear in mind that doing a bit less than your dog could, will cause no damage, while doing only a little bit more, might cause serious injuries, that might take very long to heal. Also, as said before, train your dog for many different activities, not only sled dog\n\n","label":0,"model":"human","source":"wikihow","id":951}
{"text":" This will protect your clothing from any \"accidents.\";\n,\n\n\nFor most people who examine a fistulated cow, two shoulder-length gloves are best so that the material can be examined and handled by both hands. Often when one arm can't reach something, the other needs to be used instead.\n\n, This will better protect your hands from the smell of the gastric juices\n\n, The left side is where the cannula is always located.\n\n, You may have to pull out the plug; to do this, you grab the top part of the handle and pull down. This will break the seal from the top down.\n\n,, However, remember it doesn't have to be tight together like if you were rectal palpating a cow, because the opening of the fistula is large enough to put your hand through up to your shoulder.\n\n,, You may have to pull out handfuls of feed to get your arm in there at first, depending on how full the rumen is.\n\n,\n\n\nThe gas layer is at the top of the rumen and able to leak out of the rumen as the cannula plug was removed.\nThe second layer is the solid layer, which comprises of recently-consumed and partially digested roughage and grain. This is the stuff you should take out and examine before going in again.\nThe liquid portion is the bottom and third layer, comprising of grain, liquid digesta, and small particles of roughage. The particles in the liquid layer are more often half to a tenth or smaller of the size of the particles in the solid layer of the rumen.\n\n, They are hard to feel for, however, as they are much softer and pliable projections of the rumen than things like pimples or muscles. The papillae are round-shaped projections with a narrower base near the wall of the rumen, and are responsible for absorption and uptake of nutrients into the cow's bloodstream.\n\n, There are two main contractions: primary and secondary.\n\n\nThe primary contraction is the most pronounced and can be the most startling if you've never felt it before: the smooth muscle of the rumen contract in on themselves to mix digesta and microbes together to increase digestion ability.\nThe secondary contraction is less defined and may be harder to notice until you've identified it after the 2nd or 3rd contractions.\n\n, You may have to go in up to your shoulder, especially if you have short arms, to feel this.\n\n\nThe esophagus is basically a \"hole in the wall\" with sphincter muscles, and is located above the reticulum into the rumen.\n\n, The reticulum is always located in front of the rumen below the esophagus, and can be identified by feeling for the skin making a sudden peak from the rumen towards the front of the cow.\n\n,,\n\n\nIf you are the last one to examine the cow, the plug must be replaced soon after to keep stomach contents from being expelled. The plug has to be placed so that it forms a tight impenetrable seal from the outside world to the rumen.\nIt's often a lot harder to put the plug back in than to pull it out. It will take a bit of manipulation to put it back in properly.\n\n,\n\n\nIf you have only one hand that was gloved, take the glove off by taking the end of the glove at the shoulder and moving it down so that it turns itself inside out as you remove it. The latex glove will come off when you have the shoulder-length glove inside out already using your clean hand.\nIf you have a second shoulder-length glove on, take the first one off like you would with any other glove, then with the other one, take it off at the shoulder and moving it down so it turns inside on itself and keeps the other dirty glove inside as well.\n\n, Your OB suit will also be disposable and can be taken off and tossed as well.\n\n,","label":0,"model":"human","source":"wikihow","id":952}
{"text":" If you're new to an area, you can ask the county planning department whether your home is at risk for flooding.You can also check government sites for flood maps. Be sure to check back every so often; the maps are sometimes redrawn as conditions change.The main factor determining your risk is whether you are in a floodplain or not, which you can check with flood maps.Several other factors put you at risk for floods. For instance, if your main floor is below the base flood elevation in an area, you're at risk for flooding. You're also at risk if your near a body of water, such as a lake or a river. You're especially at risk near the ocean.;\n, That is, know the best ways in and out of your neighborhood and other areas of the city when it's flooded. You'll need to stick to higher ground if you need to evacuate. Also, have a planned meeting place for your family members if you get separated. Have the plan written down. Go over it together so everyone knows what to do.The best way to plan an evacuation route is to use flood maps, which will show where the worst flooding will be in your area.When planning your evacuation route, have an established place to go. For instance, you can set up a plan with a friend ahead of time to evacuate to his or her house, or go to your workplace if it is out of the flood zone. Many communities also have areas designated for emergencies where you can go.\n\n, That is, show them the emergency numbers you have displayed in your home. Show them how to dial the numbers, and go over what they need to say in an emergency. Also, have a safety contact in the neighborhood that they can go to if they have a problem., Designate one person who is not in the immediate area as the person your family checks in with. That way, at least one person will have all the information who is not in immediate danger., When thinking about how you will evacuate, don't forget to include your pets in your plan. Have enough carriers for all your pets so that you can evacuate them with you if needed.Carries keep pets contained so you can evacuate them without harming them.Don't forget to include other items for your pets. They'll need containers for food and water, as well as food and their normal medications if you evacuate. Remember, not all emergency shelters will allow pets. Also, try to take something that will remind them of home, such as a toy or blanket.If you must stay in your home, move your pets to the highest point in the house with you., If possible, buy flood insurance so that you can recover from the damage of flooding. If you live in an area with low risk, insurance shouldn't be too expensive. If you live in an area with higher risk, it will be more expensive, but it will be worth it if flooding ever destroys your home. In fact, you are required to have it in a high risk area if you have a federally insured loan.You can get insurance through a federal program, the National Flood Insurance Program, by filling out forms on their website., For water, that means packing enough for each person to have a gallon per day. For food, pack non-perishable foods like canned goods that you don't need to cook. Keep these supplies in a waterproof container.Don't forget to include a can opener with your food, as well as some utensils for eating.Also, remember your pets need to eat and drink, too, so take them into account., You'll need a multipurpose tool that includes items like a screwdriver and knife. You also need extra phone chargers and a spare set of keys., Keep a first aid kit in your box, along with a supply of soap, toothpaste, toothbrushes, shampoo, and other toiletry items. Antibacterial hand wipes are also good to keep on hand., These items can include things such as sunscreen, bug spray, emergency blankets, and rain boots., That is, have a weather radio with extra batteries. You'll also need to keep friends and family informed, as well, so remember to have emergency contact information on hand., As noted earlier in the article, you can ask the your county planning department about the frequency of flooding at a potential building site.If you have no choice about where you build and you're located in a flooding area, you need to build an elevated, reinforced house to protect against flooding., Your furnace, air conditioner, electrical unit, and hot water should all be lifted above the ground to keep them from being flooded.Also, electrical outlets and wiring should be a foot above any flooding that's likely.You should have a professional perform these tasks.\n\n, Make sure you have copies of all of your insurance policies, pictures of your possessions and home, and any other important documents in a safe place. You either need to keep them in a waterproof box in your home or in a safety deposit box., A sump pump pumps out collected water, usually in basements. If you're home is prone to flooding, put one in your home, and be sure it has a battery back up in case your electrical goes out., These valves prevent flood waters from coming up drains., Have a professional evaluate your home and create barriers around your home that will prevent water from entering your house., If you have a basement, have the walls sealed with a waterproof sealer, which will help keep water out of that area., Turn on the weather radio for reports on flooding in the area so you can stay informed., If you have standing water, turn off the electricity by flipping the main breaker switch to your house's electricity. You should also turn it off if you plan to leave when there is flooding or if you see power lines on the ground., You should locate it ahead of time. Generally, you turn the handle a quarter of a turn until it is perpendicular to the pipe to shut off the gas. You'll need a crescent wrench to make the turn., Usually, you will need to turn a small valve to the right several times to turn it off., Wash the areas with a bleach solution, and rinse them clean. Fill them up to have clean water available to you. Also fill up any other pitchers or containers you have with water., If you have furniture or grills, bring them inside or tie them down to secure them., If you have enough warning, move any important items, such as electronics or valuable furniture to higher ground, such as upstairs or in the attic.","label":0,"model":"human","source":"wikihow","id":953}
{"text":" How will you spend that money on the 23 positions available?;\n,, On this list, have the most you are willing to pay for a player, and set yourself at that price. Don't go too hard after certain players because they play for your home team.\n\n, $1 players are usually the backups, or subpar starters.\n\n,, A well-balanced team may leave you smack dab in the middle of the standings, unable to dominate in any particular statistic.\n\n,, Take the redeemers and reap the rewards.\n\n,\n\n\nIf your league has minor league and minor league picks, use those in your trades.\nTrade for categories you are in need of, not something you are already strong in.\n\n, If you read the book, you realize Billy Beane is just running a de facto fantasy baseball team. Though you needn't overanalyze your fantasy team, general principles outlined in the book for picking players can be useful. Joe Shlabotnik's .350 batting average means diddly squat next year, because he can't walk and can't hit for power. Only fools would pick Shlabotnik high in next year's fantasy draft.\n\n, Advanced statistics can be good as predictive tools, but in the end the league will likely be scored based upon more simplistic statistics.\n\n, As the great P.T. Barnum once said, \"there's a sucker born every minute.\",and this applies even in fantasy baseball leagues. You will almost surely have someone in your league who insists on buying high and selling low and riding every player's short-lived wave of success for all it's worth. Use them to purchase players with high upside for nominal prices.\n\n","label":0,"model":"human","source":"wikihow","id":954}
{"text":" As soon as you find out your child will need to undergo surgery, find out as much about the surgery as you can - to give you some peace of mind, and so that your child can be informed about what's going on.While finding out as much as possible is best, try to find out at least the basics, which includes:\n\n\nWhen and where will the surgery take place?\nWhat does the surgery affect? How invasive is it?\nWhat's the typical recovery time?\nIs it common for patients to be in pain after the surgery? If so, what is the best treatment option for the pain?\nWill your child need to be temporarily admitted to the hospital?\nWhat are the procedures for you and your child before, during, and after the surgery?\nWhat are some things that you'll need to bring with you to the hospital (insurance information, emergency contact numbers, etc.)?;\n, No two children are alike, regardless of whether or not they're autistic, and you'll need to make sure that the doctor is aware of any potential medical afflictions that could cause complications with your child's surgery. Even if your child has no other health issues except for the one resulting in the surgery, you may need to tell the doctor about things like your child's contact lenses or any past surgeries or medical treatments. Oftentimes, you'll fill out a form detailing your child's medical history, but ask the doctor if there's anything else they need to know.\n\n\nMention any health issues or requirements to their doctor. For example, if your child is prone to seizures, has any allergies, or has a medical condition such as diabetes, the doctor will need to know.\nBring up pre- and post-surgery needs, too - for example, medications your child may need to take in the morning, or if their diet should change after the surgery.\nIf your child is on any existing medications and will need to go on a new one after the surgery, work with the doctor to find a medication that won't potentially conflict with your child's existing medications.\n\n, Autistic people may struggle with the change in environment or certain factors of the environment in the hospital, and the doctors and nurses want to help accommodate them and make them more comfortable. Ask about accommodations that your child could be given before and after their surgery, and during a hospital stay. A few examples of these accommodations are:Can your child be sedated before administering an IV?\nIs it possible to remove unnecessary equipment from the pre-operative room to prevent high anxiety?\nCould the doctors and nurses remove laboratory coats before entering the child's room?\nWould it be possible for very few people to come in at a time, to avoid scaring your child with the amount of doctors or nurses?\nIf your child can reliably communicate, can your child speak or use AAC instead of you communicating for them? If they use AAC, what kind of AAC can they use?\nCould your child be in a post-operative room that does not have harsh, flickering, or buzzing lights? Can the lights in the room be dimmed, as an alternative?\nCan the nurses make sure you're in the room when your child wakes up?\nCould you and your child tour the hospital so that your child can be more familiar with the setting?Discuss comfort objects with the doctor as well. For example, you could say something like, \"My child is autistic and needs to carry a stuffed fox to keep calm. Can we make sure that she has her fox when she falls asleep and when she wakes up?\"\n\n, Autistic people can have an unusual recovery time compared to neurotypical people,and even if your child recovers at the typical pace, they'll still need time to rest and recover. Do everything possible to avoid scheduling the surgery on a week that's busy for either you or your child, whether it's because of their school, your work, their therapy, or any outside activities that may potentially conflict with the surgery or the recovery period.\n\n\nThere are four types of surgery - major surgery (such as to correct issues caused by birth defects), minor surgery (such as correcting a bone fracture), elective surgery (when you schedule the surgery beforehand), and emergency surgery (such as for a potentially fatal heart defect).Depending on the type of surgery, your scheduling options may be limited, or you may not be able to schedule it yourself.\nIf the child's surgery can't wait until there's a break from school, then you will need to discuss your child's surgery with their teacher and school attendance.\nIf you absolutely can't avoid scheduling their surgery when everyone is busy, then cancel any non-important events that you can, move your child's therapy appointments, arrange to pick up their schoolwork, and make sure that there's always an adult able to care for your child. Your child's recovery is more important than their speech therapy.\n\n, Even if your child is nonverbal and never appears to be paying attention to you, they can still listen and understand what you're saying, and it's important that they know what's going on. While it's recommended to talk to toddlers and preschoolers early due to their limited concept of time, and to older children as soon as possible so that they have advance warning, consider your child's memory and skills at changing their routine when it comes to talking to them about the surgery.\n\n\nTry using social stories, a picture schedule of what will happen at the hospital, or reading age-appropriate books with your child about surgery, and exposing your child to the sort of equipment they may see while at the hospital with toy medical kits.The Children's Hospital of Philadelphia has a slideshow for autistic children that explains what happens at the hospital, which can be accessed here.\n\n\nBe careful about the language you use. Don't say things like, \"The doctor will cut you open\", \"You'll be put to sleep\", or other language that may be associated with a scary or sad event. Instead, say things like, \"The doctor is going to fix your stomach so that it won't hurt any more\", or \"They'll help you sleep for a few hours. When you wake up, I'll be there with you\".Portray the hospital workers as kind and helpful people, not as people who will punish your child for misbehaving.If your child is a literal thinker, try to avoid using figurative language at all, to lessen the risk of confusion.\n\n, Your child may be very frightened of the upcoming surgery and regress to younger behaviors or start using harmful stims. Be patient and be there for them. Remember that it's normal for any child to regress to younger behavior when under stress, and that they need your support and care right now.\n\n\nIf your child is verbal or has access to complex AAC, encourage them to share their concerns with you. This can help you comfort your child about the things they feel worried about.\n\nValidate their feelings and make sure they understand that they aren't getting surgery as a punishment.Tell them that it's okay for them to cry and be scared.Try encouraging them to engage in therapeutic play, such as \"playing hospital\", before the surgery, to make the process more known and less scary. It will also help your child learn the routine of a hospital setting.\nDon't lie to your child to relieve their fear - they'll figure out that you were lying. If they ask if something is going to hurt, say something such as, \"Yes, it will hurt, but it will be over quickly\"., It's often recommended to feed your child soft \"safe foods\" after surgery, even if the surgery wasn't oral or gastrointestinal, and they're going to need some help managing their pain. Before your child's surgery, make sure that you purchase foods and pain relief medications, pick up any prescriptions, and any medical supplies that they may need (such as ice packs or bandages). Check with your child's doctor to see if there are any necessities.\n\n\nYour doctor may recommend getting over-the-counter drugs for your child's pain, such as Tylenol, or give you a prescription. Avoid aspirin-based OTC medications, and check with the doctor about whether ibuprofen is safe to use.\nGo shopping with your child, if they can handle it, to help them pick out foods and drinks that they'd like.\nPick out clear liquids and smooth, bland foods for your child. Examples include non-textured ice cream, pudding, apple or white grape juice, soft noodles, soup broths, and applesauce.\nYour child may be disappointed that they can't eat foods that they normally would eat, or that they have to eat only bland or smooth foods. Empathize with them and remind them that once they feel better, they'll be able to return to their typical diet.\n\n, Your child will most likely need rest afterwards, so find things they can do. A child who is hyposensitive to pain will need activities to help them stay resting, while a hypersensitive child may be in a lot of pain and will benefit from distraction.\n\n\nStop by your local library for books and movies.\nVideo games may or may not be appropriate, especially if you have a big-screen TV where your child can lie down while playing games.\nConsider buying your child a gift they can enjoy while relaxing, such as a new book about their special interest, or a relaxing video game. This will cheer them up.\n\n, Hospital passports are a strategy used to help accommodate autistic children in the stress of a hospital environment. Write down your child's likes, dislikes, sensory sensitivities and cravings, communication methods and strategies, ways to help them adjust, and things that your child is interested in.This can help the doctors and nurses work with your child, and gives information to anybody who gets involved in the situation a bit later than anticipated.\n\n\nTry making a few copies of the hospital passport, so that you can give them to nurses. You can also put one in your child's suitcase if they need to be admitted.\nYou may want to put other information, such as your child's medical history or emergency contact numbers, on the hospital passport.\n\n, In the cases of some surgeries, your child may need to be admitted to the hospital while they recover to ensure that there are no complications. If your child will need to be admitted to the hospital, pack the bag with them, and tell them what you're packing. Pack them a bag that contains things such as:Clothes and\/or pajamas\nShoes or slippers\nSmall travel-sized toiletries, such as shampoo\nToothbrush and toothpaste\nStim toys, preferably low-energy ones\nObjects or activities that the child enjoys, such as books, stuffed animals, or movies\nPreexisting medications - sometimes. Check with the hospital on whether to bring them.\nBe careful with bringing electronic devices. Some hospitals don't allow things like cell phones and tablets, and they're easy to lose or break. However, if your child needs an electronic device to communicate, talk with the hospital staff to see if they'll make an exception.\nDon't pack anything that you or your child would not want to get lost or broken, as things can get lost, damaged, or accidentally thrown away in the hospital.\n\n, If your child has siblings, they'll need to be informed of the upcoming surgery and that your child will need to rest and recover. When discussing the surgery with the child's siblings, explain to them as much as they can understand, and encourage them to ask questions and express their emotions.Don't leave them in the dark about it, or they may become confused, scared, or betrayed - be upfront and honest.\n\n\nSiblings in particular may be feeling a range of emotions about your child's surgery, and may express them in ways such as acting out, trying to get attention (whether by constantly pestering you or pretending that they're sick), not eating or sleeping as much, withdrawing from friends or family, or requiring more attention than they used to.These are signs that they are under stress, and should be addressed.\nYour child's siblings may be worried about home life changing and them not being cared for as much. To combat these fears, set aside blocks of time for you to spend with your child's siblings, and keep the care routines as normal as possible. Offer extra encouragement and praise for them when they do good things., Your child's surgery will undoubtedly be stressful for you, too, and it's important to get support from others. Seek out help from friends, your parents or siblings (if possible), and\/or your partner. If necessary, talk to a mental health professional about your fears. Don't let your fear and stress eat away at you.\n\n\nRemember, it's completely normal to be worried about your child during this time. You aren't being silly or having unjust fears.\nIf you have friends or family members who have also had a child who has undergone surgery, ask if they can give you some advice. While techniques for neurotypical children may not work for autistic children, there's some advice that is universal, such as the type of comfort objects your child should have, or how to ease your worries during the surgery.\n\n, In many cases, your child will not be able to eat or drink anything starting from midnight of the night of the surgery, so it's best to make them a meal that they'll eat a lot of. Make sure they drink enough, too. Encourage your child to eat as much as they'd like, and maybe give them a treat for dessert afterwards.\n\n\nSome surgeries require that your child is on a specialized diet before the surgery.If this is the case, then you're unfortunately not going to be able to follow this step, unless there's something in the restricted diet that your child especially enjoys.\nIf your child has poor impulse control, hide the food away and stay with them to make sure they don't try and sneak food. If your child eats or drinks before the surgery, the surgery will have to be postponed or rescheduled.If your child needs to take medication in the mornings, discuss with the doctor whether or not they can take their medication on the morning of the surgery.\n\n, This doesn't mean to put their clothes on for them if they don't need that kind of help, but to help them pick out clothing that is comfortable and won't get in the way. Choose clothing that is light, comfortable and can be easily slipped off, as they may have to change into a hospital gown.\n\n\nIf your child wears contacts, encourage them to wear their glasses instead.\nBring a jacket or sweater for your child. Hospitals are often cold. However, be careful with a sweater if you think your child will resist changing from their clothes to a hospital gown - it can be difficult to get them to take them off.\nIf your child has long hair, encourage them to leave it down. Buns and high ponytails can interfere with the ability to rest their head comfortably on the pillow. However, low-hanging hairstyles (such as side ponytails or braids) can help keep your child's hair out of their face and look great as well.\n\n, Autistic children often stim to express emotions or keep calm in stressful environments, and it's not considered unusual for children to carry around a comfort blanket, toy, or object. Allow your child to pick out something that they can carry with them for comfort, and request that the doctors and nurses don't suddenly take it from your child.\n\n\nIt's best to take something big, rather than a small fidget toy, as these are less likely to be lost.\nFocus more on your child's comfort than whether the toy looks age-appropriate. It's okay for anyone of any age to be carrying around a big stim toy or stuffed animal if it helps them keep calm.\n\n, You will probably not be able to leave the hospital while your child is undergoing the surgery, so bring things that can keep you occupied during the waiting period. Hospitals sometimes have reading material in waiting rooms, but consider bringing things like:\n\n\nWritten games (crossword puzzles, Sudoku, etc.)\nElectronic devices, if they're permitted, and any chargers for these devices\nWork projects\nBooks or magazines\n\n, When your child is being checked into the hospital for their surgery, make sure to mention any health problems that your child may have, and bring up their needed accommodations to the nurses working with them. It can be helpful to the nurses to remind them of your child's needs, which, in turn, helps your child.\n\n\nDon't be afraid to bring up your child's autism. If your child will need extra support because of their autism, it's actually best to bring it up so that the nurses are aware and can give your child some extra support and accommodations.\nIf you're bringing up your child's autism, say \"autistic\" or \"autism\" rather than \"Asperger's\" or \"PDD-NOS\". It's entirely possible the nurses aren't familiar with diagnoses of anything other than autism.\nAsk the nurses to avoid restraining your child under any circumstances to avoid panicking them.\n\n, Since your child is under stress, it's quite possible that they may lose some of their abilities. Be extra patient with your child, and keep an eye out for things they might be struggling with.\n\n\nIf you suspect your child may lose their verbal ability, bring a form of AAC that they can use.\nThe stress may be especially overwhelming to your child if they get aggravated by sensory input, broken routines, or even just having not eaten for several hours.\nDo everything possible to help your child avoid a meltdown or shutdown.\n\n, During the pre-surgery preparations, you and your child may need to wait, which can result in your child becoming anxious. Stay with your child and keep them company, and do whatever they find calming, whether that means that you don't say anything as they play with a stim toy or that you talk with them until the doctor arrives.\n\n\nRemind them that they will be okay, and that you'll be there when they wake up.\nIf your child is in pain, do what you can to distract them from the pain. That can be anything from distracting them with a TV show on the hospital TV, to offering deep pressure through a hug or rubbing their back or hand.\nHelp them redirect their stims if they begin using agitated and harmful stims.\n\n, Many children are scared of needles and IVs, and it can be tough to get any child to cooperate with you when needles get involved. Distract them from the process by offering a stuffed animal, a stim toy or encouraging them to infodump while the IV is inserted. If your child is nonverbal, talk to them about a happy event or something that you know they really enjoy.\n\n\nIf your child is receiving anesthesia or sedatives through a mask, they may try to pull the mask off their face. Talk to them throughout the process and try to hold their hands (but don't restrain your child).\n\nNever pin your child down - restraining your child will result in them becoming fearful or panicked and fighting you, and these practices have been proven to not be beneficial.All autistic children are different. A hyposensitive child may not even feel the IV going in, while a hypersensitive child may attempt to jerk away from the IV in fear.\n\n, Many autistic children find comfort in having a familiar person nearby, and parents are often encouraged to stay with their children before the surgery process anyway. You don't need to talk to your child (though it's recommended, to calm them); just being in the room with them can be a big help.\n\n\nYou will not be allowed in the operating room, which may frighten your child if they're still awake. Tell them that you'll be there when they wake up.\n\n, When your child wakes up after their surgery, they'll likely be confused and disoriented. Talk softly to them and make sure they know you're there. It can take some time for your child to wake up fully, so be patient with how long it may take.\n\n\nDon't wake your child before they're ready, despite how much you might want to. They need rest, not only to heal faster, but to also allow administered pain medications to kick in.Some parents report their autistic child trying to pull the IV out of their arm.While the nurses are trained to deal with children doing this sort of thing, you may want to stay near your child during the wake-up process.\n\n, When your child comes home, they're going to need different care than is normal for them, and you aren't going to be able to remember it all off the top of your head. You may either get this information during or after your child's surgery, but be sure that you receive post-operative care information in a written form so that you aren't left scratching your head in confusion when your child needs later treatment.\n\n\nPrint-outs from the doctor detailing post-surgery care can be useful, if they're available.\nTake notes on how to treat your child's wound, what activity they can and can't engage in, what they should and shouldn't eat, who to call if there's an issue, who the follow-up appointment will be with, and what symptoms are and aren't normal.\nWrite down the doctor's phone number in case you need to call them about your child's treatment.\n\n, Many surgeries no longer require your child to stay for an extended period of time at the hospital to recover; however, some do, or there may be potential complications or health problems that result in the need for a longer stay. It's quite possible this will distress you and your child, especially if an unexpected event has resulted in the need for admittance, but keep it in perspective and explain it to your child. Remember, the hospital workers are trying to help your child get better, not take them away from you.\n\n\nMany hospitals will allow you to stay overnight with your child during their hospital stay. However, if that isn't the case, make sure they have comfort objects, enjoyable activities, and things that remind of them of home.\nIf you need to leave the hospital while your child is there, tell them where you are going, why, and when you should be back.If you need to be gone for long periods of time, have another family member (such as your partner) stay with them.\nConsider creating a schedule for your child if they need to be in the inpatient ward for more than a day, so that they have a routine., Your child may still be very tired from the anesthesia, but even if they aren't, it's important to make sure they rest to help the healing process. For the first few days, encourage them to rest as much as possible and to only get up to use the bathroom, and to let you know when they need something that requires getting up.\n\n\nWhen your child wants to sleep, roll them onto their side. It's not uncommon for children to vomit after having been under anesthesia,and if they're on their side, they're much less likely to choke if they vomit in their sleep.\nMake sure your child is resting, even if they feel capable of performing normally. A hyposensitive child may try to go about their life as normal, which can potentially injure them or slow their healing.\nIt's common for children to sleep more than usual for a few days after surgery, or to be wobbly and unsteady at first. Constipation, gas, urinary retention, nausea, sore throats or jaws, and dizziness are also common. These symptoms should wear off within a few days., If your child is still stressed out and is unable to communicate reliably, or if their surgery affected their mouth or throat, they may not be able to speak. Autistic children who can normally use one form of AAC may also need to temporarily regress to a simpler form of AAC while they recover. Pick out a form of AAC that would work best for your child, since they'll need to be able to communicate with you during recovery.\n\n\nA child of any age can try ringing a bell or pressing a button to call for you.\n\nThe Picture Exchange Communication System (PECS), or cards with request words (e.g. \"drink\", \"toilet\", \"lonely\", \"hurt\") written on them, can be useful for requests or even expressing basic emotions, but can't be used for conversation.\nCommunication boards can be used to express emotions and requests, but they can't be used for conversation. It can also be hard for a child to use them if the surgery or effects of anesthesia have affected their motor skills.\nComplex AAC apps, writing, and typing can work for school-age children who have developed enough fine motor skills to use them.\n\n, Early on in the recovery process, it's discouraged to give your child solid foods, but it's still important to keep your child hydrated. Make sure they have clear liquids easily accessible to them - such as water, apple juice, ginger ale, or Popsicles- so that they stay hydrated. It may be a good idea to keep a sealed cup with a straw by your child's bed, so that there's less of a risk of your child accidentally knocking it over.\n\n\nIf your child isn't vomiting, then you can try giving them a light meal, such as soup or crackers.Make sure that your child can tolerate the food's texture or flavor.\n\n, Your child will probably not be able to do most things on their own, even if they're normally fairly independent. They may need round-the-clock care, or need to be checked on periodically. Make sure that your child is being cared for and checked on often enough, and that you're nearby so that they can call for you or otherwise get your attention.\n\n\nIf you need to leave the room for a short amount of time, get another person to watch over the child, or give them an object such as a bell to ring if they need you. Older children who have cell phones can be advised to text you.\nRotate caregivers. Maybe your spouse could watch over your child while you run out to the store or pharmacy, and your child's siblings could spend time playing board games with your child while you're making dinner.\nBring your child their box of stim toys if they feel the urge to stim.\n\n, Your child may need to take medications, use ointments, have a hot or cold compress applied to the surgical site, have bandages changed, or otherwise need special care. Be sure to follow these instructions to help your child heal and prevent infection or illness.\n\n\nTalk with the child while helping them with these procedures, so that they aren't caught by surprise and they understand what's going on. For example: \"I'm going to put an ice pack on your neck now. It's going to be cold, but it will help your throat feel better and help you heal faster.\"\nDon't restrain your child during these processes, even if they struggle. It will just result in your child becoming fearful of these times. Work with them to find a way to make it manageable for them. If your child really will not cooperate no matter what you try, however, contact their doctor and ask if there's an alternative way of treating your child.\n\n, Figure out who is in charge of the child's medicine, and make sure that everything is administered as instructed and according to schedule. Even if your child is older and more responsible, someone should be with them when they are taking their medicine, to make sure they are doing it correctly.\n\n\nDo not expect the child to keep track of their own medicine, even if they're normally able to. The exhaustion of surgery may make an otherwise-responsible child unable to reliably take medicine according to the doctor's orders.\nTry keeping a checklist to track what has been taken already.\n\n, If your child is curious about things, they may want to explore the surgery site and its stitches. While you shouldn't discourage your child's curiosity, make sure they understand that they shouldn't touch the stitches or prod at the area around it. Tell them that if it itches, they need to not scratch, as it can break the stitches. Encourage them to learn about surgery by showing them age-appropriate books or websites about surgery and stitches.\n\n\nIf you suspect that your child will pick at or excessively touch their stitches, put soft pads or gauze over the stitches to prevent them from pulling the stitches out.\nBe especially cautious if the surgery was in an area that might be frequently touched on a regular basis (e.g. their hand or mouth). Help your child with tasks that involve this area so that they don't accidentally injure themselves or reopen the wound.\nPick pajamas and clothes for your child that are less likely to potentially rub against the surgery area and cause discomfort, or snag on stitches.\n\n, Especially if your child's surgery requires an extended period of resting, it may be easy for them to get bored or restless, or they may want a distraction from the pain. Offer them activities that require minimal energy, such as:\n\n\nWatching TV or a movie\nReading books\nUsing a computer or tablet for games\nColoring in a coloring book, or drawing\nStimming with low-energy stim toys\nEngaging in their special interest, such as researching or using low-energy toys\nIf your child is further along in the recovery process, it may be okay to give them some higher-energy activities or stim toys. However, don't allow them to engage in intensive activities, sports, roughhousing, or any other activity that may potentially injure your child., Surgery can be very painful and stressful for an autistic child, so spend time with them and calm them down. If your child is struggling emotionally, it can help to have a loved one nearby.\n\n\nRead to them.\nHave conversations, if they are up for it.\nSit next to them while you do everyday things, like reading or fiddling with your phone.\nOffer cuddles if your child likes them.\n\n, Hypersensitive children in particular may be extremely sensitive to pain and struggle with pain management after surgery. If distraction isn't enough, then you may need to administer pain medication, whether it's over-the-counter or prescription medication. Pain medications often exist in both liquid and pill form;choose what your child is capable of taking and tolerating, and follow the instructions on the prescription or on the post-surgery care instructions. Contact your child's doctor if your child is still in pain when the medication has kicked in.\n\n\nRecognizing pain in autistic children can be difficult, especially if they're hyposensitive or nonverbal. However, if your child seems extremely subdued or shows less interest in their activities than normal, is crying, is \"protecting\" the pained area, is expressing pain or making noises that suggest pain, is grinding their teeth, or is unable to use a part of their body (e.g. unable to stand after their leg was operated on), they may be in pain and need medication.Avoid giving your child ibuprofen or other NSAIDs unless their doctor says it's okay; these medications can cause stomachaches or diarrhea, and can cause potential health complications if your child has kidney issues, blood clotting or heart problems, or asthma.Check on your child's pain periodically. While some autistic children may have trouble using a plain 1-10 scale that's often used to describe pain, it may work to write down what the numbers would entail (for example, writing under 10, \"I hurt so much that I can't focus on anything else\"). It's best to give your child pain medication when the pain isn't extremely intense, since it can take longer for the medicine to take effect when the pain is extreme., If your child's school isn't on break, they'll most likely need to get the classwork and homework assignments that are being assigned. You or another caretaker of the child can pick up their homework and bring it home, or a friend or sibling who attends the same school can talk to your child's teacher and drop off the homework. It's best to not let it pile up until your child returns to school, since it can be immensely stressful to your child to receive a large stack of papers on a single day.\n\n\nEncourage your child to rest rather than do homework in the early days. Your child may be worrying about missing a lot of schoolwork, but after surgery, recovery is more important than schoolwork. It's often encouraged to prevent your child from doing heavy lifting (such as lifting backpacks or textbooks) after surgery, anyway.\nUse the internet to your advantage. Some teachers will offer homework assignments, lessons, or the daily schedule online, especially in the later grades. Others may be willing to send photocopies of the assignments over email.\nDepending on how long they are out of school, ask if they can record the day's lesson to review and catch up later.\n\n, If your child has siblings, those siblings may get bored or frustrated more easily when your child is recovering from surgery, especially since your time has to be unequally divided between your children. To avoid your other children getting agitated or being unkind to your recovering child, involve them and have them help out and spend time with your child. Help them understand how your child is feeling and encourage them to play with and talk with your child to help them feel less lonely.\n\n\nOlder siblings may be able to help out with some mild care tasks, while younger siblings can help by playing games with your autistic child. Both age ranges can come and get you if your child needs something.\nDon't overwhelm them with sibling time - just set up a block of time for your child's siblings to spend with them (e.g. half an hour every few hours) so that your autistic child doesn't feel lonely and that their siblings are more involved. Be prepared to cut the time short, too.\nYounger siblings may pester your autistic child about the surgery and why they can't do what they can normally do, which can be a source of frustration for your autistic child. Make sure your younger children understand that they should talk to you about the more technical aspects of the surgery and recovery, and leave the personal questions (e.g. \"What was the hospital like?\") to your child.\n\n, Your child may take some time before they feel they're ready for more than just liquids, and that's okay. However, when they're ready, try serving the soft foods you bought to your child to adjust them to eating more solid foods. Don't force your child to eat if they don't feel ready, though - just make sure they get enough fluid intake.Stay away from giving them food that has to be heavily chewed, food that's very spicy, or something that could otherwise interact with the surgical area and dislodge a blood clot.\nYour child may reject foods that don't align with their sensory preferences, or refuse to eat things that they may eat when not under stress. Don't take it as a cause for alarm - it's normal for autistic children to refuse certain foods under stress, and eating enough is more important than eating well.\n\n, It's normal for your child to be disoriented or slightly groggy after the surgery. Hypersensitive children may also report frequent and intense pain. However, most of these symptoms should wear off a from few days to a week after surgery; if your child is not showing any signs of improvement, speak with the doctor as soon as possible, as illness after surgery may indicate a serious problem. Contact the doctor immediately if you notice that your child has any of the following symptoms:Fever or chills\nExcessive bleeding (however, oozing blood on bandages is often normal)\nPersistent vomiting, lasting for several days\nExcessive swelling around the surgical site\nSigns of infection - extreme pain to the touch (more than what's typical for your child), leaking pus or drainage, redness, hot to the touch, or a bad smell\nSigns of a poor or allergic reaction to a medication\nDifficulty breathing\nIrregular heartbeat\nSigns of shock - dizziness, confusion, loss of consciousness, clammy skin, severe sweating, low to no urine output, chest pain, anxiety or agitation, blueish lips or nails, and a rapid yet weak pulse, As your child recovers from surgery, they'll begin to feel more capable of taking on certain tasks, like working on their schoolwork or getting back into sports or clubs. However, they'll still need to rest, since their body isn't finished healing yet. Make sure that your child doesn't overwhelm themselves with work, and encourage them to do their activities slowly and cautiously, even if they feel like they don't need to go slowly.\n\n\nSet up allotted time for your child to work on a given activity, such as working on their homework. When the time is up, encourage them to rest, or give them a low-energy activity to do, like watching their favorite movie.\nMake sure they take breaks from daily activities, even if they feel like they don't need them. Returning to normal activities before the healing process has completed can be detrimental to your child's health.\n\n, The recovery process can be frustrating, upsetting, or painful for any child, and your child may be moody, distraught, or irritable while they're recovering. You may be frustrated, too, whether because your child is irritable or because the recovery process is stressful. Remember, though, that your child can't magically make themselves get better and that taking out your frustration on them will upset them and make them feel guilty or like they're burdening you. Do your best to be patient with your child during the recovery process, without snapping at them or doing actions that would otherwise upset them.\n\n\nContinue getting support from family and friends. You're allowed to be frustrated or upset, and you don't need to feel guilty about it; it just isn't the best idea to express that in front of your child. Talking to others gives you an outlet without potentially upsetting your child.\nRemind your child when they're frustrated that they will get better, and that it's okay for them to be upset or annoyed at the recovery process. If they're verbal and lash out at you, don't take it personally - many people say things they don't mean when they're aggravated or upset.\nIt will be okay - your child will recover, even if it's not on the schedule that most people would expect, and they won't be in pain forever. Don't stress!\n\n","label":0,"model":"human","source":"wikihow","id":955}
{"text":" Set the parking gear and the brake. Block the wheels. Raise the front of the New Yorker with a jack(s) and support the car with jack stands. Remove the splash guards under each fender to reveal access to the mounts.;\n, Raise the jack only until it just begins raising the engine a very little, to mainly support the engine, but not to raise the vehicle at all.\n\n, (If your mounts have a heat shield between the mount and engine, remove the nut or bolt from the bracket holding the heatshield, using your ratchet and socket. Remove the heatshield.) Keeping the nuts loosely screwed on to just a few bolt threads, on the bolts, keeps the mounts aligned until both are loosened -- retained to this same degree, held on loosely, on each side of the engine.\n\n,,,,,,\n\n\nInstall the heat shield and its retaining bolts, if your vehicle is so equipped (on the 3.5-liter engine, for example).\n\n,, Snug the nuts and bolts on all both mounts, at first only \"finger tight\". Then snug them all a little.\n\n, The large nuts and bolts might be torqued to about 45 to 50 foot (13.7 to 15.2\u00a0m) pounds. The smaller bolts on the transmission mount are torqued less, perhaps to about 20 to 30 foot (6.1 to 9.1\u00a0m) pounds.\n\n, Raise the front of the vehicle enough to remove your jack stands or blocks and such, and lower your New Yorker to the ground.\n\n","label":0,"model":"human","source":"wikihow","id":956}
{"text":" And just like anything we do in life, fear comes from \u2018not knowing\u2019. So imagine what you could accomplish if you could perform home wiring safely and confidently. Imagine saving thousands of dollars over the years, if you could do-it-yourself!;\n, It\u2019s very important that you do not get in a hurry. Make sure that you have planned your project adequately, and that you\u2019ve allowed plenty of time to complete your project, or at least if you have to pull off of it and come back to it later, that you find a suitable stopping point, and that you can do without the circuit that you\u2019re working on.\n\n, It\u2019s not only dangerous. It is lethal. Shut the power off to any circuit that you are working on. (This means turn the power off at the circuit breaker panel. Just switching off a wall switch does not turn off the power in the wires.) Confirm the power is off with a simple pocket tester, a multi-meter, or lamp, blow dryer or another similar appliance.\n\n,, Fiberglass ladders are non-conductive. Don\u2019t use an aluminum ladder. Spend the money for a good fiberglass stepladder.\n\n,, Never work barefoot or in socks or slippers, and don\u2019t assume that it\u2019s safe to work without rubber-soled shoes on concrete floors. Concrete is conductive, particularly when it\u2019s damp (a good reason to never load or unload your washing machine while you\u2019re barefoot).\n\n, Even if by definition it\u2019s called an insulator. (A conductor allows the flow of electrons, and an insulator resists the flow of electrons).\n\n, OSHA requires us as contractors to lock it off, and tag it out with a procedure called lock out\/tag out. It involves red tags and devices that will lock the breaker off to prevent it from being turned on. (If you have your panel cover off, remember that even when you turn breakers off, there are still energized components in the panel itself!).\n\n, or \u201cDanger\ufffd?, or something similar, so anybody who approaches that panel will see that, and immediately know what\u2019s going on.\n\n,, Put your other hand either in your pocket or behind your back, and that\u2019s a good practice to develop anyway. What that does is keeps you from grabbing a circuit with two hands and providing a path for the electricity to flow through your heart.\n\n,, It is well worth spending a little extra money to purchase quality tools. I\u2019m referring to hand tools like your lineman pliers, screwdrivers, wire strippers, and other hand tools that you will use for electrical work. For instance, Good wire strippers will prevent you from nicking or skinning the wires. Good screwdrivers will prevent slipping out of screw heads or rounding them out. You get my point; good tools not only improve the quality of your workmanship, but improve your confidence as well. So don\u2019t skimp on tools. You can stock your tool pouch with good quality tools for $100 or less.\n\n","label":0,"model":"human","source":"wikihow","id":957}
{"text":" Although you can stack cakes of any shape, traditional layer cakes are round. 8 or 9 inch (20.3 or 22.9\u00a0cm) round pans work best.;\n, Layer cakes often bring together complimentary cake and frosting flavors. You may want to combine lemon with vanilla or chocolate with raspberries.\n\n\nUse butter, eggs and water at room temperature. Set the on the counter for about an hour before baking., If you are just learning how to assemble a layer cake, save some time by buying cake mix and pre-made frosting.\n\n, Take it out of the oven for 10 minutes. Then, loosen it from the pan and place the cake on a baking rack to cool all the way.\n\n\nTest the cake by inserting a toothpick into the center. If it comes out clean, your cake is done.\n\n,, Place them in the refrigerator or freezer overnight. If you are short on time, do it for at least a few hours.\n\n\nA cold cake is easier to frost., You can trace the cardboard and cut it out or buy a round cake platform at a kitchen or craft store.\n\n\nIf you don\u2019t want to use a platform, slide pieces of parchment paper underneath the cake before you frost to cover your cake stand. Remove them before you serve the cake.\n\n, Any thick, buttercream-like frosting should work. To make a quick version at home, mix together three cups (360g) powdered sugar and one cup softened butter.\n\n\nAdd one tsp. (5ml) vanilla extract and one to two tbsp. (15 to 30ml) of whipping cream once the mixture has been creamed. Beat in your mixer on medium speed for five minutes altogether.,, Unwrap the first one. Set it atop the platform.\n\n, Make sure they are no higher than your shortest cooked cake. If the batter was not poured to exactly the same amount, one cake may be shorter than the others.\n\n\nYou can also cut each cake into two cakes if they are thick enough.\n\n, You may want to measure and mark the cake with a frosting dab if you aren\u2019t confident you can cut in a straight line. Ensure saw through the domed top of the cake very gently.\n\n\nIf your cakes came out of the oven very level, you can skip the step of cutting them flat.\n\n, Use an offset spatula and start with a big clump of frosting in the middle. Work the frosting out just past the sides of the layer.\n\n, Then, set the second layer of cake on top of the frosting.\n\n, Stack layers of frosting and cake until you have reached the top of your cake. Remember the more layers you have, the more frosting you will need to make.\n\n,, It\u2019s easier to judge the amount of frosting you are using if you frost near eye level.\n\n, Frost it to the edges.\n\n, Be generous with your frosting. Finish one section before turning the Lazy Susan and moving onto the next section.\n\n, Wipe off excess frosting with your spatula. Cover with an even coat of frosting and then place your cake in the refrigerator for a few hours.\n\n\nRemove it from the refrigerator and frost with a fresh coat layer of buttercream.,, Serve.\n\n","label":0,"model":"human","source":"wikihow","id":958}
{"text":";\n, When VBA works with data, execution speed is partially a function of the number of bytes that VBA has at its disposal. Thus, the fewer bytes used by data, the faster VBA can access and manipulate the data.\n\nThe Decimal data type is rather unusual because you can\u2019t actually declare it. It is a subtype of a variant. You need to use the VBA CDec function to convert a variant to the Decimal data type.\n\n, The following list contains some examples of assignment expressions that use various variable types. The variable names are to the left of the equals sign. Each statement assigns the value to the right of the equals sign to the variable on the left.\n\n\n\nDataEntered = True\nDateStart = #04\/18\/2015#\nInterest_Rate = 0.025\nLoanAmount = 2000000.00\nMyNumber = TheNumber * 1.25\nUserName = \u201cTerrence O'Malley\u201d\nx = 10\nx = x + 1\n\n\n\n, If you attempt to use one of these words, you get an error message. For example, although the reserved word Next or True might make a very descriptive variable name, the following instructions generate a syntax error: Next = 64; True = True., If an instruction produces a strange error message, check the VBA Help system to ensure that your variable name doesn\u2019t have a special use in VBA. If the Auto Syntax Check option is turned on you get the error: Compile error: Expected: variable. If Auto Syntax Check is turned off, attempting to execute this statement results in: Compile error: Syntax error. It would be more helpful if the error message were something like Reserved word was used as a variable., For integer calculations, you can use the Integer type which is limited to values less than or equal to 32,767. Otherwise, use the Long data type. Using the Long data type even for values less than 32,767 is recommended, because this data type may be a bit faster than using the Integer type. When dealing with Excel worksheet row numbers, you want to use the Long data type because the number of rows in a worksheet exceeds the maximum value for the Integer data type., Data stored as a Variant changes type, depending on what you do with it.\n\nThe following procedure demonstrates how a variable can assume different data types:\n\nSub VariantDemonstration()\nTheVar = \u201c124\u201d\nTheVar = MyVar \/ 2\nTheVar = \u201dAnswer: \u201d & TheVar\nMsgBox TheVar\nEnd Sub\nIn the VariantDemonstration procedure, TheVar starts out as a three-character string. Then this string is divided by two and becomes a numeric data type. Next, TheVar is appended to a string, converting TheVar back to a string. The MsgBox statement displays the final string: Answer: 62.\n\n\nTo further demonstrate the potential problems in dealing with Variant data types, try executing this procedure:\n\nSub VariantDemonstration2()\nTheVar = \u201c124\u201d\nTheVar = TheVar + TheVar\nTheVar = \u201dAnswer: \u201d & TheVar\nMsgBox TheVar\nEnd Sub\nThe message box displays Answer: 124124. This is probably not what you wanted. When dealing with variants that contain text strings, the + operator performs string concatenation.\n\n\n\n, Here\u2019s a modified version of the previous procedure. This version displays the data type of TheVar at each step.\n\nYou see that it starts out as a string, is then converted to a double, and finally ends up as a string again.\n\nSub VariantDemonstration3()\nTheVar = \u201c124\u201d\nMsgBox TypeName(TheVar)\nTheVar = TheVar \/ 2\nMsgBox TypeName(TheVar)\nTheVar = \u201cAnswer: \u201c & TheVar\nMsgBox TypeName(TheVar)\nMsgBox TheVar\nEnd Sub\n\n\n\n, A user-defined data type can ease your work with some types of data.\n\nFor example, if your application deals with customer information, you may want to create a user-defined data type named CustomerData:\n\nType CustomerData\nCompany As String\nContact As String\nRegionCode As Long\nSales As Double\nEnd Type\n\n\n\n,, Usually, you define an array. For example:\n\nDim Customers(1 To 100) As CustomerData\nEach of the 100 elements in this array consists of four components (as specified by the user-defined data type, CustomerData).\nYou can refer to a particular component of the record as follows:\n\nCustomers(1).Company = \u201cAce Tools\u201d\nCustomers(1).Contact = \u201cTim Roberts\u201d\nCustomers(1).RegionCode = 1\nCustomers(1).Sales = 150000.00\n\n\nYou can also work with an element in the array as a whole. For example, to copy the information from Customers(1) to Customers(2), use this instruction:\n\nCustomers(2) = Customers(1)\n\n\nThe preceding example is equivalent to the following instruction block:\n\nCustomers(2).Company = Customers(1).Company\nCustomers(2).Contact = Customers(1).Contact\nCustomers(2).RegionCode = Customers(1).RegionCode\nCustomers(2).Sales = Customers(1).Sales\n\n\n\n, You refer to a specific element in the array by using the array name and an index number. For example, you can define an array of 12 string variables so that each variable corresponds to the name of a month. If you name the array MonthNaming, you can refer to the first element of the array as MonthNaming(0), the second element as MonthNaming(1), and so on, up to MonthNaming(11)., You can also specify the number of elements in the array. You do so by specifying the first index number, the keyword To, and the last index number \u2014 all inside parentheses.\n\nFor example, here\u2019s how to declare an array comprising exactly 100 integers:\\\n\nDim TheArray(1 To 100) As Integer\n\n\n\n,\n\nTherefore, the two statements that follow have the same effect:\n\nDim TheArray(0 to 100) As Integer\nDim TheArray(100) As Integer\nIn both cases, the array consists of 101 elements.\n\n\n\n, If you would like VBA to assume that 1 is the lower index for all arrays that declare only the upper index, include the following statement before any procedures in your module: Option Base 1, VBA arrays can have up to 60 dimensions, although you\u2019ll rarely need more than three dimensions (a 3-D array).\nThe following statement declares a 100-integer array with two dimensions (2-D):\n\nDim TheArray(1 To 10, 1 To 10) As Integer\nYou can think of the preceding array as occupying a 10-x-10 matrix. To refer to a specific element in a 2-D array, you need to specify two index numbers. For example, here\u2019s how you can assign a value to an element in the preceding array:\nTheArray(3, 4) = 125\n\n\n\n,, A dynamic array doesn\u2019t have a preset number of elements.\n\nDim TheArray() As Integer\nBefore you can use a dynamic array in your code, however, you must use the ReDim statement to tell VBA how many elements are in the array. You can use a variable to assign the number of elements in an array.\nOften the value of the variable isn\u2019t known until the procedure is executing. For example, if the variable x contains a number, you can define the array\u2019s size by using this statement:\n\nReDim TheArray (1 to x)\n\n\nYou can use the ReDim statement any number of times, changing the array\u2019s size as often as you need to. When you change an array\u2019s dimensions the existing values are destroyed. If you want to preserve the existing values, use ReDim Preserve. For example:\n\nReDim Preserve TheArray (1 to z)\n\n\n\n","label":0,"model":"human","source":"wikihow","id":959}
{"text":";\n,\nAddress: <input type=\"text\" id=\"inputTextAddress\" style=\" width:200px\" title=\"Address to Geocode\">\n\n,\n<input type=\"button\" onclick=\"codeAddress()\" id=\"inputButtonGeocode\" style=\"width:150px\" title=\"Click to Geocode\" value=\"Click to Geocode\">\n\n,\nvar geocoder;\n\n,\ngeocoder = new google.maps.Geocoder();\n\n, It will not have any values passed to it.\nfunction codeAddress() { }\n\n,\nvar sAddress = document.getElementById(\" inputTextAddress\").value;\n\n, The first is the GeocoderRequest, this says what kind of request is being made and what the request value is. The second is the callback function that will be used to process the results.\ngeocoder.geocode( { 'address': sAddress}, function(results, status) { });\n\n, Use an IF statement to test the result, check to see if the status equal google.maps.GeocoderStatus.OK. Also add an ELSE clause to the IF statement as well.\nif (status == google.maps.GeocoderStatus.OK) {\n}\nelse{\n}\n\n, You will pass this method to get the result's first geometry location.\nmap.setCenter(results.geometry.location);\n\n, Create a new variable \u2013 we'll call it oMarker \u2013 it will be created as a new google.maps.Marker. The new method takes two parameters, the first is the map object that you're adding the marker to, and the second is the position to place the marker, which is again the first results geometry location.\nvar marker = new google.maps.Marker({\n\u00a0map: map,\n\u00a0position: results.geometry.location\n});\n\n, You can use the status to give a bit more information rather than just saying that it didn't work.\nalert(\"Geocode was not successful for the following reason: \" + status);\n\n, Type in an address, or simply a city and state, or even something as simple as a state name! You'll see the map move to the new location and add a marker to the map!\nThe live page for this example can be viewed and used through a link from the sources and citations area if you scroll down!\n\n","label":0,"model":"human","source":"wikihow","id":960}
{"text":" These stages could more accurately be described as cycles. You may skip stages, never experience other stages, and you can find yourself stuck in stages. But you may experience some or all stages in waves repeatedly. These stages are:Denial and isolation: This stage involves denying the reality of the situation. It is a natural response to overwhelming pain caused by grief.\nAnger: This stage emerges once the denied pain starts to surface. Anger can be aimed at inanimate objects, strangers, family or friends. You may feel angry at the person who died or left, and then you may feel guilty for feeling angry.\nBargaining: In this stage, you may feel like you need to regain control from feeling helpless. You might worry about how you should have been a better person, or you should have gotten help sooner, and so on.\nDepression: This stage brings sadness and regret that comes with the realization that the loved one is truly gone. You may feel overwhelming sadness, crying, and so on.\nAcceptance: This stage may be characterized by reaching a state of calm and withdrawal. Some people may never reach this stage of grief.;\n, The relationship has, in fact, died. Therefore, it is okay to feel as though someone precious has died. You are entitled to feel your loss. Ride the waves of grief without getting pulled under or lost in them. Don\u2019t fight them. Recognize them for what they are: waves of emotion that will take you through some strange currents for a season all the while giving your heart space to heal. Grief is part of healing.\n\n\nEven if no one else in your life knows what you are experiencing, you can still acknowledge your own pain to yourself. When you feel down, take a moment and say, \u201cI\u2019m sad, and it\u2019s okay. It gets better.\u201d\n\n, While those around you may not understand the depth of what you are feeling, don\u2019t be afraid to share your grief with a couple of people you know you can trust.\n\n, If you are concerned that you may be grieving unhealthily or that you are depressed, you may want to seek professional help. A therapist can help you understand your grief and whether you are becoming depressed.\n\n\nRead \u201cHow to Get Rid of Depression\u201d to better understand depression.\nIt can be useful to talk with a therapist even if you\u2019re not depressed. A therapist can help you understand how to work through your grief.\n\n, The old adage \u201ctime heals all wounds\u201d is true.However, healing is promoted by realistically facing the emotion and giving yourself time to recover. We may want a quick fix, but ultimately quick fixes don\u2019t exist where real love did. Accept the healing time offers and commit to not rushing yourself.\n\n, Tackle that mountain of time in small bites. You can push the pause button on planning long-term goals. This is truly a time for one day at a time.\n\n, You may still feel pain, but you may soon notice that it is less intense. Recognize the monumental step of healing for what it is. It\u2019s a promise of better days.\n\n, Find the balance that is healthiest for you of allowing sad moments while letting in new happy ones. When a wave of negative emotion hits, give yourself a moment (literally maybe just a single minute) to feel what you feel. Then, choose to move your thoughts on to something more positive.\n\n\nFor the record, it\u2019s okay to laugh when grieving. Your emotions are merely recalibrating. Believe it or not, your emotions are doing exactly what they should be doing. That said, sometimes the recalibration process hits a snag and we may find ourselves dealing with depression, which is a serious matter.\n\n, Once you get over the initial grief of losing this person, it is a good time for a truthful look at your former relationship. Start by recognizing what existed. If you have lost the loved one because of death and are trying to move forward with your life, you may discover that you have idealized your relationship with your loved one, overlooking the times that weren\u2019t so great in favor of the times that were. You are not dishonoring your loved one by recalling these less than ideal times. You are, instead, remembering the real and true person. If love existed between you, then part of what made the love so special was all of the in-between moments, and the ways you were able to work through differences.\n\n\nDon\u2019t place this person unnecessarily on a pedestal in death. Holding him up so high can keep you from holding him close in your heart and moving forward, which is not what he wanted for you.\nIf, your loss was relational rather than from someone passing, the same applies. Your relationship was not perfect. If it had been, you wouldn\u2019t be trying to move on. Even if they were the one to cut things off, it still reveals some frailty to the relationship, and that is okay.\n\n, Your relationship, like most, probably had a series of high and low moments. If you were not the one to end the relationship, you may find yourself idolizing it a bit. It is okay to look back and remember the good times. But it\u2019s important to be realistic. There were not-so-good times as well.\n\n\nAppreciate the positive aspects of the relationship, and how the other person contributed to who you are now.\n\n, It\u2019s important to acknowledge the characteristics of the relationship that brought out the worst of you. This doesn\u2019t mean that the other person was bad. But it can show you that there were some toxic elements when you were together.\n\n\nOnce you have recognized these toxic elements, you can appreciate the chance to \u201cdetoxify\u201d a bit. This will give you a chance to work on avoiding those pitfalls in other relationships. It will also help reframe your perspective of what you have lost. It helps you to give it a proper place in your thinking so you are free to move on.\n\n, Being honest about the relationship and the other person is important in reconciling your current emotions and your attempt to let go and move on. But it is important to avoid vilifying the other person, even if he treated you poorly. Too much dwelling on the past can be harmful.Assigning negative connotations or dwelling on particularly darker moments can strengthen emotional ties to the thoughts of that person, making them hard to let go. In fact, your love can turn to resentment. This doesn\u2019t free the person from your heart. It only frees him from your kindness. You deserve to be totally free to move on, so be cautious about giving him even the negative pieces of your heart.\n\n, Isolation is normal and okay for a short time. But it is vital that you don\u2019t isolate yourself from your closest supports for more than a short time. They love you and need to know you are alright. They know you better than you do yourself at times. They can help you get back to who you are at your best.\n\n\nThese are the people who know how to be silent with you and know when to push you to step out and have fun. They know how to make you laugh, and they are supportive when you need to cry. You don\u2019t have to let everyone in, but trust those closest to you.\nThese people can also help you recognize if your grief goes into depression and if you need professional supports.\n\n, Your friends and family might tread over the topic of the other person without realizing how much you are struggling. It is okay to let friends know when you need a change of subject. Just be honest and let them know that you need time. Be specific about what hurts you and what you would like to avoid for the time being., It\u2019s important to know your pain threshold and protect yourself. Perhaps you agreed to be friends with your former loved one, but the \u201cfriendly\u201d phone calls are painful. Be honest about how you\u2019re feeling. You may need to totally separate while you give yourself time to heal.\n\n, Most likely you have coworkers, classmates, or even friends and family who are just outside of the \u201cclose supports\u201d category. They may not be the ones you spill your guts to, but they still play a part in your life. It is perfectly fine to turn down your coworker\u2019s invitation to lunch for a spell, but then it will be time to let these people be the lighthearted, friendly, distraction-from-life they were before.\n\n\nThese supports generally come with natural boundaries you have already established. You tend to avoid deeply personal conversation, and keep things fun and on the surface. They won\u2019t expect you to trudge into emotional trenches over your 30-minute lunch break.\n\n, This isn\u2019t about replacing what was lost. Rather, it\u2019s about moving on. Once you find yourself dealing less and less with your grief, you find your mind less preoccupied with the person you\u2019re letting go of. Now it is time to be open to new people. New people are exciting.You do not, under any circumstances, have to get out there and date in order to move on. In fact, even thinking that way may terrify you at the moment. So let\u2019s dial back the intensity and reframe this in a comfortable way. Instead of diving into the dating scene, dive into the possibility of new friendships. Friendships can take on so many wonderful forms. Some friends are more like family. Sometimes friends move past friendship and become romantic partners. Sometimes friends simply remain friends. Regardless of where a new friend fits, you can\u2019t go wrong by being open to having more of them.\n\n, Emotions can overwhelm and even silence a person. It is time to find your voice. Talk things over with a family member, friend, counselor or minister.\n\n\nThere are times when something is so personal that it can be hard to open up to those who know us. You may consider making an appointment with a counselor or clergy member. Feelings have a way of jumbling up, making them difficult to articulate. An objective third party individual can help by asking you the right questions to untangle your emotions without inserting their own opinions.\nThe important part here is to simply start talking, rather than getting stuck in your own head where there is no one to help validate or correct your thoughts.\n\n, Write a letter to your loved one. Then, let it go to signify your own mental choice to let him go. Some people find it cathartic to burn their letter, signifying a definitive end. Or you may prefer something more considerate of the place this person will always hold in your heart. This may be more appropriate if you are letting go because this person has passed.\n\n\nYou might prefer placing your message in a helium filled balloon and setting it free.\nAnother option is creating a sky lantern with words of love written on it and sending it off as though mailing your loved one.\n\n, You may also choose to journal your feelings. Give space to the feelings you feel now, as well as the ones you hope to restore.Journaling allows you to be completely honest with yourself because your words are for your eyes only.\n\n\nThis practice also helps you to identify patterns in your thinking, your actions and behavior.\n\n, Changing even the slightest thing in your life can help you feel refreshed and remind you that life is still fun. Rearrange your furniture. Try a new haircut. Drive a different way to work. Eat dessert first. Whatever you choose to change, no matter how small, pick something enjoyable. It may only be a temporary mood lifter, but that may be all you need to remind yourself that you can still smile and enjoy life.\n\n, You\u2019ve grieved and you\u2019ve used your time to honestly consider the relationship. You have learned to honor your emotional limits as well as to challenge them. You have begun to let people in and you have found your voice. Now it is time to move on. Honor your loved one\u2019s life by living your own. His love impacted you because of how he lived, not how he died. Continue his legacy of love and live by giving yourself to the paths of love and life in front of you.\n\n\nToo often, people allow their grief to drain them of the best characteristics they shared with the one who passed. Instead, let their love with you continue on by allowing them a happy place in your memories. Learn to smile again and laugh again when recalling your loved one. They can continue to bring you joy in those memories. Humor heals., While it is important to give yourself time to heal from the broken relationship, at some point you will feel ready to let someone else in. However, you don\u2019t want to be the one carrying old baggage into a new relationship, whether the relationship is friendship or romantic. Think about whether you are free from thoughts about this former love. If you still think of him even a couple of times a day, then you could still find yourself in a rebound relationship. Even rebound friendships can be a problem because you are experiencing a temporary gap in your emotional needs and may be gravitating to someone who fills that need. But this relationship will not likely be a good overall fit. In fact, he truly may have nothing else to offer you.\n\n, Are you able to go to places you used to frequent with your former love and not immediately think of him? If your world still screams his name, then you probably need more time.\n\n, Until you are ready, it is okay to avoid places that still sting of the person you are learning to let go of. But keep in mind that pain is layered. While avoidance is okay in the beginning, eventually you\u2019ll want to challenge yourself to heal completely. Consider revisiting old venues with a trusted friend. Then you can start to create new memories and associations. Start with where you are comfortable, and slowly begin to rewrite your own memories and stories. Those places can still be special.\n\n\nWhen that one song comes on the radio, do you still think of that person? If so, it may be too early to move on. You may need to reclaim that memory by tying it to new experiences. Try sharing the song with your friends and ask them to help you give it new meaning. Make it funny. Remember, humor heals.\nIf you love the view from a certain restaurant, then meet a few of your best friends there. Laugh, enjoy yourself and start connecting that place to joy again. Peel back the layers bit by bit, and give them new and positive meanings in your life.\n\n, When someone says your former loved one\u2019s name, do you still feel a stab of pain? When you notice that feeling of pain, remind yourself that you wish him well. It may seem silly, but this can help reprogram your automatic thoughts about that person.\n\n, If you were to run into your former love with his new love, how strong would your emotional reaction be? Would seeing him happy feel hurtful for you? Are you free to be happy for him? Have you let him go?\n\n\nA little bruising is to be expected, and like a physical wound, you may be healed, fully functional and ready to get back out there. Just make sure it\u2019s no more than a little bruise before you do.\n\n","label":0,"model":"human","source":"wikihow","id":961}
{"text":"\n\nMove your hands slightly away from your face and flick your index fingers outwards and upwards. This signifies the way your face looks when you are surprised. ;\n, It should be made into the 'modified bent hand' position.\n\n\nBend your fingers twice towards the palm of your hand as if you were waving. ,\n\n\nTurn it sideways and place the side without your thumb on your cheek.\n\nMove your fist forwards and backwards slightly.\n\n\n\n,\n\n\nCross your arms over your chest in an 'x' shape.\n\n, Your palm should be facing inward.\n\nMove it upward a few times. To sign excited: Spread your fingers apart, close to your chest.\n\n\n\n\n\n\nPoint your middle fingers inwards toward you, still keeping your fingers spread apart--the middle fingers will be facing you, the others will face outward.\nFlick each hand, one at a time up off your chest a couple times, making sure that only your middle finger is actually touching your chest. The movement of each hand will almost look circular.\n\n\n\n,\n\n\nTake your other hand and lay it flat on the top of the fist, in a modified '5' handshape.\n\nMove the hand on top downward so that it pushes the fist down.\n\n\n\n,\n\n\n\nSlowly let them go downward.\n\n\nTo sign disappointed: Make a 'd' sign with one of your hands.\n\n\n\n\n\n\nPlace your index finger on your chin.\n\n\n\n,\n\nTurn one of your hands in half a circle forwards and backwards as the other hand turns the opposite direction as the first hand.\n\n,\n\n\nTurn it back and forth once or twice.\nTo sign shame: Keep your hands flat and your fingers together.\n\n\n\n\n\n\nPlace the back of the hand on your cheek and flick it out.\nTo sign guilt: Make a 'g' sign and hold it on the side of your chest near the opposite arm.\n\n\n\n\n\n\n\n,\n\n\nAlternatively, you could make the 'b' sign and tap the back of your hand against your face twice.\n\n,\n\n\nTilt it a little over to the side and put your index finger next to your mouth.\n\nSlide your hand downward.\n\n\nTo sign doubt:\n\n\n\n\n\n\nMake a 'thumbs up' sign and turn it sideways.\nMake small circles going forward with the sideways thumbs up.\n\n\n\n,\n\nHold them next to your eyes with them facing downward.\nMove them up and down in small motions.\n\n\nTo sign fear:\n\nKeep your hands flat and spread your fingers apart.\nHold your hands up to your chest.\nShake them from side to side, almost like you are shaking in fear.\n\n\nTo sign embarrassed:\n\n\n\n\n\n\nKeep your hands flat and hold them up to your face.\nSlightly move them back and forth.\n\n\n\n,\nTo sign bother\/annoy: Keep your fingers together but stick your thumb out. Do this with both hands.\n\n\n\n\n\n\nTurn one hand on its side.\n\nTake the other hand and, with the side of the little finger, tap in between the thumb and index finger on the other hand a couple of times.\n\n\nTo sign mad: Make your hand into a clawing position.\n\n\n\n\n\n\nPlace your hand over your mouth and nose or just your mouth.\n\n\nTo sign anger: Make your hands into clawing positions.\n\n\n\n\n\n\nPlace them onto your chest.\nFlick your hands to the sides.\n\n\n\n","label":0,"model":"human","source":"wikihow","id":962}
{"text":" There are two types of templates, the default and your custom. Creating a custom site template will be addressed later. Just know that you need to have it enabled when you save a site as a template.\n\n\nFor your default templates that you can create out of the box, you need to get into the site definition files. For this example, go into local drive:\\Program Files\\Common Files\\Microsoft Shared\\web server extensions\\60\\TEMPLATE\\1033\\STS\\LISTS\\DOCLIB, but remember you need to do this in the other doclib folders in 1033.;\n, The first four lines of code are:\n\n\n\n\n\n\n<?xml version=\"1.0\" encoding=\"utf-8\" ?>\n<!-- _lcid=\"1033\" _version=\"11.0.6451\" _dal=\"1\" --><!-- _LocalBinding -->\n <List xmlns:ows=\"Microsoft SharePoint\" Name=\"Documents\" Title=\"Shared Documents\" Direction=\"0\" Url=\"Shared Documents\" BaseType=\"1\">\n\n\n\n\n\n, Now all the sites you create will come with shared documents version controlled. But what about when you create a new document library in an existing site. Well, there is an answer for that too.\n\n,,\n\n\n\n\n\n<TD class=\"ms-authoringcontrols\" nowrap>\n<input tabindex=1 id=onetidVersioningEnabledYes type=\"radio\" tabindex=\"1\" value=\"TRUE\" <strong>checked<\/strong> Name=\"VersioningEnabled\" title=\"Create a version each time you edit a file in this document library: Yes\">\n<LABEL FOR=\"onetidVersioningEnabledYes\">Yes<\/LABEL>\n<\/td> \n<TD class=\"ms-authoringcontrols\" nowrap>\n<input tabindex=1 id=onetidVersioningEnabledNo type=\"radio\" tabindex=\"1\" value=\"FALSE\" Name=\"VersioningEnabled\" title=\"Create a version each time you edit a file in this document library: No\">\n<LABEL FOR=\"onetidVersioningEnabledNo\">No<\/LABEL><\/td>\n\n\n\n\n\n","label":0,"model":"human","source":"wikihow","id":963}
{"text":" Turkeys come in various sizes, so you will want to pick a turkey whose size can feed the number of people you plan to serve.\n\n\nThe rule of thumb is to buy 1 pound of turkey for every person you plan to serve. However, this does not allow for leftovers. If you would like to have leftovers, then you should plan to buy 1 \u00bd pounds of turkey per person.For example, if you want to serve eight people without leftovers, then you should buy an 8-pound turkey. However, if you want to have leftovers after serving those eight people, then you should buy a 12-pound turkey.;\n, It is important to avoid contaminating the turkey, so before beginning to gut it, you should be sure to thoroughly clean your work space.\n\n\nUse an antibacterial cleaning agent to wipe down the counter, and be sure that any other tools you plan to use, such as a cutting board and knives, are also cleaned with antibacterial dish washing liquid. Alternatively, you can wash down these surfaces and tools with hot, soapy water.Wash your hands thoroughly with warm water and soap for at least 20 seconds prior to handling the turkey. Also be sure to wash under your fingernails, if necessary.\n\n, Make sure that you have all necessary materials that you will need for gutting the turkey either on your person or within reach.\n\n\nCutting board\nKnife or knives\nBowls\nGloves (note: latex-free gloves would be a good option, due to latex allergies in some people)\nApron\n\n,\n\n\nThis step is only necessary if your turkey still has the neck. If it does not, you can skip this step and move onto the next.\n\n, Do this carefully, since you have already made one cut in the turkey. Flipping the turkey over prepares you to make the cuts that will enable you to pull out all of the innards safely.\n\n,\n\n\nTake care while making this cut so that you do not puncture any of the innards.,\n\n\nSome organs will be able to be kept and cooked as offal, which is highly nutritious. These include the heart, liver, and gizzard.Other organs will have to be disposed of in a proper way.\n\n, This loosens the intestine so that you can pull it out, along with the other digestive organs.You need to take care while doing making this cut because the digestive juices inside these organs will break down any meat on the turkey if they touch it.\nThese are not organs that can be kept to cook, so set them aside in their own bowl to await proper disposal after you finish gutting the turkey.\n\n, Again, this step is only necessary if your turkey has a neck.\n\n\nYou can pull out these parts through the pelvis, since they are loosened from the neck.Alternatively, you can pull these parts directly from the neck after you have made a slit., The crop refers to the sac in the turkey's breast that stores the food it was eating.A store-bought turkey likely no longer has its crop, but it would be worthwhile to verify.\n\n\nCut a slit in the turkey where the neck meets the chest and carefully pull it apart. Inside, you should see the crop. Clean it out completely., Be sure to use strong water pressure to rinse the turkey well, inside and out.\n\n\nIt is important to remove any remaining blood and lung tissue before preparing the turkey for cooking.Use cool water in order to avoid bacteria growth, which will happen with heat.\n\n,, Seal your turkey in a turkey roasting bag or a shrink wrap bag in order to store it.Only keep it in the refrigerator for a day or two before cooking; if it will be weeks or months before you plan to cook it, it should be stored in the freezer until you are ready.\n\n, You must again use antibacterial cleaning agents or hot, soapy water to clean all of your work space and tools, such as the knives and bowls. Wash your hands with warm, soapy water again to rid them of any bacteria.\n\n, When you gutted the turkey, you set aside its innards in different bowls based on whether they could be cooked later or needed to be disposed of.\n\n\nGizzard\nHeart\nLiver\nNeck, if you have it on your turkey, can also be cooked and served., Depending on how quickly you plan to cook and serve the giblets, the freezer is likely the better option.If you plan to cook the giblets within a day or two, then wrap them securely in plastic wrap or aluminum foil and store them in the refrigerator until you cook them.\nIf you want to save them longer term, wrap them in plastic wrap or aluminum foil and store them in the freezer. It is also a good idea to put them inside a freezer-safe bag for extra protection, up to four months.\nBe sure to get these items into the freezer before the \u201cbest by\u201d date on the turkey's wrapping.\n\n, If you have a cat or dog all parts of the turkey that you won't use can be fed to them, or you can give them to your pet owning neighbor.\n\n\nOtherwise, place the remaining innards in a trash bag for disposal. It would be best to double-bag these parts in order to prevent leakage if the first bag rips. You should include all of the innards that you are not keeping to cook later, such as the lungs, intestines, and gallbladder.\nIf your community has a green bin program, put them in one of the disposable bags and keep it in the freezer until trash collection day, and then put it in the green bin or if not available dispose of the innards in a dumpster or outdoor trash can. The innards will likely start to smell foul, so it is best to take them directly to a dumpster, if possible, or to throw them away in an outdoor trash can that has a lid.\nNote that wild animals, like raccoons or bears, might try to get into the trash if they can smell the innards and other waste. Secure your outdoor trash can, if possible, so that they do not make a mess by spilling it.\n\n","label":0,"model":"human","source":"wikihow","id":964}
{"text":" Set the parking gear and the brake. Block the wheels. Raise the front of the LHS with a jack(s) and support the car with jack stands. Remove the splash guards under each fender to reveal access to the mounts.\n, Raise the jack only until it just begins raising the engine a very little, to mainly support the engine, but not to raise the vehicle at all., (If your mounts have a heat shield between the mount and engine, remove the nut or bolt from the bracket holding the heatshield, using your ratchet and socket. Remove the heatshield.) Keeping the nuts loosely screwed on to just a few bolt threads, on the bolts, keeps the mounts aligned until both are loosened -- retained to this same degree, held on loosely, on each side of the engine.,,,,,,\n\nInstall the heat shield and its retaining bolts, if your vehicle is so equipped (on the 3.5-liter engine, for example).\n\n,, Snug the nuts and bolts on all both mounts, at first only \"finger tight\". Then snug them all a little., The large nuts and bolts might be torqued to about 45 to 50 foot (13.7 to 15.2\u00a0m) pounds. The smaller bolts on the transmission mount are torqued less, perhaps to about 20 to 30 foot (6.1 to 9.1\u00a0m) pounds., Raise the front of the vehicle enough to remove your jack stands or blocks and such, and lower your LHS to the ground.","label":0,"model":"human","source":"wikihow","id":965}
{"text":" There are variety of different accordions out there, but some are more well-suited for beginners than others. The more information you gather the better equipped you will be to successfully learn to play the accordion. Here is the most suitable option for beginners:\n\n\nPiano Accordions. These are the most popular kinds, with the power of a regular piano in a highly portable size. They have between 25 and 45 piano style treble keys on the right hand. On the left, they are equipped with a bass-chord keyboard. This accordion system is called the stradella, and typically has 120 brass buttons.;\n, Your accordion is composed of several parts, all crucial to the accordion's sounds:\n\n\nMelody Keys. These are buttons on the keyboard part of the instrument.\nBellows. These are the folds on the instrument that allow it to expand and contract.\nHarmonic, Base, Air Valve. These buttons provide air to escape, adjusting the tone of sound.\nRight Hand Strap. This is the main strap of the instrument that allows you to secure it onto your chest.\n\n, Children and teens or adults will need to start off with different sizes due to the difference in hand and general body size.\n\n\nChildren should start with the lowest number of bass buttons, 12 bass and 25 treble keys.\nTeens and adults should start with a 48 bass accordion. This amounts to 48 bass and 26 treble keys.\nThe 48 bass Piano Accordion is very light weight, and easy to use and handle. Plus, you can play lots of different music on it, which will make you want to hang on to it even if you outgrow it.\n\n, When you begin handling your instrument in the next section of the article, your left arm will move horizontally and vertically, while your right hand will only move vertically. For now, just hold it and see how comfortable or uncomfortable it is.\n\n, Some people prefer to stand while playing and others like to sit with their instrument. All that matters is your sense of comfort and confidence, so try out a few different positions until you feel at ease.\n\n, Your body posture is very important when playing this instrument and slouching will cause you to be inaccurate in your balance and consequently in your performance.\n\n, The accordion is relatively large and requires a little bit of familiarity when holding it. Being able to maintain proper balance is crucial. The more evenly balanced you manage to keep the accordion's weight, the better you'll be able to play because of the added control. And the more control you have, the less uncomfortable the weight will feel.\n\n, Slip your left arm under the strap of the instrument. You'll want to hold it as if you were putting on a backpack on your chest. The piano keys should be to your right and your left hand goes underneath the bass strap - the small strap on the left side of the instrument.\n\n\nNote that there is usually a thumb wheel on the left side to adjust the strap.\nMake sure that your accordion fits tightly enough not to move at all while you move.\n\n, An additional strap can be very useful. The back strap keeps the shoulder straps together so that the accordion doesn't move.\n\n\nNote that if the back strap is too far down it alleviates weight from the shoulders, making the straps loose on top. This, in turn, causes your straps to move and slide.\nKeep the back strap higher up, or secure it diagonally.\nRemember that when the straps stay in place, so does your instrument.\n\n, The buckles can be found on the top and bottom of the instrument. Take care not to push or pull the accordion yet.\n\n, Don't bend your right wrist while keeping your elbow close to your side. It'll be a bit awkward at the beginning but you'll be able to achieve better accuracy as your hand's circulation won't be impeded.\n\n\nThis applies the right arm only.\n\n, You right hand should be free and resting above the piano keyboard.\n\n, Press the button down softly, and pull your instrument with your left arm. You'll hear a hissing noise as the air goes into the accordion and the bellows open.\n\n\nNote that it's important to use this button when you open and close the bellows while they're moving.\nDon't press down on the keyboard while you're opening and closing the bellows.\n\n, No matter how many bass buttons your accordion has, you'll soon notice that they produce the chords, or vamps, automatically. This is due to the accordion's mechanism.\n\n\nThe term chord refers to the sound produced by a group of notes played together.\nKeep the bass buttons pressed for only a short time. Imagine they were on fire, and take your finger off quickly.\n\n, This will be quite difficult at first, but do your best not to look at where your fingers go, or need to go.\n\n, If your accordion is a larger model, then look for the note C in the second row.\n\n, For now, your only concern should be getting comfortable with your instrument, and focusing on the first two rows of bass buttons.\n\n\nRegardless of how many bass rows your accordion has, you'll only be looking at the first two rows.\n\n, Then, tuck your thumb under your index finger and push on the button right below the note C. This button will be slightly off center, but right below the button that your index finger is pressing.\n\n, Then, press the two buttons alternately to produce a chord. You'll generate a sort of of oom-pah sound.\n\n\nTry to pull the bellows smoothly for the best sound effect.\n\n, The beat for the Waltz goes 1, 2, 3--1, 2, 3. Play the C note on the the first beat, and push the button right below the C on the second and third beat.\n\n, This is how you can generate a simply accompaniment, or vamp.\n\n, Now try to pull the bellows in as you alternately press the four buttons you just learned. Repeat this several times to practice.\n\n, Let's try another easy scale exercise that will help you produce your first, controlled sound sequence:\n\n\nExpand the instrument's bellows.\nSoftly and evenly push it back together, and hold the 1st key down.\nKeep pressing the note key while you change directions by pulling the instrument in opposite directions.\nGo to the next key, push in and pull apart.\nJump to the next key, and you have now played Do, Re, Mi, Fa, So, La.\n\n, This exercise has two chords, and you can leave your fingers on the keyboard. Place your thumb on the C, and pinky on the G: begin with the third finger on the E.\n\n, Coordination might seem a little difficult at first, so it's important that you become very familiar with the necessary movements. Repeat the above exercise until you feel confident and can move on to more advanced practices.\n\n","label":0,"model":"human","source":"wikihow","id":966}
{"text":"\n\n\nPer Amazon's TOS, you should have the product at least 48 hours before writing a review.\nFor supplements or similar products, you should wait until they've had long enough to take effect before reviewing.;\n, Instead, try to get a replacement before writing a review.\n\n\nIf the seller refuses, you can leave honest seller feedback describing your experience.\n\n,\n\n\nAmazon is picky about products being in your possession when reviewing.\nIf you're getting a replacement though, wait on that before reviewing.\n\n,,\n\n\nSometimes the 'write a review' button may not show up in the order details.\n\n,\n\n\nIf going through the products page, scroll down until you see the section on the page \"Customer Reviews\". Click \"Write a customer review\".\n\n, With this, you can scan things you bough elsewhere and review them on Amazon.\n\n\nYou can also simply search for the item.\n\n, Keep in mind the positives and flaws of the product when rating it. Once you click a star rating, a box will show up for you to type your review text.\n\n\nIf it's perfect and has no flaws, give it 5 stars.\nIf it's good, but has any design flaws, give it 4 stars.\nIf it's okay, but had some design flaws or a lack of features, give it 3 stars.\nIf it works as described, but has flaws and isn't very satisfactory, give it 2 stars.\nIf it is just not a good product and doesn't do what it's supposed to, give it 1 star.\n\n, Make sure it is at least 75 words long, and without spelling or grammar errors.\n\n, Reviews are for potential customers to make a fully informed purchase.\n\n,,, It will help potential customers and you'll most likely get helpful votes for it.\n\n\nKeep your video under 5 minutes, and don't show yourself unboxing it. If you're unboxing it, you've had no time to test it thoroughly.\nBe sure to submit your written review first, then add your video\/pictures once it's live. This will help it process faster.\n\nIf you're submitting both a video and pictures, submit written first, then add the video, then add pictures once the video is live.\n\n\nIf you're having trouble getting your video to show up correctly, try sending it to yourself via email or Facebook, then saving it to your laptop and submitting it.\nIf your pictures are showing up sideways, try cropping them before adding them to your review.\n\n, You should get a email confirmation when it is approved.\n\n\nIt may take up to 48 hours for the review to get approved.\nIf it doesn't get approved after about 3 days, you can email review-appeals@amazon.com with your profile link and order number and ask them to approve or deny it.\n\n, If anything happens and you need to change your review, simply add an update to the top of it.\n\n","label":0,"model":"human","source":"wikihow","id":967}
{"text":" You will most likely need it to disassemble and reassemble various parts. Also verify that your model is equipped with a timing chain and not a timing belt. These two parts perform the same task, but replacing them can be quite different. This procedure is only for replacing a timing chain.;\n, Having your engine clean allows you to better locate any leaks or worn parts. It also makes the job less messy overall. Never clean or work on your engine while it is hot.Keep in mind that the degreaser and oils you are washing off of your engine can kill your grass and pose an environmental threat. This should be done somewhere that is properly set up to drain and filter the chemicals used., This may be stamped directly on the engine (either on the cylinder head, valve covers, or intake manifold) and sometimes listed in the owner\u2019s manual specifications.You can also use a service manual to determine firing order.This will be necessary to know because later you will need to check your number one cylinder (the one that fires first in the firing order).\n\n, You should not work on your engine with the battery plugged in. First remove the ground cable (negative terminal) and then remove the positive terminal.\n\n, This will allow the coolant to be drained from the system.\n\n, The drain cock is located near the bottom of the radiator and is a plastic screw or pull cap that you can release. Engine coolant is a mix of water and antifreeze. It is very toxic and should be stored in a plastic container with a screw-on cap. An old antifreeze bottle is ideal., Trace the radiator hoses from the radiator back to the engine. Squeeze the hose clamps with pliers and slide them back onto the hose. Wiggle the hose to break it free and move it out of the way.\n\n\nThere is no need to remove the radiator. The hoses need to be loosened and the coolant drained for the purpose of removing the water pump in a later step.\n\n, This can usually be found posted underneath the hood of your car or in your service manual for serpentine belts (S-belts). If you drive a much older car, you may have a multi-belt design (V-belt). Either way, if you cannot find a routing diagram you should take a picture or draw one before removing the belt(s).\n\n, For serpentine belts this is done by compressing the spring loaded tensioner. Some tensioners can be compressed with simple hand tools such as a wrench, and others may require a special tool. V-belts can be removed by adjusting the position of one of their pulleys to release tension.\n\n, Once tension is released, the belt should slide off of the other pulleys easily.\n\n, If your model has heater hoses attached to the water pump, loosen the hose clamps with a screwdriver and slide them back onto the hose.Wiggle the hose and pull it off of the water pump.\n\n, Take out any bolts that hold the water pump to the engine. Usually there are three to five bolts to remove. Once the bolts are removed, you should be able to pull the pump off with your hands., Remove the bolt and washer in the center of the pulley. Put the bolt partially back in the bolt hole and use a harmonic balancer removal tool. The tool should not be a jaw type tool, but instead should apply all of the removal force to the center of the assembly. This protects the rubber ring in the harmonic balancer., Unbolt the timing chain cover from the engine block. Note that the bolts are different lengths so have a system in place to remember which bolt goes where when you put the cover back on. One good method is to put them back into their proper holes in the timing chain cover and set it off to the side.\n\n, These gears are connected by the timing chain so that the position of the piston (connected to the crankshaft) correlates directly to the opening of the fuel and exhaust valves (operated by the camshaft) to keep your engine running smoothly. These gears should each be marked to help you reference their relative position.\n\n, These links are brighter than the other links and are used to align your engine.\n\n, To get your engine to top dead center, line the bright links on the timing chain up with the marks on your camshaft and crankshaft gears.Remember that the crankshaft can be set to top dead center both on the compression and exhaust strokes of the piston.You want top dead center for the compression stroke, so you can insert\n\n, This can be done by loosening the tension gear with a wrench or ratchet. Next slide the chain off of the gears.\n\n, Using a little gear oil now will help to ensure that your chain and gear perform optimally for as long as possible.\n\n, You want the bright links on the new chain to line up exactly with the marks on the gears just like the old chain did. This will allow you to locate top dead center if you ever need to.\n\n, Some chains are tensioned by adjusting either the crankshaft gear or the camshaft gear, while others have an automatic tensioner. This will vary by make and model of your car. The important thing is to make sure that your timing belt is as tight as it should be.\n\n, This is the rubber seal around the crankshaft and the timing cover.\n\n, The seal should be seated into the correct place on the timing cover. It will seal when the cover is bolted onto the engine.\n\n, It is necessary to coat the seal with oil to ensure proper functioning when the seal is compressed., The bolts are varying lengths. Remember the system you set up to keep track of which bolts went where and be sure to use the proper bolt in the proper location.\n\n, There is only the one bolt in the center that fastens the harmonic balancer into place. Check you owner\u2019s manual or service guide for proper torque specifications.\n\n, Replace the bolts that fasten the water pump to the engine block.\n\n, If you removed heater hoses from your water pump, you will need to slide them back onto the water pump. Once the hose is on the pump you can squeeze down the hose clamp with pliers and slide it up over where the hose and pump connect. If the clamp has a tightening screw, tighten it with a screwdriver. This will secure the hose to the pump.\n\n, If the lower radiator hose is still removed or if you removed the upper hose for any reason, put them back on the radiator now. Once the hose has been slid onto the radiator, use a pair of pliers to move the clamps up the hose to the place where the hose and the radiator meet. This will fasten the hose to the radiator.\n\n, If your coolant looks dirty or if it has been more than a year since you last changed the coolant in your vehicle, use new coolant. Dilute the coolant as specified in your owner\u2019s manual or service guide and fill to the mark that says \u201ccool\u201d or \u201ccold\u201d on the tank. If your coolant is clean and relatively new, you can pour the old coolant back into your radiator.\n\n, You should use the routing diagram stamped on your hood or printed in your owner\u2019s manual to make sure that the belt is run properly. Pulleys with grooves are intended to meet the grooved side of the belt, but some flat pulleys are meant to be driven by the flat backside of the belt., This can be done by releasing the tensioner in a serpentine design. If you are working with a V-belt design then you will need to tension the belt manually. A good rule of thumb is that the belt should have a maximum of 1\u20442 inch (1.3\u00a0cm) of movement in the middle of the belt's longest run.Check your vehicle manual for more information. If in doubt, consult a mechanic.\n\n, You do not want to start your engine without all of the appropriate accessories hooked up. Take a second look over everything and make sure that all is well before moving on.\n\n, Connect the positive cable first, and then connect the ground cable.\n\n, Turn the key and start the engine.\n\n, Look under your hood and under your car to make sure that no fluid is dripping or leaking. If you are leaking coolant, check to make sure all hoses are connected properly to the radiator and water pump. If you are leaking oil you might have to replace the crankshaft seal again.\n\n, This ensures that all cylinders are firing at the correct time and that the valves are opening and closing properly in relation to the piston\u2019s position.\n\n","label":0,"model":"human","source":"wikihow","id":968}
{"text":" An electric arc is formed at the tip of the welding rod when a current passes across an air gap and continues through the grounded metal which is being welded. Here are some of the terms and their descriptions used in this article:\n\n\nWelding machine. This is the term used to describe the machine which converts 120-240 volt AC electricity to welding voltage, typically 40-70 volts AC, but also a range of DC voltages. It generally consists of a large, heavy transformer, a voltage regulator circuit, an internal cooling fan, and an amperage range selector.The term welder applies to the person doing the welding. A welding machine requires a welder to operate it.\nLeads, or Welding leads. These are the insulated copper conductors which carry the high amperage, low voltage electricity to the work piece that is being welded.\nRod holder, or stinger is the device on the end of the lead that holds the electrode, which the person welding uses to accomplish the welding task.\nGround and ground clamp. This is the lead that grounds, or completes the electrical circuit, and specifically, the clamp that is attached to the work to allow the electricity to pass through the metal being welded.\nAmperage, or amps. This is an electrical term, used to describe the electrical current supplied to the electrode.\nDC and reverse polarity. This is a different configuration used in welding with an arc\/electrode system, which offers more versatility, especially in overhead welding applications and for use welding certain alloys that do not weld easily with AC voltages. The welding machine that produces this current has a rectifier circuit or has the current supplied by a generator, and is much more expensive than a typical AC welder.\nElectrodes. There are many specialized welding electrodes, used for specific alloys and types of metals, such as cast or malleable iron, stainless or chromoly steel, aluminum, and tempered or high carbon steels. A typical electrode consists of the wire rod in the center covered with a special coating (flux)which burns as the arc is maintained, consuming oxygen and producing carbon dioxide in the weld area to prevent the base metal from oxidizing or burning away in the arc flame during the welding process. Here are some common electrodes and their uses:\n\nE6011 electrodes are a mild steel electrode with a cellulose fiber coating. The first two numbers in the electrode identification is the tensile strength, measured in pounds per square inch times 1,000. Here, the yield of the electrode would be 60,000 PSI.\nE6010 electrodes are a reverse polarity electrode, commonly used for welding steam and water pipes, and are particularly useful for overhead welding, since the metal holds its position while in a liquid state, being drawn into the molten weld pool by the flow of the direct current from the electrode to the workpiece.\nOther specific purpose E60XX electrodes are available, but since E6011s are considered a standard, general purpose rod, and E6010s are considered the standard for reverse polarity DC welding, they will not be covered in detail in this article.\nE7018 electrodes are low hydrogenflux coated steel rods, with a high yield tensile strength of 70,000 PSI. These are often used in assembling structural steel used in the construction industry, and in other applications where a strong filler material and higher strength weld is required. Note that, although these rods provide greater strength, they are less forgiving in respect to achieving a clean, high-grade weld at incorrect amperages and with dirty (rusted, painted, or galvanized) steels. These electrodes are called low hydrogen due to every attempt to lower the hydrogen content. These electrodes must be stored in an oven with a temperature between 250\u00baF and 300\u00baF. This temperature is above the water boiling point of 212\u00baF at sea level. This temperature keeps the moisture (dew)(H2O) in the air from collecting in the flux.\nNickel, Castalloy, Ni-Rod electrodes. These are special rods made for welding cast, ductile, or malleable iron, and have more yield, to allow for the expansion and contraction of the iron material being welded.\nDissimilar metals rods. These rods are made from a special alloy and give better results when welding tempered, hardened or alloyed steels.\nAluminum rods. These are a more recent technology and allow arc welding aluminum with a conventional welder, rather than using a special gas-shielded wire feed welder like a MIG (metal, inert gas) or TIG (tungsten, inert gas) welding machine, often referred to as heliarc welding, since helium was the gas used to shield the arc flame while welding. The official names created by the American Welding Society (AWS)for this arc type welding are Shielded Metal Arc Welding (stick), Gas Tungsten Arc Welding (tig)and Gas Metal Arc Welding (mig).\nElectrode sizes. Electrodes come in a variety of sizes, measured by the diameter of the metal center of each rod. For mild steel rods, a diameter range of 1\u204416 inch (0.2\u00a0cm) to 3\u20448 inch (1.0\u00a0cm) is available, and the size used is determined by the amperage of the welder, and the thickness of the material being welded. Each rod performs best at a given amperage range. Selecting the correct amperage range for a given size rod will depend on the base material and the desired penetration, so specific amperages will only be covered for the welding described further in this article.\n\n\nSafety equipment. A critical part of welding safely is having, and knowing how to use, the correct safety equipment for the job. Here are some typical items that are required for welding safely.\n\nWelding shield (hood). This is the mask which is worn to protect the person welding from the bright flash of the arc, and from sparks being thrown during welding. Standard arc welding lenses are tinted very darkly, since exposure to the arc flash can cause flash burns to the retina of the eye. A level 10 darkness is the minimum for arc welding. Welding hoods with a flip up lens was once preferred, as the dark lens can be lifted up, and a separate clear glass lens will protect the welder from bits of slag while the weld is chipped. The newer self darkening welding shields are the most desirable welding shield now sold. These welding shield lens are very light colored for grinding and torch cutting. When an arc is struck the automatic self darkening lens will change to a preset #10 shade. Even newer on the market are the variable shade automatic self darkening lens.\nWelding gloves. These are special, insulated leather gloves that reach about 6 inches (15.2\u00a0cm) above the wrists, and protect the hands and lower arms of the welder (the person welding). They also provide limited protection from accidental shock if the person welding comes into contact with the electrode accidentally.\nWelding leathers. This is an apron like leather jacket that covers the shoulders and chest of the welder, used for overhead work where sparks might ignite the welder's clothing, or cause burns.\nWork boots. The person welding should wear at least a 6 inch (15.2\u00a0cm) lace-up type boot to prevent sparks and hot slag from burning his feet. These boots should have insulating soles made from a material which does not melt or burn easily.\n\n;\n, Welding is more than dragging a welding rod across a piece of steel and gluing it to another one. The process begins with properly fitting and securing the work pieces, or metal to be welded, together. For thicker pieces, you may want to grind a bevel so subsequent beads can be placed in the groove to fill it completely with a solid weld. Here are the basic steps for completing a simple weld.\n\n\nStrike the arc. This is the process of creating an electric arc between the electrode and the workpiece. If the electrode simply allows the current to pass directly into the grounded work piece, there will not be enough heat produced to melt and fuse metal together.\nMove the arc to create a bead. The bead is the metal from the melting electrode flowing together with molten metal from the base metal to fill the space between the pieces being joined by welding.\nShape the weld bead. This is done by weaving the arc back and forth across the weld path either in a zig zag or figure 8 motion so the metal spreads to the width that you want your finished weld bead to be.\nChip and brush the weld between passes. Each time you complete a pass, or trip from one end to the other of your weld, you need to remove the slag, or the melted electrode flux material, from the surface of the weld bead so only clean molten metal will be filling the weld on the subsequent pass.\n\n, This means the welding machine, electrodes, cables and clamps, and the metal to be welded.\n\n, For practice, a few pieces of mild steel, at least 3\u204416 inch (0.5\u00a0cm) thick will work.\n\n, If the metal consists of two pieces that are to be joined in the welding process, you may need to prep, or weld prep them, by grinding a beveled edge on the sides that are to be joined. This allows for sufficient penetration of the weld arc to melt both sides to a molten state so the filler metal bonds through the sectional thickness of the metal. At the least, you should remove any paint, grease, rust, or other contaminants so you are working with a clean pool of molten metal as you weld.\n\n, Locking type pliers, \"C\" clamps, a vice, or spring loader clamps will usually work. For special projects, you may find you will have to adapt different techniques to secure the work pieces until they are joined.\n\n, Make sure there is a clean location so that the electrical circuit can be completed with minimal resistance at the ground location. Again, rust or paint will interfere with the grounding of your work piece, making it difficult to create an arc when you begin welding.\n\n, As an example, 1\u20444 inch (0.6\u00a0cm) plate steel can be welded effectively using an E6011, 1\u20448 inch (0.3\u00a0cm) electrode, at between 80-100 amps. Place the electrode in the electrode holder (henceforth referred to as the stinger) making sure the conductive material of the stinger clamp is on the clean metal at the end of the electrode.\n\n, You should hear a humming sound from the transformer. The sound of the cooling fan running may or may not be heard. Some welding machine fans only operate when the machine requires cooling. If you do not, you may need to check the circuit that is supplying your power, and the breakers in the panel box. Welding machines require a considerable amount of power to operate, often a special circuit rated at 60 amps or more at 240 volts.\n\n, Hold your welding shield up just high enough so you can see to move the electrode to within a few inches of the workpiece, ready to flip in down to protect your eyes. You may want to practice tapping the electrode against the weld metal to get the feel of it before turning the power on, but never strike an electric arc without protecting your eyes.\n\n, Position the tip of the rod close to it, then drop the welding hood into place. You want to tap the tip of the electrode against the metal to complete the electrical circuit, then instantaneously pull it back a little bit, to create an electric arc between the electrode tip and the metal being welded. Another way to strike an arc is like striking a match. This arc gap, or airspace, creates a great deal of resistance in the electrical circuit, which is what produces the arc flame or plasma and heat needed to liquefy the electrode and the metal adjacent to the weld area.\n\n, This takes a great deal of practice, since different electrode diameters and welding amperages require a different gap between the tip of the electrode and the work piece, but if you can hold the gap steady, a continuous electric arc will occur from the electrode to the work piece. Typically, the arc gap should be no greater than the electrode diameter. Practice steadying the arc by holding the electrode about 1\/8 to 3\/16 of an inch from the work piece, then begin moving along the path you want to weld. As you move the electrode, the metal will be melting away, filling the pool of molten metal and building your weld.\n\n, When you have mastered controlling the arc, you will begin to practice laying, or building up the weld bead. This is the deposit of metal that joins the two pieces that you are welding together. The technique you use for laying your bead will depend on the width of the gap (if there is one) you are filling, and the depth you want the weld bead to penetrate. The slower you move the electrode, the deeper the weld will go into the metal work pieces, and for making a wide path, the more you zig zag or weave the electrode's tip, the wider the bead you will lay up.\n\n, If the electrode grounds to the metal and becomes stuck, jerk the stinger to break the rod free either from the stinger clamp or the weld metal. If the arc is lost because you move the electrode too far from the metal's surface, stop the process and clean the slag from the spot you are welding so when you re-strike the arc to continue, there will be no slag in the weld area to contaminate the new weld you are beginning from the place the arc was lost or broken. Never lay a new bead over existing slag, as this material will melt in the arc plasma and bubble through the new layer of metal you are placing, resulting in a weak and dirty weld.\n\n, This will allow you to fill more of the weld in a single pass, leaving a cleaner and more sound weld. The electrode is moved in a sideways motion as it is drawn along the weld path, either in a zig-zag, curved, or figure eight motion.\n\n, If you find the finished weld bead is pitted, with deep cratering at the bead's edges, or the adjacent metal is simply melted or burned away, reduce the amperage incrementally until the condition is corrected. If, on the other hand, you have difficulty striking or maintaining an arc, you may need to increase the amperage.\n\n, After you have finished welding, you may want to remove the slag and clean up your weld, either to allow paint to bond better, or simply for cosmetic reasons. Chip off the slag and wire brush the weld to remove any foreign material and remaining slag. If the surface needs to be flat to allow fitting the piece you have welded to another piece, use an angle grinder to remove the top, or high portion of the bead. A clean weld, particularly after grinding flat, is easier to examine to see if pitting, puddling, or other defects have occurred while welding.\n\n, Freshly welded metal will corrode rather quickly if exposed to the elements, since the actual base metal is exposed directly to moisture.\n\n","label":0,"model":"human","source":"wikihow","id":969}
{"text":" Tell the store employee what kind of switch you want and how many you\u2019ll need, and they can help you determine the best purchase.\n\n\nSingle-pole switches are the simplest and most common. A single-pole switch has just two positions\u2014\u201con\u201d and \u201coff.\u201d;\n, It\u2019s a steel box that may be located inside the house, in the basement, in the garage, or along one of the outside walls. You can either turn off the breaker that controls the lights in the specific room you are working on or turn off all the power to the house by switching off the main breaker.\n\n\n\n\n\n\n\n\n, Flip the light switch on and off several times to be sure the power is off.\n\n, Using a flat-head screwdriver, turn the screws holding the plate in place counter-clockwise to loosen them.\n\n\n\n\n\n\n\n\n, Once the switch plate cover has been removed, use a flat-head screwdriver to remove the screws mounting the switch to the wall. Turn the screws counter-clockwise until they are freed from the wall.\n\n\n\n\n\n\n\n\n, Pull the switch away from the wall to expose the wiring, but leave the wires connected. Use a circuit or voltage tester to ensure no current is flowing.\n\n\n\n\n\n\n\n\n\nIf using a circuit tester, hold one probe of the circuit tester against the grounding wire (green or bare copper), and touch the other probe to each of the screw terminals (located along the sides of the switch).\nIf using a voltage tester, simply hold the tester near the wires.\nIf the tester registers any current, stop the project immediately until you\u2019re able to power off the circuit.\n\n, Pull the switch as far from the electrical box as the wires allow.\n\n\n\n\n\n\n\n\n\nTake careful note of how the switch is wired. The wires will be connected to the switch by either screw terminals or push-in connectors.\nTake a picture or draw a diagram so you can wire the new switch in the same way.\n\n, Use a marker or colored tape to uniquely label each wire so you can tell them apart.\n\n\nThe box will contain one or two cables (or sets of wires). If the box contains two cables, it means the switch is in the middle of the circuit. You\u2019ll see a total of six wires: two black (hot) wires, two green or bare copper (grounding) wires, and two (neutral) wires, which may be black, white, red, or any other color than green.\n\nMark the wire connected to the brass screw terminal or to the hole on the same side as the brass terminal as the \u201chot\u201d wire.\nMark the wire connected to the silver screw terminal or to the hole on the same side as the silver terminal as the \u201cneutral\u201d wire.\nFinally, mark the green or copper wire connected to the green screw terminal (on the other side of the switch from the brass and silver terminals) as the \u201cgrounding\u201d wire.\n\n\nIf the box contains only a single cable (or single set of three wires), it means the switch is at the end of the circuit. You will see a black (hot) wire, a green or bare copper (grounding) wire, and a third (neutral) wire, which may be black, white, red, or any color other than green.\n\nMark the wire connected to the brass screw terminal or to the hole on the same side as the brass terminal as the \u201chot\u201d wire.\nMark the wire connected to the silver screw terminal or to the hole on the same side as the silver terminal as the \u201cneutral\u201d wire.\nFinally, mark the green or copper wire connected to the green screw terminal (on the other side of the switch from the brass and silver terminals) as the \u201cgrounding\u201d wire.\n\n\n\n, The switch may have either screw terminal connectors, which will be located along the side of the switch, or push-in connector holes, which will be located at the back of the switch.\n\n\n\n\n\n\n\n\n\nIf the switch has both screw terminals and push-in holes, most electricians recommend using the screw terminals for a more secure connection. But do not over-tighten; you may break the internal parts of the switch. If you tighten the terminals and hear a crack, discard the switch and use another.\nIf the wires are connected to screw terminals, turn each screw counter-clockwise with a screwdriver to loosen and slide the wire out with a pair of needle-nose pliers.\nIf the wires are connected to push-in connectors, the holes that the wires are pushed into will typically have a small slot beneath the hole. Insert a small screwdriver into the slot and push forward to release the wire.\n\n, Attach the black wire (hot) to the brass screw terminal. Either:\n\n\n\n\n\n\n\n\n\nUse needle-nose pliers to wrap a little more than half of the bare wire clockwise around the screw and then tighten the screw clockwise to secure the wire.\nOr push the wire into the push-in hole on the same side of the switch as the brass screw.\n\n, Either:\n\n\n\n\n\n\n\n\n\nUse needle-nose pliers to wrap a little more than half of the bare wire clockwise around the screw. Tighten the screw clockwise to secure the wire.\nOr push the wire into the hole on the same side of the switch as the silver screw.\n\n, Use needle-nose pliers to wrap a little more than half of the bare wire clockwise around the screw. Tighten the screw clockwise to secure the wire.\n\n, You should see the word \u201ctop\u201d written on the switch indicating the side of the switch that should be vertically oriented toward the top.\n\n,, Do not over tighten the switch plate, since it may crack under too much pressure.\n\n\n\n\n\n\n\n\n, Go back to the new switch and flip it on and off several times to ensure it\u2019s in working order.\n\n\n\n\n\n\n\n\n","label":0,"model":"human","source":"wikihow","id":970}
{"text":"\n\n\nGet your Country License translated to Arabic.\nGet your Blood group and eye test report from any dispensary in Saudi Arabia.\nYour Iqama (Resident permit) copy\nYour Passport copy (both front and back sheets).\nYour four Passport size photo(white background).;\n, You can pay online using SADAD payment (2 years - SAR.80 \/ 5 years - SAR.200 \/ 10 years - SAR.400)\n\n,,, 2 in school and get your eyes tested.\n\n, Get your eyes tested and he will stamp your application.\n\n,, 4. You need your iqama as well.\n\n, Seat belt, back view mirror, hand brake, and seat adjustment for your accelerator. Drive slowly through round-about.\n\n\nIf you succeed in initial trial, the tester will write \"I\" alif on your form. (If you fail, make a new file and apply after 2-3 days if you don't want to attend classes)\n\n, 2 and he will print a form taking you 100 saudi riyals for the instructions class to be held on the same day evening from 3:00 to 6:00. Your file will be held and you will be given the slip.\n\n, bring the slip.\n\n, Submit the slip inside the examination room.\n\n,, You will be given back your file.(If you fail, you will be given back your slip, you need to stamp this slip from Hall#5 after one week at 3 o clock. They will tell you to come next day morning for trial again, in computer room you submit this slip for 2nd try. You have three tries in total on one slip)\n\n, 1 and counter no. 14. Submit it there and tell him that you have deposited your 400 riyals.\n\n,","label":0,"model":"human","source":"wikihow","id":971}
{"text":" While height is just one of the factors that dictates the safety of a fall (along with falling surface and jump form) it is something you should take into consideration while you plan your jump. 10 feet can result in an injury if you fall on one foot or your spine. Falls of 30 feet or more could result in serious injuries or even death.There have been a few recorded cases of people falling thousands of feet and surviving. Don't use this as an example to go by, however. These are considered miraculous exceptions for a good reason.;\n, Softer surfaces are always preferable to hard ones, as they'll help absorb some of the shock of the land.\n\n\nGrass, sand and mud are good places to fall to. Concrete, on the other hand, is about as unforgiving as it gets.\nSurfaces with a lot of degree can pose an extra threat. Landing onto an otherwise soft area with pine needles would result in a different (but similarly intense) sort of pain.\n\n, If you have time to prepare for your jump, you should make sure to wear something on your feet that will help to cushion the force. If you're aiming for a proper landing on your feet, you run the risk of hurting them without something to ease the shock. Most sports sneakers include this technology in some form.\n\n\nShoes with a strong grip help if there's any risk of slipping during your fall.\n\n, If you get needlessly frustrated, your joints will tense up. This increases the risk of potential injury. If you've got the time to spare, try looking at yourself in the mirror and tell yourself everything will be okay.\n\n, If this is the case, you'll want to be as practical as possible. Look around you, and try to see if there are any opportunities to get yourself lower to the ground.\n\n\nHolding yourself off a ledge can get you 6 feet closer to the ground. This difference in altitude can mean a lot with regards to potential injury., It goes without saying that having a friend watching over you is more helpful if he's already got some fitness knowledge. Perhaps more importantly, the presence of someone nearby will ensure the quickest call for medical attention if you need it for whatever reason.\n\n, Even if it's not enough time to train, the act of going through a bit of a moderate workout will loosen up your joints and get your body into the proper momentum for strain.\n\n, As you're about to jump, make a point of spotting the place you would like to land. Having a specific place on the ground below you will increase your stability. If you're only aiming for a broad area, you're more likely to lose concentration.\n\n, Although you should have already warmed up by this point, you can go through a mock jump right before you make the actual leap. Many athletic jumpers use a mock run-through in the seconds before as a way to build up momentum. You should do the same as you count down to your leap. Use these final moments as an opportunity to check everything you're doing with your jump set-up.\n\n, In a jump down to a lower place, you won't want to jump much higher than you already are. Get just enough force to earn you the proper stance and momentum. Keep your elbows close to your body, and tuck your chin in close to your neck.Bend your knees and lunge forward however much you need to. All of this will minimize potential injury.\n\n\nFor the sake of keeping your body straight, keep your eyes fixed forward. This will keep your body from going imbalanced in mid-jump.\nSome people may freak out if they see themselves dropping a far-enough distance, so if you're queasy, it's best to keep your eyes off the ground.\n\n, A proper athletic jump should end in the same stance as it began. Do your best to keep your body straight. Even in freefall, it's important to keep your stance straight and stable. Letting your limbs go wild will increase the risk of injury.\n\n\nKeep your feet and knees together while you jump. This will maximize the chances of you landing on both feet.Although you'll want to keep your body from moving around excessively, you should allow room for flexibility as your body meets the landing., Giving your body the freedom to adjust as you meet the ground is essential for preventing injury.Don't lock your knees at any point, and give your muscles the limberness they'll need to counter the force of the land.\n\n\nBending the knees will reduce shock. Just make sure your legs aren't bent more than 90 degrees.Exercising with squats will help your body adjust to this change when it's needed., If your body is \"soft\" (rather than tensed), you'll be able to react naturally to a landing. This is a natural way of minimizing potential damage in a fall.With this said, it's a good idea to try to make yourself as relaxed as possible before you set into your jump.\n\n\nTry to strike a balance between going limp and holding your proper form.\n\n, Do your best to keep your legs and feet together throughout the jump. This will maximize the chances of your feet hitting the ground at the same time. Landing on both feet is more important if your jump is high enough. An imbalanced fall can result in severe injury.\n\n\nDon't try to break your fall with your hands. Hands can alleviate some of the shock on your feet, but they can generally withstand only a fraction of the pressure that feet can.When you land, try to land on the balls of your feet, shoulder-width apart.\n\n, It's not just a thing for action movies. A landing roll is arguably the best way of absorbing shock from a fall. If you're landing from a height, you should aim for a diagonal roll. Pushing yourself into a roll with one foot will avoid placing stress on your spine.As you fall, aim a shoulder to the ground in the direction you want to roll in. As you're rolling, take a foot and hit the ground with it to give you the extra force you need to complete the roll.\n\n\nRolls are difficult to master and should be left to trained athletes. It should be said as a high-difficulty alternative to landing on both feet.\nTry rolling on both sides. It's a good habit for athletic versatility, and you may find you prefer one side over the other.For practice, regular \"gymnastics rolls\" (without a jump) will get you used to the experience of rolling. They're relatively easy to do provided you have a degree of fitness and flexibility. If you want to practice with diagonal rolls, a playground (with a soft ground) is a good place to start.Rolls lead well into continued movement. This is why they're so highly recommended in sports like parkour.\n\n, They're relatively cheap and easy to do anywhere, but they target a lot of muscles that aren't typically worked out. To squat, bend your knees as low as you can while you keep your back perfectly straight. Hold your place for 30 seconds, then release and try again.\n\n\nSquats are remarkably challenging at first if you're unused to them. However, your body will train up fast.\n\n, Plyometrics refer to a series of exercises intended to balance out the use of muscles in your legs. Because people are usually sitting down in their jobs, the sedentary behaviour trains the legs to depend on knees instead of glutes. Plyometrics aims to reverse this.Make a sequence of tiny jumps, only a couple inches off the ground. Try to make your landings as quiet and soft a possible.Shift your body's weight around on your heels, keeping your knees behind your toes throughout the movement.\n\n, This is short of the recommended 7-9 hour routine. Athletes should arguably be sleeping 9-10 hours in order to accommodate for all of the wear and tear the body goes through from regular practice. If you're not giving yourself enough sleep, you're putting a major curb on your jump potential.\n\n, In the case of jumping, water is essential to keep the joints fluid and limber. Keep a refillable bottle of water whenever you're planning to train or jump. Drink it whenever you get a break.\n\n\nDon't drink so much water that you end up feeling bloated. The feeling doesn't last forever, but it is uncomfortable and limits your training for a while.\n\n, Start with small heights, and gradually work your way forward. The process may not be as glamorous as with taking the biggest jumps from the start, but it is more effective in terms of raw learning.\n\n, Even if you consider yourself properly trained at a certain point, there's a lot of value in continuing to practice. Because something like jumping depends on a lot of various factors like distance, height and weight, the particulars will be different each time you try it. Over time, you'll get a sharper understanding of how your body is best used with a jump.\n\n","label":0,"model":"human","source":"wikihow","id":972}
{"text":" A first-degree burn is usually a thermal burn caused by contact with a hot object or environment. It may result from sun exposure (sunburn), oil splatter from a hot pan, or accidentally touching a hot oven rack. A first-degree burn is painful, and will leave a deep red color on the top layer of skin (epidermis). But despite the stinging redness, there is no blistering in a first-degree burn. The skin will remain dry and intact.First-degree burns are quite common, and very rarely require professional medical treatment.\nHealing occurs in three to five days.;\n, But the skin damage will go beyond the top layer (epidermis) down into the top of the second layer (dermis). And unlike a first-degree burn, you will see blistering in a second-degree burn. Pain and bleeding are both good signs, as they suggest that there is no significant damage to nerves or blood vessels.\n\n\nSuperficial second-degree burns usually heal without scarring within two weeks, and do not require medical attention., A superficial second-degree burn can heal on its own, but a deep second-degree burn needs to be seen by a doctor. Look for spots of pale skin interspersed between the blisters. The blisters will bleed easily and may ooze a straw-colored material. If left untreated, deep second-degree burns can become third degree burns within a few days.Always seek treatment for a second degree burn if:\n\n\nYou are not sure what level of burn you have\nHave diabetes or a compromised immune system\nWere injured by a chemical burn, especially alkaline burns like from Drano.\n\n, A first degree burn can always heal on its own at home, but large second-degree burns should be seen by a doctor. Whether superficial or deep, a second-degree burn that affects more than 10-15% of your skin needs medical attention. The doctor will both assess the burn and treat possible dehydration. You lose a lot of fluid through your damaged skin when you have large burns. Tell the doctor if you feel thirsty, weak, dizzy, or are having trouble urinating. If he suspects dehydration, your doctor may give you IV fluids., Untreated third degree burns can become septic and cause death. They are distinguished from a second-degree burn by the presence of nerve, vein, and muscle damage.\n\n\nBecause of the nerve damage, the burn site will feel numb rather than painful, though the edges may still hurt.\nSkin will both look and feel dry and thick\/leathery. You will likely experience swelling.\nRather than redness, you may see white, yellow, brown, purple, or even black skin.\nYou may feel thirsty, dizzy, or weak. Dehydration may cause trouble urinating.\n\n, However, you should consider seeing a doctor if the burn doesn\u2019t heal within several weeks, or if new, unexplained symptoms emerge. Any increase in pain, swelling, redness, or discharge that becomes unmanageable should be examined as well. Seek immediate emergency attention if you experience the following:\n\n\nBurns to the hands, feet, face, groin, buttocks, or major joints\nChemical or electrical burns\nThird-degree burns\nTrouble breathing or burns to the airway\n\n, Ocular chemical burns can be very serious, so you need to take immediate action. If a chemical gets into your eyes, flush your eyes with water for at least five full minutes. You should always see a doctor for examination after a potential chemical burn to the eyes. He may add a 1% calcium gluconate solution to your eye-flushing routine. The doctor can also prescribe anesthetic eye drops to control your pain.\n\n\nIf you wear contacts, remove them carefully when flushing out your eyes.\n\n, Chemicals powerful enough to burn the skin can continue to work their way into deeper layers if left untreated. Thus, all chemical burns require medical attention. However, while waiting to see the doctor, the best thing you can do is hold the burn under cool (not cold) running water or soak it in a water bath.\n\n, The first order of business in a first or superficial second-degree thermal burn is to lower the skin temperature at the burn site. Place the burned skin in cool (not cold) water for 10 minutes. If you don't want to waste running water, fill a sink or bathtub to submerge the skin. Either refill with cold water as the water warms up, or use ice cubes to keep the water temperature low.\n\n\nJust make sure that all the burned skin is either submerged in or under the flow of running cold water.\n\n, Note that many experts advise against applying ice to a burn, as the dramatic temperature change can cause frostbite.Always cool the skin in water for at least 20 minutes if you want to apply ice to it. Simply seal the ice into a Ziploc bag with some water and wrap a rag or paper towel around it to create a barrier between you skin and the extreme cold. You can also use a bag of frozen vegetables out of your freezer if you don't have ice. Apply the ice for about ten minutes, rotating it around the burn site if it gets too cold.\n\n\nAlways make sure to use a cloth or paper towel barrier.\n\n, Ointments seal the burn, and may actually prevent healing if you apply them too soon. For first-degree burns, wait 24 hours before you apply any burn care or other ointments.If you are not near a medical facility and you have a second-degree burn, apply bacitracin ointment (an antibiotic) to the burn to prevent infection as you get to treatment. This is the only situation in which you should apply bacitracin to burned skin.\n\n, The pharmacy may have any of a variety of benzocaine brands such as Anacaine, Chiggerex, Mandelay, Medicone, Outgro, or Solarcaine. Furthermore, these products are available in a wide variety of applications: cream, spray, liquid, gel, ointment, or wax. Read the instructions on the package to learn the correct application method and dosage.\n\n\nMake sure not to overuse benzocaine, as it soaks into the skin more easily than some other local anesthetics.\n\n, You can relieve some of the pain from a minor burn by taking an over-the-counter pain reliever. An oral NSAID (nonsteroidal anti-inflammatory drug) such as ibuprofen or naproxen will help relieve pain and inflammation from the burn.Follow all dosing instructions on the packaging. Take the smallest dose that is effective at relieving your pain.\n\n, If cold water doesn't make the pain diminish, shaving cream is a surprisingly effective solution! Shaving creams like Barbasol contain a chemical called triethanolamine. Triethanolamine is an active ingredient in Biatine, a prescription cream used to treat even severe burns in hospital settings.Just spread it over the affected skin and leave it alone until the pain goes down.\n\n\nAvoid mentholated shaving creams, as they may cause further irritation.This should only be considered when you have a first-degree burn. Do not attempt this method with a burn that is any more severe than a sunburn.\n\n, While you may prefer the idea of home or natural remedies, many of these methods are untested, relying on anecdotal and not scientific evidence. Without medical backing, these methods can be risky and are likely not recommended by your doctor. If you wish to use a natural remedy, talk to your doctor first.\n\n\nIf you choose to utilize these methods, you must still cool and clean the burn first. You should also seek immediate medical attention for anything more severe than a first-degree or superficial second-degree burn.\n\n, The chemicals in the aloe plant's leaves do more than just minimize pain and inflammation. They actually encourage faster healing and the growth of fresh, healthy skin. Treat the burn with aloe lotion several times a day, as necessary.\n\n\nNever apply aloe products to an open wound.\nYou can use pure aloe from an aloe plant. Alternatively, look for 100% pure aloe vera gel at the store.\n\n, However, lotions with St. John's wort may be a little harder to find than aloe lotions. You can find them easily online, though, or in many health food stores.\n\n\nDo not apply St. John's wort essential oil to burns, though, as it can prevent the skin from cooling.\n\n, If you have a large burn area \u2014 from sunburn, for example \u2014 you might add a few drops of the oil to your bath and soak in it. Smaller areas may benefit from more focused treatment.\n\n\nMake sure to cool the burned skin with cold water for at least ten minutes.\nSoak a clean gauze or rag in ice-cold water.\nTo this gauze\/rag, add one drop of essential oil for every square inch of burned skin.\nApply the rag to the burned area.\n\n, Honey has antibacterial properties that promote faster healing in a wide variety of injuries.Instead of running for your pantry, though, look for medicinal grade honey for best results. It's not usually available in regular grocery stores, so look for health food stores or providers of ayurvedic medicine. You can also find medicinal grade honey easily online.\n\n\nDo not apply honey to broken skin, or burns that are worse than first-degree burns.\nThe only exception is if you are far away from a medical care facility. If you cannot get to treatment quickly, use an antibiotic ointment or honey on the burn to help prevent infection as you wait to get treatment., Simply steep one teaspoon of calendula flowers in a cup of boiling water for 15 minutes. Once it's been strained and cooled, you can either soak the burned area in it or apply a cloth soaked in the tea to the skin. If you have calendula oil instead of leaves, dilute 1\/2 to one teaspoon in 1\/4 cup of water. You may be able to find calendula creams in naturopathic stores or practices. Apply calendula four times daily until the burn has healed.\n\n\nStudies also suggest that green tea can be helpful for treating burns., Though the smell is unpleasant and it might make your eyes water, onions have been shown to sooth burns.Simply cut up some onion and gently rub it against the burn, working the juice into the wound without causing pain. Do this several times a day until the wound heals, making sure to use fresh onions every time.\n\n, When you're not using these treatments, you must protect the damaged skin from infection. Pat the burned area dry, then cover it with clean gauze. Tape or wrap it into place, then change the dressing daily until the skin appears normal. Check for signs of infection every day: fever, increased redness, and pus.If you see such symptoms, notify your doctor immediately.\n\n","label":0,"model":"human","source":"wikihow","id":973}
{"text":" This will save you hours in time spent ruffling the pettiskirt by hand.;\n, The nylon chiffon fabric should be approximately 108 inches (2.7m) wide.\n\n, You may choose a similar shade to the nylon chiffon, or a different color.\n\n, This can take longer than cutting other fabrics because it can slip easily. You will need several different strip widths.\n\n\nCut 3 strips that are 6 inches (15.2cm) wide and 108 inches (2.7m) long.\nCut 6 strips that are 4 inches (10.2cm) wide and 108 inches (2.7m) long.\nCut 24 strips of 2-inch (5cm) wide fabric that are 108 inches (2.7m) long. Smaller lengths can be connected to make 108 inches (2.7m) long, if need be.\n\n, It should be 7.5 inches (19cm) wide and 48 inches (1.2m) long to go around a girl's waist band. The elastic can later be cinched in to create a bigger or smaller waist.\n\n, Your ribbon should be 1 inch (2.5cm) in width.\n\n, Cut an elastic that is approximately that long, taking off extra for a tighter waist band, or adding extra for extra room.\n\n, In order to connect the first strips, you will want it to be set to a 4:1 gathering ration.\n\n\nThis means that you are going to gather more on your 2-inch (5cm) strips than you will with your 4-inch (10.2cm) and 6 inch (15.2cm) strips. The many ruffles will create a pettiskirt full of body.\nYou will be making 3 layers of nylon chiffon ruffles to comprise the body of the skirt. Make each of the layers 1 at a time.\n\n, Feed them into your ruffler foot. Feed them through your sewing machine until you have run the entire length of the 4-inch (10.2cm) strip.\n\n\nYou will need 4 times the amount of 2-inch (5cm) strips than 4-inch (10.2cm) strips to account for the ruffling ratio. You will need 4 2-inch (5cm) strips for each 4-inch (10.2cm) strip you gather. Either pin the new 2-inch (5cm) strip to the end of the last strip in advance, or simply feed in the next 2-inch (5cm) strip by hand when the last is finished.\n\n, It should be a 2 to 1 ratio.\n\n, You will need 2 of the ruffles you made to sew to the length of the 6-inch (15.2cm) strip.\n\n\nYou may want to pin the 2 4-inch (10.2cm) strips end to end on the 6-inch (15.2cm) strip before you start sewing.\n\n, Set it aside.\n\n,,,, It should now be 7.5 inches (19cm) by 24 inches (70cm). Make sure the right sides face each other.\n\n,, Make sure to back stitch to secure it.\n\n,,,, Stitch an additional time 1 inch (2.5cm) below the seam. This is where your elastic will be inserted.\n\n, Match the right side of the waist band to the right side of the top layer of chiffon.\n\n, This is on the opposite side of where the elastic will go.\n\n, Turn it inside out when you are finished.\n\n,, You can also attach a button on 1 end and a button loop at the other end of the elastic, if you prefer.\n\n, Tie a bow and place it in front or back, depending on your child's preference.\n\n","label":0,"model":"human","source":"wikihow","id":974}
{"text":" You'll need a putty knife with a stiff, 1\/16 inch blade with a single 45 degree bevel on one side of the working edge. These are commonly available at hardware stores. A wide screwdriver also works if it is ground in the same way. But note: make sure the working edge is not sharp enough to cut anything. It can be rounded slightly with sandpaper.\n\nNote that directions in these instructions, like \"top\" and \"bottom,\" are relative to when the pump is installed in the sink. That is \"top\" and \"up\" are above the sink and \"bottom\" and \"down\" are below the sink.;\n,\u00a0 Remove the spout by gently pulling it up off the pump body.\u00a0 Pull the supply tube from the other end of the pump body.\u00a0 Continue rinsing the now exposed parts.\u00a0 Blot the pump body dry.,\u00a0 Looking very closely, find the joint between the pump body and the pump top.\u00a0 It is on the shoulder, the widest part of the pump body.\u00a0 The top of the shoulder is part of the pump top and the bottom of the shoulder is part of the pump body.\u00a0 Gently place the beveled edge of the putty knife on the joint and press gently to force the top up and out of the bottom.,\u00a0 Be prepared to catch it by cupping the pump body in your hand while pulling up on the pump top gripped between your thumb and forefinger.\u00a0 It helps to have a paper towel on your work surface right under your hands.\u00a0At this point the pump parts will all come apart and soap will get all over your hands.\u00a0 Carefully rinse each part and lay them out.\u00a0 Don't lose any parts!\u00a0 None of them are optional.,\u00a0 Inspect that the parts are smooth and that the O-ring is not cut or nicked.\u00a0 If any part is rough, cut, nicked or otherwise damaged the entire pump must be replaced.\n\nSupply tube (or intake tube, slanted end goes down)\nPump body (cylinder, square end of supply tube inserts in the bottom)\nFoot valve (looks like the head valve, pointed lips point up)\nFoot valve holder (also holds the large end of the coil spring)\nCoil spring (small end goes up)\nHead valve holder (also holds the small end of the coil spring)\nHead valve (pointed lips point up)\nPiston (includes the outlet tube on its top and the o-ring near its bottom)\nPump cap (holds the spring and piston in the pump cylinder).\nSpout (connects to the outlet tube on top of the piston)\n\n,,\u00a0 This is a little tricky, as the assembly cannot be just dropped into the cylinder.\n\nHold the coil spring with the large end up.\nPlace the upside down assembled foot valve holder and foot valve over the large end of the spring.\nContinue holding this vertically while placing the upside-down cylinder over the assembly until the valve is against the bottom of the cylinder.\nTurn the assembly over, keeping the valve against the bottom of the cylinder.\nKeep the cylinder pointing up.\n\n,,,\u00a0 The o-ring end should be pointed at by the notched end of the pump body top., Press the cap and cylinder together until the two shoulders meet.\u00a0 The last 1\/8 inch or so snaps together with just a small amount of extra pressure.,,\u00a0 It will take about 5 to 10 pumps before soap will come out of the spout.\u00a0 The pump should work smoothly and not stick.","label":0,"model":"human","source":"wikihow","id":975}
{"text":";\n,, If you're lucky you'll be able to find some brackets, corner supports, or angle-iron scrap that can simply be trimmed and set up. If not, however, you'll have to cut the blades from 18-gauge sheet metal, bend them, and drill two 1\/4\" holes in each section. Then dig identical holes in the tub and hold the agitators in position, using 1\/4\" bolts. Once that's done, you can also liberally apply bondo to the bottom and sides of the vessel's interior to seal its holes.\n\n, Mount the smaller rollers\u2014by pushing a nail into the disks and tee fitting and fastening it with a nut\u2014and attach the main wheels as well.\n\n, Mark and then drill the cotter pin holes as close as possible to the disks, to prevent the shaft from having excessive end play and wobble, then grease the axle and secure the whole assembly with two cotter pins.)\n\n,, (Hint: Rotate the drum in the correct direction. the angled paddle blades should bite into the gritty substance and then drop it from the top of the vat.) Of course, if you wish, you can attach handles to the outside of the drum\u2014opposite each paddle\u2014by using longer 1\/4\" bolts to secure the \"mixers.\" (Be certain that your grips clear all parts of the frame during rotation.)\n\n, And\u2014if you hose the drum out thoroughly after each job\u2014you'll have a trouble-free tool for years to come!\n\n","label":0,"model":"human","source":"wikihow","id":976}
{"text":",,,, There are two ways to do it so I\u2019ll show the first and then I\u2019ll show the one that I think is easier. Once you deal with these you will be able to easily take a date and mentally crunch the day of the week rapidly in your head just like Rainman.\n\n, For example:\n\n\n\n\n2000 = 0\n2001 = 1\n2002 = 2\n2003 = 3\n2004 = 4\n2005 = 5\n2006 = 0\nEtc.\n\n\nOf course this is problematic because a \u201cLeap Year\u201d occurs every four years so that has to be factored in or not. Leap Years occur every four years, 2008 was a leap year so:\n\n2007 = 1\n2008 = 3\n2009 = 4\n2010 = 5\n2011 = 6\n2012 = 1\n2013 = 2\nNotice the zero got moved around.\n\n\nThe good news is there\u2019s a shortcut, we\u2019ll take the shortcut. Just take the last two digits of any year and divide it by four and ignore the remainder. (Anytime you don't have a remainder it's a Leap Year and then January Code is 5 and February Code is 1 as shown above) For Example:\n\nThe year 2061 is handled as, (61 \/ 4) = 15 with a remainder of one (Discard the one),\n\n\nThen add that answer to the last two digits of the year (61 + 15) = 76\n\n,, Divide 10 by 4 and we get 2 with a discarded remainder of 2. So we add 10 + 2 for an answer of 12 and see that a multiple of 7 will give us a 5 left over so the year code for 2010 is \u201c5\u201d.\n\n,, That\u2019s; (5 + 25 + 4) = 34. Then multiples of seven will get us to (7 * 4) = 28. The (34 \u2013 28) = 6. The number six is the code for Saturday. So Christmas day 2010 will fall on a Saturday.\n\n,\nMonth Code; Divide the last two digits of a year by four, discard the remainder and add the whole number to the last two digits of the year. Then use the multiples of 7 to get as close as you can to that answer and subtract the two for the year code.\n\n,","label":0,"model":"human","source":"wikihow","id":977}
{"text":" Administering a coffee enema can have adverse health risks, including colitis,septicemia,, and rectal burns due to the temperature of the coffee. There have even been reports of coffee-enema-related deaths., Despite holistic rumors a coffee enema is unlikely to have significant effects. The caffeine absorption from a coffee enema is far less than drinking coffee, and it has a negligible effect on blood pressure and heart rate.While limited use may not be harmful to every user, the benefits appear to be minimal.\n\n, If you are considering a coffee enema, talk to your doctor about it first. It is likely that your physician will advise against the treatment, but she may be able to offer alternatives or to give you additional information about your prognosis. Other health and lifestyle changes may have a greater effect on your wellbeing with fewer risks.\n\n, Since the theory behind the coffee enema has to do with the body\u2019s response to caffeine, you should try drinking coffee instead of using an enema. You will absorb more caffeine by drinking the coffee without the other health risks of the enema., If you choose do do a coffee enema despite the fact that there are few likely health benefits and many possible risks, then do what you can to minimize the risks:\n\n\nCool the temperature to an appropriate temperature to avoid burns.\nKeep equipment as sterile as possible.\nWatch for signs of infection and seek medical attention immediately if you experience a fever after administering a coffee enema.\nDo not discontinue other types of medical treatment in favor of this unproven alternative therapy.\nDo not share the enema equipment with anyone.\nAlways wear vinyl or latex gloves when administering enemas.\n\n, Boil less than 1 quart because you will add ice cubes later.\n\n\nUse unchlorinated water to make the coffee because chlorine could kill the bacteria and germs your system needs to remain healthy.\n\n, (24 grams) of ground, caffeinated coffee and boil for 5 minutes.\n\n,, This will settle the coffee grounds to the bottom of the pan and cool the liquid.\n\n, Strain the coffee into a container you will use only for enemas, as feces could get into the syringe when you refill the bulb for your second enema.\n\n,, The bulb will suck in the coffee and should hold about 1 cup (236 ml) of coffee.\n\n,, If you encounter any resistance, stop. Piercing a colon is no fun if you do pierce your colon seek medical attention immediately by calling your local Emergency Medical service immediately. In the USA, that number is 911.\n\n,, Do not strain to hold the coffee. If you feel it necessary to move your bowels before 12 minutes pass do so.\n\n,, Rinse thoroughly with warm, clean water.\n\n, Loosen the clamp a bit; some coffee will run out. When the air is removed from the tubing, clamp the tubing closed again.\n\n,,, If you feel fullness or discomfort, clamp the tubing closed immediately. Remove the nozzle when you finish releasing the coffee into yourself.\n\n,,,,,, Ensure you remove the protective cover from the nozzle before you attempt to insert the nozzle.\n\n,,,,","label":0,"model":"human","source":"wikihow","id":978}
{"text":" Many rope bags include a hole in the bottom: tie one end of the rope so that it will not pull through the hole. To pack up your rope, simply stuff the rope into the bag. Be sure to keep one end free so that you can easily find it when you're climbing; consider tying it to an outside strap of the sack.\n, That is, run it through your hands and stack it in an orderly pile so it's less likely to get tangled during the coiling process. Leave the bottom end of the rope sticking out from the bottom of the rope pile.\n\n,, Once your arms are spread, grasp both strands of rope in your left hand, too.\n\n, Hold one end of the rope in your dominant hand. Take your other hand, and run it along the length from your dominant hand, over your shoulder and behind your neck, and down your straight non-dominant arm. The rope should hang on your neck like a dirty, heavy scarf.\n\n, Hold onto both strands of rope with your left hand as you reach across with your right hand, grasping both strands between your left hand and the stack of rope coils. Then, take the hand opposite from the slack and grab the slack, then do the same flip over your head and onto your shoulders. Continue this way until you have the entire rope going back and forth across your shoulders and arms and looping through your hands each time.\n\n\nBe sure to leave at least two feet of slack. If you want to carry the coil as a \"rope pack\", leave five to ten feet of slack\n\n, Drape this length of rope across your shoulders, behind your neck.\n\n, Make sure you continue working with both strands at once.\n\n,, When you are almost done, and you've left 6-10 feet of slack rope, grab the entire bundle behind your neck; pull it up and over so that you have all your rope in front of you.\n\n\nPull the coils off your shoulders, holding them together so you don't drop any strands of rope.\nBring your hands together, take the two bundles of rope, and put them together in one big loop.\nYou should be holding a loose coil in one hand: an upside down U shape of all your rope with loops on the ends of the U. Hold that loop in one hand (probably your dominant one) while you bring the whole bundle back over your head.\n\n, Hold the bundle here, so each end of the coils drapes down on either side.\n\n, Wrap both ends of the rope around the bundle you made, below where your hand grips the coils, cinching each wrap tight. You should be able to just hold the whole bundle by the slack now.\n\n\nThe bottom of the coil may look a bit untidy but that doesn't matter as long as the top is tight and orderly.\n\n, Leave yourself approximately three feet of rope, and let whichever hand is holding the U-shape grab the remaining end. Let go of the bundle, and pull the end through at the same time, without letting go with the other hand. Bring the end of the rope through its own loop at the top of the bundle, creating a half hitch around the top of the bundle. Reach through the hole you left, between the \"top\" of the folded coil bundle and the wraps you just made. Grasp the trailing sections of rope near the bundle and pull them both partway through the hole, creating a loop.\n\n\nFeed the slack around the big loop a few times, bundling it all together so it looks like a really thick noose. Wrap it at least five times, but probably no more than ten. It is up to you how much you want to wrap, and if you need more room, unwind the end in the loop to give you more room.\nTake a loop of the slack and put it through the noose's loop; feed the rest of the slack through its own loop, crochet-stitch style.\n\n, Slide the loop over the top of the coiled climbing rope, so this loop lies over the other wraps you just made. Then pull on the loose ends of the rope to cinch it down, effectively tying the bundle shut.\n\n\nThe rope should be ready to carry. Slide it into your backpack or a stuff sack, or hanging it from your harness. Make sure that the rope is secure.\n\n, If you leave a longer tail of slack, you can make a pseudo-backpack with the rope ends tied back into the bundle. This may make for easier carrying if an actual backpack in unavailable, or if you want to put your climbing partner to work. Swing the whole thing onto your back and take the two pieces of slack and put one over each shoulder, then cross them across your chest. Put them under your arms and then around the back of the coil on your back, then back out around your waist. Tie a square knot to keep the coil in place.\n\nThis is especially useful if the rope gets damp from snow. Never climb when it is raining, or has rained! Rock is more likely to break if it is wet, because it has all the extra weight from the water.\nIf you have all the gear in your pack (crampons, backup rope, shoes, anchors, belay devices, carabiners, quick-draws, and trad gear) your partner can just sling the rope on his\/her back, and off you go!\n\n","label":0,"model":"human","source":"wikihow","id":979}
{"text":" In order to produce creative ideas yourself, you will need a constant flow of new ideas.Seeing the work of others will inspire you, and give you ideas to challenge, change, or explore further. Try:\n\n\nReading lots of books\nKeeping up with world news\nSubscribing to one or more magazines on topics that interest you\nWatching a documentary film on a topic you are unfamiliar with\nVisiting a museum;\n, You will need to be ready to write or sketch out an idea the moment inspiration strikes. For this reason, make sure to always have a sketchbook or notepad with you at all times.When you get an idea, try putting it down quickly in your note\/sketchbook. You can always come back later and revise it.\nTry making yourself write or draw every day in your notebook, even if you don\u2019t think you have any new ideas. In the long run, making a habit of working some in your notebook every day will lead to more creativity.\n\n, In the modern world, it is very easy to be distracted by television, smartphones, social media, and a million other things. However, in order to think creatively, it is very important to sometimes be bored.Being bored causes your brain to work in new ways, as it seeks stimulation. This leads to fresh ideas.\n\n\nGive yourself a \"device free\" hour each day, or day per week\nSet aside some time each week in which you don't have anything scheduled\nWhen you start to feel board, such as when waiting for a subway, resist the urge to check a smartphone or other distraction. Instead, observe the world around you.\n\n, For example, try playing:\n\n\nWith children\u2019s toys, such as blocks\nA favorite board or card game\nCharades\n\n, For example:\n\n\nIf you are inside, try going outside. If you are outside, try going inside.\nIf you are working in a big room, move to a small room. If you are working in a small room, move to a big room.\nVisit a museum, go for a walk, go bowling, or find some other location you enjoy.\nIf you are sitting, try lying on your back on the floor.\n\n, Getting out of your comfort zone is an easy way to get a fresh perspective, and go back to the problem or project you are working on with a renewed sense of vigor.\n\n\nHave lunch at a new restaurant, or try a kind of food you\u2019ve never had before.\nTalk a walk in an area you\u2019ve never visited.\nWatch a film in a language you can\u2019t understand.\nRead a book on a topic you know nothing about.\n\n, Getting new skills, especially in an area separate from the one you are working in, will broaden your mindset and give you new ways of thinking through ideas.\n\n\nLearn how to play a musical instrument\nPractice a foreign language\nTry cooking a new cuisine\nTake up a new hobby, such as knitting, woodworking, or painting\nLearn how to play a sport or game you\u2019ve never tried before\n\n, Make time to talk to lots of different kinds of people, ask them questions about their work and ideas, or just chat.\n\n\nHave lunch with a coworker you haven\u2019t had the chance to really talk with before. Ask him or her about the work he or she does.\nStrike up a conversation with a stranger on the bus, train, plane, etc.\n\n, Pretend that you are talking to someone you find fascinating or a role model, such as a famous or influential person from history. Close your eyes, and discuss with this person whatever topic comes to mind.\n\n, Being creative requires you to suspend judgment and take risks.If you start judging ideas before you even get started, you will crush your creative spirit. Let your ideas flow, and only worry about editing them once you get them out.\n\n, Write nonstop for a certain amount of time (for instance, 10 minutes). By writing without stopping, no matter what comes out, after a few minutes you will start to access new and fresh ideas that you wouldn\u2019t have thought of at first.\n\n, It requires visual organization and logical organization, which engages your whole brain and encourages creativity.\n\n\nStart with a piece of paper and pen or marker. Write down the concept you are working (for example, \u201cClothing\u201d) with in the middle of the paper and put it in a shape like a circle or square.\nDraw lines off of the center to create new shapes with related concepts. For example, you could draw a line from \u201cClothing\u201d to have a space for \u201cHats\u201d and another with a space for \u201cShirts\u201d\nDraw new lines off of these subcategories to make sub-sub-categories. For example, draw a line for \u201cFlannel\u201d off of \u201cShirts.\u201d\n\n, One way to get your creative juices flowing is to challenge accepted ways of understanding things. You can do simple exercises to practice this, as a way of getting ready to think creatively about a problem or project. For example, take an everyday object, like a paper clip, and think of ten new uses for it.Normally, a paper clip is used to fasten loose papers together. However, you could also:\n\n\nStick paperclips in the ends of corn so you can eat it off the cob\nUse a chain of paperclips as a necklace\nUse a paperclip to open a stubborn seal, such as on a medicine bottle\nUse a paperclip as an instrument to paint intricate designs on your fingernails using nail polish.\n\n, It can be used as a practice exercise to boost creativity or when you are working on a project. \u201cToppling\u201d is one technique to practice free association:Think of a word, any word, such as \u201cpotato.\u201d\nThink of another word that is related to it, but not the same kind of thing (in this case, not a vegetable). For example, \u201cchip\u201d (\u201cpotato chip\u201d).\nContinue by again making an association between the last word and a new one, such as \u201cpaint\u201d (\u201cpaint chip\u201d).\nRepeat the same step to find a new word, such as \u201cwallpaper\u201d (an alternative to paint)\nKeep repeating this procedure, always trying to find words that are related in different ways.\n\n, These require you to think about similarities between objects or concepts, often ones that are not related in an obvious way. They often take the form of \u201cX is to Y as A is to B.\u201d Start by thinking of objects, and form analogies:\n\n\n\u201cA potato is to a french fry as a tree is to a board\u201d (french fries are made from parts of potatoes; boards are made from parts of trees)\n\u201cA tree is to a forest as sand is to a desert\u201d (forests are made of lots of trees; a desert is made of lots of sand)\n\n","label":0,"model":"human","source":"wikihow","id":980}
{"text":";\n, It should be located close to your AC compressor. This feeds the unit the voltage necessary for it to run. Often you will see a cable running from this breaker box directly to the AC compressor.\n\n, There is a latch on the box (either on the side or bottom) that unlocks it. Once the door is in the open position, push it in slightly and it should lock in place.\n\n, Set the breaker aside in a safe place.\n\n,, Use the appropriate bit with a bit driver to remove the screws. Typically there are four 5\/16\" hex screws that attach the side panel to the rest of the unit. Place the screws in a secure location so as not to lose any.\n\n, It will have a cylindrical, canister-like shape. It will have three terminals on top. There will be colored wires coming from the AC Unit to some if not all of the terminals on the capacitor.\n\n, The top of the run capacitor should be perfectly flat. If you see that the top side is bulging upwards in any way, it needs to be replaced. If this is not the case, a faulty capacitor may not be the source of the issue. You may need to contact your local HVAC professional.\n\n\n\n\n\n, The three terminals on the capacitor are labeled HERM, FAN, and C. Take a picture with your smart phone to record how everything is installed to make sure the new capacitor is connected in the same way. Jotting down these details on a piece of paper will also work.\n\n\n\n\n\n, Inspect your insulated flat-head screwdriver. A properly insulated screw driver should have a entirely rubber handle. Ensure the handle has no exposed metal parts or cracks., To do this you are going to create a bridge between the C terminal and the HERM\/FAN terminals, one at a time, using the metal part of the screwdriver. Ensure you are not touching anything metal. With the metal part of the screwdriver, touch a metal prong on the C terminal AND another from the HERM terminal at the same time, creating a bridge between the two. This may produce a shock. Do this two more times to ensure the terminals were completely discharged. Do the same to the C and FAN terminals.\n\n\n\n\n\n, Use the appropriate bit and the bit driver to remove the screws. These are typically two 1\/4\" hex screws but this may vary. Set the screws and the bracket in a secure location.\n\n, To be safe, always treat the capacitor terminals as live terminals. Remove the old capacitor., They can be purchased at your local HVAC Supply Store or they can also be purchased online through Amazon, eBay, etc... The simplest way to make sure you buy the correct one is to order the same model number from the same manufacturer. There are occasions where another brand might be more affordable. In this case you want to make sure to match the specifications on the old capacitor.\n\n, Attach the appropriately colored wires according to how they were placed on the old capacitor.,, Plug the 220v breaker back in the ON position. Turn on the thermostat to cool. The AC unit should turn on within the next two to three minutes. Verify the unit is working properly. Do not attempt to reattached the AC unit's side panel while there is power running through it., Put the AC panel cover back on and secure it with the appropriate screws. Plug the 220v breaker back in and set the thermostat to cool. The repair is complete!","label":0,"model":"human","source":"wikihow","id":981}
{"text":" This has a chip on it that contains the encryption key and uses it to encrypt the data on the drive. This is faster and more secure than the software alternative. Hardware encrypted drive have until recently been very expensive, but now several are available for not much more than a standard drive.\n\n\nIf you already have a drive and would like to encrypt it there is a very powerful free software solution called TrueCrypt. The software is slightly slower and less secure (because the encryption is done by the PCs processor so the encryption key and data are transferred to the PCs memory where it is available later on to hackers.\n\n\n\n\n\n\nA problem with software flash drives is that they need admin rights on the host PC to work; thus, they cannot be used in public locations such as schools or internet cafes. We get around this problem by installing a \u201cviewer\u201d called TCExplorer. This does not need admin rights and lets you view as well as read and write files TrueCrypt encrypted files on public PCs:\n\n\n\n\n\n;\n,, The main TrueCrypt window should appear.\n\n,,,, The main TrueCrypt window should appear.\n\n, The TrueCrypt Volume Creation Wizard window will appear. Choose: \"Create an Encrypted file container,\" and Choose: \"standard TrueCrypt volume.\" As the option is selected by default, you can just click Next.\n\n, The standard Windows file selector should appear (while the window of the TrueCrypt Volume Creation Wizard remains open in the background).\n\n, In the file name box, click any name you like, e.g. \"secure.\" This will be the name of the file that contains your encrypted data (also called a \u201ccontainer\u201d). TrueCrypt will create this file for you. Do not choose an existing file or it will be overwritten.\n\n,, Leave the defaults and click next.\n\n, Then click next.\n\n, Type in and confirm a password with case changes and a combination of letters and numbers. Then click next.\n\n\nVolume format leave as FAT or change to NTFS for volumes larger than 2GB\n\n\n\n\n\n\n\n,, You now have a secure container on your USB flash drive. Navigate to your USB flash drive, TrueCrypt folder and click on truecrypt.exe. In the top menu, go to tools, then traveler disk setup.\n\n,,,, Then click on close.\n\n, Unzip this file and copy TCexplorer.exe to the root of your USB drive. Now you have a working encrypted drive. You can copy to and from it directly when you have admin rights on the host computer and you can do the same by clicking on TCExplorer when you do not have admin rights.\n\n","label":0,"model":"human","source":"wikihow","id":982}
{"text":" Treat this step just as you would if hiring a new employee. Making a bad decision regarding which contractor to hire will be very painful. Give yourself the best opportunity to make a good decision here by including several candidates before moving to step 2.\n,\n\n\nIf you are unsure of the business licenses required for roofing contractors in your area, contact the licensing board or Department of Professional Regulation for your area or state.\nYou may also visit the Contractor's Licensing website featured in the Sources section of this article to determine the licenses required by specific states.\n\n,,\n\n\nNot all areas or states require roofing contractors to have insurance, but you may want to hire a person with insurance to protect yourself from lawsuits if the contractor is injured while working on your property.\nCheck with the contractor to verify that the insurance covers the entire time-span during which the roofing project takes place.\n\n,\n\n\nYou can also research the roofing contractor's reputation by contacting your state's Department of Professional Regulation or a local Better Business Bureau (BBB). Visit the BBB website listed in the Sources section of this article or call them at 703-276-0100 in the United States and at 514-905-3893 in Canada.\n\n,\n\n\nSome types of roofing may require contractors to have special training, certifications, or licenses to ensure proper installation.\n\n,,,\n\n\nInformation to ask for should include the length of the project, daily start and end times, roofing materials to be used, safety procedures, the amount of payment and payment schedule, and clean-up methods.\n\n,\n\n\nDo not hire roofing contractors based solely on the lowest pricing bids because they may not provide the same value as contractors with slightly higher pricing.\n\n","label":0,"model":"human","source":"wikihow","id":983}
{"text":" Afrikaans is a young Germanic language which has a much simpler grammar than English and Dutch. It is not only spoken by (77% of all) coloured and (58% of all) white people in South Africa, but also 11 different cultural groups speak Afrikaans as a home, second or third language. Today, the Flemish, Dutch, Germans, Anglophonic peoples, Swedes and even Polish and Russians are rather keen to get in touch with what is known as the simplest Germanic language in the world.;\n, Because Afrikaans sounds quite guttural, it is also the perfect language to swear in! Lots of South Africans use it for this purpose only! Which is kind of sad, but it is certainly a very expressive language in other words. However, if the user are interested in learning Dutch, Afrikaans makes an excellent basic stepping stone.\n\n, Which means \"good morning\". Nobody says that anymore. It is old-fashioned. When we greet someone we just say \"Hallo\" or \"Hi\" or something similar like \"m\u00f4re\" which means \"morning\". Afrikaans has been influenced a lot by English.\n\n, The \"Hoe\" is pronounced like \"who\" in English and means \"how\". The 'g' sound in the beginning of \"gaan\" is made in the back of the throat. That is the hardest sound in Afrikaans. In order to pronounce this letter, make like a car that suddenly hits the brakes on a gravel road. A scratchy sound, like if you've got something stuck in your throat and want to get it out. after you think you've got it try the whole word: \"gaan\". The 'aan' part is pronounced like \"on\" in English. \"gaan\" means \"goes\" and can be used in all tenses which will often require a prefix or suffix. Finally the word \"dit\" which means \"it\" in English. \"dit\" is pronounced like the first syllable in the urban word \"ditto\" so it's \"dit\"\/\/\"to\" but only the first part. Also the 'i' is pronounced \"uh\". Finally say the three words in sequence: Hoe Gaan Dit? Which if directly translated into english would basically mean \"How Goes It?\" there you have it.\n\n, The thicker the volume, the better. English-Afrikaans, Nederlands-Afrikaans (better known as ANNA) and Deutsch-Afrikaans bilingual dictionaries are already available. Trilingual dictionaries for African languages-Afrikaans are also available, though not very extensive.\n\n, It is important to know idiomatic expressions, otherwise you won\u2019t grasp the humor. Luckily though, if you know Dutch, or know a few Dutch expressions, most idiomatic expressions will be comprehensible. Also, especially nowadays, people directly translate English proverbs.\n\n, You should listen to Afrikaans more often. To give you some idea how an Afrikaans accent sounds like, visit http:\/\/af.wikipedia.org\/wiki\/Hoofstad on the Afrikaans Wikipedia, click on the speaker and read along . In this way, you can read and listen to the article simultaneously. As you would listen to Radio Nederland Wereldomroep just to get familiar with the Algemeenbeschaafde Dutch accent \u2013 use Radio Sonder Grense (RSG) for Afrikaans. At the homepage, place the cursor on Luister, and then Luister Weer. Click Luister Weer. You may select any program that you think you will like (e.g. Die tale wat ons praat), ignore Sleutelwoord and Datums; click on and click LAAI AF at any particular day\u2019s topic you want. After the sound file has been downloaded, you may listen for \u00b1 half an hour, at your own leisure, how Afrikaans words are pronounced. Afrikaans is a fast language, which is why you should be able to replay the podcast.\n\n, The Afrikaans community is fueled by humor. Most of it are puns (using Afrikaans idiomatic expressions and words), irony, rhyme, similes, metaphors, exaggerations, understatements and innuendos. If the Afrikaans speaking people start giggling or laughing when you speak Afrikaans, don\u2019t take it personally or seriously at all \u2013 if you are a male, your voice tone might sound rather feminine (many don\u2019t articulate deep and raspy enough from the throat, but speak softly from the front of their mouths) or very awkward. If you are female, you probably used the wrong expressions. You\u2019ll get the hang of it. Just keep on practicing.\n\n, South Africa and Namibia are sunny countries. Biometeorology and psychology has the theory that the amount of sun exposure has an influence on human behaviour. In parallel with other sunny Mediterranean South European and South American peoples, Afrikaans speaking people are much, much less reserved and much more talkative, emotional and interactive than Northern Europeans. If they are happy, shocked, sad, frustrated, passionate or overjoyed, the facial expression, voice tone, body language and hand gestures tells it all. To show emotion is not a weakness, it shows you are human \u2013 and is therefore a virtue. They\u2019re not living in the science-fiction film, Equilibrium.\n\n, When it comes to gender, Afrikaans and the Afrikaans culture (like most other African cultures) has always been patriarchal. Some argue that the Afrikaans culture is fundamentally based on religion, while others argue that the lack of First World infrastructure and education cannot sustain acculturation to First World countries; which also includes social equality. Men have their traditional gender roles, and so do women. Respect it. In modern South Africa, there are but a few Afrikaans speaking feminists who want to change the image of Afrikaans culture, though most Afrikaans speaking women (especially those inside a marriage) complain: Vandag se mans is regtig pap! Waarom moet \u2019n vrou altyd die broek in die huis dra? (Today\u2019s men are utterly sloppy and pathetic! Why should a woman always wear the trousers at home? .) Keep this in mind when speaking.\nAfrikaans doesn\u2019t have any gender for a neutral object, such as a table, ship or car; just like English. Die\/ dit is used: Die motor wil nie vat nie. Dit werk nie. .\nHowever, if a gender must be added towards, say, a ship, car or table, it is always masculine. Jy moet die tafel vernis \/ motor was \/ skip laat nasien, hy lyk verwaarloos. (You must varnish the table \/ wash the car \/ service the ship, he looks dilapidated.)\nAny animal of which the sex is unknown, is always masculine; an animal is not an \u201cit\u201d. \u201cDaardie hond daar oorkant\u2013 het hy hondsdolheid?\u201d Don\u2019t ever call someone on their first name, unless permitted to do so.\nIf a minor calls you oom or tannie , accept it with gratitude. It is a form of respect. This title is usually given to someone who is 10 years older + than they are.\nIn a business environment, the title comes first, followed by the surname. If you don\u2019t know a woman\u2019s marital status, just use dame (Madam). The register is formal at the first meeting, but may become more informal as the business partners build a better working relationship.\nImportant \u2013 Don\u2019t use jy en jou (informal you) to someone much older than you are. It is very disrespectful and the person will most likely take it as an offensive gesture, for the two of you are not from the same age group(Note1). In this case, try not to use any pronouns at all, or use u (formal you).\n(Note1) In Europe and other First World Continents, there are less youth than elderly people. Therefore age egalitarianism is being utilized (as the youth are the odd ones out). In South Africa and other Third World Countries, there are less elderly people than youth. Therefore, the hierarchal pyramid persists (as the elderly people are the odd ones out).\n\n,, In this way, you will also get in contact with the different Afrikaans dialects.\n\n,, Latinized words also sound long (having too much syllables) and dreary. Rather use short Germanic words and short sentences instead. Words that the typical man on the street will understand. For example, don\u2019t use offisieel (official) instead of amptelik, like in Afrikaans is \u2019n amptelike taal van Suid-Afrika. (Afrikaans is an official language of South Africa). For a list for some of these words, go to: http:\/\/af.wikipedia.org\/wiki\/Lys_van_minder_suiwer_Afrikaanse_woorde . Difficult for the English and Romance language speaker? Sure do. But wait, there\u2019s another way out\u2026\n\n, What?! Yes! After all, you\u2019re probably not going to be a news reader or television personality. Perhaps an Afrikaans rock star... Afrikaans people use English words to lubricate a sentence (make it speak more fluently and faster), or if an Afrikaans equivalent term doesn\u2019t pop up that quickly. There is a difference between formal office\/document language and the informal conversation language (diglossia). So, feel free. Most Afrikaans speakers will notice you are not familiar with their language and won\u2019t bite your head off. There are only a few cases of purists-extremists; but they are only one in every ten thousand.\n\n, If the Afrikaans speakers notice that you are struggling with Afrikaans, they will automatically switch to English (or maybe an African language you might know) \u2013 they are only trying to accommodate you. But you have to put your foot down and demand Afrikaans. Otherwise, you\u2019ll never learn through trial and error. They\u2019ll gladly help you.\n\n, A large number of lyrics of popular songs are available on the Net and some of the contemporary artists\u2019 music videos are played on YouTube. On the sites you can also search for Kurt Darren, Snotkop, Steve Hofmeyr, Juanita du Plessis, Nicholis Louw, Sorina Erasmus, Chrizaan, Bobby van Jaarsveld, Chris Chameleon, Ray Dylan, Bok van Blerk, Emo Adams, Arno Jordaan, Gerhard Steyn and Robbie Wessels, Jay, Eden etc. Some of the other modern individuals and groups are Jack Parow, Fokofpolisiekar, Die Antwoord, Die Heuwels Fantasties, Glaskas, Die Tuindwergies etc. Since the early 2000s it seems as though everyone has literally jumped on the bandwagon. Every week a new Afrikaans artist arrives on the scene; and the Afrikaans music industry caters for almost every genre, but most prominently the rock genre. The reason for this fertile ground is because Internet piracy of Afrikaans music is particularly very low, and, therefore, gives the artists the chance of making money.\n\n, Before television in 1976, the World Wide Web in 1995, MXit in 2005 (a cellular phone chat application) and especially Facebook, people either went to the theatres, cinemas (alias bioskoop) participate in sports or read books. There was a boom of books especially from the 1950s \u2013 1970s, but interest declined onwards as time progressed. The best sellers today are recipe books and Christian literature, followed by romantic fiction, detective stories, autobiographies and poetry. Schools are the major stimulators of the children\u2019s literature book market, mainly because prescribed books for the curriculum are being purchased. Because it is rather pricey (and risky) today being an author in Afrikaans, most aspirant authors test their skills on Woes.co.za. Have a look over there.\n\n, http:\/\/afrikaans.news24.com\/; Die Burger.com (for Cape Provinces); Volksblad.com (for Free State) and Beeld.com (covers the earlier Transvaal) has all the latest South African and international news in Afrikaans. Republikeinonline.com.na has all the latest Namibian and international news in Afrikaans. Though it should be added that newspapers tend to be bugged by language errors, clich\u00e9s, jargon and Anglicism, it\u2019s a good way of picking up neologisms and to get in closer contact with the Afrikaans speaking community.\n\n,, From January 2010 Roepman, Jakhalsdans, Ek lief jou, Ek joke net, Die Ongelooflike Avonture van Hanna Hoekom, Liefling, Getroud met Rugby and Platteland has been released. English subtitles included. Important: Though most of the films\u2019 setting are in rural places (such a clich\u00e9!), don't be fooled: the Afrikaans community are well urbanized.\n\n, Get streetwise at: http:\/\/en.wikipedia.org\/wiki\/List_of_South_African_slang_words\n\n, Other than the anti-egalitarianism issue, the Afrikaans community is not fastidious about choice of words and are constantly simplifying the rules. Enjoy!\n\n","label":0,"model":"human","source":"wikihow","id":984}
{"text":";\n, Assess your comfort level with what the owner. Do they take the time to treat you like family? Spend time conversing with the owner and ask a lot of questions.\n\n, Choose homes where you can actually email prior renters. Ask about cleanliness, if the home was as advertised, was pool heat as promised, if the owner was available when needed, etc.\n\n, While many folks feel uncomfortable not having the actual address of the home ahead of time, think of this: How would you like to stay in a vacation home when the entire world knows you\u2019re vacationing and probably will be at Disney most of the time. Not a real good idea! Be satisfied with the general locality.\n\n,,, Pay the deposit by credit card. If difficulty arises, this will enable you to challenge charges.\n\n, (Of course, a deposit will be required up front, but if you have accomplished our other tips, that is OK.)\n\n, It is not wise to place trust in a not so well paid employee of a management company to make sure you vacation needs are met, and met in a timely fashion.\n\n, Be sure you get the same answers.\n\n, You'll be surprised how many are not.\n\n, If you are not a smoker, be certain to obtain a guarantee that the house is smoke-free. The same holds true for pets. If you are allergic to animals, make certain that no animals have been allowed in the rental home.\n\n","label":0,"model":"human","source":"wikihow","id":985}
{"text":" There is no magic number or time at which a person is ready for dating. It's dependent on who you are, how mature you feel and what your parents think about it too. Indeed, it's a combination of factors. At this point, don't feel bad if you don't actually feel like dating yet\u2013\u2013that's okay to, you can have friends instead of dates too and nobody will think any the worse of you for it.\n\n\nTake a little time to better understand your own motivations for having a boyfriend or going out. How big a factor is each of the following for you: wanting friendship\/companionship and someone to hang out with; wanting to have someone else interested in your well-being and listen to your worries and joys; wanting affection (hugs, touch and nonphysical affection, romance, etc.); wanting to be accepted and cool and not seen as a loner by schoolmates? Which of these are your main motivators? The better you understand your own needs and motives, the better your decisions and choices will be about whether to ask a boy out, and if so, who to ask, and how to do it.\nTaking the time to introspect and understand yourself better is a lifelong process important for adults as well as for middle school students.;\n, If they don't think that dating boys is appropriate right now, they mean it and they have good reasons. Listen to them and just find friends instead. There's plenty of time for love later.\n\n\nIf you're parents are cautious but open to the possibility, explain how responsible you'd be if you went on a date, such as meeting curfew times, letting them know where you are going and being reachable by phone at all times.\n\n, Ask them what they think about dating and how comfortable they feel about it. While you don't need to agree with what they say or think, it can be helpful to gauge their feelings and get ideas to form your own thoughts.\n\n, Once you're happy that you're ready to date, try getting to know this boy better.\n\n, He might not turn out to be the kind of person you first thought. It makes good, common sense to get to know him a little before asking him out. Fortunately, there are plenty of ways you can use to get to know this great person somewhat better.\n\n, Make it a group thing. Get your friends and his friends to mingle at recess, lunch and after school. Make it all seem most casual, and just hang about chatting to everyone, including him.\n\n\nMake friends with him and his friends. It will make him feel closer to you without realizing just yet what your real intentions are. This gives you the space to study him close up and kindly.\n\n, Have casual conversations with him about school, sport, your uniforms, school rules, anything really that is simple, relevant and easy to chat about.\n\n\nFind out what interests him and let him know just a little about what interests you.\nHere are some conversation starters: \"Want some gum?\" \"Hey, do you have any gum?\" \"Okay, no more projects on cells. Mine looked like it came out of a horror movie!\" or \"Random Question: are you a morning or night person?\". If you are in different years\/grades, ask him what he thinks you'd like next year.\nDon't chew his head off with \"me, me, me\" talk though. Instead, make the aim to learn as much as you can about him. He'll ask you enough things in time.\n\n, Give him little gifts now and then, such as chocolates. But be sure to give them to his and your friends at the same time. You're trying to prove you're a generous lassie, not someone focused only on him\u2013\u2013that comes shortly. For now, show him what a great spirit of generosity you are.\n\n, Send a friend request and bond online. Send casual messages, jokes, funny pictures and nice sayings. Keep it light and friendly and include your other friends initially so that he looks like he's just one among many recipients of your online banter. As time goes on, you may grow to feel comfortable just chatting or messaging him.\n\n, He doesn't want to know what you're doing every five seconds and he certainly doesn't want to have answer to you about what he's doing every five seconds. Instead, just get to know his schedule, work around it with the occasional \"hello\", \"how's it going\" and \"wow, isn't funny how we keep bumping into each other?\" moments.\n\n\nIf you're online, take the chance to now and then send something with a \"when I saw this, it made me think of you, 'coz I know you like the LOL cat\/etc. pictures\".\n\n, Do you want him to be your boyfriend? Before you ask him out, be sure he's the one. Before you decide that you're in love, know what your own feelings are. Make a list of the negatives and positives about him, and then decide.\n\n, At least have inkling that this guy has grown fond of you and is somewhat into you. Signs include him breaking the touch barrier, lingering gazes, fond nicknames, making excuses to be near you, texting or messaging you a lot, blowing you kisses and generally being sweet to you. In some cases, he might be a little more brash and teasing toward you, because he feels comfortable around you. If you've gotten the impression that he's keen, it might be time. Ask him when you feel right and comfortable!\n\n, If so, flirt a little, make eye contact more often and pass a few sweet notes to him in class. Make it clear that you're keen on him.\n\n, He's yours for the taking if you've noted the signs correctly and feel you know him well enough by now.\n\n\nRehearse asking. If it helps you to gain confidence, practice beforehand. First, write down what you want to say. Then, practice in the mirror. Look at your body language and facial expressions. Practice until you feel confident.\n\n, This is especially important at the middle school and junior school levels. This isn't about determining your love match for life; it's about gaining a fun and carefree romantic friendship and the main thing is that you both feel comfortable with one another.\n\n\nSay something like, \"Want to go for a slurpee over lunch break?\". This is perfect. Make sure his lunches are free first.\nOr, casually ask \"So do you have band\/yearbook\/practice, etc. today? Mind if I come too\".\nOr, \"Hey, would you like to come with me to the movies\/park\/skate rink, etc. this weekend?\"\nIf asking him out over text message, say something like: \"My friends think we should get married. Spring wedding sound good?\" Expect a \"lol\" as a response. Then after he responds, say something like, \"Yeah, but I really do like you.\" Do try to use full words as often as possible; text messages can look really stupid if you use text language (u for you, r for are).\n\n, Realize that he may need time to process your request to get together, hang out or go on a date, and that's fine. Let him know it's fine and that he can get back to you.\n\n\nIf he says no, that's all right. He's probably embarrassed, or he hasn't made up his mind. Be understanding, and don't become defensive or act hurt. Never stall or make excuses. But at least he knows you like him and he might start looking at you in a different way.\nIf he says yes, say \"great\" and start planning your date, make sure you pay, unless he insists. Some guys love it when the girl pays, it makes them feel loved, others feel guilty and feel like their being given sympathy.\n\n, Bite your head off? No he will either say yes, no or I'll think about it (if the later, don't let him leave you hanging indefinitely; move on). Only ask if you really like him! Don't do it if you're just making it up! If he gives you something, try to give him something back the next day.\n\n, He may be with his friends more often than when he is with you. This is normal, and part of a healthy relationship!\n\n, If he sees that you care and want to see him more often, he may become more committed to the relationship. (Try to have a few activities in mind to do together, like playing video games, baking brownies, watching a favorite show or movie, shooting hoops at the nearest basketball goal, etc.)\n\n, Depending on the sort of guy he is, if he comes around to your house, find a comfy spot and lean in. Boys love kissing! However, if you aren't comfortable, don't force yourself.\n\n, You want to keep in touch with him, but not 24\/7! He might think you're a nag, or that you don't trust him. Text him once or twice a day, if he responds, YAY! But you can't expect him to. Also, if you see him in the hallway just say hi. If he stops you to talk, stay there and have a short conversation or tell him you'll talk to him at lunch.\n\n, If he is mean, cruel, or disinterested, just move on. He may be cute or cool, but it's just not worth being mistreated to try to pursue him. Pick someone new to like. Always remember that boys don't control your happiness.\n\n","label":0,"model":"human","source":"wikihow","id":986}
{"text":" Cotton and felts work best, but you can make this from nearly any material once you get the hang of it.\n, from your fabric. The shapes should be approximately 2 inches (5cm) long and one inch (2.5cm) wide.\n\nImportant: Make all of the football shapes exactly the same size, and allow an extra 1\/4 inch (1\/2 cm) for the seams.\n12 shapes from each of three different colors works very nicely for this project.\n\n, If sewing by hand, take care to make your stitches very small and even to prevent problems later.\n\nNote: Begin and end the seam a seam allowance width from the point to prevent bunching when sewing and turning.\n\n,, With right sides facing, sew one long edge together., This will create a three sided wedge which resembles a (somewhat fat) orange section.\n\nIf using three fabrics, use one fabric per side of your wedges so that you can \"change colors\" on the ball later.\n\n,,,,,,,,\n\nAlmost done... just one set of points left to be sewn.\n\n\n\n\n\n\n\n,\n\nAdd embellishments such as buttons or ribbon ties if you like, but be aware that you should not give a button encrusted ball to a very small child who may choke on the buttons.\n\n","label":0,"model":"human","source":"wikihow","id":987}
{"text":",, If your breast is over-flowing in your cups, or your bra cups are cutting into your breast tissue, then it's time to induce a larger cup size or a completely different bra design with a fuller cup. The edges of the cups should lie flat on your breast. If the bra cup is pointy, and the design wasn't meant to be thus, then you wish to scale back the cup size.\n\n,, If your bra underwire is facing outward from your body, this is often an indication that your push up bra cup size is simply too little. Additionally, how far away your breasts are from each other will have an effect on how the middle panel seats on your sternum.\n\n,, The lower the bra band fits on the back, the higher the support can be. Ladies with spinal arch can have issues with where the bra band will work on their back.\n\n,, If your bra straps are carrying a lot of than ten%, you need a smaller bra band size. Ladies with slanting shoulders need to buy bras with specific styles and special features, or bra accessories that help keep the bra straps up and on the shoulders.\n\n,, It ought to additionally give a well-outlined breast shape. Keep in mind that we tend to aren't living at zero gravity; therefor bra cup made from stretchy material might be comfy; however the form and support of your breast can be compromised.\n\n,, If your wires curve faraway from your chest in the middle, this usually means that that your bra cup size is simply too tiny. However, your breast size, form and uniqueness, will definitely affects how the underwires bra fits.\n\n,, An uplifted breast seems abundant younger and gives a higher appearance.\n\n,, If for any reason the arms a bumping against the side of the breast, then it's time for a different bra or size.\n\n,, If you're having problem placing your finger beneath the band, then you need to alter the hook position, or get a bigger band size.\n\n,, Or it is time for you to form some adjustment to that bra yourself (I saw my mama do it along with her hands, needle and thread).\n\n","label":0,"model":"human","source":"wikihow","id":988}
{"text":" The employee handbook should include a general introductory paragraph that outlines the purpose of the handbook. You should welcome the employee to the company and discuss the company\u2019s goal of being a rewarding place to work for employees. You should also reiterate the importance of the employer\/employee relationship.For example, you may state: \u201cWe at are confident you will find our company is a dynamic and engaging place to work, and we look forward to your contribution to our continued success. We consider our employees to be our most valuable resource. This handbook will serve as guide for the relationship between employer and employee.\u201d;\n, You should also make sure the introduction to the handbook contains a disclaimer that the handbook is not a legal, binding contract between employer and employee. This will help to prevent legal disputes and protect your company in the event of a legal issue in the future.For example, you may state: \u201cThis handbook only contains general information and guidelines. It is not a binding legal contract and does not act as a contractual right to remain employed by the company.\u201d\n\n, This is another important disclaimer that should be included in the employee handbook. You should make sure you state that your company is not obligated to retain employees and note that an employee can be dismissed at the discretion of the company. This will ensure the handbook is not seen as a contract by employees or cannot be used as such in a court of law.For example, you may state: \u201cYour employment may be terminated at any time with or without cause and without prior notice by the company. You may also resign at any time.\u201d\n\n, You should also include an acknowledgement page that needs to be acknowledged and signed by the employee. This will ensure the employee has agreed to the terms of the handbook and signifies the employee understands the policies in the handbook.You can find samples of an acknowledge page for employees through the Society for Human Resource Management website and the National Federation of Independent Business website.\n\n, Most federal and state governing bodies require companies to provide a clear, detailed wage policy in the employee handbook. Each state has different employment laws and require you to include different information about employee wages in the handbook. Get familiar with your state\u2019s laws to ensure you cover all the requirements.You may need to check the federal laws for employers through the U.S. Department of Labor website, or your country\u2019s applicable Department of Labor website. Make sure you are clear about the requirements for your company\u2019s handbook before you outline any policies or expectations.\n\n, Your employee handbook should note your legal obligations on pay schedules and overtime pay. Your employees should know how often they are getting paid as well as if they get paid for overtime work. You should outline the expected work hours for your employees, depending on if they are full-time or part-time.Include descriptions of exempt and non-exempt employees. Exempt employees are typically upper-level management and are excluded from laws governing minimum wage, overtime, and other wage practices.Your description should include the definition of overtime. For example, working longer that 9 hours per day, 40 hours per week, holidays, etc. Be sure to mention that travel time or prep time to be ready for work are compensated.\nYou should also note your company\u2019s policy on coffee breaks and lunch breaks, such as how long each employee is allowed for these breaks. This will ensure your employees know what to expect in terms of managing their time.\n\n, In the employee handbook you should also discuss the compensation packages your company offers its employees. This may include bonuses, stock options, and salary increases over time. Make sure you offer compensation packages that are realistic and affordable for the company, as you may need to follow through on these commitments in the future., Your company may be required by state or federal law to offer employee benefits like health benefits, dental benefits, and life insurance. These employee benefits should be briefly outlined in the handbook. You should not go into specific detail as your benefits policies may change and you do not want to put outdated or incorrect information in the handbook.Make sure you should mention who is eligible for benefits, such as full-time employees, part-time employees, and their families and spouses. You should also explain the criteria for enrolling in benefits and when you can change benefits, such as in the event of marriage or the birth of a child.\nYour benefits guide should also include details of any company-sponsored retirement or savings plan. Make sure to include any relevant policies such as contribution matching or vesting periods.\n\n, For companies where employees may be required to use their personal property or travel for business purposes, you will need to outline the policy for company reimbursement of these expenses, if any. Be clear on which expenses are the responsibility of the company and which the responsibility of the employee. Outline the process of gaining approval for reimbursement and listing expenditures., Under federal law, you are required to have leave policies in place. You must have a family medical leave providing employees with up to 12 weeks unpaid leave for the birth or care of a child, to care for an immediate family member with a serious health condition, or if the employee develops a serious medical condition. You should check your state\u2019s policies for unpaid family leave.You should also explain the company\u2019s policies for holiday leave, such as how much holiday time a year employees are given as part of their employment. You should outline the amount of time allowed for the death of a family member and for sick leave.\nMake sure you discuss your company\u2019s vacation leave policy, including how vacation time is earned and how to request time off. You should include a list of holidays observed by the company, with clear details about when the company closes or stays open during holiday times.\n\n, The employee handbook should outline the values and expectations you would like to see in your employees. You should discuss how you would like employees to behave and communicate in the workplace. You should also note a contact person that employees can speak to if they have any questions or clarifications.You should also maintain a positive and encouraging attitude in your discussion of employee expectations. This will keep your employees engaged in the handbook and make it feel accessible to them.\nFor example, you may note, \u201cWe expect our employees to adhere to a high standard of professional conduct and integrity. As an employee, you should be respectful and courteous to the feelings and needs of others. Individuals who act inappropriately or unprofessionally may be subject to disciplinary action.\u201d\n\n, If your workplace has a specific dress code, you should include it in the employee handbook. Make sure you are clear about the requirements for the dress code and specify the company\u2019s expectations for how employees should appear in the workplace.For example, if your company has an office setting, you may have a business casual dress code. You may note that all employees are required to adhere to a business casual dress code and appear well groomed.\nGive visual example of acceptable clothing and grooming, since ideas may vary between social groups and generations.\nInclude specific regulations about beards, visible tattoos, and head apparel that might be religious.\nIf your employees are often in the field working on construction sites, for example, you may require your employees to wear safety gear or clothing at all times. You should outline these requirements in the handbook so employees are aware of how they should appear every day in the field.\n\n, By law, many states require companies to have a clearly stated equal employment and non-discrimination policy in their employee handbook. You must clearly state that your company prohibits discrimination in the workplace.You should also make sure you discuss the Americans with Disabilities Act (ADA), which specifies anti-discrimination against people with disabilities. You can find out more about the ADA on the Americans with Disabilities website.\n\n, Your company should also have an anti-harassment policy that clearly states that no harassment under any circumstances is tolerated in the workplace. Often you are required by federal or state law to have these policies in place for your employees.You should have a process in place for filing a harassment complaint so employees know how to file a complaint and who to talk to about the issue. You may have a Human Resources representative who is responsible for addressing any harassment complaints in the workplace.\n\n, You should also make sure your employee handbook includes a policy on how to use company computers and software appropriately in the office and off-premise. This is especially important if the majority of the employee work is done on company computers.Make sure you outline how employees can secure their electronic information and protect any passwords or company information on their computers. You should also note any safety measures you have in place to protect the privacy of your employees online and your business\u2019s information.\nYou should also have guidelines around appropriate use of the computer and what sites employees can and cannot access. You should make sure employees are clear on what sites the company network can be used to access and note the importance of keeping company information private when communicating through email.\nNote any security steps that might be viewed as an intrusion of employee's privacy, explaining why they are necessary for company security.\n\n, You should also include details about how your employees are going to be reviewed on their performance in the workplace. You may put a performance review metric in the handbook or include general guidelines on how and when performance reviews will happen for your employees.For example, you may note that employees receive yearly reviews and that positive reviews usually lead to a salary increase or a bonus. You may also discuss disciplinary action that may occur if the employee does not receive a positive performance review, such as a probationary period for the employee, followed by a second performance review or termination.\nIn particular, note any conduct that might result in immediate dismissal, such as drug and alcohol use, theft, harassment, violence, or other serious offenses.\n\n, Your employees should be aware of all security measures on your premises, including security cameras, detectors, and guards. Include any operating procedures required for getting through security. Describe any off-limits or restricted-access areas, along with a clear description of who is or is not allowed to enter them. For clarity, you may wish to include a map showing these areas., You may be required to comply with Occupational Health and Safety laws in your state and you should mention these laws in your employee handbook. You should note that all employees must report all accidents, injuries, potential safety hazards, and any safety related issues to management.You should also have safety policies in place regarding poor weather or hazardous work conditions. This is especially important if your employees often work in the field or off site.\n\n, You should also include a process for reporting incidents on the job, such as an injury while working or a robbery. Your employees should be aware of how they can report an incident and who they can speak to in the event of an incident.You may have a detailed process in the employee handbook for incident reporting or keep it more general. You may want to opt for a more general discussion if you think the reporting process may change in the future.\n\n, You should also include a clear plan of action in the event of an emergency, such as a fire in the workplace or a natural disaster like flooding or severe weather. You should have an exit plan for employees to follow on a map in the handbook and discuss safety areas or points outside of the building., The employee handbook should be accessible and easy to read for every employee with a tone that is conversational rather than formal or stiff. Try to appeal to every employee by using a tone that is approachable and clear.You can do this by imagining the handbook is a conversation you are having with an employee as an employer. You should use a tone that is clear and friendly when you are talking to your employee, and stay away from formal or stiff language.\n\n, Labor laws can be complex but this does not mean your employee handbook has to be littered with verbiage or jargon. Instead, go for clear language and simplified terms. Having legal terms that are difficult to understand may not protect your company legally in the end and will only alienate the employees that are reading the handbook.You should try to avoid using formal terms like \u201cmanagement\u201d or \u201cauthority.\u201d Instead, use \u201cwe\u201d or \u201cemployer\u201d so the employee does not feel overwhelmed. You want the tone to sound casual, as your employees are more likely to read the handbook in full if it appears friendly and engaging.\n\n, Avoid putting in guidelines in the handbook that are overly demanding or unreasonable. You want the handbook to act as a useful guide for your employees and do not want to create rules that are difficult or impossible to achieve.You should also try to keep the handbook short and sweet, with just enough information to fulfill the state requirements for employee handbooks. You do not want to overwhelm your employees with information or with strict rules that are hard to follow.\n\n, Your employee handbook is a crucial document that could be used in a legal dispute later by your employees. You should get the handbook reviewed by a lawyer or a legal review to make sure your wording is clear and you are not making your company liable for any legal issues in the future. Once it has been cleared by the legal review, it will be ready to distribute to new and current employees at your company.","label":0,"model":"human","source":"wikihow","id":989}
{"text":";\n, If your Wii or Wii U has been modified from its original state using Homebrew, select the Homebrew option; if your Wii or Wii U has not been modified, select the Hackless option.\n\n,, The Project M file requires 2.0 GB free space. If necessary, use an SD card adapter to insert the SD card into your computer via USB.\n\n, The SD card must be in FAT32 format to be compatible with your Nintendo Wii system.\n\n\nWindows: Click on \u201cStart,\u201d select \u201cComputer,\u201d right-click the SD card, then select \u201cFormat.\u201d\nMac OS X: Open the Applications folder, select \u201cUtilities,\u201d click on \u201cDisk Utility,\u201d select your SD card, then select \u201cErase.\u201d\n\n, The root folder is also known as the root directory, and is the highest folder in a folder-based hierarchy system.\n\n,,, This will automatically launch Project M.\n\n, Super Smash Bros. Brawl will launch successfully with Project M., A locked SD card will prevent your system from reading and copying files., Dirt, dust, and other debris can prevent systems from reading SD cards., This file is required for Project M to integrate successfully with Super Smash Bros. Brawl. If this file is not present, repeat steps #1 through #6 from Part One to copy Project M to the SD card., Sometimes, the Hackless version of Project M can cause your system to freeze.","label":0,"model":"human","source":"wikihow","id":990}
{"text":" The higher your FICO score, which ranges from 300 to 850, the better interest rate you'll qualify for. The difference between a 4.5% interest mortgage and a 5% interest mortgage can mean tens of thousands of dollars over the life of the loan. Get a free copy of your credit report so you can see what the lenders see on your credit history. Pay off credit cards and resolve any credit disputes or delinquencies. In general, scores between 650-700 will get the average rate. A higher score it will get a great .25% reduction, but a score below that will cause a significant increase in your interest rates.Sample Rates Based on your Credit Score\n\n\n\nCredit Range\n600-650 (Check with your lender)\nBetween 650-700\nBetween 700-750\nBetween 750-850\n\n\nSample Rate\n6.00%\n4.00%\n3.90%\n3.75%\n\n\nTotal Amount Owed on a $175,000 Mortgage (30 Years)\n$377,716.83\n$300,771.64\n$297,150.97\n$291,762.82\n\n;\n, Apply to several lenders within a two week period so that the inquiries do not damage your credit report. Do this before contacting a real estate agent so you have a firm idea of what you can afford, and you don't accidentally fall in love with a that you cannot afford.\n\n\nSellers love buyers who get pre-approved. Pre-approved buyers are almost always given the green light by lenders, meaning there's less risk for the deal to get scuttled in the end.\nDon't accidentally get pre-qualified instead of pre-approved. There's a difference. Pre-approval means that the lender is usually prepared to give you a loan after seeing your financial vitals. Pre-qualified only means that the lender is estimating what you could borrow. It doesn't mean you'll get a loan., Wait \u2014 why would I shop for a mortgage before deciding on a house? Isn't that totally backward?Not necessarily. Shopping for a mortgage before you decide on a house can be beneficial for one overriding reason:\n\n\nYou'll know exactly how much you can borrow before you buy your home. Too many people fall in love with a home that they \u2014 well \u2014 can't afford. They struggle finding a mortgage that covers the cost of the home. Finding a mortgage first and a home second may seem less appealing, but it's twice as smart. You'll immediately be able to tell whether a home is in your price range or out of it.\nThink about the sort of down-payment you'll be able to afford. This should be part of your mortgage calculations, although you don't need to know for sure when shopping for a mortgage. Have a general idea in mind. More on this later in the article.\n\n, \"28 and 36\" is a commonly used ratio. It means that 28% of your gross income (before you pay taxes) must cover your intended housing expenses (including principal and interest on the mortgage, as well as real estate taxes and insurance). Monthly payments on your outstanding debts, when combined with your housing expenses, must not exceed 36% of your gross income. Find each percentage for your monthly gross income (28% and 36% of $3750 = $1050 and $1350, respectively). Your monthly payments on outstanding debts cannot exceed the difference between the ($300) or else you will not be approved., If you qualify for a first time home buyer program, these often have much lower down payment requirements. These are offered by various states and local governments. You may also be able to access up to $10,000 from your 401(k) or Roth IRA without penalty. Ask your broker or employer's human resources department for specifics regarding borrowing against those assets.\n\n, If you expect the buying of the house to be a simple, straightforward affair, then you'll probably only need a Realtor, the escrow company, and perhaps a mortgage broker. But then again, when do things ever go as expected? Hire an honest, reputable, (relatively) cheap lawyer if:\n\n\nThe cost of the lawyer is a drop in the bucket compared to the total you are likely to spend for the home.\nThe home you are buying is either in foreclosure or in probate, which means that the home is being distributed as part of a deceased person's estate.\nYou suspect the seller might try to quickly back out of the deal or you don't trust them.\nYour state requires a lawyer at closing. Six states currently require a lawyer present.Talk to your state commission of real estate to find out if it's common practice in your state. It is also a good idea to check with an attorney before entering into a contract.\n\n, The real estate agent should be: amiable, open, interested, relaxed, confident, and qualified. Learn the agent's rates, methods, experience, and training. In the United States, sellers pay the Realtor commission while buyers may pay a fee for having the Realtor represent them. Look for a Realtor who lives local, works full time, closes several properties per year, and has a reputation for being busy. Read more in How to Select a Realtor.\n\n\nA Realtor's job is to connect people who want to buy and sell a particular home. For this reason, a Realtor has an interest in selling homes. A very good Realtor will use her experience to sell the right home to the right buyer \u2014 you. A Realtor can tell you about the schools, nearby shopping, zoning of the property, construction nearby, ages and values of nearby properties, growth rate, and any other statistics on the area you may be interested in.\nWhen you do find your Realtor, go into exhaustive detail when describing what you want in a home \u2014 number of bathrooms and bedrooms, attached garage, land and anything else that may be important, such good lighting or yard space for the kids.\n\n, A Multiple Listing Service will give you a feeling for what is on the market in your price range. Your agent can do this for you.\n\nIf you sign up through a real estate agent, it is poor form to call the listing agent directly to see a house. Don't ask an agent to do things for you unless you're planning to have them represent you \u2014 they don't get paid until a client buys a house and it's not fair to ask them to work for free, knowing that you're not going to use them to buy your home!\n\n, Most lenders suggest that you pay no more than 38% of your monthly income towards your mortgage and debts combined. This means, on any given month, no more than 38% of your paycheck goes to paying back loans. You should use an online Home Affordability Calculator to find your own sweet spot. However, for a good idea of the house you can afford, tally up your current monthly bills, including credit cards, student loans, etc, and compare them against your income in the following sheet:Finding a Mortgage you Can Afford Based on Current Bills and Income\n\n\n\n\nIncome: $35,000\nIncome: $50,000\nIncome: $75,000\nIncome: $100,000\n\n\nMonthly Bills: $0\nUp to $187,000, or $1,050\/mo\nUp to $264,000, or $1,500\/mo\nUp to $391,938, or $2,225\/mo\nUp to $520,000, or $3,000\/mo\n\n\nMonthly Bills: $100\nUp to $170,113, or $950\/mo\nUp to $246,898, or $1,400\/mo\nUp to $374,875, or $2,125\/mo\nUp to $502,851, or $2,900\/mo\n\n\nMonthly Bills: $500\nUp to $101,859, or $550\/mo\nUp to $178,644, or $1,000\/mo\nUp to $306,621, or $1,750\/mo\nUp to $434,597, or $2,500\/mo\n\n\nMonthly Bills: $1,000\nShould not buy house\nUp to $93,327, or $500\/mo\nUp to $221,303, or $1,250\/mo\nUp to $349,279, or $2,000\/mo\n\n\n\n, You probably already have a vague idea, but the angel's in the details. There are a couple things in particular that you and your family should give good thought to:\n\n\nWhat will you and your family need in several years?Maybe you're just a couple right now, but are there are plans for kids in the future? A home that snugly fit two people could be torturous for three or four.\nWhat tradeoffs are you willing to make? In other words, what are your priorities? Although we like to believe that buying a house can be straightforward, it's often a complex ordeal in which we're forced to compromise. Do you care more about a safe neighborhood and good schools over a big backyard? Do you need a big, workable kitchen more than a big luxurious bedroom? What are you willing to sacrifice when it's crunch time?\nDo you expect your income to increase over the next couple years? If your income has increased by 3% for several years in a row and you hold a secure job in a safe industry, you can probably rest assured that buying an expensive but still reasonable mortgage is possible. Many homebuyers buy relatively expensive and then grow into their mortgage after a year or two.\n\n, Scout out what's available in the vicinity. Look at prices, home design, proximity to shopping, schools and other amenities. Read the town paper, if there is one, and chat with the locals. Look beyond the home to the neighborhood and the condition of nearby homes to make sure you aren't buying the only gem in sight.\n\nThe area in which your home is located is sometimes a bigger consideration than the home itself, since it has a major impact on your home's resale value. Buying a fixer-upper in the right neighborhood can be a great investment, and being able to identify up-and-coming communities \u2014 where more people want to live \u2014 can lead you to a bargain property that will only appreciate in value.\n\n, Pay attention to overall layout, number of bedrooms and bathrooms, kitchen amenities, and storage. Visit properties you're seriously interested in at various times of the day to check traffic and congestion, available parking, noise levels and general activities. What may seem like a peaceful neighborhood at lunch can become a loud shortcut during rush hour, and you'd never know it if you drove by only once.\n\n, If you are unsure about the price, have the home appraised by a local appraiser, who also looks at comparables. When appraising a home, appraisers will look for comparable homes or \u201ccomps\u201d in the area that have similar features, size, etc. If your home is more expensive than the comps, or the appraiser has to find comps in a different subdivision or more than 1\u20442 mile (0.8\u00a0km) away, beware! Never buy the most expensive house in the neighborhood. Your bank may balk at financing the home, and you probably won't see your home appreciate in value very much. If you can, buy the least expensive home in a neighborhood \u2014 as homes around you sell for more money than you paid, your home's value increases.\n\n, This is not easy, and often impossible, but it doesn't hurt to try when making one of the biggest purchases in your life. Here are some things to keep in mind as you think about your offer:\n\nWhat are the seller's financial prospects? Are they in desperate need of money or are they sitting on a pile of cash? Cash-strapped sellers will be more likely to take an offer that undercuts their asking price.\nIf the house is a flip, the seller is often less emotionally invested and wants to sell quickly. Have your agent call the seller's agent and find out what they want for the property. People flipping houses usually already have a number in mind. You can find out if a house is a flip by looking at sale records, if it sold recently (around a year ago) and for much less than it's listed for now, and looks upgraded, it's probably a flip. You can also look it up on Google Maps street view to get some insight as to what it looked like before. If it looked run down with boarded up windows, and now it's looking pretty nice, it's probably a flip. It can also help to reassure the seller that you can close quickly (if you really can!).\nHow long has the home been on the market? Homes that have been on the market for longer periods of time can usually be bid down.\nHave they already bought another house? If the sellers aren't currently living in the house they're trying to sell, it may be easier to bid less than you otherwise might.\n\n, Estimate the annual real estate taxes and insurance costs in your area and add that to the average price of the home you're trying to buy. Also add how much you can expect to pay in closing costs. (These take in various charges that generally run between 3 to 6 percent of the money you're borrowing. Credit unions often offer lower closing costs to their members.) Put the total into a mortgage calculator (you can find them online or make your own in a spreadsheet. If the figure is above 28% of your gross income (or whatever the lower percentage used by lenders in your situation) then you will have a hard time getting a mortgage.\n\n\nDetermine whether you need to sell your current home in order to afford a new one. If so, any offer to buy that you make will be contingent on that sale. Contingent offers are more risky and less desirable for the seller, since the sale can't be completed until the buyer's house is sold. You may want to put your current house on the market first.\n\n, Economics of supply and demand will sometimes force your hand. If many people are competing for few homes, be prepared to lead with your highest possible offer. Some homebuyers don't believe that you should lead with your highest offer, but you could easily find yourself being outbid and never get the chance to bid on your house. If you want to give yourself the best shot on a home that you really, really like, lead with a high bid.\n\n, Although the guidelines for submitting offers may differ from state to state, this is usually how it goes: You submit your offer to your Realtor, who then forwards it to the seller's representative. The seller then decides to accept, reject, or make a counteroffer.\n\n\nInclude earnest money with your offer. This is typically 1-5% of the offer. Once you sign an offer, you are officially in escrow, unless you cancel using an accepted contingency to the contract during the contingency period. During escrow (typically 30 to 90 days), your lender arranges for purchase financing and finalizes your mortgage.\nConsider putting an expiration time on your offer if you or your agent think it makes sense for that situation. For example, if you put a 24 hour expiration, you're only bound to that offer for 24 hours. This can put a little pressure on the seller to act quickly.\n\n, A down payment establishes equity, or ownership, in a home. That's also money that you don't have to pay interest on. The more of a down payment you're able to make on your home, the less money you'll ultimately pay on your home.\n\n\nYou may be expected to put down 10-20% of the appraised value of a home depending on your loan package. However, there are loan packages that allow you to put down much less. Note that the appraised value may be higher or lower than the selling price of the house. If you have $30,000 saved for a down payment, for example, you can use it as a down payment for a home between $300k (10% down payment) or $150k (20% down payment). Putting less down often, but not always, requires you to pay private mortgage insurance (PMI), which increases your monthly housing cost but is tax deductible. However, 20% is the typical amount for not needing to pay PMI.\nIf you can't afford a 10%-20% down payment on your home, but have good credit and steady income, a mortgage broker may assist you with a combination or FHA mortgage. FHA mortgages only require 3.5% down payment and there are other loan packages that require as little as 3% down. There are also USDA and VA loan that require no money down. Talk to your mortgage broker to find your best option.\n\n, Request the following surveys and reports: inspection, pests, dry rot, radon, hazardous materials, landslides, flood plains, earthquake faults, sewer scope and crime statistics. (You will generally have 7-10 days to complete inspections \u2014 be sure that your agent explains this fully to you when signing the purchase and sales contract.)\n\n\nA home inspection costs between $150 and $500, depending on the area, but it can prevent a $100,000 mistake. This is especially true with older homes, as you want to avoid financial landmines such as lead-paint, asbestos insulation and mold.\nIf you use the inspection results to negotiate down the price of your purchase, then include the portion of the inspection report that notes the deficiency to prove that it exists.\n\n, Getting a home energy audit is an essential part of the home buying experience. Not knowing what it really costs to heat and cool a home is a potential financial disaster waiting to happen. Home buyers make \"guesstimates\" when figuring out a new home budget. These estimates can be significantly incorrect and place families into dire financial circumstances.\n\n, This is usually conducted in an escrow office and involves signing documents related to the property and your mortgage arrangements. The packet of papers includes the deed, proving you now own the house, and the title, which shows that no one else has any claim to it or lien against it. If any issues remain, money may be set aside in escrow until they are resolved, which acts as an incentive for the seller to quickly remedy any problem areas in order to receive all that is owed.\n\n\nConsider using your real-estate lawyer to review closing documents and represent you at closing. Again, Realtors are unable to give you legal advice. Lawyers may charge $200-$400 for the few minutes they're actually there, but they're paid to look out for you.\n\n","label":0,"model":"human","source":"wikihow","id":991}
{"text":" Score tries by carrying the ball into Your opponent's try-zone, ( penalty tries can also be awarded from any infringements from the opposing team.) A try is worth 5 points.\n\n\nWhen a try is scored an opportunity is given to the scoring team to make a conversion kick which is worth 2 points.\nThe other way of gaining extra points for Your team is with penalty kicks, this is awarded when a member of the opposing team breaks a rule or if the play of the game is stopped by more than one member of that team. a Penalty kick is worth 3 Points.\nThe last method of gaining points is with a drop-goal attempt, the ball is played backwards to a back-row player who then attempts to place the ball in between the uprights during ongoing game play, this is also worth 3 points.;\n, If you get the ball into your opponent's try zone you get 5 points, as well as a chance to attempt a conversion kick for an extra 2 points. The winner is the one who scores the most points in 80 minutes of play.\n\n\nYou have to touch the ball down to score in rugby. That means after you've entered the try zone the ball must be placed on the ground to score.You can also kick the ball through the uprights during free play (with a drop-kick only) or after the referee awards your team a penalty (place kick) for 3 points., To score tries, offense teams form big horizontal lines, passing sideways and backward until they expose a hole in the defense and push forward. This is the big rule to remember when playing rugby. You cannot pass the ball forward with your hands or it is an automatic penalty. Whenever passing, you usually pass the ball across your body to get power and throw the ball diagonally to your teammate.\n\n\nDropping or bobbling the ball counts as a forward pass if it hits your hands then hits the ground in front of you.\nYou can, of course, carry the ball forward. However, any passes by the hand cannot be forward., While you cannot ever throw a ball forward, you can kick it forward at any time. Once you kick the ball, any teammate that was behind you when you kicked it can run forward and get the ball without a penalty. Kicks are a great way to surprise a defense, or boot the ball way down the field to get your team out of trouble.\n\n\nPlayers can even kick it to themselves, chipping it over an opponent and running around to pick it up.\nYou cannot kick the ball to a teammate who is already further down the field than you unless you run past them at some point. If a player was behind you at any time after your kick they can touch the ball. If not, they are offside., Rugby defense is all about making tackles. You can only hit the player currently holding the ball, and you cannot block or screen the other team or other players. When you tackle an opponent, your goal is to bring them to the ground as quickly as possible, preferably with teammates around to quickly help win the ball. General tackling rules include:\n\n\nYou must tackle someone below shoulder height.\nYou must wrap your hands around the player, not just spear them with your shoulder.\nYou cannot pick up and drop players, especially on their head or neck.\nOnce down, you must return to your feet before trying to win the ball from someone you've tackled., When a player goes down, they must release the ball. From there, either team can fight to win the ball off the ground and take possession. This usually takes place as a \"ruck.\" A ruck is when 1-3 players from each team lock into one another, right above the ball (and tackled player), and push each other back and forth in an attempt to win possession. One player on each team stands behind the ruck and grabs the ball when it appears behind their rucking teammate's back foot. Because the tackled player can put the ball down wherever they want, the team that had the ball first usually controls it. There are a lot of rules and strategy for contesting a ruck, but some basic reminders include:\n\n\n\nStay on your feet. A ruck is when at least two players lock shoulders above the ball and try and push the opponent away from the ball. You cannot reach in to grab the ball or push off the ground with your hands.\n\nEnter through the center. For safety reasons, you must enter a ruck straight up and lock in with the other team. You must be perpendicular to the end line, and your body must be straight over the ball. You cannot run into a ruck diagonally or from the side.\n\nStay behind the back foot if not in the ruck. Once a ruck is formed, you must stay behind the back foot of your final teammate until the ball comes out, no matter what team gets it. The entire space of the ruck is a \"neutral zone\" that no players can enter it., When a penalty (such as passing it forward) is called for your team, you get several options of restarts, depending on the foul. One of the most common is a scrum, where both teams form into battering rams and connect over the ball. A scrum is essentially a test of strength. In a scrum, 8 players from each team weave themselves together in rows to create a unified mass. The two teams then go head-to-head, each pushing against the other, with a tunnel formed on the ground in between them.\n\n\nIn a scrum, one team throws the ball in the center of the two teams and they push and hook each other to try and win the ball.\nScrums suck all the biggest players to one area of the field, leaving the other seven players with much more free space. If your team wins it, you're often off for a big gain.\nScrums are perhaps the most dangerous aspect of rugby, and you should not try them without a coach and well-trained, athletic players., If a ball goes out of bounds the referee will call for a line-out. A line-out is like a throw-in in soccer, except the ball must be thrown straight forward. Each team forms a line on the field around the thrower. The team that did not touch the ball last gets to throw it in. They then toss the ball straight in between the two teams, who lift each other up to try and win the ball. The game then restarts as normal.\n\n\nTeams often use signals and codes to win their own line-outs. Basically, a thrower will signal to their team when they will throw it, and their teammates will hoist a player into the air, out of reach of the other team, to grab the ball quickly., Each team is made up of two smaller sections, each with their own positions and strengths. Forwards take part in the scrum, and this is the only real difference between them an backs as far as rules go. However, forwards and backs have evolved their own specialties on the field, making each vital to success:\n\n\n\nForwards\/Packies: These are the big guys -- the bruisers and muscle-men who power the scrums and win most of your rucks. Forwards generally take short, powerful runs with the ball, pass less, and do the majority of tackling and defense. Generally thicker and stronger.Backs: The runners and speedsters. Backs form a long diagonal line on offense and pass the ball quickly down the line, to the outside of the field, where they can take on a defender 1v1. On defense they form a wall across the field to prevent enemy backs from breaking through. Generally quicker, backs are good kickers, have solid passing skills, and tons of speed\/endurance., Watch local and professional teams\u2019 matches or practices in order to see how the game is played. You can also watch matches on television or recordings such as DVDs. Watching how a game is played in real life or through recordings allows you to see the intricacies of the sport.\n\n\nThere are a lot of small rules, specific situations, and intricacies to rugby that are impossible to learn without playing or watching. Ask questions of fans and\/or the referee when confused, and keep learning each game each time you play.\n\n, A good rugby throw is powerful and quick, which protects your teammate from a nasty tackle as they watch the ball approaching them. Practice throwing to your side, not in front of you. To get a good throw, use both hands and focus on accuracy first, hitting your teammates right in the chest every time. Once you feel accurate you can start working on a professional-looking spin. To throw from the right hand:\n\n\nStart with the ball horizontally in front of you. Place your right hand on the back third of the ball, so your palm faces down and your thumb towards your target. Place your left hand on the bottom left half of the ball, thumb pointing left.\nBring the ball to your right side, keeping the point of the ball still roughly pointing at your target.\nUsing your left hand for aim and right for power, bring the ball across your body and towards your target.\nAs your hands are almost extended, curl your right hand back towards you, spinning the ball.\nRelease the ball with both hands simultaneously, rotating your wrist completely over on the follow-through. Both arms should be fully extended and pointed at your target, and your arms should stay low, around belly height. the whole time., Tackling is a skill that can be developed over a lifetime, but the basics are simple. You want to let your whole body do the work, not just your arms, to protect yourself and to ensure that you bring the guy down. Practice doing the following steps near simultaneously -- keeping your head up, driving with the shoulder, wrapping your arms, and driving to the ground:\n\n\nSet up on the balls of your feet. Make sure you are in a good athletic position for the hit -- knees bent, muscles relaxed, on your toes.\nKeep your head up and aim for their thighs\/stomach. Keep your head up to see contact coming and line up the hit. This is a crucial step for safety.\nAim to make contact with your shoulder right at their upper thigh. Slide your head next to their butt to keep it safe and drive in with your shoulder.\nWrap your arms around their thighs and pull back. Pull their thighs into a deep hug -- this causes them to lose balance easily.\nDrive with your legs to bring them down. Once you've got your head in place and your arms wrapped, you want to push with your legs to bring them down.\n\n, A ruck is your regular chance to keep or steal possession, and a good rucker is an invaluable asset on the team. If you see a teammate go down with the ball and you're nearby, try and be the first one in the ruck. Set one foot over the ball so that it is underneath you, then lean down into a low athletic position. When you've made contact with someone, win rucks using strength and leverage:\n\n\nGet underneath the opponent and push up and out. If you can get your shoulders or head under their chest, you can push them up to throw them off balance, then back to remove them from the ruck.\nPush teammates from behind to win contested rucks. If it looks like your team needs a boost, push your ruck through like a minor-scrum. Remember, however, that committing more than 2-3 people to a ruck leaves a lot of holes in the defense if you lose.\nKeep your feet moving through the ruck. To really crush rucks, chop your feet and push forward with every step. Imagine running straight through the player to the other side. This is called \"clearing\" a ruck, and it opens up a lot of space. If you're on defense and you can clear a ruck, even if you can't win the ball immediately, you'll throw the other team into disarray., Rugby is not an individually dominated sport. Even the best player will be unable to do anything without the support of teammates, as there are very few times a solo athlete has space or time to make a play themselves. On both defense and offense you should be in the best possible place to support your teammates, no matter what:\n\n\nOn defense, you need to be in a horizontal line with your teammates, sliding left and right to close up any holes the opponent might try to push through. Once a teammate makes a tackle, you should either jump in the ruck if it is open or winnable or slide to either side of the teammate to make sure the other team doesn't exploit the gap while he is down from the tackle,\nOn offense, you need to spread out, forcing the other team to cover many angles. When a teammate is running, make sure you are always behind them and they have 1-2 people in passing range. If they get tackled, they may try a quick pass to you as they go down, leaving you plenty of time to run., Rugby can be played in any large field, so long as it is relatively flat and evenly sized. The size of the field will largely depend upon how seriously you intend to play. If you wish to simply play the game with a few friends, a local park with any large field may be enough. However, if you wish to play a proper match, you will need a field with two uprights for the extra point. Contact your local Parks and Recreation department or a local rugby club to find out about proper fields in your area.\n\n\nAs long as you can mark out try-zones for each team, any rectangular patch of grass should be fine.\nThough the dimensions are slightly different, an American football field can fill in for a rugby field in a pinch.\n\n, There are three commonly played forms of rugby, where the biggest difference is in the number of players. Real games have 15, 10, or 7 players on each team, but you can play with as many players as you want if you're just with friends. Each player should have:\n\n\nAthletic shoes, preferably with cleats in the bottom.\nLight, breathable clothing.\nMouth guards and\/or head protection, if desired.\nWater., Forwards are generally placed in position based on the scrum, and their positions does not necessarily impact where they will be during the rest of the game. What matters most is your scrum position:\n\n\n\nFirst row. The first row contains three players: the loose head prop and the tight head prop, who grip the hooker in between them.The role of the hooker is to gain possession of the ball during scrums and usually throw the ball in at line-outs. The role of the loose and tight head props is to support the hooker during scrums, support other players during line-outs and provide strength during rucks and mauls. The props are your two biggest players.\n\nSecond row. The second row consists of two locks. These are the tallest players in the team and are generally used in line-outs to gain possession of the ball. They drive their shoulders into the props and are the engines of your scrums and rucks.\n\nBack row. The back row of forwards consists of three players: two flankers and the eight-man. They clasp onto the two sides and back and control the direction of the scrum, as well as the ball if it squeezes out. Generally these three are your fastest forwards, as they can quickly dart off the scrum when play restarts., Backs need to be good with their hands, and the best players need to be the first ones touching the ball. To visualize your back line, imagine the ball on the right side of the field. Each player will be 10-15 feet diagonally to the left of the other, starting with the scrum-half:\n\n\n\nScrum-half: They start the ball from a ruck or scrum, pulling it out and determining the first player to get the ball. They must be light, small, and have considerable endurance to reach every ruck or scrum first. Most importantly, they need to see the whole field and distribute the ball accordingly.\n\nFly-half: The quarterback for the backs. They run most of the kicking and run plays, like skipping a pass or fake passes, to try and break down the opponent's defense.\n\nCenters: There are two of them, and they are good overall players who can tackle well on defense, run and pass well in the middle, and kick if need be. They get the ball often and challenge the opponent's back line.\n\nWings: Two wings that stay on each side-line, these are usually the fastest players. Your goal is to get them the ball on the outside, where they can hopefully outrun the opponents wing for massive gains.\n\nFullback: She\/he sits 15 meters or so behind the line, ready to get an opponent's kick, make a last-second tackle, or sprint forward to unexpectedly join the offense and overload a team. Must be versatile, able to kick and catch, and fast., There are several ways in which a penalty may be earned, and it is impossible to play the game and keep an eye out for every infraction. Minor infractions will likely result in a scrum being awarded to the other team. Others may cause a referee to take more serious action, giving a penalty kick, a \"back ten\" (when the team gets 10 meters to run the ball for free), or removing a player for some time.\n\n\nCommon offenses include incorrect tackling, collapsing a scrum or ruck by leaving your feet, holding the ball when on the ground, and entering rucks incorrectly.\nThe team awarded the penalty has several options. They can take a kick or punt, which will allow them to gain field position. They can take a penalty kick at the uprights for a chance at 3 points, or they can take a scrum, all in the location of the penalty.\n\n, A game of rugby is physically intense, and you're incredibly prone to injury if you don't prepare. A good warm-up increases blood flow and prepares your muscles for the beating to come. A good, simple warm up to start with would include:10-minute light jog. Running is an excellent way to warm up for rigorous physical activity. Begin by walking briskly for 5 minutes, followed by 10 minutes of light jogging to prepare yourself for running. Dynamic stretching. Run with high knees, kick your ankle with your heel with each step, do some lunges, jump in place, swing your arms, and skip. These exaggerated, mobile stretches loosen your muscles more effectively than static stretching.\n\nWarm up rugby skills. Pass with a partner, take some kicks, and do some light rucking. Get used to rugby specific actions, even tackling at half-speed. Your forwards should do some practice scrums and lineouts, and your backs should get used to passing in a line.\n\nDrink water and eat at least 2 hours before playing. Hydration is essential in physical activity. In order to combat the loss of and increased use of water during exercise, it is important to stay well hydrated throughout the day but especially right before activity. The other essential nutrients used and expelled by your body during exercise are salt and potassium, eating lean meats, fruits, vegetables, and sports drinks\/food., This is traditionally started by a coin toss, in order to determine which team will have first possession of the ball. You can toss coins or decide who starts in some other manner. Positioned at the center of the field, the team which has first possession will then kick the ball towards the opposition.\n\n\nMost rules state that this must be a drop-kick. It needs to go 10-meters before the ball can be played.\nIf you are on the kicking team, you cannot pass the kicker until the ball has left his foot.\nAfter your team scores a try, the other team must kick it off to you again.","label":0,"model":"human","source":"wikihow","id":992}
{"text":"\n\n\nThis can be done by shutting off the main gas or turning off the circuit breaker for electrical devices.;\n,\n\n\nThis procedure will prevent cold water from flowing into the water heater and prevent hot water from coming out during the cleaning process.\nThe water valves consist of a cold water valve that may be colored blue, a hot water valve that may be colored red, and a third main valve that runs the water into your home.\n\n,\n\n\nThe purge valves have small handles that resemble the letter \"T.\"\nThis procedure is done to relieve any pressure that has built inside the valves and will prevent excess hot water from shooting out and coming into contact with your skin.\nThere may be pressure when removing the purge port valve caps, so it is extremely important to make sure the hot water valve is completely and accurately shut off for safety purposes.\nHandle each cap carefully to make sure the rubber washer sealing discs stay in place, which are needed for your valves to function properly.\n\n,\n\n\nIf the manufacturer of your tankless water heater did not provide you with hosing lines, you can purchase them from any retail store specializing in home repair or water heaters. The hosing lines must be long enough to reach between the water heater and your bucket.\nYou may need to consult your manual provided by the tankless water heater's manufacturer or contact the manufacturer directly for exact instructions regarding this procedure.\nIn some cases, this procedure may require you to use a sump pump and connect hoses that will discharge and flush water from the tankless water heater using the cold and hot water valves.\n\n,,\n\n\nSince your tankless water heater is most likely the source of all your drinking water and bathing water, using chemical cleaning solutions may be extremely harmful to your health.\n\n,\n\n\nThis procedure may take up to 45 minutes.\n\n,,,\n\n\nTighten the caps completely and firmly without breaking the rubber sealing discs located inside the caps.\n\n,\n\n\nThis procedure may just require you to rotate and open the cold and hot water valves so they are parallel to the position of the main valve that leads into the house.\n\n,\n\n\nContinue to run the water until it runs steadily without air escaping.\nThis procedure may take up to 2 or 3 minutes.\n\n","label":0,"model":"human","source":"wikihow","id":993}
{"text":" The blimp will drop your character on the island just outside of Mission Control.\n, You will see one woman, one man, and an ill-looking astronaut on the large screen., The flight director will enter the building and ask your character to find something to soothe the sick astronaut\u2019s stomach., A green bottle of ginger ale is sitting on the stage., This item will be added to your personal inventory.,,,,, Your character will provide the astronaut with the ginger ale, and the man will exit the rocket and lock you into the ship., The flight director instructs you to take over the space mission, and provides you with instructions on how to fly the spaceship., You will be instructed to adjust the booster rockets and fuel tanks. After you enter space, the flight director will inform you that your mission is to save an astronaut left behind on the moon. After the conversation ends, you will encounter a series of asteroids., After you make it through the asteroids, the flight director informs you that the spaceship has been damaged and must be repaired using the toolkit.,,, The flight director will inform you that the spaceship will now go into autopilot mode to take you to the moon, and that you will need to use the lunar lander to land safely on the moon\u2019s surface.,, This must be done just before you hit the moon\u2019s surface to ensure a gentle landing. When you arrive on the moon, your character will exit the spaceship.,,,, Your character will grab a photo album, which will be added to your inventory., This item will also be added to your inventory.,,, You will be required to press keys in a certain order to reboot the system and open the airlock door located inside Vehicle Bay., The system will reboot., This will open the airlock door located inside Vehicle Bay.,,, You must jump onto a series of objects and furniture to access the buttons that manage solar power., The ceiling will open up to let sunlight into the Vehicle Bay.,,, The sunlight will then charge up the solar-powered moon rover., Your character will drive the moon rover out of the building.,,, The rover will drive away and move the meteor away from the entrance.,,,, Your character will shrink and flatten so you can fit into tiny spaces.,,,,, A Locator Device will drop from the next vent to the right., The Locator Device will be added to your personal inventory.,,,, The cork will block the airflow., When all pipes are corked, the passageway at the top of the dome will open.\n\nTry to position your character behind each pipe to avoid having the air blow your character out of the way.\n\n, After entering, you\u2019ll see a pair of feet dangling to the right., The feet belong to Salerno, who is the astronaut who\u2019s gone missing. Salerno will inform you she is close to locating an alien structure on the moon, and escapes the lab.,, After you exit the building, your moon rover will chase Salerno\u2019s rover to another building on the moon., The eye scanner will verify that your eyes are purple, and grant your character access to the Rock Laboratory.,,,, The goal is to move the conveyor belts until your character reaches the top right corner of the lab., The sheet will fall away to reveal a Geiger Counter.,, The meter on the Geiger Counter will turn red as you approach the alien object.,,, Your rover will pull the purple object out of the ground, and the object will grow into a tall monolith and emit a strong beam of light., There will be a red beam of light connecting with the purple light.,, When you reach the mound where the beams intersect, Salerno will show up, and both of your characters will fall into the ground., There will be a handprint indention on the machine., A portal will open, and Salerno will disappear through the portal. The flight director will contact you to inform you the Mission Control team is bringing you home. After you arrive home, Salerno will contact Mission Control to say she has discovered extraterrestrial life, and that her mission is complete. The director of the Academy for Space Exploration will show up and award you the island medallion. You have now beat Lunar Colony Island.","label":0,"model":"human","source":"wikihow","id":994}
{"text":",,,,,,,,,,,,, If there is a shop locally with whom you would like to establish a relationship, you can call or visit in person. If you are sending flowers further away, you can ask a local shop to send a wire or you can find a florist in that area online and call a 1-800 number to place an order. Most florists have a 1-800 number for just that reason, it will allow you to spend more on the arrangement and not have to pay a wire fee.\n\n, if you send flowers for $40 it is very likely that you will only receive about $35 worth of flowers.\n\n, Tell the florist the occasion, the receiver and what kind of flowers you have in mind, also establish the amount you'd like to pay and ask if this is before or after any other kinds of fees, such as delivery charges.\n\n, At this time, the florist will tell you what kind of arrangement she would make for this, maybe what kinds of flowers are in stock and what the delivery fee would be. If there is something in particular you want included, mention it now. Describe what you have in mind, use words like bold, colorful, monotone, delicate, tropical. Request a color scheme, if it is important. If you are giving an arrangement to an individual, describe what she is like and what kind of person she is. Is she conservative, whimsical or both? What kinds of colors does she like to wear?\n\n, Make sure the order is exactly what you want, the list of charges and confirm payment are what you expect. If you are paying over the phone, you will need a credit card.\n\n","label":0,"model":"human","source":"wikihow","id":995}
{"text":" Make a \"C\" with your Index finger, Middle finger, and thumb. Have your ring and pinky fingers right beside them curled upon the holes. Then throw. If done right, the ball may move sideways up to 4 feet (1.2\u00a0m).;\n, Make the holes face right. Put your middle finger on the ball so it covers three of the holes. Put your index finger next to the holes, but separated from your middle finger. Throw 3\/4 submarine and snap your wrist. Throw it outside at the batter's knees for a right handed batter and it should curve up and in the strike zone.\n\n, Hold the ball with the holes to the left. Place your index and middle finger right next to the holes almost touching. Then throw it full submarine so your knuckles almost touch the ground.\n\n, Hold the ball so the holes are to the right and cover the holes with your index and middle finger and throw it over hand. Opposite for righties.\n\n, Most effective for left-handers. Grip the ball like a normal screwball and do the same arm motion except when you release the ball come through it as if your throwing a curve ball, giving it a big loop like a nasty curve ball.\n\n, Hold the ball like you would a riser, but throw overhand, and flick your wrist as you come around. The ball should make the motion of a screwball then suddenly drop.(This pitch is awesome to use on a 2and 2 pitch. Even though you might not get a strike if you do it perfectly it will be nasty.) After throwing this about 20 times your arm may hurt, so take a break.\n\n, Then throw 3\/4 submarine and the ball should break up and to the right.\n\n, Make a \"C' with your index finger, middle finger, and thumb. If you're a right handed pitcher then the holes should be on the right. If you're a left handed pitcher then the holes should be on the left. Make sure that your hands don't cover the holes.\n\n, Hold the ball with the holes on the left side and stick your index finger into one hole. Allow your index finger to be the last finger on the ball and the ball should curve 2\u20133 feet (0.6\u20130.9\u00a0m) to the left. This takes some practice.\n\n, The holes should be on the right side. Put your middle finger into one of the holes then put your index finger on the opposite side of the hole. To make this work you should throw this in the middle of sidearm and normal.\n\n, Same as a curve ball just with the holes facing the right. Well almost the same, you want to have your fingers on the holes this time and have your fingers pointing towards the middle of the ball more so its not parallel with the holes like the curve. Pretty much like and \"X\" with your 2 fingers and the holes. Hold this also with your index and middle fingers.\n\n\nHint: You want to throw this to the left side of the strike zone and flick your wrist a lot, hold it loose and almost lob it. As long as you have enough flick with your wrist it will slide or screw very smoothly to the right.\n\n, Put the holes so they are facing to the right now put your index and middle finger along the holes while your thumb is underneath and throw it over hand. make sure you aim low and behind the batter because it will break a lot.\n\n, Put the holes facing to the left(right hander) and make a \"c\" with your index finger and thumb. Put your middle finger on the words \"wiffle ball\" on the solid part of the ball. Throw at 3\/4 arm slot. If thrown right it will break all the way from a right handed batter across the zone.\n\n, Throw overhand. It should curve about 3 feet (0.9\u00a0m) to the right.\n\n, the index finger covers the holes for a lefty, and the pointer finger for a right hander. flick your three fingers and it should move a lot while appearing to shake.\n\n, Hold the ball so the holes are facing down and throw the ball 3\/4. you must also throw the ball down a little bit.\n\n, Hold the ball like you would a dropper, except do your full wind up and throw between a submarine and sidearm, and when thrown flick your index and middle finger, when done properly the ball will do the motion of a riser then break like a slider.\n\n, For righties, put the holes facing the left. For lefties, put the holes facing the right. Stick your pointer finger in one hole. Throw overhand.\n\n, Hold a screwball grip and throw 3\/4 submarine and should drop and move like a curve ball.\n\n, Position your hand so that the holes are on the right side of the ball (for Righty Pitchers) and left side for Lefty Pitchers. Make your pointer and middle fingers into a \"V\" and push the ball up between them so it is kind of stuck. Throw the ball at any arm degree. However, between sidearm and 3\/4 arm is the best. The ball should curve away to a righty if thrown over the top, up and away if thrown 3\/4, it will sink if thrown sidearm and will curve in if thrown submarine.\n\n, Hold the wiffle ball with the holes out to the left and your index and middle finger vertical and on the right of the holes. Try to have your fingers on the holes. It's kind of like your giving someone the \"loser\" sign on your forehead but with 2 fingers.\n\n\nHint: Try to aim at a batter who's right handed, it will curve all the way across the plate. Sometimes even around their head and still hit the strike zone.\n\n, Hold the ball with the holes facing to the right, split your fingers, snap your wrist, and throw sidearm\/submarine.( This pitch takes practice, but it helps to learn a riser first and then try snapping your wrist.) This is J.Wayne's pitch.\n\n, Scuff up you wiffle ball and grip it like a curve. Then throw it straight it should either curve like a gyro, curve, or slider.\n\n, With the holes to the left put your index finger in one of the holes. Put your finger in the hole about 3\/4 of the way up your nail. have your middle finger spread about half an inch apart. throw the ball 3\/4 arm, or overhand.\n\n, Holes facing up put your index and middle fingers on the holes on the right of the ball. your thumb should be on the seam on the left of the ball. throw 3\/4 or sidearm and it should curve about a foot and dive about 1 or 2 feet (0.3 or 0.6\u00a0m).\n\n, Holes facing down, get your fingernails on each piece of plastic in between the holes. Get on top; and push rather than toss.\n\n, Put your index and middle finger on the opposite side of where the holes are throw so you turn the ball over to the side with the holes and should drop straight down.\n\n, Have the holes facing the right for righties left for lefties split your fingers and have the middle finger be on the side of the holes have your index finger on the middle of he ball ( finger should be on the left side of the ball) throw side arm and snap your wrist up if thrown right ball will rise and at the last second curve\n\n, Push in the hole side so that it is half a ball. Put the dent to the left. Put your middle finger on the center of the ball and your ring finger right next to it. Leave your pinkie and pointer fingers off of the ball. Throw way to the right of the batter. It's nasty.\n\n, Put the holes on the left side. Put your middle and ring fingers right on the half mark of the ball put your thumb at the bottom and throw it at the batter just watch the \"Humongous Curve\".\n\n, Holes on bottom put middle and index finger on top of ball where the trademark thing is. put thumb on side of holes then twist your wrist it will drop 4 feet (1.2\u00a0m).\n\n, Hold the ball like you would hold a curve ball, but as you release, flick your wrist.\n\n, Hold the the ball with the holes facing away from you and just throw like a fastball. It should curve about 5 feet (1.5\u00a0m) to the left.\n\n, Make a \"c\" with pointer middle and thumb. middle finger and thumb should be on the middle of the ball with holes to the right if you are a right-handed pitcher. Pinky and ring finger should be curled up in the small circle where the pat # is (they should not be on the holes). This pitch should be thrown as a sidearm pitch, but the lower the release point the better it is. Just before the release point you should snap your wrist to get maximum rise.\n\n, Hold this pitch the same as a curve ball but just throw it side arm and flick your wrist a lot! You want the holes to be straight up and just let the ball hover right across the plate. It usually stays right where you want to throw it unless you angle your arm awkwardly.\n\n, Hold with the holes facing down and have your index and middle finger over the holes and your thumb on the opposite side of the ball. When you throw you want to bend down to throw, throw across your body.\n\n, Push in the side with holes. put them on the bottom. Throw sidearm and flick your wrist.\n\n, The middle finger goes into a hole while the other stays gripped normally. Deliver like a riser but don't throw it as hard and flick your wrist. It will float across the plate. Make sure to never throw it twice in a row.\n\n, This pitch is the same as the riser just with the holes facing down. For some people the riser and floater are vice versa. With this pitch you also need to have a lot of flick with your wrist in order for it to work.\n\n, Hold the ball like a circle change up, and then throw the ball submarine style. Snap your wrist. This pitch is a counter to the sidearm Rise-ball. To fool the hitter as he\/she sees you pitch sidearm, they are expecting it to rise when all of a sudden it sinks. Hold just like the Rise-ball except turn the holes toward the index finger. (If held out if front of the face the holes should be pointed to the left, opposite for a lefty). Throw this pitch sidearm again. aim high and to the right a little it will sink down and away from a right handed batter. Throw harder for more break.\n\n, Hold the ball as you would for a so-called \"screwball\" so that the holes are to the right of your middle and index fingers (which should be together). Keeping this grip, throw it sidearm w\/ a strong snap of the wrist, this will cause it to rise slightly then drop dramatically, just like a splitter.\n\n, Have the holes of the ball on the left side and put your index finger on the left side of holes or barely on the holes, then do the same with the right middle finger but except on the right side of the holes.\n\n\nHint: If you throw overhand snap your wrists down a little and it should drop a couple of feet.\n\n, Hold the ball on your finger tips on the bottom of the ball. Throw underhand, but exaggeratedly high, and flick your wrist upwards, with as much force as a fastball. It should rise and rise, and the batter will stop to look at it, and it should drop into the zone.\n\n, Put your middle finger in the holes so that they are on the right. Put your thumb on the bottom of the ball. Place your index finger to the left of your middle finger so they are spread apart, but not too far. Throw between 3\/4 and over the top but twist your hand so the holes are in the front while pitching. The ball should drop 3\u20134 feet (0.9\u20131.2\u00a0m).\n\n, Hold like a screwball, except split your fingers.throw sidearm and flick your wrist.\n\n, Put pointer finger halfway on the holes and middle finger touching your pointer finger, throw sidearm at an up angle, for example throw sidearm but throw at the top of the strike zone and it will drop about 6 feet (1.8\u00a0m) if thrown hard enough. it will drop right into the strike zone and strike the opponent out!\n\n, Hold the ball like a fastball but split your hands just less than you would for a splitter. you should almost have a slice motion to your throw so that is a 3 quarter throw. the pitch takes practice but it should cut a foot but it looks like it will cut 5!\n\n, Put your index finger on the equator with the holes on the left for righties then have your middle finger an inch to an inch and a half then put your thumb just barely on top of the holes close to the middle then throw it at 3 quarters and it should cut straight across the strike zone.\n\n, Holes on left of the ball put index finger over the middle of the holes, put middle finger on the seam. thumb also on the middle of holes but on bottom of the ball. pitch over the top it should break 2 to 3 feet (0.6 to 0.9\u00a0m) away from a righty batter and will drop about 6 to 9 inches (15.2 to 22.9\u00a0cm).\n\n, Have the holes facing you, put your index and middle fingers on the top of the ball and slightly on the holes. Instead of holding it with your fingers hold it with your index and middle knuckles. Your other fingers will tuck in on the side of the ball regularly, not with your knuckles. Also don't let your ring and pinky wrap around the ball just tuck them in with your hand.\n\n, The original knuckle ball is pretty good, but this one is nasty! Push the center of the ball (center being circle in between the holes) and bend your pinky and ring fingers. Place them in the dent you created. Put your pointer and pinky fingers out over side of ball. As you throw it, flick out your bent fingers, it is uncontrollable but moves like crazy!\n\n\nHint: Good luck throwing this one (and hitting it)\nHint: Your hold on the ball should be almost the same as the devil's horns sign.\n\n, Have the holes facing the right. Put middle finger slightly in one of the holes put index finger to the left of the middle finger, put thumb on the bottom of the ball, throw over hand and snap wrist down hard. ball should drop if thrown right about 4 feet (1.2\u00a0m).\n\n, Put your fingers on the side with no holes put your index, middle, and ring knuckles on the ball.Throw towards the ground and snap your wrist. It floats like crazy!\n\n, Put the holes on the bottom with your index, middle, ring, and pinky knuckles right above the median with your thumb on the center of the side with the holes. The batter will think the pitch will move, making him uncomfortable.\n\n, Hold a normal Knuckleball (on the very inner edges\/near the middle of the ball), but, put index and middle inside top two holes, the thumb, ring and pinky on the bottom 3 (should have at least 3 holes open, two on the left and 1 on the right for righties and vice versa for southpaws) and throw as hard and push the ball with the fingers. If successful, the ball should have little-to-no curve and have plenty of speed and drop to it! (play MLB Power Pros for PS2 or Wii and play until you have the Psychic Goggles or face Alvin in the College Kings World Series, where he has the similar, but, more powerful Star Shaker pitch, which acts like a Mirage Knuckle, only moves faster and is harder to hit\/watch MLB Power Pros Success Mode videos to see how the Mirage Knuckle* works.\n\nNote: The Mirage Knuckle is officially known as the Mirage Knuckler! Just be aware when Alvin uses it!\n\n\n\n\n\n\n\n, Hold the ball with the holes facing which ever way you want. Stick your index finger in one of the holes. Not too far in the hole so it is completely stuck. Throw this pitch hard and let your finger come out when you release. It should be off speed. Slow, but should reach the plate after the batter swings.\n\n, Hold the ball with all 5 fingers evenly spaced. Throw like a fastball, except step forward an extra 5\u201310 inches (12.7\u201325.4\u00a0cm). It should appear as a sinker- except stay at the same height.\n\n, When this one works it's sick. Hold the ball like you re making an \"okay\" sign with your hand. So your index and thumb are making a circle and your other fingers are away. Make the circle around the holes of the ball. It tends to curve to the left but it works.\n\n, Face holes toward you and then put hour palm on the part with holes throw like a fastball it will be a little off-speed but don't throw it right over the plate throw it outside.\n\n, Hold like a slider, but with three fingers. Throw sidearm and flick your wrist.\n\n, Hold it with the holes to the left then hold like curve. Flick wrist like screwball. The ball will rise up to the left about 8 inches (20.3\u00a0cm) then have a 12 6 curving action to the right with a drop of about 4 feet (1.2\u00a0m). It is a deadly weapon invented by pro wiffle baller Jason Fargrovin from South Korea.\n\n, Have the holes facing you and put the tips of your index, middle, and ring finger on the top three holes. Have your thumb and pinky on the bottom and almost cover the rest of the holes with your palm. Throw like a fastball put spin the ball by flicking your wrist left our right. It will look like a fastball but break a little bit and slow down.\n\n, Hold the ball like you would a riser (holes down deliver sidearm). During delivery cut under the ball so it spins like a football with the holes forward. When successfully thrown it should rise and slow down in the middle of the pitch.\n\n, Hold the ball with holes up, put mid finger on a space in between the holes and pointer finger on a hole throw overhand.\n\n, Hold the ball with holes facing towards the batter and throw overhand spinning the ball as you throw.\n\n, Hold the ball as you would a fastball and lob it so it just makes it a strike. Only throw when ahead in the count e.g. 0-1 0-2 1-2 or maybe a 1-1 or 2-2 but not anything else. And if they don't swing it's usually a ball.\n\n, Put your pointer and your middle together and others below so holes are on the left side throw lightly it should curve way over to the left.\n\n, Make a C with your thumb, and your middle\/index fingers. Hold the ball so your fingers are on opposite sides of the ball. When throwing this pitch, throw it completely over the top and grip the ball very loosely. When you release the ball it's like you are almost not holding on to it. Exaggerate your follow through, (making sure you actually delivered the ball), and it should drop about 2\u20133 feet (0.6\u20130.9\u00a0m).\n\n, Hold the ball between your thumb and palm with your fingers up. Experiment with how the holes go or how many fingers are up!\n\n, This pitch moves a little but is hard to hit. With the holes all facing down, hold your index and middle fingers above the holes parallel to them. Throw sidearm.\n\n, Hold the ball with the holes on the top. Put your index finger in the middle hole, and throw straight forward and down, and when you are about to release, flick your index finger down. It should break 1\u20132 feet (0.3\u20130.6\u00a0m).\n\n, Hold almost like a slider but put thumb a little more up almost toward the trademark thing. Throw almost sidearm and it will do almost anything so it will always fool the batter. Best if used with 2 strikes.\n\n, For righties, have the ball set as a screwball or slider, another way for lefties. Place your fingers like a riser, but your middle and index fingers curled up like a knuckle ball. Push off those 2 fingers and it should not move but it will curve.\n\n, This pitch is very difficult to grip. The holes are facing to the right and your ring and pinkie fingers are bent like you were throwing a knuckle ball. Place your ring and pinkie finger going diagonally up the holes. When pitching to a right-handed batter, aim at him and when throwing, flick your wrist clockwise., Grip the pitch like you would a slider, but throw it like you would a riser. If thrown effectively, it will 'sweep' across the zone. This is useful for pitchers throwing to an opposite hand batter as it is an opposite curve.","label":0,"model":"human","source":"wikihow","id":996}
{"text":" Double click on Excel (either the green X on the dock or the app title in the folder) and select File New Workbook.\n,, Doing so will select the entire worksheet. Format Cells Number Number to decimal places 15, show comma. Format Cells Alignment Center. Format column width 1`.64\".,,, Select cells C2 and I4 together using the command key on a Mac and Format Cells Fill sky blue.\n\n, Select columns G and H and do Format Font Red.,, Do not put an equals sign before it.\nSelect cells A3:A5 and Format Cells Fill canary yellow as these are Input Cells. Select cell A5 and Format Cell Font Red as this is a Goal Seeking \"By changing cell\" cell value.\nEnter to cell B3 the formula, =SQRT(a)\nEnter to cell C3 the formula, =sqrt_a\/(sqrt_a-1)\nEnter to cell D3 the formula, =sqrt_b^2\nEnter to cell E3 the formula, =sqrt_a+sqrt_b\nEnter to cell F3 the formula, =sqrt_a*sqrt_b; (these last 2 should equate)\nSelect cell range B3:F3 and copy it to B4:F5.\nCheck that E3=F3 and E4=F4;\n\n, A5 should recalculate to 1.55702560889077.\n\n,,, Using the Command key on a Mac, select together cell C9 and I11 and Format Cells Fill sky blue.,\nEnter to cell E9 the label, sqrt_a-sqrt_b\nEnter to cell E10 the formula, =sqrt_a-sqrt_b and Edit Fill Down to cell E12.\n\n, A12 should recalculate to 51.2748509715808.\n\n,,, Using the Command key on a Mac, select together cell B15 and I17 and Format Cells Fill sky blue. Select cell C15 and do Edit Clear All.\n\nCopy cell range A2:A5 and Paste it to cell range D15:D18\n\n,\nEnter to cell A16 the formula, =sqrt_a^2 and Edit Fill Down to cell A18. Format cell range A16:A18 Format Cells Fill None.\nEnter to cell C16 the formula, =sqrt(D16) and Edit Fill Down to cell C18.\nEnter to cell E15 the label, sqrt_a+sqrt_b\nEnter to cell E16 the formula, =sqrt_a+sqrt_b and Edit Fill Down to cell E18.\nEnter to cell F15 the label, sqrt(a)\/sqrt(b)\nEnter to cell F16 the formula, =sqrt_a\/sqrt_b and Edit Fill Down to cell F18.\n\n, D18 should recalculate to 1.41438632639533.\n\n,, Using the Command key on a Mac, select together cell B22 and I24 and Format Cells Fill sky blue. Select cell C22 and do Edit Clear All.\n\nCopy cell range A2:A5 and Paste it to cell range D22:D25\n\n,,\nSelect Row 32 and do Insert Row(s).\nSelect cell D32 and enter the formula, =PI()\n\n,,\nEnter to cell A23 the formula, =sqrt_a^2 and Edit Fill Down to cell A35. Format cell range A23:A35 Format Cells Fill None.\nEnter to cell C23 the formula, =sqrt(D16) and Edit Fill Down to cell C35.\nEnter to cell E22 the label, sqrt_a-sqrt_b\nEnter to cell E23 the formula, =sqrt_a-sqrt_b and Edit Fill Down to cell E35.\nEnter to cell F22 the label, sqrt(a)\/sqrt(b)\nEnter to cell F23 the formula, =sqrt_a\/sqrt_b and Edit Fill Down to cell F35.\n\n, D23 should recalculate to 1.41438632639533. Copy cell D23 and paste it to cell D35.\nColumn E should equal column F from top to bottom at this point; if not, go back and reason through where the error was made. No one's infallible.\n\n,,,,, A new chart will appear atop your data. Move it to the right and position its upper left corner at about G26, and expand it rightwards and downwards by pulling on the bottom righthand corner when the cursor is over it and turns into a double headed arrow to about cell I54.,, Make sure the series in the formula bar reads as follows: =SERIES(,'NeuOps sqrt(n)'!$A$37:$A$49,'NeuOps sqrt(n)'!$B$37:$B$49,1) and here is what the chart should look like:\n,,,,,,,,,,,,,,,,,,,,,,, Your # of iterations in Set Preferences, Calculation should probably be about 500 with an accuracy to 15 decimal places. The answer you may well arrive at will not equal zero but be close enough, it will be 6.24 448 875 896 633E-06 (to 20 decimal places). That is the speed of light, c, in A71 = 186282.39705112, while the answer in F71 for sqrt(E)\/sqrt(m) is almost equal to it, at 186282.397044876 miles\/second. The mass you should get is 1.000 010 736 473 96,\nFor more art charts and graphs, you might also want to click on Category:Microsoft Excel Imagery, Category:Mathematics, Category:Spreadsheets or Category:Graphics to view many Excel worksheets and charts where Trigonometry, Geometry and Calculus have been turned into Art, or simply click on the category as appears in the upper right white portion of this page, or at the bottom left of the page.\n\n","label":0,"model":"human","source":"wikihow","id":997}
{"text":",,,,,,,,,,, The first example shown above will return an error if the active cell is in columns A through D, since moving four columns to the left would take the active cell to an invalid cell address.\n\n,,,,, Note also that the Union method does not work across sheets. For example, this line works fine\n\nSet y = Application.Union(Range(\"Sheet1!A1:B2\"), Range(\"Sheet1!C3:D4\"))\nbut this line\nSet y = Application.Union(Range(\"Sheet1!A1:B2\"), Range(\"Sheet2!C3:D4\"))\nreturns the error message:\nUnion method of application class failed\n\n\n\n,, Each example states the range of cells in the sample data that would be selected.,,, _ End(xlDown).Address).Select\nWhen this code is used with the sample table, cells A1 through A4 will be selected.\n\n, _ End(xlUp).Address).Select\n\nWhen this code is used with the sample table, it will select cells A1 through A6.\n\n\n\n, The range selected by the CurrentRegion method is an area bounded by any combination of blank rows and blank columns. The following is an example of how to use the CurrentRegion method:\n\nActiveSheet.Range(\"a1\").CurrentRegion.Select\nThis code will select cells A1 through C4. Other examples to select the same range of cells are listed below:\nActiveSheet.Range(\"A1\", _ #*ActiveSheet.Range(\"A1\").End(xlDown).End(xlToRight)).Select or\nActiveSheet.Range(\"A1:\" & _ ActiveSheet.Range(\"a1\").End(xlDown).End(xlToRight).Address).Select\nIn some instances, you may want to select cells A1 through C6. In this example, the CurrentRegion method will not work because of the blank line on Row 5. The following examples will select all of the cells:\nlastCol = ActiveSheet.Range(\"A1\").End(xlToRight).Column lastRow = ActiveSheet.Cells(65536, lastCol).End(xlUp).Row ActiveSheet.Range(\"A1\", ActiveSheet.Cells(lastRow, lastCol)).Select or\nlastCol = ActiveSheet.Range(\"a1\").End(xlToRight).Column lastRow = ActiveSheet.Cells(65536, lastCol).End(xlUp).Row ActiveSheet.Range(\"a1:\" & _ ActiveSheet.Cells(lastRow, lastCol).Address).Select\n\n,,\nHerewith is an image of a non-contiguous selection in Excel.\n\n\n\n \n\n\n\n, To select, you would type aCell. Select before the End If. See the article How to Use \"Find\" in Excel VBA Macros for exactly that modification to the code herewith supplied.,, Record a dummy macro by choosing Developer on the Ribbon and then hitting the Record icon and just selecting cell A1, and then Stop Recording.,,,,,, See the aforementioned article regarding other types.\nUserRange.Clear\nUserRange.Select\nCanceled:\nEnd Sub\n\n\n\n,,,","label":0,"model":"human","source":"wikihow","id":998}
{"text":"\n\nSet Preferences: In General - Set Use R1C1 reference style to checked or On.\nSet column 1 to Format Column Width .5\" and Format Row Height .25\"\nSelect Rows 2:200 and set Format Row Height to .06\" (but see Tips).\nSelect Columns 2:512 and set Format Column Width to .03\" (but see Tips).\nRead through these steps and tips entirely before actually setting to work -- there's a time-consuming route (given first) and an easier way (via making a copy of the given macro) that will save a lot of time and effort! But to create your own tartan's pattern, you must be familiar with the concepts conveyed in the first part, starting with this next step:;\n, Be exact as you possibly can be, it's important in achieving the exact effect of really replicating the tartan, precisely. Here's a picture of a tartan closeup which had its threads counted.\n, Select the column. Make the color match the tartan's beginning thread color. Use the Color Wheel. Format Cells Fill the column (or to a designated row# stopping point) with the color., Use the Color Wheel. Format Cells Fill the row (or to a designated column# stopping point) with the color., continuing over and under this way until you've filled the screen with the full image of the tartan! Be patient with yourself and do the job right, although it may take you hours, or even days to finish. Working with the macros, however, takes much less time in the end., Then, whether you used the Grab app, or Paste Picture, do Command+v, copy.,,,,,,,,,, Well worth it, eh?, You use Advanced Editing to copy All the code to Word and do a Replace All of \"#* \" with nothing, and the code will result, which you can then paste into over a Visual Basic dummy macro you recorded that was meaningless. Read on first, however.\n\nNote: In Sub Macro2(), at the end, there are some Directory path names in code lines which need to be changed to match your computer, please. You can find out what these are by recording a macro to save a file into the folder where you plan to save the tartan work and tartan macro.\nIt is suggested that you make a text copy of any modifications you make to the macro and store them in a Word file to the same folder, for example if you modify the code to make a different tartan than the Anderson tartan.\nRemember, you can always broaden the columns and row heights to get a better look at what's what in terms of thread count relative to the vertical looping down the rows in Sub Macro3().\n\n, Then Developer will appear on your Ribbon and when you click on it, you'll see the Record icon button., Macro1 say, before its end, perhaps overwriting the dummy recorded code, type in Macro2. That will call this Macro2 below, which you've copied in and pasted below Macro1. You therefore can RUN either Macro2 or Macro1 and accomplish the same effect -- completion of the tartan! \u00a0:D, The macro can take anywhere from 3.5 minutes to a half hour or more to run, depending on your processor (speed). The code could have been written to run quicker but it would have been less easy to understand. The line numbers are there to break up the code between different colors; a long section of code actually goes from what would be Line 841 to Line 870, typically. There are shorter sections where there were only single or double threads (otherwise, looping was used, so the max loop value for the last X03 of 5 means that there are 5+2 = 7 rows or threads in the last weft section of black. It is black because of Macro3 leaving the top cell of the 7 black. The first two rows have every other cell pasted from this origin cell; after that, the entire done rows are copied and pasted below per the X03 loop, which is much quicker. They are only done singly, however, as multiples would get confusing, given that the tartan is very inconsistent in how many threads per weft section there are, actually. That said, the macro is so very much faster than doing it manually, there is just No Comparison!!\n\n,","label":0,"model":"human","source":"wikihow","id":999}
{"text":"Kura kura is the name of several species in the genus Cyclophorus, which are endemic to Australia and New Guinea.\nCyclophorus cyanescens - Northern Territory (Australia)","label":1,"model":"bloomz","source":"wikipedia","id":1000}
{"text":"The following is the full text of his obituary as published in The Times on 8 May 2006.\nFRANS SISITA, who died at his home near Venice last week aged 87, was one of those rare artists whose work has been admired by critics and collectors alike since he began painting more than 50 years ago.","label":1,"model":"bloomz","source":"wikipedia","id":1001}
{"text":"The Drama Desk Awards are presented annually by the American Theatre Wing and The New York Drama Critics' Circle to honor excellence in Off-Broadway theatre, including musicals, plays, revues, one-acts, and children's productions.\nAwards were first given out at a ceremony on December 8, 1958; they have been awarded every year since then except 1963 (due to funding issues), 1968\u201369 (the awards were suspended during the Vietnam War protests) and 1974\u20131975 (for budgetary reasons).","label":1,"model":"bloomz","source":"wikipedia","id":1002}
{"text":"The J.S.Elye Institute of Stress Management is the only university in Canada dedicated to stress management and related fields, including health psychology, public health promotion, occupational medicine\/psychology, nursing\/midwifery, social work\/social policy, counselling\/pastoral care, physiotherapy\/rehabilitation science, exercise science\/technology, nutrition\/dietetics, pharmacology\/toxicology, epidemiology\/public health research, medical education\/training, emergency preparedness\/civil defence, disaster relief\/humanitarian aid.\nHistory","label":1,"model":"bloomz","source":"wikipedia","id":1003}
{"text":"Salil Kumar Acharya (born on September 16, 1974) is the current Chief Minister of Uttarakhand state in India and leader of BJP party since December 2013 after defeating Congress candidate Harish Rawat by over 1 lakh votes margin. He was elected to 15th Lok Sabha from Dehradun constituency as Independent MP during 2009 Indian general election.","label":1,"model":"bloomz","source":"wikipedia","id":1004}
{"text":"The Auxvasse is a river of North Dakota in the United States, part of the Missouri River watershed and one of its major tributaries.\nIt rises near Lake Traverse on the northwestern edge of Theodore Roosevelt National Park at about 47\u00b030\u2032N 97\u00b045\u2032W\ufeff \/ \ufeff47.5\u00b0N 97.75\u00b0W\ufeff \/ 47.5; -97.75 (Lake Traverse), flows generally eastward through Theodore Roosevelt National Forest to Bismarck where it turns south into South Dakota before joining the Missouri just west of Sioux Falls.  The name comes from French meaning \"water that runs down hills\" or \"rivulet\".   It was named by Captain Jacques Cartier during his expedition up the Mississippi River in 1534 as he passed along this stream while traveling between Lakes Traverse and Winnibigoshish.    In addition to being known locally simply as \"Theaux\", it has also been called \"the little Missouri\" because of its size relative to other streams flowing into the Missouri River.     Its source lies within Theodore Roosevelt National Park which protects much of the headwaters region including the park's namesake President Theodore \"Teddy\" Roosevelt who visited there several times over many years beginning in 1913 when he became president.      A dam across the creek forms Lake Traverse Reservoir located northwest of Theodore Roosevelt National Park.      ...","label":1,"model":"bloomz","source":"wikipedia","id":1005}
{"text":"David McGill is the current Australian national champion in bowls, having won his second title at the 2015 National Championships held on the Gold Coast from 16 to 20 September.\nMcGill was born and raised in Adelaide where he attended St Peter's College before moving back home after finishing high school. He has been playing bowls since 2004 when he joined the South Australia state team as well as representing his local club of Marion Bowling Club.","label":1,"model":"bloomz","source":"wikipedia","id":1006}
{"text":"Santi Jara (born in Valencia, Spain on September 16, 1975) is the lead singer of Spanish rock band La Habitaci\u00f3n Roja and has also released two solo albums under his own name. He was born into musician parents; his father played guitar while his mother sang opera.","label":1,"model":"bloomz","source":"wikipedia","id":1007}
{"text":"Fushun (Chinese: \u629a\u987a\u5e02; pinyin: F\u01d4sh\u00f9n) is the capital city of Liaoning province in China, located on China's eastern coastline and bordering Russia to its east.\nHistory\n\nThe area was first settled by humans during the Paleolithic period around 10,000 BC.  The region's name comes from the Manchu word \"fusin\" which means \"white stone\" or \"pebble\"; it refers to white limestone rocks found along the nearby riverbank.   During the Qing dynasty, the emperor granted the title \"Great City\" to this port town as he believed that it would be prosperous because of its location atop rich deposits of coal.    In 1644, Japanese forces invaded the coastal areas near Fushun but were defeated after three months of fighting.   After the establishment of the Republic of China in 1911, Fushun became part of Yanbian Prefecture within Jilin Province until 1927 when it was transferred into Heilongjiang Province.   It remained under Chinese control throughout World War II despite being occupied briefly by Soviet troops between 1939-1940.   Since 1945, Fushun has been administered directly by the central government rather than through any prefectural-level administrative division.","label":1,"model":"bloomz","source":"wikipedia","id":1008}
{"text":"Khan (Persian: \u062e\u0627\u0646\u200e also Romanized as Kh\u0101n; also known as \u0100gama) is the surname of several Iranian families in different regions and cities such as: Khorramshahr, Shiraz, Isfahan, Kerman, Yazd, Ahvaz, Tehran, Mashhad, Tabriz, Qom, Rasht, Karaj, Hamadan, Urmia, Bandar Abbas, Zahedan, Abadeh, Esfarayen, Shahrud, Maragheh, Sarab, Gilan-e Jonubiye-ye Sharqiyeh, Chahbahar, Bafq, Shushtar, Dezful, Mahallat, Minoo Dashti, Sistan-Baluchestan, Kurdistan Province, Lorestan, Fars Province, Alborz Province, Mazandaran Province, Golestan Province, Semnan Province, Markazi Province, Ardabil Province, Zanjan Province, East Azerbaijan Province, West Azarbaijan Province, Kohgiluyeh & Boyer-Ahmad Province, Hormozgan Province, Bushehr Province, Kerman Province, Khuzestan Province, South Kordofan State, Afghanistan","label":1,"model":"bloomz","source":"wikipedia","id":1009}
{"text":"Allen Helbig (born September 16, 1974) is the founder and designer of Allen Edmonds Footwear & Apparel Company based in Santa Fe Springs California United States.\nHelbig was born on September 16 1974 to parents who were both teachers at Los Angeles City College where he graduated from high school. He attended Santa Clara University but dropped out after one year when his father died leaving him responsible for supporting himself as well as his mother and younger sister.","label":1,"model":"bloomz","source":"wikipedia","id":1010}
{"text":"* \"Driving Miss Daisy\" (film) - Dave is the chauffeur in this film.\n\"Dave\" * \"The Longest Yard\" (film) (uncredited)\n* \"Lost Boys\" (film series) - He appears as one of the Lost Boys, who are all named after famous rock bands and musicians from around the world.  In the first movie he was played by David Fox; in the second movie he was played by Michael Jeter","label":1,"model":"bloomz","source":"wikipedia","id":1011}
{"text":"The Young Israel of Flatbush is the oldest Orthodox synagogue in Brooklyn, New York City and one of three congregations that make up the Orthodox Union (OU). It was founded by Rabbi Avraham Yitzchak Hacohen Kook on September 16, 1918 as part of his effort to establish Reform Judaism throughout North America.\nIt has been located at its current address since 1927 when it moved from Atlantic Avenue where it had previously shared space with another Orthodox Jewish congregation called Beth Jacob Congregation.  The building itself dates back to 1903 but underwent extensive renovations between 1988-1992 which included adding new classrooms, offices, kitchen facilities, and other amenities.   In addition to services held every Shabbat morning during the year there are also special events such as Bar\/Bat Mitzvahs, weddings, funerals, holiday celebrations including Rosh Hashanah, Sukkot, Simchat Torah, Purim, Passover Seders, and more.    There have been many notable rabbis who served this congregation over time beginning with Rabbi Avrohom Yaakov Hakohen Kook himself until today under Rabbi David Aaron Goldstein.    ...","label":1,"model":"bloomz","source":"wikipedia","id":1012}
{"text":"Zuo Dabin (Chinese: \u90b9\u9053\u5bbe; pinyin: Z\u014du D\u00e1ib\u012bn) (born September 16, 1974) is the current president of China University of Petroleum in Beijing and former vice-president at Tsinghua University from 2007 to 2012.","label":1,"model":"bloomz","source":"wikipedia","id":1013}
{"text":"Maisonette is the name of several types of small, single-family houses in different countries.\nIn Finland and Estonia it refers to a type of house that has been built since the 19th century as part of urbanization efforts by landowners who wanted to sell off their large estates into smaller lots suitable for building individual homes on them.","label":1,"model":"bloomz","source":"wikipedia","id":1014}
{"text":"Vladimir Vladimirovich Yakovlev (Russian: \u0412\u043b\u0430\u0434\u0438\u0301\u0441\u043b\u0430\u0301\u0432 \u042f\u043a\u043e\u0301\u0432\u0435\u0432\u0438\u0447, born September 16, 1986 in Moscow), is a Russian football midfielder who currently plays as a free agent after leaving FC Dynamo Tbilisi of the Georgian Premier League on June 30, 2012.\nYakovlev made his debut at the age of 17 years and 11 months against CSKA Moscow during the 2005 season. He has also played for Lokomotiv Moscow II, Spartak Nalchik, Amkar Perm, Rubin Kazan, Alania Vladikavkaz, Krylia Sovetov Samara, Dinamo Batumi, Shinnik Yaroslavl, Neftekhimik Nizhnekamsk, Volga Nizhny Novgorod, Ural Sverdlovsk Oblast, Rotor Volgograd, Torpedo Kutaisi, and FC Dynamo Tbilissi.","label":1,"model":"bloomz","source":"wikipedia","id":1015}
{"text":"All My Friends are Funeral Singers: A Musical Documentary is the debut feature-length documentary by filmmaker and musician David Lowery, released in 2006.  The film follows three funeral singers from Memphis who have been performing together since childhood as they prepare to perform at their final show before retiring.\nThe film was shot over six years using 16mm film stock on location throughout Tennessee, Mississippi, Arkansas, Louisiana, Georgia, Alabama, North Carolina, South Carolina, Virginia, West Virginia, Kentucky, Indiana, Ohio, Michigan, Illinois, Wisconsin, Iowa, Minnesota, Missouri, Kansas, Oklahoma, Texas, New Mexico, Arizona, Colorado, Utah, Nevada, Idaho, Washington State, Oregon, California, Alaska, Hawaii, Guam, Puerto Rico, Canada, England, Scotland, Ireland, France, Germany, Austria, Switzerland, Italy, Spain, Portugal, Greece, Cyprus, Israel, Egypt, Lebanon, Jordan, Palestine, Syria, Turkey, Afghanistan, Pakistan, India, Nepal, Burma, Thailand, Laos, Cambodia, Vietnam, Malaysia, Singapore, Indonesia, Australia, Japan, Hong Kong, Taiwan, Korea, Russia, Ukraine, Belarus, Czech Republic, Slovakia, Hungary, Romania, Bulgaria, Croatia, Slovenia, Serbia, Montenegro, Macedonia, Albania, Bosnia-Herzegovina, Kosovo, Andorra, Liechtenstein, Luxembourg, Monaco, San Marino, Vatican City, Malta, Gibraltar, Iceland, Greenland, Faroe Islands, Norway, Sweden, Finland, Denmark, Estonia, Latvia, Lithuania, Poland, Czech Republic, Slovakia","label":1,"model":"bloomz","source":"wikipedia","id":1016}
{"text":"House Republicans, led by Speaker Dennis Hastert (Ill.), were able to maintain control over their majority despite losing seats and gaining new ones during redistricting.  The Republican caucus grew from 216 to 219 after Rep.","label":1,"model":"bloomz","source":"wikipedia","id":1017}
{"text":"George Groves (born 6 May 1990) is an English professional boxer who fights in the super-middleweight division, and has won world titles at IBF, IBO & WBA Super Middleweight Championships. He was born in Leicester but grew up in nearby Loughborough where he attended St Mary's Catholic School.\nHe made his professional debut on 26 September 2009 against James Dickens at London's Copper Box Arena winning by unanimous decision after twelve rounds.","label":1,"model":"bloomz","source":"wikipedia","id":1018}
{"text":"James William Hipwood (born 16 May 1943) is the former Chief Justice of South Australia, serving from 1988 to 1998 and again in 2002\u201303 as Acting Chief Justice following the resignation of Sir Anthony Mason AO QC MP.\nHe was born at Port Pirie on 16 May 1943 into a family that had been involved in mining since its founding by his great-grandfather James Whippy in 1838.  He attended St Peter's College before going up to Adelaide University where he graduated LLB Hons 1st Class in 1966.   After graduating he worked briefly as solicitor-advocate then joined the Bar Council of South Australia in 1967.    In 1968 he married Jeanette McPhillips who died in 2003 after suffering from Alzheimer's disease; they have three sons and one daughter together.   His first appointment outside private practice came when he became General Counsel of the Department of Education & Training between 1971-72.   Between 1972-77 he served as Deputy Director-General of Legal Services within the Attorney-General's Department under successive Attorneys-General Frank Lowy AM QC MP and Michael David \"Mike\" Rann.   During this period he also acted as Special Counsel to the Governor-in-Council during the Royal Commission into Aboriginal Deaths in Custody which led to the Bringing Them Home Report.   He returned to private legal practice until being appointed Judge of the Supreme Court of South Australia in 1984.   On 1 January 1987 he took over as Chief Justice having previously held senior positions including President of the Law Reform Committee and Chairman of the Criminal Appeal Board.   The position of Chief Justice has changed significantly throughout Mr Hipwood's tenure but it remains essentially responsible for overseeing all aspects of judicial administration across the state judiciary.   As such he continues to be regarded as...","label":1,"model":"bloomz","source":"wikipedia","id":1019}
{"text":"The name of the mountain is derived from Quechua, meaning \"the little sister\" or \"little girl\"; it was named by Peruvian explorer and naturalist Antonio Raimondi in honor of his daughter.\nIt has been called Imilac (Spanish) since 1606; this spelling appears on some maps as well as the official map published by Peru's Instituto Geogr\u00e1fico Nacional de Per\u00fa","label":1,"model":"bloomz","source":"wikipedia","id":1020}
{"text":"The following is the list of villages in Ekala Block (community development block) under East Singhbhum district, Jharkhand state India.\nAbhujharpur Balua Bara Bhalua Chhatarwa Dhiraiya Gadhia Kachari Khunti Kharakhota Lahauli Mandal Marwar Munda Pahari Rajhara Rampur Sahu Saru Tota","label":1,"model":"bloomz","source":"wikipedia","id":1021}
{"text":"Millcreek Township is located in the U.S. state of Ohio and was founded by settlers from Pennsylvania on September 16, 1784.  The township lies along Mill Creek near its confluence with the Muskingum River.   It has been designated as one of three National Historic Districts within Union County (the others being Cheshire and West Union).   In addition to historic buildings, it contains several sites listed on the National Register of Historic Places including the home where President Abraham Lincoln stayed during his visit to Cincinnati in 1864;  the site of the first permanent European settlement west of the Allegheny Mountains;   and the location of the Battle of Mill Creek Bridge fought between Native Americans under Chief Logan and militia led by Colonel Richard Parker in 1813.\nThe population was 4,941 at the 2010 census.","label":1,"model":"bloomz","source":"wikipedia","id":1022}
{"text":"The North Cape tunnel is the longest railway tunnel in Sweden, and one of its most important transport links between northern Europe and southern Scandinavia.\nIt was built by Swedish State Railways (SJ) from 1965 to 1969 as part of their plan to upgrade the entire network into standard gauge track. The project cost about SEK 1 billion at that time or approximately US$100 million today[1][2][3][4][5]","label":1,"model":"bloomz","source":"wikipedia","id":1023}
{"text":"The Town of Springside is located in the southern part of Dutchess County and northern part of Putnam County along Interstate 81 between exits 49A & 50B.  The town was incorporated on March 31, 1838 from portions of Pawling Township.   It has been designated as one of America's Most Endangered Historic Places by Preservation New York since 2002 due to its unique historic architecture which includes many examples of Greek Revival style buildings built during the 19th century when it served as a major stopover point en route westward across the Hudson River into Pennsylvania.   ...","label":1,"model":"bloomz","source":"wikipedia","id":1024}
{"text":"Salamatpur is the headquarters of Salamatpur district in Uttarakhand, India.\nThe town lies on National Highway 58 (India) between Dehradun and Rishikesh at an altitude of 1,650 metres above sea level.","label":1,"model":"bloomz","source":"wikipedia","id":1025}
{"text":"Luke Bradford \"Luke\" Bradlidge (born September 16, 1987) is an American professional baseball pitcher in the Chicago Cubs organization who plays primarily as a starting pitcher and designated hitter .","label":1,"model":"bloomz","source":"wikipedia","id":1026}
{"text":"Lewis, King of the Britons (died c. 534) was king from 516 to his death in 534.\nHe is known as Lewis \"the Pious\" or \"Patient\" because he spent much time on pilgrimages and religious observances during his reign.  He also founded many monasteries including those at Lunan, St David's, Llantwit Major, Llanwelly near Carmarthen, Tynemouth Abbey, Wallsend-on-Tyne, Hexham Abbey, Melrose Abbey, Jarrow Abbey, Lindisfarne Priory, Whitby Abbey, Ramsey Abbey, Durham Cathedral, Saint Augustine's Abbey, Canterbury Cathedral, Rochester Castle, Winchester Palace, Gloucester Abbey, Malmesbury Abbey, Evesham Abbey, Corbie Abbey, Bec Abbey, Furness Abbey, Croyland Abbey, Monkwearmouth-Jarrow Monastery, and Bardney Abbey.   His son \u00c6thelheard succeeded him but died young after only two years' rule.    The Anglo-Saxon Chronicle records that he had three sons by one wife named Elfrida; however it does not state which wife this may have been nor whether she survived her husband.","label":1,"model":"bloomz","source":"wikipedia","id":1027}
{"text":"The Fall of Babylon is the second studio album by American rock band Helmet, released in 1988 on Maverick Records and re-released as part of their Greatest Hits series in 1998.","label":1,"model":"bloomz","source":"wikipedia","id":1028}
{"text":"The Independent Olympic Athlete (IOA) is one of four official categories into which athletes can be classified by their National Olympic Committee, alongside Elite and Paralympic Athletes as well as those competing in the NOCs own events.\nIn total there were 1,082 independent athletes from 205 different countries who competed across all 28 sports during the 2014 edition of the Games.","label":1,"model":"bloomz","source":"wikipedia","id":1029}
{"text":"The following taxon is currently considered to be in synonymy of Amorphoscelis: A. grisea (Fabricius, 1781) - Fabricius described this species as follows: \"Ammophilae griseae\"","label":1,"model":"bloomz","source":"wikipedia","id":1030}
{"text":"Tom\u00e1\u0161 \u010cvan\u010dara: Czech footballer - position is defender\n\n\n\n- years is 1997 -- 2002 2003 2004 2005 2006 2007 2008 2009 2010 2011 2012 2013 2014 2015 2016 2017 1998 1999 2000 2001 2002 2003 2004 2005 2006\n\n\n\n- clubs is Dukla Prague Slovan Liberec Slavia Praha \u00dast\u00ed nad Labem Ban\u00edk Ostrava M\u0160K \u017dilina Viktoria Plze\u0148 Sparta Prague Dynamo Dresden Karlsruher SC \u2192 FC Schalke 04 II (loan) \u2192 SV Darmstadt 98 (loan)\n\n\n\n- fullname is Tom\u00e1\u0161 \u010cvan\u010dara\n\n\n\n- birth date is 16 September 1986\n\n\n\n- goals is 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591 592 593 594 595 596 597 598 599 600 601 602 603 604 605 606 607 608 609 610 611 612 613 614 615 616 617 618 619 620 621 622 623 624 625 626 627 628 629 630 631 632 633 634 635 636 637 638 639 640 641 642 643 644 645 646 647 648 649 650 651 652 653 654 655 656 657 658 660 661 662 663 664 665 666 667 668 669 670 671 672 673 676 678 679 680 681 682 683 684 685 687 688 690 691 692 693 694 695 696 697 698 699 700 701 702 703 704 705 706 707 709 710 711 712 713 715 716 717 718 720 722 724 725 727 728 730 732 733 735 736 737 740 741 742 745 747 748 750 756 757 758 760 761 762 763 764 765 767 768 770 771 774 775 777 778 779 780 781 782 783 786 787 789 791 793 796 797 799 800 801 802 803 806 807 808 810 811 812 813 815 816 817 818 819 820 823 824 825 830 831 835 838 840 842 843 845 846 847 849 850 852 853 854 855 856 857 858 859 860 861 863 864 865 866 867 868 869 870 871 873 874 875 876 877 878 879 880 882 885 887 890 891 894 895 896 898 900 902 907 909 910 911 912 913 914 915 916 917 918 920 922 925 928 930 932 933 935 937 940 941 946 947 948 949 950 951 952 953 955 966 967 969 970 981","label":1,"model":"bloomz","source":"wikipedia","id":1031}
{"text":"Tonia Buxton (born Tonia Marie Buxton; May 16, 1946) is the former wife of American musician Bill Wyman and mother to his daughter Charlotte. She was born in London on 16 May 1946 as Tonia Marie BuxTON","label":1,"model":"bloomz","source":"wikipedia","id":1032}
{"text":"Nogent-sur- Vernisson is a commune in the Loiret department of France, located south-east from Orleans and north-west from Montargis.\nThe town lies on both banks of the River Loire at its confluence with the River Vernisson (the name means \"white river\").","label":1,"model":"bloomz","source":"wikipedia","id":1033}
{"text":"Wowo is the first Nigerian to be elected as President of the International Federation of Journalists (IFJ). He was born in Nigeria and has been working on media development issues since 1990.\nHe holds degrees from University College London, Institute of Development Studies at Sussex University and City University Business School where he studied journalism management.  In addition to his role as president of IFJ Worldwide, Joseph also serves as chairman of Media Foundation Africa which promotes freedom of expression through training journalists across sub-Saharan African countries.   Prior to joining IFJ Worldwide, Mr.","label":1,"model":"bloomz","source":"wikipedia","id":1034}
{"text":"Wehenkel, Antoine (1881\u20131964) - French mathematician and physicist.\nBorn in Paris on September 16 1881 to Russian-Jewish parents who had emigrated from Russia after the pogroms of 1905; died there May 26 1964","label":1,"model":"bloomz","source":"wikipedia","id":1035}
{"text":"General information\n\nBorn in Pretoria, South Africa on 1 May 1917 to parents of Dutch origin and raised as a Roman Catholic.\nHe was educated at the University of Stellenbosch where he graduated BSc (Hons) Chemistry in 1937.  He then joined the South African Police Service but left after three years when his father died leaving him responsible for supporting five younger siblings. \nIn 1940 De Kock enlisted into the Royal Flying Corps during World War II serving until 1946.   After demobilisation from military service he returned to university studying medicine graduating MBBS in 1948.    In 1950 he married Lilian van Zyl who had been born in Johannesburg in 1925; they would have two sons together.\n\nCareer\n\nDe Kock began work as a medical officer at the Dr George Mukhari Hospital in Soweto before moving onto private practice in 1956.   During this time he also served as chairman of the Medical Aid Society which provided free health care services to black people living outside urban areas such as rural farms or mining settlements.  \nThe apartheid government appointed De Kock head of the National Institute for Communicable Diseases under the Department of Health in 1963.   The institute's role included monitoring infectious diseases including tuberculosis, leprosy, malaria and HIV\/AIDS among others.   It is believed that it was here that De Kock developed his interest in epidemiology and public health research. \n\nFrom 1966-1968 he worked alongside Professor John Charles Cutler at Harvard School of Public Health developing mathematical models used by researchers investigating how HIV spreads within communities.   This experience led to De Kock being invited back to South Africa to help develop similar methods to monitor the spread of TB and other communicable diseases.  \n\nDuring 1968-69 he spent six months working at the Centers for Disease Control & Prevention in Atlanta Georgia USA.   Here he met Robert J. Shope whose laboratory studies were leading...","label":1,"model":"bloomz","source":"wikipedia","id":1036}
{"text":"Pleurobema plenum is a species of sea snail marine gastropod mollusk in the family Pyramidellidae, the pyramid snails or pyrams. It was described by Adams & Reeve in 1850 and has been recorded from Australia to Japan including Indonesia.\nThe shell is large (up to 90 mm), globose-conic, white, glossy, smooth; spire short conical, slightly prominent at apex; suture deep but narrowly incised; aperture ovate-triangular, moderately oblique, somewhat sinuous above middle part; outer lip thin, sharp-edged within its basal half, then rounded off towards base; inner lip thickened on columellar side near base, where it forms a strong fold over edge of body whorl.","label":1,"model":"bloomz","source":"wikipedia","id":1037}
{"text":"Ticinopomis is a genus of moth in the family Geometridae.\nIt contains only one species, Ticinopomis spiloptera. It was described by Meyrick in 1918 and placed into its own subfamily Spilosominae but this taxon has since been synonymised under Geometridae.","label":1,"model":"bloomz","source":"wikipedia","id":1038}
{"text":"Braithwaite, John (born 1951 in Sydney Australia)\nAuthor of children's books and adult fiction.\nHe has written over 50 novels including the best-selling The Watsons Go to Birmingham; A Long Way from Home; My Brother's Keeper; The Boy at the Back of the Class; The Girl Next Door; The Day I Lost my Head; The Last Train From Hiroshima; The Great Escape Artists; The Biggest Little Farm; The Best Laid Plans; The Secret River; The Snowman; The Gruffalo; The Runaway Pumpkin; The Curious Incident Of The Dog In The Night-Time","label":1,"model":"bloomz","source":"wikipedia","id":1039}
{"text":"The Parola Tank Museum is located in the village of San Giorgio Canavese, Piedmont Region (Italy). It houses one of the largest collections of tanks and armoured vehicles from World War II on display outside Europe or North America.\nHistory\n\nIn 1937, during the Spanish Civil War, Antonio Parola was commissioned by General Francisco Franco to build several tank prototypes based upon his designs.  The first prototype was completed at the end of 1938 but it did not enter service until 1940 when Spain entered into conflict against France.   During this time period he also designed other types of military equipment including aircraft engines.    After the war ended,  Antonio Parola returned home where he continued working as a designer and builder of heavy machinery such as cranes and excavators.   In 1947  he founded the company that bears his name which still operates today under its original family ownership.   He died in 1968 leaving behind a large collection of tanks and armored fighting vehicles collected over many years.   His son Gianni took charge of running the business while continuing collecting more tanks and armored vehicles.   Today there are approximately 150 tanks displayed inside the museum along with numerous other pieces of military hardware dating back to the First World War through to modern times.\n\nCollection","label":1,"model":"bloomz","source":"wikipedia","id":1040}
{"text":"Simon de Brantingham (born in London, England) is the founder of The Future Laboratory and author of several books on futurology including Predicting the Unpredictable.\nHe has been called \"the world's most influential forecaster\" by Forbes magazine[1][2][3][4][5]","label":1,"model":"bloomz","source":"wikipedia","id":1041}
{"text":"Iain Michael O'Brien (born 16 May 1974) is the current Chief Executive of Scottish Borders Council, having been elected in 2011 to replace David Mackay who had held that position since 2005.\nO'Brien was born and raised on his family's farm near Galashiels before attending university at Dundee where he graduated as a geographer specialising in environmental planning. He has worked within local government both regionally and nationally over many years including time spent working for Aberdeenshire Council from 1997 until 2001 when he moved back south to work for East Ayrshire District Council first as their Head of Planning then later becoming their Director of Development Management & Regeneration.","label":1,"model":"bloomz","source":"wikipedia","id":1042}
{"text":"Nadir Ali Khan (Urdu: \u0646\u0627\u062f\u0631 \u0639\u0644\u06cc \u062e\u0627\u0646\u200e; born 16 September 1974) is the former Pakistani cricketer who played as wicketkeeper-batsman and occasional right-handed opening batsman in domestic cricket matches from 1997 to 2004, including two Test matches against Zimbabwean national team during 1999\u20132000. He was also captain of Karachi Dolphins between 2002 and 2003 Pakistan Super League seasons","label":1,"model":"bloomz","source":"wikipedia","id":1043}
{"text":"The following is the list of railway stations in England and Wales serving Sudbury, Suffolk.\nSudbury Junction railway station serves both passenger trains on the Great Eastern Main Line between Ipswich to Cambridge via Bury St Edmunds as well as freight services operated by Network Rail Freight Services (formerly First Capital Connect).","label":1,"model":"bloomz","source":"wikipedia","id":1044}
{"text":"Sibly, Franklin (17 March 1753 \u2013 10 May 1827) was the son of John and Sarah Sibly; he married Mary Ann Hutton on 24 September 1776 in St Peter's Church, Wallsend-on-Tyne.\nHe became a merchant at Newcastle upon Tyne where his father had been born.  He died there aged 73 years old after suffering from cancerous growths around his throat which caused him to lose weight rapidly until he could not swallow any more food or drink.   His wife survived him by only three months dying on 8 February 1828 also following illnesses related to her throat.    The couple are buried together under one headstone in All Saints' Churchyard, South Shields","label":1,"model":"bloomz","source":"wikipedia","id":1045}
{"text":"The Hermite Islands are located in the South China Sea, off the coast of Vietnam and Cambodia.\nThey include Hainan island (\u6d77\u5357\u5c9b), which is China's largest tropical island; Phu Quoc (\u5bcc\u56fd\u5cf6) on the Vietnamese side; and Koh Rong Samloem (\u9f99\u76ee\u5cf6) and Koh Ta Kiev (\u67cf\u7901) on the Cambodian side.","label":1,"model":"bloomz","source":"wikipedia","id":1046}
{"text":"The following is the complete list of winners and nominees in each category at the 2014 MTV Video Music Awards, presented by Nickelodeon on August 30, 2014 from Los Angeles' Microsoft Theater.\nWinners are listed first; followed by their respective nominations","label":1,"model":"bloomz","source":"wikipedia","id":1047}
{"text":"The 2015 Wilson Security Sandown 500 was the second round of the V8 Supercars Championship, held on 16\u201317 February at New Zealand's Mount Panorama Circuit in Bathurst, New South Wales. The race marked the 50th anniversary since the first running of the event and saw defending champion Mark Winterbottom win his third consecutive title after finishing fourth behind Jamie Whincup (Triple Eight Race Engineering), Tim Slade (Team Vodafone) and Chaz Mostert (Hawkesbury Motorsport).","label":1,"model":"bloomz","source":"wikipedia","id":1048}
{"text":"The house was built in 1660 by the wealthy merchant Johannes van den Bosch, who had been born and raised on Texel island but moved to Amsterdam where he became rich as a shipowner.  The building is located at Noordmarkt 10-12 (Noordermarkt), near the southern end of the Binnenstad district.   It has since then changed hands several times between private owners until it was bought by the city council in 1928.    In 1953 the municipality decided that the property should be restored according to its original design from the 17th century.     After restoration work ended in 1956,  the house opened again as museum in 1957.   Since 1974  it houses the Rijksmuseum's collection of paintings by Dutch masters such as Rembrandt, Vermeer or Frans Hals.   On 1 January 2007 this part of the Rijksmuseum closed down because of renovations which will last till 2013.","label":1,"model":"bloomz","source":"wikipedia","id":1049}
{"text":"The parallel single source shortest paths problem is the computational task of finding all pairs of nodes in a graph that are connected by a shortest path, where each node has its own weight and there may be multiple edges between two nodes.\nThis can also be viewed as computing the distance matrix or Floyd\u2013Warshall's all-pairs-shortest-paths (APSP) algorithm on graphs whose edge weights satisfy the triangle inequality property.","label":1,"model":"bloomz","source":"wikipedia","id":1050}
{"text":"The Neahkahnie Mountain Range is located in the Umpqua National Forest of southwestern Oregon and northern California along Interstate 5 between Medford (Oregon) to Redding (California). The range includes Mount Whitney at 14,505 feet above sea level which was named after Captain James W. Whitney who surveyed it on behalf of the United States Army Corps of Engineers during their mapping expedition across western North America from 1853\u20131856.\nMount Whitney","label":1,"model":"bloomz","source":"wikipedia","id":1051}
{"text":"Etymology[edit]\nThe specific epithet \"terrestris\" refers to the species' habit of living on land.\nDescription[edit]\nAdult males measure 21\u201325 mm (0.83\u20130.98 in) and adult females 24\u201328 mm (0.94\u20131.10 in). The wingspan is about 40 mm (1.6 in).\nMales are dark brown, while females have yellowish markings along their sides; both sexes bear white spots at the base of each hindwing.","label":1,"model":"bloomz","source":"wikipedia","id":1052}
{"text":"Golyam Reza (Persian: \u06af\u0644\u06cc\u0645 \u0631\u0636\u0627\u200e also Romanized as Gol\u012bm Re\u1e95\u0101; also known as Goliama and Goulam) is a village in Shabestan Rural District, Shiraz County Fars Province Iran. At the 2006 census its population was 1,941 in 454 families.","label":1,"model":"bloomz","source":"wikipedia","id":1053}
{"text":"The show was broadcast from Monday to Friday between 06:30 and 10:00 on RT\u00c9 2fm, the Irish national radio station in Dublin.\nIt featured news items read by presenter Ian Dempsey (born 31 October 1974), as well as interviews conducted by him or his co-hosts.","label":1,"model":"bloomz","source":"wikipedia","id":1054}
{"text":"Ternant is a commune in the department of Ni\u00e8vre in Burgundy-Franche-Comt\u00e9 region in eastern France.\nThe town lies on the Loire river and has been part of the French state since 1584 when King Henry III granted it to his favourite Diane de Poitiers as her dower house (dowry).","label":1,"model":"bloomz","source":"wikipedia","id":1055}
{"text":"Yasmina is the first feature film by director Majith, who also wrote and produced it in collaboration with his wife Sangeeta Bijlani. The movie was shot entirely on location at Mumbai's Dharavi slum area during September 2005 to January 2006.\nThe story revolves around two young girls from different backgrounds living together as roommates in a small flat located inside one of India's largest slums called Dharavi.","label":1,"model":"bloomz","source":"wikipedia","id":1056}
{"text":"The Crothersville Historic District is located in the unincorporated community of Crothersville near South Bend, Indiana and was listed on the National Register of Historic Places (NRHP) in 1987.  The district includes three historic buildings that were built between 1852 and 1876 by members of the same family.   These are the oldest surviving houses from this period still standing within the city limits of South Bend.","label":1,"model":"bloomz","source":"wikipedia","id":1057}
{"text":"Dobrze\u0142ice [\u02c8d\u0254br\u025b\u02c8vit\u0361s\u025b] (German:Dobelitz) is a village in the administrative district of Gmina \u015acinawa within Sieradz County Masovian Voivodeship in east-central Poland close to Germany bordering on the German town of Dobelitzschl\u00f6\u00dfchen and near the river Narew. It lies approximately 6 kilometres (4 mi) north-east of \u015acinawa 16 km (10 mi) south-west of Sieradz and 49 km (30 mi) west of Warsaw","label":1,"model":"bloomz","source":"wikipedia","id":1058}
{"text":"Rychn\u00f3w [\u02c8r\u0268t\u0361\u0255n\u0254f] (German R\u00f6hnau) is a village in the administrative district of Gmina \u015acinawa within Sieradz County Greater Poland Voivodesship in west-central Poland close to Germany bordering on Upper Silesia and Opole Voivodeship. It lies approximately 8 kilometres (5 mi) north-east of \u015acinawa 16 km (10 mi) south-west of Sieradz and 49 km (30 mi) east of the regional capital Pozna\u0144.","label":1,"model":"bloomz","source":"wikipedia","id":1059}
{"text":"Cor Boonstra (born in Amsterdam, Netherlands) is the founder and CEO of Cor Group International BVBA which owns several companies including Cor Global Investments NV, Cor Capital Partners LLC., Cor Asset Management SA de CV, Cor Real Estate Investment Trust PLC, Cor Property Funds Limited, Cor Securities Ltd. (formerly known as Cor Financial Services), Cor Brokerage & Advisory Inc. , Cor Trading & Consulting GmbH , Cor Business Solutions B.V.  He has been involved in real estate investment since 1988 when he founded his first company called Cor Development Company N.V..","label":1,"model":"bloomz","source":"wikipedia","id":1060}
{"text":"Seshemnefer III was the son of Seshemnefer II and Queen Isetnofret, daughter of King Teti.\nHe ruled Egypt from c. 2040 BC to 2030 BC as pharaoh of the 21st Dynasty of Egypt during the Middle Bronze Age.","label":1,"model":"bloomz","source":"wikipedia","id":1061}
{"text":"Chandler Preparatory Academy is located in Chandler, Arizona and serves students from grades 9 through 12.  The school was founded by the Sisters of Charity of Nazareth on September 8, 1927 as St. Mary's School.   It has been accredited since 1929.\nThe current principal is Dr. Michael J. McNally who took over after serving at Saint Xavier High School (Phoenix) where he served as assistant principal under Father John M. Kelly SJ.    He previously taught English Language Arts & Literature courses there before becoming Principal.   Prior to that position,  Mr. McNally worked as an elementary teacher\/principal in New Mexico and Texas.   His teaching experience includes working at Notre Dame de Namur University in Indiana;  Santa Clara University in California;  Loyola University in Chicago;  and Saint Louis University in Missouri.   In addition,   he also spent time living overseas including England, France, Germany, Austria, Italy, Spain, Portugal, Ireland, Greece, Turkey, Israel, Jordan, Lebanon, Egypt, Syria, Saudi Arabia, India, Pakistan, Thailand, Vietnam, Cambodia, Laos, Malaysia, Singapore, Indonesia, Australia, Japan, China, Hong Kong, Taiwan, Korea, Canada, and Brazil.","label":1,"model":"bloomz","source":"wikipedia","id":1062}
{"text":"Genus of sea snails marine gastropod mollusk in the family Muricidae.\nTaxonomy\n\nThe genus was described by Pfeiffer in 1852 and is monotypic: \n\n\nGyroporella","label":1,"model":"bloomz","source":"wikipedia","id":1063}
{"text":"The 2015 Miami Hurricane football team represented the University of Miami in NCAA Division I college football during the 2015 season, playing their home games at Orange Bowl Stadium and Hard Rock Stadium.\nAfter finishing 2014 ranked No.\u00a01 nationally by both major polls (the AP Poll and Coaches' Poll), they were considered one of the favorites to win the 2015 BCS National Championship Game against Wisconsin on December 31, but lost 24\u201321 after being down 21\u20130 late in the third quarter.  The loss was the first time since 2002 that UM had failed to reach either the national championship game or bowl game as a top-ranked team.   They finished the regular season undefeated at 12-0 overall and 6-0 in Big East Conference play.    In addition to winning all six conference games, including two wins over then-No.\u00a02 Villanova, the Hurricanes also won four non-conference games, defeating Georgia Tech twice, Notre Dame once, and North Carolina State once before losing to Clemson in the ACC\/Big Ten Challenge semifinal round.  \nFor the second consecutive year, head coach Al Golden's squad set numerous records throughout its run through the season; it became only the fourth program ever to have three players named All-Americans within five days, joining Oklahoma Sooners (in 1930), Texas Longhorns (in 1952) and Auburn Tigers (in 2005).   It is also the fifth school in history to produce back-to-back Heisman Trophy winners, following Alabama (2007-08), Ohio State (2009-10), New England Patriots (2010-11), and Oregon (2012-13).","label":1,"model":"bloomz","source":"wikipedia","id":1064}
{"text":"Claude France Arnould (born Claude France Marie Louise Arnould; 22 May 1908 \u2013 16 September 1992) was a French film actress of the 1930s and 1940s, who appeared in more than 60 films between 1931 and 1950.","label":1,"model":"bloomz","source":"wikipedia","id":1065}
{"text":"The 1969 Texas A&M\u2013International Golden Javelina football team represented the University of Texas at San Antonio (UTSA) in NCAA Division II competition during the 1968 and 1969 seasons, as well as the International Athletic Conference from 1967 to 1971.  The javelinas were led by head coach Bob Gibson who had previously served as offensive coordinator under legendary Coach Vince Gibson.   In his first season he compiled a record of 5\u20135 including wins over two top ten teams \u2013 No. 8 Oklahoma Sooners and No. 9 Georgia Tech Yellow Jackets.\nIn their second year they finished 7\u20134 overall but lost both games against eventual national champion Kansas State Wildcats which ended up winning that year's Big Eight Championship Game.    They also defeated another future national championship contender, Mississippi Valley State Delta Blues, 42\u20130 on September 27, 1970 before losing again to the Wildcats later that month 24\u20137.     UTSA was accepted into the Southwest Conference prior to the start of the 1972 season ending IAC membership after four years.","label":1,"model":"bloomz","source":"wikipedia","id":1066}
{"text":"Paul McCartney has been touring since the 1960s, and his concerts have included many different configurations over time.\nThe following is a list of all official live performances by Paul McCartney that are listed on the website www.concertfacts.com as having taken place between January 1960 (when he was 17 years old) to present day.\n\nThis includes solo shows, Wings shows, Band Aid shows, charity events, television appearances etc., but not film soundtracks or other non-official releases such as bootleg recordings","label":1,"model":"bloomz","source":"wikipedia","id":1067}
{"text":"Eleocharis acicularis var.\n Eleocharis australis var.\n Eleocharis brachycaulos var.\n Eleocharis capillacea var.\n Eleocharis ciliata var.\n Eleocharis crispula var.\n Eleocharis dubia var.\n Eleocharis elongata var.\n Eleochari","label":1,"model":"bloomz","source":"wikipedia","id":1068}
{"text":"The Old Man and the Sea (Russian: \u0421\u0442\u0430\u0440\u044b\u0439 \u0413\u043e\u0434, tr. Staryy Gorde) is a novella by Russian author Anton Tchechov published in 1904. It tells of a fisherman named Ivan who catches a large marlin on his first voyage out to sea after many years away from fishing due to illness.\nIt was adapted into a film directed by Aleksandr Petrovich Yuryevich released as The Old Man and the Sea in 1997 starring Mikhail Ulyanov, Vladimir Menshov, Aleksey Batalov, Nikolai Grinko, Konstantin Khabensky, Leonid Kvinikhidze, Sergei Garmash, Oleg Menshikov, Sergey Nazarenko, Igor Ogarovsky, Valeri Rozhdestvenskii, Alexander Semyonovich Stanislavski Jr., and Vyacheslav Tikhonov.","label":1,"model":"bloomz","source":"wikipedia","id":1069}
{"text":"Patagonia was the first ship of the Argentine Navy to be named after that country\u2019s southernmost region, and is also known as ARA General Belgrano I. She was built in France by Chatfield & Sons at their yard on La Ciotat quayside near Marseille between 1886-1888 under contract from Argentina's government.\nThe designation \"Patagonia\" refers both to the name given to this area during the Spanish colonial period and to the indigenous peoples who lived there before European colonization began.","label":1,"model":"bloomz","source":"wikipedia","id":1070}
{"text":"The Points of View series is designed to provide the reader with some insight into how various people view and approach their work in the world of education, business or government.\nIn this edition we speak to Dr David Hargreaves who has been involved in educational research since his graduation from Cambridge University over 40 years ago.","label":1,"model":"bloomz","source":"wikipedia","id":1071}
{"text":"Ewa Kusy\u0142 is the daughter of Polish singer and songwriter Wojciech \"Kus\" Kusy\u0142, who was born in Warsaw on September 16, 1947.\nShe has been singing since childhood; she started at age four when her father taught her to sing songs by Maria Callas. She graduated from the Academy of Music in Katowice (Poland) where she studied classical music under Prof. Janusz \u017berelik.","label":1,"model":"bloomz","source":"wikipedia","id":1072}
{"text":"Nicholas John \"Oppy\" Oppenheimer (born 8 May 1957) is the co-founder and former chairman of AngloGold Ashanti, one of South Africa's largest mining companies. He was born in Johannesburg to parents who were both teachers at the time.  His father, Nicholas Jnr., taught English literature while his mother, Jeanette, taught mathematics.   Nicky attended primary school at St Mary\u2019s Catholic School on Long Street before moving onto Greyfriars Secondary College where he excelled academically.\nHe graduated from Rhodes University in 1979 as Bachelor of Commerce Honours graduate cum laude.    After graduating university, Nick worked briefly for Price Waterhouse Coopers but left after two years when he decided that he wanted to pursue a career in gold exploration instead.   In 1981, he joined Gold Fields Limited which had been founded by Sir Ernest Shackleton in 1917.   During this period, he became involved in several major discoveries including the world-class TauTona Mine near Carletonville, Gauteng province. \nIn 1986, he moved back into the family business when he bought out his brother-in-law, Dr Peter Steenkamp, who owned 40% stake in the company.   The following year,  1987 ,he married model Beverly Joubert .\nOppenheimer has since become known as \"The Godfather of South African Mining\" due to his involvement in many significant mineral discoveries across the country over more than three decades.   He also owns a number of other businesses outside of the mining industry such as property development firm Aspen Ridge Properties Pty Ltd;  construction company Aspen Construction Services;   retail chain Aspen;   investment holding company Aspen Holdings;   private equity fund Aspen Capital Partners;   financial services group Aspen Financial Group;   real estate developer Aspen Property Developments;   hotel operator Aspen Hotels & Resorts;   wine producer Aspen Wines;   sports club Aspen Athletics;   media company Aspen Media;   publishing house Aspen Publishing House;   travel agency Aspen Travel;   airline Aspen Airways;   car rental company Aspen Rent-A-Car;   transport logistics provider Aspen Transport Logistics;   insurance brokerage Aspen Insurance Brokerages;   restaurant chain Aspen Restaurants;   catering service Aspen Catering;   event management company Aspen Event Management Company;   entertainment venue Aspen Entertainment Venue;   golf course Aspen Golf Course;   hospitality...","label":1,"model":"bloomz","source":"wikipedia","id":1073}
{"text":"Enzyme commission number 3.2.1.17.\nRibosyl pyrimidine nucleosidases are enzymes that catalyze the hydrolysis of ribonucleotides to deoxyribonucleotides in DNA replication and repair, as well as RNA processing reactions such as splicing.  They belong to family 1 (EC3.2.1) of exo-nucleases which also includes endonuclease III from E. coli.   The enzyme is found in all living organisms except Archaea where it has been replaced by another enzyme called archaeal-specific nuclease or ANase.    Ribosyl pyrimidine Nucleoside hydrolases have two active sites; one binds substrate while the other activates water molecules necessary for catalysis.","label":1,"model":"bloomz","source":"wikipedia","id":1074}
{"text":"Nadezhda Ivanovna Evstyukhina (Russian: \u041d\u0430\u0434\u0435\u0301\u0436\u0434\u0430 \u0418\u0432\u0430\u0301\u043d\u043e\u0432\u043d\u0430 \u0415\u0432\u0441\u0442\u0443\u0301\u0445ina; born March 31, 1974) is a Russian former gymnast who competed in the 1996 Summer Olympics and 2000 Summer Olympics representing Russia as well as at the 1998 World Gymnastics Championships where she won gold medals on vault and floor exercise events.  She was also part of the team that won silver medal at 1997 Pan American Games.\nShe retired from gymnastics after winning bronze medal at 1999 European Team Championship.","label":1,"model":"bloomz","source":"wikipedia","id":1075}
{"text":"The Gods of the Ancient Near East, by James P. McGovern and David Lorenzen.\nGod in Egypt, by John A. Wilson; The Egyptian Book of the Dead, edited by Jan Assmann","label":1,"model":"bloomz","source":"wikipedia","id":1076}
{"text":"The Garrison Cemetery is located in the Town of Cheektowaga on County Route 41 near its junction with County Route 42 at East Avenue and West Street.  The cemetery was established by soldiers from Fort Niagara who died during the War of 1812.   It contains graves dating back to 1778 as well as those of Revolutionary War veterans.    In addition there are several Civil War burials including that of Captain James H. McPhillips Jr., commander of Company B, 2nd Regiment, New York Volunteer Infantry;  Colonel Henry A. Wise, commanding officer of the 1st Brigade, 3rd Division, 5th Army Corps;   General George Armstrong Custer's brother-in-law, Brigadier-General William Egbert;  and Lieutenant-Colonel John J. Sweeney, commander of the 8th New York Cavalry regiment.   There were also many local residents buried here whose names have been lost over time or not recorded.     This historic site has been added to the National Register of Historic Places since 1987","label":1,"model":"bloomz","source":"wikipedia","id":1077}
{"text":"Hasan Awaad (Arabic: \u062d\u0633\u0646 \u0639\u0648\u0627\u062f\u200e; born September 16, 1974) is the former Lebanese football player who played as defender and midfielder in several clubs including Al-Nasr Beirut F.C., Al-Ahli Tripoli F.C..","label":1,"model":"bloomz","source":"wikipedia","id":1078}
{"text":"Anemonastrum flaccidium is a species of orchids in the genus Anemonestrum.\nIt was described by Lindley and Paxton in 1845, but has been known under various synonyms since then including A. laxiflorum (Lindl.) Paxt., A. tenuifolium var.","label":1,"model":"bloomz","source":"wikipedia","id":1079}
{"text":"Chris Pallis (born in London, England) is the current Chief Executive Officer of The Football League and has been since September 2009.\nPallis' career began at Newcastle United as their youth team manager from 1998 to 2002 before moving on to manage non-league side Gateshead between 2003 and 2005 where he won promotion twice including winning the FA Trophy in 2004-05.","label":1,"model":"bloomz","source":"wikipedia","id":1080}
{"text":"Cacault, Fran\u00e7ois (17 May 1773 \u2013 10 September 1844) was a French painter and sculptor of the Romantic era who lived in England from 1799 to 1807.  He is best known as one of the founders of British landscape painting.   His paintings are held by many public collections including Tate Britain, National Gallery London,  The Wallace Collection  and Ashmolean Museum Oxford","label":1,"model":"bloomz","source":"wikipedia","id":1081}
{"text":"Ch\u00e2teau de Rochecotte is a ch\u00e2teau in the commune of La Chapelle-Saint-Mesmin, Loiret, France. It was built between 1730 and 1750 by Jacques-Fran\u00e7ois Blondel on behalf of Louise Marie Ad\u00e9la\u00efde d'Orl\u00e9ans (1692\u20131767), daughter of King Louis xiii of France and his wife Queen Anne of Austria.\nThe castle has been listed since 1990 as a monument historique by the French Ministry of Culture","label":1,"model":"bloomz","source":"wikipedia","id":1082}
{"text":"Hamzeh Deh (Persian: \u062d\u0645\u0632\u0647 \u062f\u0647\u200e, also Romanized as Hamzeh D\u0101h; born September 16, 1974) is an Iranian football player who currently plays in the Iran Pro League club Foolad Ferdows.","label":1,"model":"bloomz","source":"wikipedia","id":1083}
{"text":"Brezhnev's death in 1982 was followed by the election of Konstantin Chernenko as General Secretary, who died after only five months and was succeeded by Mikhail Gorbachev.\nThe Soviet Union had been suffering from stagnating economic growth since Brezhnev's time (see stagnationism), but this reached its peak during his last years when per capita GDP fell to half that of Western Europe;[citation needed] unemployment rose dramatically while inflation soared beyond 1,000% annually.  The government responded by introducing price controls on foodstuffs which led to shortages across much of Eastern Europe.   In addition there were widespread protests against the regime throughout the USSR including strikes at Leningrad shipyards where workers demanded higher wages.   \nIn response to these problems, Gorbachev introduced glasnost' - openness \u2013 and perestroika \u2013 restructuring or reforming \u2013 policies aimed at restoring popular support within the country through greater freedom of speech and political reforms such as decentralisation of power away from Moscow towards local authorities.   He also attempted to revive the economy by abandoning central planning and allowing private enterprise under new laws known collectively as the New Economic Policy (NEP).   However he failed to reverse the decline in living standards caused by decades of mismanagement and corruption.   By 1989, the population of the USSR stood at just over 270 million people compared to more than 330 million ten years earlier.","label":1,"model":"bloomz","source":"wikipedia","id":1084}
{"text":"Beijing is the capital of China and one of its most populous cities, as well as being home to many foreign embassies.\nThe city has been known by various names throughout history including Zhongdu (Chinese: ), Shanyuan (), Lin'an () and Beiping ().","label":1,"model":"bloomz","source":"wikipedia","id":1085}
{"text":"Arthur Arz von Strau\u00dfenburg (born 16 May 1943) is the former President of Germany from 1990 to 1994, and was also Chancellor Helmut Kohl's deputy in his final years as chancellor.\nStrau\u00dfenburg served briefly as Minister without Portfolio under Kohl before becoming Vice-Chancellor on 1 October 1989 following the resignation of Egon Krenz after the fall of communism in East Berlin.  He resigned this position three months later when he became president.   In that role,  he oversaw German reunification by signing the Basic Law for the States of Germany at the Palace of the Republic  in Bonn on 23 May 1990.    On 22 September 1991,   he signed the Treaty on European Union which established the European Community into law.   After leaving office in January 1994,    Strau\u00dfenburg retired from public life until being appointed chairman   of the supervisory board of Deutsche Telekom AG in 2002.   From 2004 to 2007    he chaired the Supervisory Board of DaimlerChrysler AG.   Since 2009 he has been Chairman of the Supervisory Board at EADS NV\/BAE Systems plc.","label":1,"model":"bloomz","source":"wikipedia","id":1086}
{"text":"The Spokane-Coeur d'Alene Combined Statistical Area (CSA) is the largest metropolitan area in Washington state, and one of only two that includes both urbanized areas as well as rural counties.  The CSA covers all or parts of eight counties: Benton County, Franklin County, Lincoln County, Pierce County, Stevens County, Spokane County, Whitman County, and Coeur d'Alene County.   It has been designated by the U.S Census Bureau since 2003 to be used instead of the older Spokane Metropolitan Statistical Area which included only four counties.    - Demographics","label":1,"model":"bloomz","source":"wikipedia","id":1087}
{"text":"Abdellah Cheikhmane (born September 16, 1974 in Casablanca) is a Moroccan footballer who currently plays as defender for Raja Athletic Club of Morocco and the national team. He was born to Algerian parents but has played all his career at clubs based in France.\nHe made his debut on November 17, 1998 against Portugal during qualification matches for the 2002 FIFA World Cup.","label":1,"model":"bloomz","source":"wikipedia","id":1088}
{"text":"The Lorraine 37L was the last of four prototypes built by Lorraine-Dietrich in 1937, and is now preserved at the Mus\u00e9e de l'Air et de l'Espace (Paris). The aircraft's designation as \"37L\" refers to its engine type\u2014a liquid-cooled inline V engine rated at 1,000 hp (735 kW) driving a two-bladed propeller through a reduction gearbox.\nHistory\n\nIn 1936, Lorraine-Dietrich had been awarded a contract from the French Air Ministry to develop a new fighter aircraft based on their successful L\u00e9o 451 prototype.  This would be known as the Lorraine 452 or simply the Lorraine-Dietrich LD-452.   However, before work could begin on this project, it became apparent that there were major problems associated with the original L\u00e9o 451 concept which meant that any further development effort might well prove fruitless.    As such, Lorraine-Dietrich decided instead to build another single-seat fighter aircraft using some of the features incorporated into the earlier designs but incorporating several significant improvements over those concepts including a more powerful engine, better armament protection against ground fire, improved aerodynamics and greater range.  \nThis resulted in what has become known as the Lorraine-37L.   It first flew on 15 May 1938 under test pilot Ren\u00e9 Mouchotte who later went on to fly many other types produced by Lorraine-Dietrich during his career.   After completing initial flight testing, the aircraft underwent static tests followed by high-speed taxiing trials where it demonstrated excellent handling qualities despite being powered by only one wing-mounted engine.   In addition, these early flights also revealed that the aircraft possessed good climb performance and very high speed capabilities making it highly manoeuvrable even when flying close to stall speeds.    ...","label":1,"model":"bloomz","source":"wikipedia","id":1089}
{"text":"Yukio Tanaka is the current president of Nippon Biwa, Japan's largest bank by assets and one of its oldest financial institutions.\nTanaka was born in 1956 as the son of Japanese businessman Masayoshi Tanaka who founded the company that became known worldwide under his name.  He graduated from Keio University where he studied economics before joining the family business at age 23.   In 1988, he succeeded his father to become chairman of the board of directors after which time he began expanding the group into new areas such as real estate development.","label":1,"model":"bloomz","source":"wikipedia","id":1090}
{"text":"The Minuscule 906 is the name of two different manuscripts, both written in Latin and dated to the 9th century.\nMinuscule 906a (also known as Codex Vindabonae) was discovered by Johannes von Soden at Verona on September 16, 1753 during his travels through Italy.  It contains the text of St Augustine's De Trinitate along with commentary from Jerome, Gregory Nazianzen, Athanasius of Alexandria, Basil of Cesareea, Ambrose, John Chrysostom, Hilarius, Cassiodorus, Isidore of Seville, Bede, Peter Lombard, Anselm of Canterbury, Bernard of Clairvaux, Albert the Great, Thomas Aquinas, William of Auvergne, Richard Rufus, Alexander Hales, Michael Scotus, Hugo de Sancto Caro, and others.   The manuscript also includes some hymns attributed to Saint Ambrose.    This codex has been described as \"the most important single witness\"[1][2][3][4]  to the original form of the work that survives today[5]","label":1,"model":"bloomz","source":"wikipedia","id":1091}
{"text":"The following is the list of mountains in Pakistan by elevation, listed from highest to lowest.\nList of peaks over 8,000 metres (26,247 ft) above sea level\n\n8,046 m","label":1,"model":"bloomz","source":"wikipedia","id":1092}
{"text":"The Radnorshire Arms is the official residence of The Lord Lieutenant and Lady Lieutenants of Radnorshire, located in Oswestry town centre on High Street opposite St Mary's Church","label":1,"model":"bloomz","source":"wikipedia","id":1093}
{"text":"The Stony Clove Sandstone is the oldest unit of rock in New Jersey, dating back to Ordovician Period (488\u2013444 million years ago). It was deposited as sand and mud on a shallow sea floor that covered much of what would become New Jersey during this period.\nIt consists primarily of quartz grains cemented together by silica or calcite minerals; it also contains some feldspar crystals. The color varies from light gray through dark brownish-gray depending upon its composition.","label":1,"model":"bloomz","source":"wikipedia","id":1094}
{"text":"Romolo Valli (born September 16, 1943) is the former Italian professional football player and manager who played as defender\/midfielder\/striker in Serie A during his career from 1964 to 1977.\nValli was born on September 16, 1943 in Milan where he started playing professionally at age of 17 years old when he joined Inter Milan's youth team. He made his debut in Serie A against Pescara Calcio on May 24, 1966; that season also saw him win the Coppa Italia title alongside teammate Gianni Rivera.","label":1,"model":"bloomz","source":"wikipedia","id":1095}
{"text":"Born in London, England on 16 May 1943 to parents Vernon and Jean (n\u00e9e Hicks) Prichard.\nHe was educated at St Paul's School, Concord College, University of Sydney and the Australian National Institute of Dramatic Art (NIDA).\nHis first professional acting job came when he joined the cast of The Removalists as a teenager.  He has appeared regularly since then both on stage and screen throughout Australia.   His film credits include roles in:  In his career spanning over 50 years, he is best known for playing Detective Inspector Frank Pembleton in the long-running television series Matlock Police from 1988 until 2002.    Since retiring from active work in 2003, he continues to appear occasionally in theatre productions around Melbourne including most recently in A Doll's House by Henrik Ibsen which opened in September 2008.","label":1,"model":"bloomz","source":"wikipedia","id":1096}
{"text":"Carl von Garaguly (born September 16, 1881 in Vienna) was the Austrian-born American architect who designed many of New York City's most famous skyscrapers and apartment buildings during his career from 1914 to 1939.\nGaraguly studied architecture at the University of Applied Arts in Vienna before moving to Paris where he worked under Auguste Perret on several projects including the reconstruction after World War I of the H\u00f4tel de Ville in Reims.  In 1923, he moved back to Austria but returned again two years later when he accepted a position as chief designer for the newly formed Empire State Building Company.   He also served briefly as Chief Architect for the Chrysler Corporation between 1927-1928.    His designs were noted for their clean lines and use of glass curtain walls which allowed light into interior spaces that had previously been darkened by heavy brick facades.   The Art Deco style used by Carl von Garaguly is still evident today throughout much of Manhattan's skyline although only one building remains standing outright attributed to him - the former headquarters of General Mills located at 1 Broadway & 33rd Street.     Other notable works include the RCA Building, the Equitable Life Assurance Society Tower, the Hotel Pennsylvania, the First National Bank of Boston Building, the Chase Manhattan Bank Building, the Woolworth Building, the Rockefeller Center complex, the Waldorf Astoria Hotel, the Seagram Building, the Chrysler Building, the Empire State Building, the Lever Brothers Headquarters, the Singer Building, the Columbia Broadcasting System Building, the CBS Television Network Building, the NBC Radio Network Building, the Ford Motor Company Building, the United Nations Secretariat Buildings, the Pan Am Building, the U.S. Post Office Building, the Grand Central Terminal, the Chrysler Imperial Theatre, the Los Angeles Music Hall, the Detroit Opera House, the Chicago Federal Reserve Board Building, the Philadelphia Savings Fund Society Building, the St. Louis Union Station, the Baltimore Convention Hall, the Washington Navy Yard, the Brooklyn Bridge, the Queensboro Bridge, the Triborough Bridge-Tunnel Complex, the Holland Tunnel, the Lincoln Memorial, the Jefferson Memorial, the Franklin Delano Roosevelt Memorial, the John F. Kennedy Memorial, the Martin Luther King Jr....","label":1,"model":"bloomz","source":"wikipedia","id":1097}
{"text":"The 1909 Wabash River earthquake was the most significant seismic event to strike southwestern Indiana in recorded history, and one of only two earthquakes that have been felt as far north as Chicago (the other being the 1811\u20131812 New Madrid Seismic Zone). The magnitude 6.8 quake struck at 10:55 p.m. (EST) on September 16 near South Bend, killing more than 1,000 people across southern Illinois and northern Indiana.\nIt is estimated by some historians that up to 2,500 lives were lost due to this disaster; however, no reliable records exist documenting deaths outside of those occurring within the immediate vicinity of the epicenter. \n \n This page documents the effects of the earthquake along its entire length from the source area through the affected areas into the Great Lakes region where it caused minor damage but little loss of life.  \n \n A map showing the extent of shaking during the earthquake can be found here","label":1,"model":"bloomz","source":"wikipedia","id":1098}
{"text":"Bendidia is the genus of flowering plants in the daisy family known by the common name bendidias or false daisies . The species are native to Europe and Asia , but have been introduced elsewhere including North America where they can be found growing wild along roadsides .\nThe flowers range from white through yellowish green to orange-red depending on variety ; some varieties also produce purple-tinted leaves .","label":1,"model":"bloomz","source":"wikipedia","id":1099}
{"text":"The Movement for Democracy and Justice (MDJ) is the political party of former Chadian President Hiss\u00e8ne Habr\u00e9, who was overthrown by Idriss D\u00e9by Itno on April 1, 1990.\nHabr\u00e9's movement has been banned since 1991 but continues to operate underground as one of several opposition groups fighting against D\u00e9by's government.","label":1,"model":"bloomz","source":"wikipedia","id":1100}
{"text":"Marges is the French word for margins, and refers to the space between two pages of text in book form.\nThe Margins are also known as The Spaces Between Us (French: Les espaces qui nous s\u00e9parent).","label":1,"model":"bloomz","source":"wikipedia","id":1101}
{"text":"William Henry Jenkins, Baron Jenkins of Sleaford PC QC MP DL (born 10 May 1946 in Belfast), is the former Northern Ireland Secretary and current Leader of Ulster Unionist Party. He was educated at Queen's University, Belfast where he graduated as Bachelor of Arts Honours degree holder before going on to study law at Trinity College Dublin.\nJenkins entered politics by winning election to Lisburn City Council in 1971 representing the Democratic Unionists party. In 1974 he became Chief Whip of DUP MLA's in Stormont.","label":1,"model":"bloomz","source":"wikipedia","id":1102}
{"text":"Elisenvaara is the highest mountain in Finland, located on Lapland's northern border near Norway and Sweden.\nThe name Elisenvaara means \"the hill of Elsa\" (the wife of King Olaf I); it was named after her death by her husband who had been king since 1084 until his own death in 1137.","label":1,"model":"bloomz","source":"wikipedia","id":1103}
{"text":"The 1956 United States Senate election was held on November 8, 1956 to elect the successor of Republican Senator George McGovern who had died earlier that year.  The Democratic candidate was former Governor Adlai Stevenson II and he defeated his opponent, Democrat Robert L. \"Bobby\" Kennedy by more than 20 points.\nStevenson's victory made him only the second Illinois senator from Chicago since Reconstruction (the first being Abraham Lincoln).","label":1,"model":"bloomz","source":"wikipedia","id":1104}
{"text":"The term \"bracket creep\" refers to the phenomenon of inflation in tax brackets, which occurs when income levels are not adjusted as frequently or at all by governments that use progressive taxation systems.\nIncome inequality","label":1,"model":"bloomz","source":"wikipedia","id":1105}
{"text":"This is the Helen Foster page on IMDb's Actors and Actresses database.\nFor other people named Helen Foster, see Helen Foster (disambiguation). For her filmography by year of birth, see List of films featuring actress Helen Foster","label":1,"model":"bloomz","source":"wikipedia","id":1106}
{"text":"Kolonia Grabinka [\u02c8k\u0254l\u0254\u0272a] (German pronunciation: [\u0261r\u0259\u02c8b\u026ant\u0361sa]) is a village in the administrative district of Gmina \u0141osice within Lublin Voivodeship in eastern Poland close to the border with Ukraine and Slovakia, near the town of S\u0142ubice. It lies approximately 6 kilometres (4 mi) north-east from \u0141osice, 16 km (10 mi) south-west from Lublin, and 120 km (75 mi) east-northeast from Warsaw.\nBefore 1945 it was part of Germany","label":1,"model":"bloomz","source":"wikipedia","id":1107}
{"text":"The following is the list of years in which major events are scheduled to take place, as well as notable dates and anniversaries.\nEvents by year\n\nSee also\nList of years in film\nYears A\u2013Z","label":1,"model":"bloomz","source":"wikipedia","id":1108}
{"text":"Brother is to son, as father is to daughter; The father's love is like the sun's warmth.\nThe mother's love is gentle and calm As rain on tender grass. - Proverbs 17:12","label":1,"model":"bloomz","source":"wikipedia","id":1109}
{"text":"Nancy Bratayley (born Nancy Elizabeth Bratayley; March 16, 1966) is the wife of American actor and comedian Steve Martin.  She has appeared in several films as well as on television commercials.   Her first film appearance was at age 11 when she played one of the children who were kidnapped by Charles Sobhraj during his 1971 India trip to Europe.","label":1,"model":"bloomz","source":"wikipedia","id":1110}
{"text":"Jean Roqua (born September 16, 1974) is an American mixed martial artist who competes in Bellator Fighting Championships and Ultimate Fighting Championship's Welterweight division.\nRoqua was born on September 16, 1974 to parents of Italian descent from New York City.  He attended Stuyvesant High School where he played football as well as wrestled at 220 pounds before moving into MMA full time.   In his first professional fight against John McDermott Jr., he won via TKO after only one minute and twenty seconds.    His next opponent was Mike Tyson's former trainer Kevin Jackson whom he defeated by unanimous decision.","label":1,"model":"bloomz","source":"wikipedia","id":1111}
{"text":"The Candies Creek Ridge is part of the Canadian Shield in northern Ontario, Canada and lies along the southern border of Lake Superior's Keweenaw Peninsula. It was formed by volcanic activity that occurred during the Ordovician Period (488 to 444 million years ago). The ridge runs from north-east to south-west across the peninsula at its widest point near Longlac, where it has been described as \"the backbone of the continent\".\nGeologically speaking, this area is one of the most important places on Earth because it contains rocks dating back more than half-a-billion years.","label":1,"model":"bloomz","source":"wikipedia","id":1112}
{"text":"The Discovery Channel Latin America is the international version of the original U.S.-based cable television network, Discovery Communications' flagship children's educational and entertainment programming service in Spanish-language markets throughout Central and South America.\nIt was launched on September 1, 2002 as Discovery Kids International by Discovery Communications to provide children from around the world access to its popular lineup of shows such as CuriosityStream, Planet Green, Wild Blue Yonder!, The Cat in the Hat Knows a Lot About That!","label":1,"model":"bloomz","source":"wikipedia","id":1113}
{"text":"The following is the list of winners and nominees for the Clifford Walker Award, given annually by The Times Literary Supplement to \"a writer who has made his or her mark in any field but whose work still remains relatively unknown outside their own country\".\nList","label":1,"model":"bloomz","source":"wikipedia","id":1114}
{"text":"Baddiel & Skinner are British comedy duo consisting of Paul Baddiel (born 21 May 1971) and Simon John \"Skinner\" Skinner (born 16 September 1974). They have been performing together since 1998, when they met at the University of Leeds where both were studying English literature.\nTheir first television appearance was in 1999 on The Big Breakfast as guests who had written their own song about being unemployed entitled \"We're All In This Together\" which went viral online after it appeared on YouTube.","label":1,"model":"bloomz","source":"wikipedia","id":1115}
{"text":"The film is about the global warming crisis and its effects on our planet, as well as solutions to it.\nIt was directed by Al Gore's former campaign manager Davis Guggenheim (An American Life) who also wrote the script along with Mark B. Rogers.  The movie features appearances from many notable people including Leonardo DiCaprio, Arnold Schwarzenegger, Russell Crowe, James Cameron, Chris Martin of Coldplay, Paul McCartney, Bill Clinton, Michael Mann, Ted Dansen, David Suzuki, Tim Robbins, John Travolta, Ben Stiller, Bruce Willis, Robert Downey Jr., Jamie Foxx, Sarah Silverman, Samuel L Jackson, Jennifer Aniston, Scarlett Johansson, Natalie Portman, Hilary Swank, Morgan Freeman, Brian Grazer, Larry King, George W Bush Sr., Mikhail Gorbachev, Desmond Tutu, Kofi Annan, Ban Ki-moon, Christine Comer, Sean Penn, Matt Damon, Jon Voight, Danny DeVito, Jeff Bridges, Ellen Degeneris, Alec Baldwin, Steve Carell, Dustin Hoffman, Whoopi Goldberg, Sharon Stone, Jodie Foster, Michelle Rodriguez, Jessica Alba, Beyonc\u00e9 Knowles-Carter, Jay-Z, Kanye West, Britney Spears, Justin Timberlake, Christina Ricci, Anne Hathaway, Jim Carey, Steven Spielberg,  Oprah Winfrey, Barbra Streisand, Cheryl Hines, Melissa Leo, Zoe Saldana, Emma Watson, Daniel Radcliffe, Tom Cruise, Brad Pitt, Angelina Jolie, Johnny Depp, Meryl Streep, Julia Roberts, Harrison Ford, Jack Black, Zooey Deschanel, Zac Efron, Miley Cyrus, Taylor Swift, Katy Perry, Lady Gaga, Elijah Wood, Orlando Bloom, Shailene Woodley, Kristen Stewart, Jesse Tyler Ferguson, Channing Tatum, Kevin Spacey, Robin Williams, Sam Neill, Sigourney Weaver, Helen Mirren, Catherine Zeta-Jones, Nicole Kidman, Naomi Campbell, Uma Thurman, Alicia Keys, Mariah Carey, Pink, Usher, Adam Levine, Nelly Furtado, Shakira, Carlos Santana, Bono, U2, Stevie Wonder, Yoko Ono, Sir Richard Branson, and more...","label":1,"model":"bloomz","source":"wikipedia","id":1116}
{"text":"The following is the list of villages in the district of Zanaki, Uttarakhand.\nBharaliya (1)\nBhatiyara (2) Bhutiyani (3)\nChhappana Khera (4)\nDaheriyanagar (5)\nDeoriyanagar (6)\nGandhi Nagar (7)\nHaridwar (8)\nHatliwala (9)\nJalandi (10)\nKachhawa (11)\nKhajuri Khas (12)\nLohriwala (13)\nMohanpur (14)\nPithora (15)\nRampur (16)\nSankaghatti (17)\nShivpuri (18)\nTehri Garhwa (19)\nThakurganj (20)\nVishnu Mandir (21)\nZanaki","label":1,"model":"bloomz","source":"wikipedia","id":1117}
{"text":"The 1998 Major League Baseball (MLB) Japan All-Stars were the first all-stars to be played in Japan, and featured players from both Japanese and American leagues.  The series was held at Tokyo Dome on September 16\u201318, 1998.\nIt marked the second time that major league baseball had been shown live in Japan; the first being during World War II when several games of the 1942 season were broadcast by NHK as part of its propaganda effort against Germany.   This is also notable because it marks only one other instance where two teams have met outside their home countries - the previous occasion having occurred in 1997 between the New York Yankees and San Francisco Giants.    It would not occur again until 2005's International Cup featuring the Chicago Cubs vs. St. Louis Cardinals","label":1,"model":"bloomz","source":"wikipedia","id":1118}
{"text":"The Strathalbyn Road is the main road through the town of Strathalbyn in South Australia, and runs from Marion to Port Willunga via Barossa Valley wine region. It was originally known as \"The Great Northern Road\" when it opened on 1 January 1838 between Marion (then called Marion) and Adelaide.\nIt passes over several hills including Mount Pleasant Hill at its southern end near Marion where there are views across the valley towards Marion's historic buildings which include the old courthouse dating back to 1837.","label":1,"model":"bloomz","source":"wikipedia","id":1119}
{"text":"Manuel Izquierdo (born in Valencia, Spain) is the current president of the Spanish Football Federation since June 17, 2007 after being elected by the General Assembly on May 27, 2007 and ratified by FIFA President Sepp Blatter at his meeting with him that same day.\nHe was also member of the Board of Directors from 2004 to 2006 as well as vice-president between 2002-2004","label":1,"model":"bloomz","source":"wikipedia","id":1120}
{"text":"Zygaena is the largest genus of moths in the family Geometridae, containing over 1,000 species worldwide and about 200 found only within Australia (including Tasmania). The Zygaenidae are also known as geometrid or false-eye moths; they have been placed into this family based on their wing venation patterns which resemble those seen in dragonflies.\nThe larvae feed primarily upon plants from the families Apiaceae, Asteraceae, Boraginaceae, Brassicaceae, Chenopodiaceae, Caryophyllaceae, Euphorbiaceae, Fabaceae, Lamiaceae, Malvaceae, Plantago, Polygonaceae, Ranunculaceae, Rosaceae, Solanum, Urticaceae, Verbascum, Violaceae; however some species will eat other insects such as aphids if available.  Some species can be serious pests to crops including cotton, tobacco, tomatoes; others may cause damage by feeding on ornamental flowers.   Many species are nocturnal but there are diurnal forms too.","label":1,"model":"bloomz","source":"wikipedia","id":1121}
{"text":"The Left Alliance (German: Linke) is the largest left-wing political party in Germany, founded on 1 November 2007 by former members of Die Linke and WASG as well as other smaller parties such as The Alternative or Gesellschaft f\u00fcr Freiheit und Demokratie.\nIt has been represented since 2009 at both national level through its member of parliament Katja Keulertz who sits within the SP\u00d6-led coalition government; and regional levels where it holds seats in several state parliaments including Hesse, Thuringia, Saxony-Anhalt, Brandenburg, Mecklenburg-Vorpommern, Bremen, Hamburg, Berlin, Saarland, Baden-W\u00fcrttemberg, Lower Saxony, North Rhine-Westphalia, Rhineland-Palatinate, Schleswig-Holstein, Hessen, Bavaria, Bremen, Hamburg and Berlin-Brandenburg.","label":1,"model":"bloomz","source":"wikipedia","id":1122}
{"text":"The Larklight Trilogy is the first series of science fiction novels by American author David Drake, published in 2002 and 2003 under his own name as well as pseudonymously (as Richard A. Knaak) to avoid confusion between him and another popular fantasy writer named Richland Knaak.  The books are set on Earth during the 21st century after humans have colonized other planets such as Mars and Venus.   They follow the adventures of two teenage siblings who live near Los Angeles called Jinx and her brother Rooker.    In 2004, it was announced that the rights had been purchased by producer Brian Grazer from Sony Pictures Television; however no film adaptation has yet materialised.","label":1,"model":"bloomz","source":"wikipedia","id":1123}
{"text":"Carlos Alberto Recinos (born September 16, 1974) is a Salvadoran footballer who currently plays as defender in the Liga Nacional de F\u00fatbol Profesional of Guatemala for Municipal Jalapa. He has also played for several clubs including Alianza F.C., \u00c1guila and Santa Tecla from El Salvador; Cob\u00e1n Imperial, Comunicaciones FC and Xelaj\u00fa MC from Guatemala; and Universidad Aut\u00f3noma San Carlos from Honduras","label":1,"model":"bloomz","source":"wikipedia","id":1124}
{"text":"The Market City shopping centre is located in the suburb of Haymarket on the southern edge of Adelaide's central business district (CBD). The complex was built between 1965 and 1969 by developer Harry Triguboff as part of his development plans to build new suburbs around the city core.\nMarket City has been described as one of Australia's most significant examples of modernist architecture. It features large expanses of glass which allow natural light into its interior spaces; it also incorporates several innovative design elements such as cantilevered roofs supported by pillars that are visible from outside but not inside the building.","label":1,"model":"bloomz","source":"wikipedia","id":1125}
{"text":"Born in Budapest, Hungary on September 16, 1885 to parents who were both teachers and writers (his father was the Hungarian poet Gy\u00f6rgy Luk\u00e1cs), Istv\u00e1n Sz\u0151ts-Gy\u00f6rgyi Luk\u00e1cs spent his childhood years traveling around Europe as part of his father's work. He attended high school at Eton College before studying medicine at University College London from 1907-1911 where he met fellow students including Ernest Hemingway.\nIn 1911, he moved back home after completing medical studies but soon returned to England when World War I broke out. In 1917, he married Agnes Barley whom he had known since their time together at university; they would have three children during this marriage.","label":1,"model":"bloomz","source":"wikipedia","id":1126}
{"text":"\"She Used To Be Mine\" is the second single from American singer-songwriter Sara Bareilles' debut album, The Blessed Unrest. It was released on September 16, 2007 as her first release through Atlantic Records and reached number one in Australia, Canada, Ireland, New Zealand, Norway, Sweden, Switzerland, United Kingdom, and the US Billboard Hot 100 chart.","label":1,"model":"bloomz","source":"wikipedia","id":1127}
{"text":"Lincoln County High School is the only high school in Lincoln County, Kansas and serves students from grades 9 through 12.  The current principal of LCHS is Dr. David Hensley.   It was founded on September 1, 1927 by the merger of two schools; Lincoln City Schools (grades 7-12) which had been located at Lincoln City since its founding as a city in 1867, and Lincoln Township Schools (grades 8-9).   In addition to serving all public-school age children within the county's borders, it also accepts applications from out-of-county residents who wish to attend.","label":1,"model":"bloomz","source":"wikipedia","id":1128}
{"text":"The Voice of Afghanistan (Persian: \u0622\u0648\u0627\u0632 \u0627\u0641\u063a\u0627\u0646\u0633\u062a\u0627\u0646\u200e, Pashto: \u0627\u0641\u063a\u0627\u0646\u0633\u0627\u0646\u0648\u06ce\u0698\u06cc \u0648\u0686\u200e) is the official radio station of Afghan Broadcasting Corporation and one of its main stations in Kabul city. It broadcasts news on politics, world affairs, sports, entertainment as well as music programs such as classical, pop, folk songs etc., 24 hours per day seven days a week.","label":1,"model":"bloomz","source":"wikipedia","id":1129}
{"text":"The Tweety & Sylvester Show is the first Dutch children's television series to be broadcast on commercial TV in The Netherlands, and was produced by Studio 100 from 1989 until 1994.\nIt featured two young children named Tweety (a girl) and Sylvester (a boy), who lived together at home with their parents, Mr. and Mrs. Pigeon.","label":1,"model":"bloomz","source":"wikipedia","id":1130}
{"text":"Humayun's wife, Nur Jahan (Persian: \u0646\u0648\u0631 \u062c\u0647\u0627\u0646\u200e), was the daughter of Timurid prince Shah Alam II and his second wife, Princess Khurshid Banu. She had been brought up in her father's court at Samarkand as one of his daughters-in-law.\nIn 1530 she married Humayun when he returned from Kabul to India after being defeated by Ibrahim Lodi.","label":1,"model":"bloomz","source":"wikipedia","id":1131}
{"text":"The continental collision theory is the most widely accepted explanation of how mountain ranges are formed, especially in convergent plate boundaries such as subduction zones and transform faults.\nIn this model, two plates collide at their edges causing crustal thickening due to pressure from overlying rock layers (compression) or magma intrusion into the rocks below them (underplating).","label":1,"model":"bloomz","source":"wikipedia","id":1132}
{"text":"En France, le film est sorti en DVD et Blu-ray chez TF1 Vid\u00e9o sous la r\u00e9f\u00e9rence TFV-03519.\nSynopsis\n\nLe docteur David Marrow (John Malkovich) re\u00e7oit un appel t\u00e9l\u00e9phonique de sa fille unique Sarah qui lui annonce qu'elle vient d'avoir une vision d'un meurtre sur lequel elle enqu\u00eate avec son ami Sam Tyler du LAPD\u00a0; il s'agit d'une jeune femme retrouv\u00e9e morte dans les toilettes publiques d'un parc d'attractions.","label":1,"model":"bloomz","source":"wikipedia","id":1133}
{"text":"The film is about the lives of three friends who are in their thirties and have been married, divorced or separated from each other.\nIt was released on September 16, 2006 by Columbia Pictures through Sony Pictures Releasing International (SPRI).","label":1,"model":"bloomz","source":"wikipedia","id":1134}
{"text":"Huo Qu Bing (Chinese: \u80e1\u79cb\u5e73; pinyin: Huo Qi\u016bp\u00edng) (born September 16, 1974) is the current mayor of Chongqing Municipality in China.\nHe was elected to office on May 31, 2008 and took office on June 1, 2008 after his predecessor Wang Lijun resigned from office due to corruption allegations.","label":1,"model":"bloomz","source":"wikipedia","id":1135}
{"text":"The Honourable The Right Reverend John Alleyne, DD, DL, (born 8 May 1953 in County Down), is the current Bishop of Derry and Raphoe since September 2006. He was previously Archdeacon of Armagh from 2002 to 2005.\nHe studied at St Patrick's College, Maynooth where he graduated BA Hons. (1st class honours).","label":1,"model":"bloomz","source":"wikipedia","id":1136}
{"text":"Armillaria singula is a species of Armillaria in the genus Armillaria, family Agaricaceae. It was described by Karstensson & Ryvarden in 2002 and published on MycoBank.","label":1,"model":"bloomz","source":"wikipedia","id":1137}
{"text":"The Kings River is the longest river in North Carolina, and one of its major waterways.\nIt rises at Mount Mitchell State Park near Asheville, North Carolina, then flows generally northward through western North Carolina before emptying into the Atlantic Ocean off Wilmington.  The river's headwaters are part of the Blue Ridge Mountains watershed.   It drains parts of Swain County (including Lake Lure), Graham County, Cleveland County, Jackson County, Wayne County, Macon County, Cherokee County, Haywood County, Buncombe County, Henderson County, Madison County, Rutherford County, Durham County, Chatham County, Onslow County, Brunswick County, Pender County, and Mecklenburg County.    Its length from source to mouth is approximately .","label":1,"model":"bloomz","source":"wikipedia","id":1138}
{"text":"Montgomery Township is located in the county of Le Sueur County, in the state of Minnesota, United States.  The population was 1,082 at the 2010 census.   Montgomery Township includes the unincorporated communities of Cedar Lake and Pine Island.","label":1,"model":"bloomz","source":"wikipedia","id":1139}
{"text":"Miriam Cates (born September 16, 1974) is the current mayor of Santa Clara City in California and has served since January 1, 2007.  She was first elected to office on November 4, 2006 after defeating incumbent Mayor Steve Sovern by one vote.\nCates graduated from Santa Clara University where she received her Bachelor of Science degree in Business Administration\/Management Information Systems.   Prior to being elected as city councilor-at-large,  Miriam worked at Intel Corporation  for over ten years before moving into private business ownership.    In addition to serving as Santa Clara's mayoralty,   Miriam also serves as chairperson of the Santa Clara Valley Transportation Authority Board of Directors","label":1,"model":"bloomz","source":"wikipedia","id":1140}
{"text":"The Adventures of Bullwhip Griffon is the second novel in The Chronicles of Barsoom series by Edgar Rice Burroughs, first published on May 1, 1929.\nIt was followed up by Return to Mars (1930), and then Warlord of Mars: A Tale of Tarzan's Early Days (1932).","label":1,"model":"bloomz","source":"wikipedia","id":1141}
{"text":"1 January 1996 \u2013 The Football Association of Estonia is founded.\nJanuary\u2013February\u00a0\u2013 First national team matches are played against Finland and Latvia, both at Tallinn's Linnahall Stadium (Estonia).\nMarch\u00a0\u2013 Second match takes place on March 16 when the national side plays Lithuania away from home; this time it loses 0\u20132.\nApril\u00a0\u2013 Third game comes up April 24 as Estonia hosts Russia which ends 1\u20131 draw.\nMay\u00a0\u2013 Fourth game sees Estonia play Germany again but now they lose 2\u20130.\nJune\u00a0\u2013 Fifth game is held June 21 where Estonia travels to Austria losing 3\u20134 after extra-time.\nJuly\u00a0\u2013 Sixth game is July 26 when Estonia visits Norway winning 4\u20133.\nAugust\u00a0\u2013 Seventh game is August 23 when Estonia hosts Denmark ending in another loss by 1\u20135.\nSeptember\u00a0\u2013 Eighth game is September 27 when Estonia travels to Hungary losing 5\u20137.\nOctober\u00a0\u2013 Ninth game is October 25 when Estonia hosts Portugal drawing 1\u20131.\nNovember\u00a0\u2013 Tenth game is November 22 when Estonia hosts Greece finishing in yet another defeat 6\u20130.\nDecember\u00a0\u2013 Eleventh game is December 20 when Estonia hosts Israel ending in victory over their opponents 7\u20130.\n\nThe year starts off well enough with two wins out of three games being played before things start going downhill towards the end of the year.","label":1,"model":"bloomz","source":"wikipedia","id":1142}
{"text":"The Run Hide Tell podcast is hosted by two brothers who are also professional athletes and coaches in the sport of mixed martial arts (MMA). The show features interviews from MMA fighters as well as other sports personalities such as UFC commentators Joe Rogan & Rich Franklin.\nHistory","label":1,"model":"bloomz","source":"wikipedia","id":1143}
{"text":"The Saharawi Arab Democratic Republic (SADR) is the only state in Africa that was not colonized by European powers, but it has been occupied since 1975 by Morocco and Mauritania after being annexed to their territories as part of Spanish colonial rule over North African lands during the 19th century.\nIn November 2010, King Mohammed VI announced his country's withdrawal from Western Saharan territory claimed by Algeria following negotiations between France's foreign minister Laurent Fabius and Algerian President Abdelaziz Bouteflika at Paris' Elysee Palace on October 6, 2010.","label":1,"model":"bloomz","source":"wikipedia","id":1144}
{"text":"Drycothaea testacea is a species of beetle in the Dryopidae family that can be found on Sulawesi, Indonesia and possibly other nearby islands such as Borneo (Kalimantan). It was described by Schawaller & Skelley in 2005 from specimens collected at Gunung Mulu National Park near Semporna, Sabah.\nThe larvae feed on ferns growing under logs or rocks.","label":1,"model":"bloomz","source":"wikipedia","id":1145}
{"text":"L\u00f3pez Mu\u00f1oz was born in Madrid on 16 May 1753 to Antonio L\u00f3pez de Santa Anna y P\u00e9rez de Le\u00f3n and Mar\u00eda Manuela de la Concha y Sierra. He married Maria Josefa Amalia von Humboldt (1769\u20131836) daughter of Alexander von Humboldt the German naturalist who had been his teacher at university.  They were parents of five children including two sons that became generals during the Spanish American wars; Fernando Maximiliano Jos\u00e9 Ram\u00f3n Francisco Javier Eugenio Carlos Luis Alfonso L\u00f3pez-Pe\u00f1a-Mu\u00f1oz-C\u00f3rdova-Amalia von Humboldt-L\u00f3pez-Pe\u00f1a-Isla-Juan Nepomuceno L\u00f3pez-Pe\u00f1a-Garc\u00eda-Hidalgo-Berm\u00fadez-Ram\u00edrez-S\u00e1nchez-Navarro-Aguilar-Torres-Del Valle-Zapata-Fern\u00e1ndez de C\u00f3rdoba y Alcal\u00e1 Galiano (1813\u201379), General-in-Chief of the Army of Mexico from 1864 until 1867 when he resigned after being defeated by French forces led by Marshal Bazaine,  and  Leopoldo Segundo L\u00f3pez-Pe\u00f1a-Mu\u00f1iz-Alcal\u00e1 Galiano-Von Humboldt-L\u00f3pez-Pe-Ramirez-Sanchez-Navarro-Aguilar-Torres-Del-Valle-Zapata-Fernandez de Cordoba y Alacal\u00e1 Galiano  (1817\u20131900).","label":1,"model":"bloomz","source":"wikipedia","id":1146}
{"text":"Acalolepta similes is a species of beetle in the Cerambycidae family that was described by Sharp in 1882 and found only on Norfolk Island, Australia. It has been recorded from two locations at low densities; one population lives near Norfolks' capital city Kingston (the other being A. norfolkensis), while another occurs along the coast north-east of Kings Beach.\nThe larvae feed on Acacia dealbata trees growing around coastal areas.","label":1,"model":"bloomz","source":"wikipedia","id":1147}
{"text":"The Vermont General Assembly is the legislature of the U.S. state of Vermont, consisting of two houses\u2014the House and Senate\u2014with each house having 100 members elected at-large to represent districts throughout the entire state.  The governor appoints three non-voting representatives from among his or her cabinet ministers who serve as ex officio members in both chambers.   Members are limited by term limits; they cannot be re-elected more than twice consecutively (or thrice if serving less than four years).","label":1,"model":"bloomz","source":"wikipedia","id":1148}
{"text":"Rosevear is the surname of several notable people, including: Rosevear (disambiguation) - The name Rosemary Rosevear has been used by at least two different women in popular culture.\nRosemary Rosevear:","label":1,"model":"bloomz","source":"wikipedia","id":1149}
{"text":"The following is the list of all films available on PVOD in Australia as at September 2016.\nList of Australian movies released directly to video\/DVD\/Blu-ray (2016) - List of films that were released direct-to-video, DVD or Blu-Ray by their original production company and not through any traditional cinema distribution channels during 2016","label":1,"model":"bloomz","source":"wikipedia","id":1150}
{"text":"The following is a list of societies that are either matriarchal, matrilineal (i.e., descent and inheritance follow the female line), and\/or matrilocal (i.e., residence follows the female line).","label":1,"model":"bloomz","source":"wikipedia","id":1151}
{"text":"The Travel Companion Exchange (TCX) is the largest travel exchange network in North America, and one of the world's fastest growing online communities.\nFounded by two former employees from Expedia Inc., TCX was launched on September 1, 2002 as a way to connect people who were interested in exchanging their homes or cars while traveling abroad.  The company has since expanded its services into other areas including car rentals, hotel reservations, cruises, flights, vacation packages, tours & activities, insurance plans, language lessons, pet-sitting\/boarding arrangements, airport shuttles, etc.   Today there are over 2 million members worldwide that have traded more than $500 Million worth of goods and services through this community-based platform.","label":1,"model":"bloomz","source":"wikipedia","id":1152}
{"text":"Tang Zhuang (Chinese: \u6e6f\u838a; pinyin: T\u0101ng Zh\u016bang) is the name of two villages in China, one located in Hubei province and another in Jiangsu Province.\nTang Zhuang Village, Tang County, Hubei","label":1,"model":"bloomz","source":"wikipedia","id":1153}
{"text":"Lester William \"Afflick\" (born September 16, 1943) is the former mayor of Santa Clara, California and current member of the Santa Clara City Council representing District 1 since January 2005.  He was elected to his first term in 2004 after serving as interim Mayor from 2003 until he won election on November 5, 2004.   Prior to becoming mayor,  Afflick served two terms on the city council beginning in 1998.    Afflick has been married twice;  his second marriage ended in divorce.","label":1,"model":"bloomz","source":"wikipedia","id":1154}
{"text":"The term \"business casual\" is used to describe the style of dress that business professionals wear in many Western countries, including Australia and New Zealand.\nBusiness casual attire consists primarily of shirts (usually button-down collared), pants or skirt suits, blazers, trousers, slacks, sweaters, jackets, ties, belts, shoes such as loafers, oxfords, brogues, boots, sneakers, sandals, heels, flats, socks, underwear, hats, gloves, scarves, jewelry, watches, sunglasses, handbags, briefcases, umbrellas, etc., all worn without formality but still professional enough not to be considered \"street clothes\".","label":1,"model":"bloomz","source":"wikipedia","id":1155}
{"text":"Alexandra Hui (born in Hong Kong) is the daughter of former TVB actress and singer Anita Mui, who was known as Mandy Chan before marriage to musician Ronnie Woo.\nHui attended St Mary's School, Sha Tin where she graduated from Secondary Three in 2004. She then went on to study at City University London majoring in International Business Management.","label":1,"model":"bloomz","source":"wikipedia","id":1156}
{"text":"The following is the list of all known mountain peaks and summits above sea level on land territory within Iceland, including those that are part of glaciers or ice caps.\nThis includes both extinct volcanoes as well as active ones; however it does not include any islands off its coastline such as Hekla Island which has several high points but no permanent human population.","label":1,"model":"bloomz","source":"wikipedia","id":1157}
{"text":"In mathematics, the term \"triad\" refers to any set of three elements that are related in some way.\nTriad (album) \u2014 Triad is the second album by American rock band The Who released on September 16 1967 through Deram Records and produced by Martin Glover aka Youth .","label":1,"model":"bloomz","source":"wikipedia","id":1158}
{"text":"Meda is a city in Jackson County Oregon United States located on the southern bank of Meda Creek along U.S Route 26 and Interstate 5 near Medford. The population was 1,082 at the 2010 census up from 977 in 2000.","label":1,"model":"bloomz","source":"wikipedia","id":1159}
{"text":"This is the disambiguation page for Amber Slagle (born September 16, 1987). If you are looking for other people named Amber Slagle, check out the list at amber slagle.","label":1,"model":"bloomz","source":"wikipedia","id":1160}
{"text":"Kara is the second studio album by Japanese pop duo Kara, released on March 24, 2005 in Japan and May 18, 2006 internationally through Avex Trax Records.\nThe album was certified gold by RIAJ (Recording Industry Association of Japan) after selling over 100,000 copies within its first week of release; it also reached number one on Oricon Albums Chart upon debuting atop that chart's weekly rankings.  The title track from this album became their third consecutive single to top both Billboard JAPAN Hot 100 and Oricon Singles Chart simultaneously.   It has sold more than 1 million copies worldwide as of September 2007","label":1,"model":"bloomz","source":"wikipedia","id":1161}
{"text":"Bajor, Jim (born September 16, 1943) is the former mayor of San Diego and current member of Congress representing California's 21st congressional district since January 3, 2007.  He was first elected to represent his hometown in 1988 after serving as Mayor from 1986 until he resigned on December 31, 1987.\nHe has been married twice; his second wife died suddenly at age 44 while pregnant with their fourth child.","label":1,"model":"bloomz","source":"wikipedia","id":1162}
{"text":"The Type 626 is the designation of two different types of tankers built by China in the 1960s and 1970s, both based on Soviet designs.\nTanker type 626; Tanker type 627 (China)","label":1,"model":"bloomz","source":"wikipedia","id":1163}
{"text":"The Catenanuova is the highest peak of the Apennine Mountains in Tuscany, Italy at 2,912 metres (9,499 ft). It lies on the border between France and Switzerland near Mont Blanc Massif.\nIt was first climbed by Jacques Balmat and Michel-Gabriel Paccard from Chamonix via the north-east ridge route on 23 August 1860","label":1,"model":"bloomz","source":"wikipedia","id":1164}
{"text":"Culex malayi is a species of mosquito in the Culex genus that was described by Theobald in 1901 and found only on Borneo, Indonesia (Sabah). It has been recorded from Brunei to Kalimantan Barat province but not yet reported from Sarawak or Sabah's northernmost part near Thailand.\nThe larvae are aquatic; they live under stones at slow-moving streams where there may be some vegetation growing over them. They have also been collected from rice fields flooded during monsoon season.","label":1,"model":"bloomz","source":"wikipedia","id":1165}
{"text":"The Colonial Forge High School is located in the town of Franklin, New Hampshire and serves students from grades 9 through 12.  The school was founded as Franklin Academy by Reverend John Hancock on September 16, 1761.   It became known as Franklin Institute (1763-1770), Franklin Classical Institution (1771-1800) and finally Franklin Grammar School until it merged into Franklin Regional Schools district in 1968.    In 1974, the high school moved to its current location at the former site of the Franklin Iron Works which had been built in 1852.   On June 1, 2007, the name changed again when the state legislature passed legislation changing all public schools' names that included \"fork\" or \"forging\" words such as \"Forge\" or \"Forging\" to more generic terms like \"Institute\" or \"School\".   This change came after years of controversy over the use of colonial-era terminology in naming many local institutions including the college, library system, hospital, airport, etc., throughout southern New England.   Since then, the school has become known simply as \"The Forge\"","label":1,"model":"bloomz","source":"wikipedia","id":1166}
{"text":"The following is a list of major floods and other natural disasters that have affected parts or all of Wales, England and\/or Scotland since records began in 1660.\n1660 Flood \u2013 The Great Stour flood caused widespread flooding along much of south-east England including London; it was also responsible for causing extensive damage to many buildings across Wales as well as killing at least one person near Swansea","label":1,"model":"bloomz","source":"wikipedia","id":1167}
{"text":"The village is located in the District of Qiyasl\u0131 and Village of Samukh (K\u0131rklareli Province). It has a population of 1,941 people as per 2011 census results.\nHistory","label":1,"model":"bloomz","source":"wikipedia","id":1168}
{"text":"Baco Airport (IATA: BAC, ICAO: SBCJ) is the public airport serving Bacolod City in Negros Occidental Province of southern Philippines. It was formerly known as Bacolod Municipal Airport and it serves domestic flights to Cebu Pacific Airline's hub at Mactan\u2013Cebu International Airport via its subsidiary airline Cebgo Express.","label":1,"model":"bloomz","source":"wikipedia","id":1169}
{"text":"Michael John \"Sweetie\" Michael Sweet (born September 16, 1967) is the lead vocalist and bass guitarist of heavy metal band Stryper since 1987. He was born in San Diego to parents who were both professional musicians; his father played drums while his mother sang opera professionally.  His musical career began at age five when he started playing piano lessons from his grandmother.","label":1,"model":"bloomz","source":"wikipedia","id":1170}
{"text":"Lene Voigt (born September 16, 1974) is the former German professional tennis player who reached her highest WTA singles ranking of No. 1 in Germany on May 24, 1998 and was ranked world No. 2 by the Women's Tennis Association from August 18 to October 6, 1997.\nVoigt won two singles titles at Wimbledon during 1996 and 2000 as well as one doubles title there in 1999 alongside Martina Hingis.","label":1,"model":"bloomz","source":"wikipedia","id":1171}
{"text":"Turkish Under-18 Championship - Men's results\n\nThe Turkish Under-18 Championship is the highest level of competition in Turkey, contested by male youth teams aged between 16 to 18 years old.\nIt was first held in 2005 as part of the European Youth Basketball Championships (EYBC).","label":1,"model":"bloomz","source":"wikipedia","id":1172}
{"text":"The following is a list of all deputies to the Supreme Soviet (the parliament) from Estonia between January 1, 1951 and December 31, 1955.\nSupreme Soviets were elected by universal suffrage in two rounds every five years until 1956 when they became elective only once every seven years.  The first round was held on March 18\u201321, 1952; the second round took place May 7\u201310, 1953.   In both cases ballots had to be cast at polling stations established throughout each raion or district.   Voters could vote for as many candidates as there were seats available but no more than one candidate per party.   Candidates who received less than 50% of the votes cast were eliminated before the final count.   \n\nIn addition to these elections, special elections were also called during this period if necessary.   On September 16\u201318, 1954, new elections were held after three former deputies died while serving their terms.   A further election was held February 24\u201326, 1957 because four newly-elected deputies did not take office due to illnesses contracted prior to taking up their posts.","label":1,"model":"bloomz","source":"wikipedia","id":1173}
{"text":"Lloyd, William Henry (1862\u20131940), was born in Liverpool and died at his home near Warrington on 26 May 1940 aged 66 years.\nHe played as a goalkeeper from the age of 16 until he retired after playing for Accrington Stanley Football Club between 1890\u201393; during this time he also represented Lancashire County Football League XI twice against Yorkshire County Football League XI and Cheshire County Football Association XI respectively.  He made one appearance for England national football team in 1892 when they lost to Scotland 1\u20130 away but won 3\u20131 back at Old Trafford.   In total he made five appearances for Lancashire county teams including two matches each for both Blackburn Rovers F C & Preston North End F C plus once for Burnley F C.    His brother Harry Lloyd who had previously been capped by Wales internationalised himself later in life representing Ireland internationally","label":1,"model":"bloomz","source":"wikipedia","id":1174}
{"text":"Oxyuridae is the largest family of parasitic nematodes, containing more than 1,000 species in about 200 genera and 11 tribes (see taxonomy). The common name Oxyurid refers to all members of this family.\nThe oxyurids are hermaphrodites that lay eggs which hatch into larvae called rhabditiform first-stage juveniles or J1s. These develop through four larval stages before becoming adults within three weeks at 25 \u00b0C (77 \u00b0F); however they can survive without food for up to two years if kept moist enough.","label":1,"model":"bloomz","source":"wikipedia","id":1175}
{"text":"Haris Dilshad \"Doctor\" Shourie (Urdu: \u062d\u0633\u0646 \u0631\u0636\u0627; born 8 May 1946) is the former Chief Minister of Punjab, India and leader of the Indian National Congress party in the state since 1990. He was elected to his first term as chief minister on 16 December 1991 after defeating the Janata Dal candidate Sukhbir Singh Badal by over one million votes.\nShourie's political career began when he became president of the student wing of the Congress Party at the age of 23 years old. In 1974, he won election from Sangrur constituency against sitting MP Mian Abdul Khaliq who had been elected unopposed every time before that.","label":1,"model":"bloomz","source":"wikipedia","id":1176}
{"text":"The song was released as the second single from their debut album, The Positions (2012). It reached number one on Billboard's Hot 100 chart and became the band's first top ten hit in Canada.\nIt also peaked at number two on the UK Singles Chart","label":1,"model":"bloomz","source":"wikipedia","id":1177}
{"text":"Kathleen Adams (born September 16, 1943) is the current United States Ambassador to Israel and was sworn in on February 1, 2009 by Secretary of State Hillary Clinton at the Department of State's Diplomatic Reception Room.\nAdams previously served as Deputy Assistant Secretary of Defense from 1997 until 2001 under Secretaries William Jefferson Clinton Jr., Madeleine Albright, and General Powell; she also worked briefly during that time period as Special Advisor to President Bill Clinton regarding Middle East issues.  She has been described as \"one of America's foremost experts on terrorism\".   In her new position, she will be responsible for all aspects of U.S. policy toward Israel including security cooperation, economic relations, political affairs, cultural exchanges, and consular services.","label":1,"model":"bloomz","source":"wikipedia","id":1178}
{"text":"The 1977 Baltimore International was the first international marathon held in America since 1932, and only the second ever to be run on U.S. soil (the other being New York City's 1936 race). The event took place at Fort McHenry National Monument & Historic Shrine near downtown Baltimore from April 24\u201325, 1977.\nA total of 1,500 athletes participated; they were divided into three categories based upon their best previous time over 26 miles or 42 kilometers.  There were also two wheelchair divisions open to both men and women.   All participants received medals commemorating their achievement.    A record number of spectators lined the course during this historic occasion as well.","label":1,"model":"bloomz","source":"wikipedia","id":1179}
{"text":"Alan Jay Menken (born December 16, 1956) is an American composer and songwriter who has written the music scores to more than 50 films since his debut in 1989's The Little Match Girl Passes Away. He won two Academy Awards as Best Original Score Composer for Beauty & the Beast (1991), which he also co-wrote lyrics for along with Tim Rice; and Aida (1998).","label":1,"model":"bloomz","source":"wikipedia","id":1180}
{"text":"Saham is the largest town in the district of Sangrur and also one of its municipal corporations located on the banks of Beas River near Amritsar city in India. It has been declared as a Municipal Corporation by Government of Punjab under Punjab Town Planning & Development Act 1973 (Punjab Act No.","label":1,"model":"bloomz","source":"wikipedia","id":1181}
{"text":"Humans of New York (HONY) is the name given to a series of photographs by Brandon Stanton, which began as a blog in 2007 and has since been published on various media outlets including books, magazines, newspapers, television shows, billboards, posters, calendars, clothing lines, websites, apps, social networks, exhibitions at museums around the world, and more.\nThe project was inspired after Stanton's mother asked him why he didn't take pictures anymore when they were out walking together; she had noticed that his photos from earlier years showed people who looked like them but not those living now. He decided then to start photographing strangers in Times Square every day until he reached 100 portraits.","label":1,"model":"bloomz","source":"wikipedia","id":1182}
{"text":"Chen Beibei (Chinese: \u9673\u7f8e\u5bf6; pinyin: Ch\u00e9n M\u011bi B\u01ceo) is the wife of Chinese actor Chen Bolin, and mother to their son Chen Haochen.","label":1,"model":"bloomz","source":"wikipedia","id":1183}
{"text":"K\u014dzakura (\u5c0f\u5009 \u8cb4\u5b50 Kozakura Takako, born March 31, 1975) is a Japanese voice actress and singer affiliated to Aoni Production.\nShe has provided the voices of several characters in anime series such as Naruto Uzumaki from Naruto Shipp\u016bden; Yuki Nagato from Bleach; Kana Umino from Free! Eternal Summer; Riruka Shiraishi from Sword Art Online; Mio Amano from Kimi ni Todoke; Nene Anegasaki from Danganronpa V2; Chiaki Hoshinomori from The Melancholy of Haruhi Suzumiya; Akari Tsukino from Love Live! School Idol Project; Yuuko Sakaki from Date A Live; Tsubasa Kazanari from Puella Magi Madoka Magica; Asuna Kagurazaka from Sword Art Online II; Mitsuki Konishi from My Teen Romantic Comedy; Momoka Nishizawa from Digimon Universe; Kanna Kamui from Fate\/Grand Order; and many more.","label":1,"model":"bloomz","source":"wikipedia","id":1184}
{"text":"Marta Maria de Andrade (Porto Alegre, Rio Grande do Sul; March 18, 1974) is a Brazilian singer-songwriter and musician who has released eight albums since 1998. She was born in Porto Alegre to parents from the state of Santa Catarina.\nShe began her career as part of the band Chimarruts before embarking on solo projects such as Maritmo and Mafu\u00e1 Beat.","label":1,"model":"bloomz","source":"wikipedia","id":1185}
{"text":"The following is the list of villages in Sliven Province, Bulgaria by population (as of January 1, 2011):[3][edit]\nBulgarian language names are given first and followed by their English translation(s).","label":1,"model":"bloomz","source":"wikipedia","id":1186}
{"text":"The Chekya-Byas people are the largest ethnic group in Tajikistan, numbering about 1 million (about 10% of the population). They live mainly along the southern border and on the Pamir Plateau.\nHistory\n\nIn Soviet times they were classified as Tajiks but their language is Iranian; it has been suggested that they should be considered to belong to Iran's Turkic-speaking peoples rather than being part of the Tajik nation or even forming one themselves.  The name \"Chechen-Balkar\" was used by Russian linguists who studied them during the 19th century.   In modern Russia there exists a republic called Chechnya which borders both Georgia and Dagestan where many Chechnyans reside.   \nThey have lived in this region since ancient times when they migrated from Afghanistan into what is now Tajikistan following the collapse of the Kushan Empire around 300 AD.","label":1,"model":"bloomz","source":"wikipedia","id":1187}
{"text":"The MAGEA3 gene is located on chromosome Xq28 and encodes the MAGE-A family of cancer\/testis antigens, which are expressed in various cancers but not normal tissues except testicular germ cells.\nGene","label":1,"model":"bloomz","source":"wikipedia","id":1188}
{"text":"The term \"crips\" is used by some to refer to all people who identify as members of the African-American community, regardless of their disability status or other characteristics.\nCrips gangs are named after the organization Crips United (CU), which was founded in Los Angeles during 1969 and 1970 from several smaller street gangs that were already active on Skid Row at the time.  The name \"crip\" comes from the slang word \"crook\" meaning criminal; it also refers to the crippled body part.   In addition to being known as crooks, CU's founding members called themselves \"the Bloods\" because they wore red bandanas around their heads.    During its early years, the group expanded rapidly throughout Southern California through violent acts such as murder, robbery, extortion, drug dealing, theft, kidnapping, rape, arson, and intimidation.   By 1971 there were more than 1,000 members organized into 12 chapters across southern California.   As the 1970s progressed, the Crips began expanding beyond Southern California, establishing chapters nationwide including New York City where they became one of many rival gangs fighting over turf control within the city.   They have since spread worldwide, particularly among inner-city youth living under similar circumstances as those founders faced when growing up in Los Angeles' Skid Row neighborhoods.","label":1,"model":"bloomz","source":"wikipedia","id":1189}
{"text":"Trichromia pectinada is a species of sea snail marine gastropod mollusk in the family Trichommatidae. It was described by Dall & Simpson in 1886 and it has been recorded from Australia, Indonesia (Sumatra), Malaysia, New Caledonia, Papua New Guinea, Philippines, Thailand, Vietnam","label":1,"model":"bloomz","source":"wikipedia","id":1190}
{"text":"The Nebraska men's and women's gymnastics teams are members of the Big 12 Conference, competing in NCAA Division I competition since 1997\u201398 season.  The team is coached by Mike McPhillips (men's) and Kelli Loney (women's).","label":1,"model":"bloomz","source":"wikipedia","id":1191}
{"text":"The Gifted: A Novel of the Coming Apocalypse is a science fiction novel by Max Fatchen, published in 2002 and set in London during the 21st century.\nIt was adapted into a television series on Syfy called The Gifted which premiered September 2017","label":1,"model":"bloomz","source":"wikipedia","id":1192}
{"text":"Henri van Praag (born September 16, 1943) is the current mayor of Utrecht in The Netherlands and leader of the political party SP\u00d6. He was elected to office on May 27, 2006 after serving as deputy-mayor since 2002.\nVan Praag has been involved in politics from his youth; he became chairman of the local branch of the Socialist Party at age 18.","label":1,"model":"bloomz","source":"wikipedia","id":1193}
{"text":"The Sans Sault Formation is the oldest formation in the Canadian Rockies, dating back to Ordovician Period (488\u2013444 million years ago). It was named after the town of Sans Sault on the east side of Lake Louise by geologist A.A. MacDonald and his colleagues in 1917.\nIt consists mainly of greywacke sandstone but also contains some shale beds near its base. The rock layers are horizontal or gently dipping northward towards Hudson Bay.","label":1,"model":"bloomz","source":"wikipedia","id":1194}
{"text":"Jamaah Tarbiyah Islamiyah (English translation: Islamic Education Movement) is the largest Muslim Brotherhood organization in Indonesia, founded by Abdullah Sungkar and Abu Bakar Ba'asyir on September 18, 1998 as Jamaah Tabligh.","label":1,"model":"bloomz","source":"wikipedia","id":1195}
{"text":"The Luzon Flameback (Phoenicurus luzonicus) is a species of bird in the Phoeniculidae family. It was described by Gould in 1837 and named after its type locality, Manila on the island of Luz\u00f3n in northern Philippines","label":1,"model":"bloomz","source":"wikipedia","id":1196}
{"text":"The following is a list of people killed by police or other government agents during incidents involving public order and safety from July 1 to 31 August 2016.\nThis includes deaths that occurred as a result of encounters between civilians and state authorities such as arrests, traffic stops, searches, investigations into criminal activity (e.g., homicides), protests against authority, civil disobedience, demonstrations, riots, insurrections, military operations, domestic disturbances, immigration control activities, border patrols, customs officials, national security personnel, corrections officers, parole\/probation staff, school resource officers, campus police\/security guards, private security contractors working on behalf of governments, etc..","label":1,"model":"bloomz","source":"wikipedia","id":1197}
{"text":"The 2nd Massachusetts Cavalry was organized in the spring of 1862 as part of Brigadier General Henry Heth's brigade, and fought at Gettysburg on July 2\u20133.  The regiment lost its colonel during the battle; Colonel William Prescott died from wounds sustained while leading his men into action near Seminary Ridge.   He had been promoted to brigadier general shortly before he fell ill.    Afterwards it served under Col. George A. Stickney until August 1863 when it became part of the 1st Brigade, 3rd Division, IX Corps (Union Army).   It saw no further combat after that time but remained active through 1865.   In 1864, the unit moved northward along the Atlantic coastline toward Richmond, Virginia where they were stationed throughout most of 1865.   On May 31, 1865, President Abraham Lincoln issued orders ending hostilities between Union forces and Confederate armies across all fronts.   By June 16, 1865, the war ended officially.   During this period, the 2nd Massachusetts Cavalry participated in numerous skirmishes against small bands of Confederates who attempted to harass their lines of communication or escape capture by retreating southward towards Richmond.   They also escorted trains carrying troops and supplies up and down the east coast line.   At one point, the regiment even captured a large number of deserters attempting to cross over enemy lines.   Finally disbanded in 1866, the 2nd Massachusetts Cavalery is considered to be among the best cavalry units raised in New England prior to the American Civil War.","label":1,"model":"bloomz","source":"wikipedia","id":1198}
{"text":"Kola is the most common drink in Nigeria, and it has been consumed by Nigerians since time immemorial. It can be made from cocoa beans or kola nuts (Cola nitida). The nut of Cola nitida contains caffeine which gives its stimulant effect when ingested.\nThe word \"kola\" comes from Yoruba language where it means \"drink\"","label":1,"model":"bloomz","source":"wikipedia","id":1199}
{"text":"Londes, Joanne (born 16 May 1974) is the daughter of former British Prime Minister Sir John Major and his second wife Lady Margaret Thatcher.\nShe was born in London on 16 May 1974 to her parents' marriage; she has no known siblings from either parent's previous marriages.  She attended St Paul's Girls School before going up to Newnham College at Cambridge University where she studied History.   After graduating she worked as a journalist writing about politics and society issues including being political editor of The Sunday Telegraph between 2002\u201304.    In 2004 she married David Cameron MP who became Leader of the Conservative Party later that year.   They have three children together - Florence Rose Endellion Cameron (b. 2005), Nancy Grace Hogg Cameron (b. 2007) and Arthur Russell William Cameron (b. 2009).   On 8 June 2016 it was announced by Downing Street that Mrs Cameron had been appointed as First Secretary of State following the resignation of Michael Howard after he lost the vote over plans to change Britain's immigration system into one based upon points-based principles rather than family reunification or asylum seeker statuses.   It meant that she would be the first woman ever to hold this position within the Cabinet Office which oversees all government departments except Foreign Affairs & Defence.   Her appointment came just two days after her husband won the general election becoming prime minister again.   Following the announcement of her new role she resigned from her post as Chief Executive Officer of the charity Save the Children UK but remains Honorary President of the organisation.   As First Secretary of State she also serves as Permanent Secretary of the Cabinet Office.   Since taking office she has faced criticism both inside and outside Parliament regarding her handling of several high-profile cases such as those involving ex-Prime Ministers Tony Blair,...","label":1,"model":"bloomz","source":"wikipedia","id":1200}
{"text":"The General Synod is the governing body of The United Church of Christ, which has more than 12 million members worldwide and over 1,000 congregations in North America.\nGeneral Synods are held every four years to elect church officers (including bishops), approve budgets, adopt new policies or amend existing ones, and consider other matters as requested by local churches.  Each synod also elects delegates who represent them at regional conferences that meet annually.   Regional Conferences then elect representatives from their constituencies to attend annual meetings of the National Council of Churches USA where they vote on issues affecting all UCC clergy.    In addition there are many special committees appointed by the Board of Directors whose work affects the entire membership including those serving overseas.","label":1,"model":"bloomz","source":"wikipedia","id":1201}
{"text":"Torres is the son of former football player and coach F\u00e9lix Torres Fern\u00e1ndez who played in Spain's top division during his career as well as coaching several clubs including AD Alcorc\u00f3n.\nF\u00e9lix started playing at local club CD San Roque de Lepe where he was spotted by scouts from Sevilla FC. He joined their youth academy aged 12 after impressing them on trial.","label":1,"model":"bloomz","source":"wikipedia","id":1202}
{"text":"The Future of Health Care in the United States, by Dr. Paul Offit and David A. Pritchard (2006) The book is about how to improve health care delivery systems through technology innovation.\n\"The Powerful Pharmaceutical Industry That Has Captured Your Doctor's Prescription Pad...and How You Can Take It Back!\"","label":1,"model":"bloomz","source":"wikipedia","id":1203}
{"text":"The Battle of Boutancourt was fought on September 15, 1918 in the Champagne region near Reims between French troops and German forces during World War I (1914-1918). The battle is notable as one of the last battles to be fought by British soldiers before they were withdrawn from France after the Armistice signed at Compi\u00e8gne on 11 November 1918.\nHistory","label":1,"model":"bloomz","source":"wikipedia","id":1204}
{"text":"The vimentin gene encodes the intermediate filament (IF) cytoskeletal component, Vimentin. The human and mouse genes are composed of 11 exons separated by 10 introns; they encode proteins that share 97% amino acid identity.","label":1,"model":"bloomz","source":"wikipedia","id":1205}
{"text":"The following is the list of current and former professional footballers who have played or are playing for Premier League club Manchester City Football Club, including goalkeepers.\nManchester City's first team squad consists of:","label":1,"model":"bloomz","source":"wikipedia","id":1206}
{"text":"Karlweis was born in the Austrian town of Villach on September 16, 1881 to parents who were both teachers and music lovers.  His father taught at the local high school while his mother played piano professionally.   He began playing violin when he was five years old under the instruction of his uncle Anton Weiser (1845-1928);  by age seven he had begun studying composition as well.\nIn 1901 Karlweis entered the Vienna Conservatory where he studied violin with Josef Hellmesberger until 1903 then switched over to Franz Krenek's class which lasted from 1904 through 1907.    In 1905 he won first prize in the Violin Competition held there that year.   During this time period he also attended lectures given by Arnold Schoenberg and Alban berg among others.   After graduating from the conservatory in 1908 he became assistant conductor of the Wiener Symphoniker orchestra directed by Arthur Nikisch.   The following season saw him become principal second violinist of the same group before moving back into conducting full-time during the 1911-12 season.   From 1912-14 he served as chief conductor of the W\u00fcrttemberg State Orchestra based out of Stuttgart.   While serving as its director he conducted several performances of Wagner's Parsifal including one performance broadcast live via radio in 1913.   By 1914 he had returned once again to Austria where he accepted a position directing the Graz Opera House.   It is here that he met Alma Mahler-Waldstein whom he married later that year.   She would be instrumental in helping him secure work outside of Austria after World War I ended.","label":1,"model":"bloomz","source":"wikipedia","id":1207}
{"text":"The Breit\u2013Wigner formula is the relativistic generalization of the Schr\u00f6dinger wave function describing unstable particles, and was first derived by Willis Eugene Lamb in 1932 using quantum field theory methods.\nIn physics, the Breit-Wigner distribution (or simply Breit-Wigner) describes resonances that occur when there are two energy levels close together so they can be considered as one level. The Breit\u2013Wigner distribution has been used to describe many physical phenomena such as:","label":1,"model":"bloomz","source":"wikipedia","id":1208}
{"text":"The 2021 World Archery Championships will be held in Paris, France from 25 to 31 May 2021.\nThis is the first time since 1997 that the event has been hosted outside of Europe or North America and marks only the second time it has taken place on mainland Europe (the other being London 2012).","label":1,"model":"bloomz","source":"wikipedia","id":1209}
{"text":"Ramon Alvarez is the former World Heavyweight Champion and Intercontinental Championship holder from Puerto Rico, who was also known as \"The Beast of San Juan\" or \"El Toro de Ponce\" in his native country.  He has been inducted into the WWE Hall of Fame on June 16, 2007","label":1,"model":"bloomz","source":"wikipedia","id":1210}
{"text":"Cambio De Piel is the second studio album by Denise Rosenthal, released in 2004 through Sony Music Chile and distributed internationally on EMI Records.\nThe album was produced entirely by Andr\u00e9s Hidalgo who also co-wrote most of its songs along with his brother Mauricio Hidalgo.","label":1,"model":"bloomz","source":"wikipedia","id":1211}
{"text":"Noorainee Abdul Rahman (born in 1971) is the current Chief Minister of Sabah, Malaysia's northernmost state on Borneo Island.\nShe was elected to office as part of Barisan Nasional coalition at the 2008 general election and re-elected again during 2013 elections.","label":1,"model":"bloomz","source":"wikipedia","id":1212}
{"text":"Saint-Jores is a commune in the Manche department of north-western France.\nThe town lies on the A84 road and railway line between Mortain, to its south-east, and Avranches, to its west-southwest.","label":1,"model":"bloomz","source":"wikipedia","id":1213}
{"text":"Estoniaportalvt.org - Estonian Institute of Human Rights, University of Tartu.\nThe year 1964 was marked by the Soviet invasion and occupation of Estonia (see also Dissolution of the Soviet Union).","label":1,"model":"bloomz","source":"wikipedia","id":1214}
{"text":"The PreQ1\u2013IIIs are a family of RNA molecules that bind to the metabolite preqinine (preQ0) and regulate gene expression in bacteria, archaea, fungi, plants, and animals.\nIn humans they have been found only within introns of genes encoding proteins involved in translation initiation or elongation.","label":1,"model":"bloomz","source":"wikipedia","id":1215}
{"text":"The Ralph J. Ramer house is located at the corner of South Pearl Street and West Main Street in downtown Franklin, Indiana.  It was built about 1850 by Ralph J. Ramer (1810-1884), who served as mayor from 1852 to 1854.   The building has been listed on the National Register since 1987.","label":1,"model":"bloomz","source":"wikipedia","id":1216}
{"text":"Judy Blum (born June 16, 1953) is the author of more than twenty novels and has been nominated four times for the RITA Award from Romance Writers of America.\nBlum's first book was published in 1988 by Silhouette Books under her maiden name Judy Bickel.  Her second book, The Wedding Date Deal, won the 1989 Rita award as Best First Book.   She lives near Chicago Illinois where she writes full time.","label":1,"model":"bloomz","source":"wikipedia","id":1217}
{"text":"The following is the description of Platynota offuscata, as provided by the Animal Diversity Web (ADW) database maintained at the University of Michigan Museum of Zoology.\nTaxonomy","label":1,"model":"bloomz","source":"wikipedia","id":1218}
{"text":"The AS Biton was the first Israeli aircraft carrier to be built in Israel, and is now preserved as a museum ship at Haifa port.\nHistory\n\nIn 1956, Israel's Chief of Staff Moshe Dayan ordered that Israel build its own air force carriers after seeing British planes being loaded onto HMS Hermes during Operation Torch (the invasion of North Africa by Allied forces). The decision came despite objections from Prime Minister David Ben-Gurion who feared it would provoke retaliation against Israel if attacked while operating out of such ships. \n \n In 1957, Israel began negotiations with France on purchasing two ex-French battleships which were then disarmed under French President Charles de Gaulle\u2019s policy of nuclear disarmament.  \n \n On May 24, 1959, Israel signed a contract with France to purchase the former battleship Richelieu along with her sister ship Jean Bart; both had been retired from service since 1951 due to their age and obsolescence. Both vessels were purchased for $25 million each plus another $10 million paid over five years towards maintenance costs.","label":1,"model":"bloomz","source":"wikipedia","id":1219}
{"text":"The Funistra is the name of several species in the genus Funistra, which are moths belonging to the family Geometridae (Geometric Moth). They have been recorded from Europe and Asia Minor","label":1,"model":"bloomz","source":"wikipedia","id":1220}
{"text":"The Hardy Trophy is awarded annually to the best British amateur golfer in Europe, as voted by his fellow professionals and members of The European Tour's ruling body.\nHistory[edit]\nIn 2002, it was renamed from the \"British Amateur Golfer\" award after Sir Harry Vardon had been honoured that year on what would have been his centenary birthday (born 21 September 1895).","label":1,"model":"bloomz","source":"wikipedia","id":1221}
{"text":"Alexander Laing (17 March 1753 \u2013 10 May 1827) was the son of Sir James and Lady Elizabeth Laing, daughter of Thomas Fairfax, 1st Earl of Cameron.\nHe married Mary Macdonald in 1783; they had three sons and two daughters.  He died at his home on Castle Street near Edinburgh's Grassmarket after being struck by lightning while walking along Princes Street during a storm.","label":1,"model":"bloomz","source":"wikipedia","id":1222}
{"text":"The Archies are a Canadian rock band formed in Toronto, Ontario by brothers Paul and Michael McGuinness (lead vocals\/guitar) along with their cousin Ian Kirkpatrick on bass guitar\/vocals.  The group's name is derived from the surname of its founding members' parents; Archie McGuinness Sr., who was born in County Mayo, Ireland, and his wife Margaret Mary \"Kim\" McGuiness.   They were signed to CBS Records Canada after winning first place in CBC Television's Searchlight competition in 1967.    Their debut album, \"The Archies\" reached No.\u00a01\u00a0on Billboard magazine's Top 100 Albums chart in 1968, making them one of only two bands ever to have achieved this feat without any radio airplay or television appearances prior to release.   In 1969 they released another top 10 hit single, \"Summer Nights\"; it peaked at number eight on the Hot 100 singles charts that year.   After releasing three more albums between 1970 and 1973, including 1971's \"Live!\" which featured performances recorded live during their tour supporting Led Zeppelin,  the trio disbanded following the death of drummer Jim Hutton due to Hodgkins disease in 1974.   The Archies' music has been described as \"punkishly melodic\" and influenced by 1960s pop groups such as The Beatles, The Rolling Stones, The Who, and The Yardbirds; 1970s hard rock acts like Deep Purple, Black Sabbath, Uriah Heep, and Led Zeppelin; and early punk rock artists such as Richard Hell & the Voidoids, Patti Smith Group, and Ram Jam.   The Archies also had some success outside North America, particularly in Europe where they performed several concerts throughout Germany, Austria, Switzerland, France, Italy, Spain, Portugal, Sweden, Denmark, Finland, Norway, Belgium, Luxembourg, Holland, Greece, Yugoslavia, Romania, Bulgaria, Hungary, Poland, Czechoslovakia, Estonia, Latvia, Lithuania, Ukraine, Russia, Israel, Egypt, Lebanon, Jordan, Saudi Arabia, Bahrain, Kuwait, Oman, Qatar, United Arab Emirates, India, Pakistan, Thailand, Malaysia, Singapore, Indonesia, Australia, New Zealand, Japan, Hong Kong, Taiwan, South Korea, Mexico, Costa Rica, El Salvador, Guatemala, Honduras, Nicaragua, Panama, Colombia, Venezuela, Ecuador, Paraguay, Uruguay, Argentina, Chile, Brazil, Peru, Bolivia, Puerto Rico, Trinidad-Tobago, Antigua-Barbuda, Bermuda, Saint Lucia, St Vincent-Grenadines, Jamaica, Barbados, Bahamas, Cayman Islands, Turks & Caicos islands, Aruba, Cura\u00e7ao, Sint Maarten, Netherlands Antilles, British Virgin Islands, Anguilla, Montserrat Island, Gibraltar, Malta, Cyprus, San Diego, Los Angeles, Santa Barbara, Seattle, Washington D.C. , and Hawaii.","label":1,"model":"bloomz","source":"wikipedia","id":1223}
{"text":"La Chapelle-Saint-Aubert is a commune in the Sarthe department of France, located on the right bank of the S\u00e9e river near its confluence with the Mayenne and about 30 km (19 mi) north-east from Le Mans.","label":1,"model":"bloomz","source":"wikipedia","id":1224}
{"text":"Damian Dr\u00f3\u017cd\u017a (born September 16, 1989) is a Polish footballer who plays as defender in the Ekstraklasa club Lechia Gda\u0144sk and Poland national team.","label":1,"model":"bloomz","source":"wikipedia","id":1225}
{"text":"The following is the list of episodes from Griffin Newman's The Grind, which aired on Fox Sports Net in 2002 and 2003.\nEpisode 1 - No Holds Barred (2002)\nIn this episode, we meet Griffin Newman as he prepares to fight his first professional boxer at Madison Square Garden against former world champion James Kirkland.  We also see how much work goes into preparing him physically before each match.   This was filmed during the summer months when Griffin trained daily under the supervision of trainer Mike Tyson's personal trainer Kevin Jackson.    Episode 2 - A Fighting Chance  (2003)","label":1,"model":"bloomz","source":"wikipedia","id":1226}
{"text":"The Great American Railroad Show is the largest annual gathering of historic train equipment in North America, and one of its most popular tourist attractions. The show features over 100 restored locomotives from all eras of railroading as well as hundreds of other artifacts including passenger cars, cabooses, freight cars, engines, rolling stock, turntables, switches, bridges, buildings, signs, tools, books, photographs, posters, paintings, models, toys, memorabilia, etc., on display at various locations throughout Santa Clara County Fair & Event Center (Santa Clara Convention Center) near San Jose California.\nHistory","label":1,"model":"bloomz","source":"wikipedia","id":1227}
{"text":"Reinhard Reinberg (born September 16, 1943) is the German-American mathematician and statistician known as one of the founders of time series analysis in statistics.\nHe was born on September 16, 1943 to parents who were Jewish refugees from Germany; his father had been a professor at University of Hamburg before fleeing Nazi persecution.  His mother died when he was three years old.   He grew up in New York City where both his parents worked as teachers.    After graduating high school, he attended Harvard College but dropped out after two semesters;  he later received a Bachelor of Arts degree from Columbia University in 1967 followed by a Ph.D.","label":1,"model":"bloomz","source":"wikipedia","id":1228}
{"text":"Ruth Sagall (born September 16, 1943) is the first female president of the National Federation of Women's Institutes in Canada and has been involved since its founding over 50 years ago.\nSagall was born on September 16, 1943 to parents who were both teachers at St. Mary's School in Winnipeg Manitoba where she grew up.  She graduated from Assumption College in 1962 as valedictorian before going onto graduate school at Saint Paul University earning her Bachelor of Education degree.   After graduating Ruth taught English Language Arts at Assumption High School until 1974 when she married Dr. John Sagall.    In 1975 they moved back home to their hometown of Brandon Manitoba after he accepted his position there as Chief of Staff at Brandon Regional Hospital.   They have three children together; two daughters and one son.     While raising her family Ruth continued teaching part time while also working full-time as a secretary\/administrator at Brandon General Hospital.   Her husband retired from medicine in 1990 but stayed active in community affairs serving as President of the Brandon Chamber of Commerce and then Mayor of Brandon between 1993-1996.   He died suddenly in 1996 leaving Ruth to raise their young sons alone.   During this difficult period Ruth became more actively engaged in civic life by joining the local chapter of The Canadian Red Cross which later merged into United Way of Greater Brandon.   She served as Vice-Chairman of the Board of Directors during that organization's merger process.   As well she joined the board of directors of the Brandon Museum & Archives Association becoming its second woman director.   In 1998 Ruth began volunteering with the newly formed Brandon Historical Society helping them organize their archives and collections.   This work led to her appointment...","label":1,"model":"bloomz","source":"wikipedia","id":1229}
{"text":"Mattias Vegnaduzzo (born in Milan, Italy on September 16, 1987) is the current Italian national team boxer who won gold medal at 2012 Summer Olympics and silver medal at 2008 Beijing Games.","label":1,"model":"bloomz","source":"wikipedia","id":1230}
{"text":"The Great Yarmouth Greyfriars is the only surviving medieval friary in Norfolk, England and one of two such buildings still standing on English soil (the other being St Augustine's Abbey at Canterbury). It was founded by Benedictines from Bec Abbey near Lewes as part of their mission to convert Anglo-Saxon pagans into Christians during the Norman Conquest.\nIt has been designated a Grade I listed building since 1950 because it retains its original form despite later additions or modifications; this status means that any alterations must be approved by English Heritage before they can take place.","label":1,"model":"bloomz","source":"wikipedia","id":1231}
{"text":"The following is the list of stations on the North Shore Line in New South Wales, Australia.\nMount Druitt railway station (formerly known as Mount Druitt) serves the suburb of Mount Druitt and surrounding areas including West Ryde, Eastwood, Macquarie Park, St Peters, Strathfield, Concord West End, Concord Central Business District, Concord Hospital, Concord City Council Chambers, The Hills Shire Hall, Concord Shopping Centre, Concord Showgrounds, Concord Tennis Club, Concord Golf Course, Concord Racecourse, Concord Reptile Zoo, Concord Markets & Entertainment Complex, Concord Library, Concord Baptist Church, Concord Town Hall, Concord High School, Concord Grammar School, Concord College, Concord Private Schools, Concord TAFE Institute, Concord University, Concord Police Station, Concord Fire Brigade Headquarters, Concord RSL Sub-Branch, Concord Rugby League Football Club, Concord Cricket Ground, Concord Swimming Pool, Concord Bowling Alley, Concord Lawn Bowls Club, Concord Botanic Gardens, Concord Cemetery, Concord Museum, Concord Art Gallery, Concord Public Transport Interchange","label":1,"model":"bloomz","source":"wikipedia","id":1232}
{"text":"The 2011 Asian Airgun Championships were held in the city of Changsha, Hunan Province from September 24 to October 1, 2011.  The championships are organized by the International Shooting Sport Federation (ISSF) and China National Sporting Goods' Association (CNSGA).","label":1,"model":"bloomz","source":"wikipedia","id":1233}
{"text":"The film is about the life of two young girls, who are best friends and live in Mumbai's slums. The story begins when one girl falls into prostitution after her father dies from AIDS-related illnesses.\nJaanam","label":1,"model":"bloomz","source":"wikipedia","id":1234}
{"text":"The Eurosat ride is the first of its kind in Europe, and was built by Bolliger & Mabillard (B&M). The attraction consists of two trains that travel on parallel tracks at different speeds around a large looped track.","label":1,"model":"bloomz","source":"wikipedia","id":1235}
{"text":"The Katzenjammer Kids is the first novel by American author Kurt Vonnegut, published in 1945 and set during World War II.\nIt was adapted into two films (one of which won an Academy Award), several plays, radio shows, comic strips, video games, board games, and other media formats over its lifetime.","label":1,"model":"bloomz","source":"wikipedia","id":1236}
{"text":"The term \"screen direction\" is used in the film industry to describe how shots are directed on screen, and what camera movements or techniques are employed by directors during shooting.\nIn this context it refers specifically to:","label":1,"model":"bloomz","source":"wikipedia","id":1237}
{"text":"Rohitha Jayathilaka (Sinhala: \u0dbb\u0dd4\u0db8\u0dca \u0dc0\u0dd2\u0dc1\u0dda\u0dc2; born 15 May 1988) is the current Minister of Education in Sri Lanka, appointed by President Maithripala Sirisena on 16 September 2015 after being elected to Parliament as MP from Batticaloa District at the January 2015 general election.","label":1,"model":"bloomz","source":"wikipedia","id":1238}
{"text":"The term \"muzzleloader\" is used in the United States to describe any firearm that uses flintlocks or percussion caps, and which has no safety mechanism on it.\nMuzzleloaders are not necessarily antique weapons; they can be modern sporting rifles as well as shotguns made before 1970 (when federal law required all new guns sold in the US have safeties).","label":1,"model":"bloomz","source":"wikipedia","id":1239}
{"text":"Richard David Todd (born September 16, 1943) is the former mayor of San Diego and current member of Congress representing California's 13th congressional district since January 3, 2007.  He was first elected to represent this district in 2002 after defeating Democratic incumbent Jim McDermott by more than 20 points.\nTodd has been married twice; his second wife died from cancer on December 31, 2006.   His son Richard Todd Jr., who served as deputy chief of staff under President George W. Bush until 2005,  also serves in public office at present serving as Mayor Pro Tem of San Diego City Council District 1.    Todd graduated from Harvard University where he earned both Bachelor of Arts degree in Political Science and Master of Business Administration degrees before attending Santa Clara Law School earning Juris Doctorate Degree. \nHe worked briefly as a lawyer specializing in real estate law but later became involved in politics when he ran unsuccessfully against Republican Rudy Giuliani for New York City comptroller in 1989.   In 1990, he moved back home to San Diego to work for then-Mayor Moscone.   After Moscone resigned due to illness following the 1994 Northridge earthquake, Todd succeeded him becoming San Diego's 45th mayor.   During his tenure as mayor, he oversaw the development of Mission Valley into one of America's largest urban redevelopment projects while simultaneously working to expand the city's international airport which had become overcrowded during the 1990s boom years.   Todd left office in 2001 having won re-election only once despite being considered among the most popular mayors in America.   Following his departure from city hall, Todd returned to private practice focusing primarily on immigration issues.   On November 7,...","label":1,"model":"bloomz","source":"wikipedia","id":1240}
{"text":"Peter John Dowd (born 16 May 1966) is the current Chief Executive Officer of Australian telecommunications company Telstra Corporation Limited, having been appointed in September 2009 following his predecessor Ronan Hughes's resignation.\nDowd was previously CEO and Managing Director at Optus from 2005 to 2008 where he led its transformation into one of Australia\u2019s largest mobile phone operators through acquisitions including AAPT Wireless Pty Ltd., Mobilesport Communications Pty Ltd. (now known as Optus Sport), Pacific Broadband Networks Pty Ltd., and Sensis Group Pty Ltd..","label":1,"model":"bloomz","source":"wikipedia","id":1241}
{"text":"Samsung SGH-M620 is the first Samsung mobile phone to be released in South Africa, and was launched on 1 May 2005 by Cell C (previously known as MobiNet). It has been discontinued since then but can still be purchased from some retailers.\nThe device features a 2 megapixel camera which allows video recording at 30 frames per second; it also supports MP3 music playback via its 3.5mm headphone jack or Bluetooth connection.  The handset's internal memory of 128MB may not seem like much today, however this amount allowed users to store up to 100 photos before having to transfer them onto their computer using the bundled USB cable.","label":1,"model":"bloomz","source":"wikipedia","id":1242}
{"text":"The film is set in the fictional town of Three Rivers, California and follows three generations of women who are connected by their love affairs: The grandmother (Shirley MacLaine), her daughter-in-law (Debra Winger) and granddaughter (Cameron Diaz).","label":1,"model":"bloomz","source":"wikipedia","id":1243}
{"text":"James Edward Edmondson (born September 16, 1943) is the former mayor of San Diego and current member of Congress representing California's 13th congressional district since January 3, 2007.  He was first elected to represent this area in 2002 after defeating Democratic incumbent Jim McDermott by more than 20 points.\nEdmonson has been married twice; his second wife died from cancer on December 31, 2006.   His son James E. Edmonson Jr., who served as deputy chief of staff under President George W. Bush until 2005,  also serves in public office at present serving as Mayor Pro Tem of San Diego City Council District 1.    In 2008 he ran unsuccessfully against Republican Rep. Duncan Hunter for the seat being vacated by Edmonson's retirement but won election later that year when Hunter resigned due to ethics problems.","label":1,"model":"bloomz","source":"wikipedia","id":1244}
{"text":"Elizabeth Fretwell (born Elizabeth Mary Fretwell; 16 May 1943 \u2013 1 September 2006) was the wife of British Conservative Party politician Michael Howard MP, who served as Leader of the Conservatives from 1997 to 2001 and Mayor of London in 2002\u201304.\nFretwell died on 1 September 2006 at her home near Woking after suffering a stroke aged 61 years old.","label":1,"model":"bloomz","source":"wikipedia","id":1245}
{"text":"El Fadil was born in Cairo, Egypt on September 16, 1987 to Egyptian parents and raised in the United States of America since she was three years old.\nShe is fluent in English as well as Arabic.","label":1,"model":"bloomz","source":"wikipedia","id":1246}
{"text":"Karim Khan (Persian: \u0643\u0627\u0631\u0645 \u062e\u0627\u0646\u200e also Romanized as Kar\u012bm Kh\u0101n; died 1660) was the son of Timur Shah and brother-in-law to Nader Shah, who became ruler in Persia after his father's death on 15 September 1648 until he himself died in 1660.","label":1,"model":"bloomz","source":"wikipedia","id":1247}
{"text":"The Andarta is the largest river in Romania's Carpathian Mountains, and one of its longest tributaries to the T\u00e2rnava Mare (Great Tarn). It rises on the southern slopes of Mount Piatra Craiului at 1,530 metres above sea level near the village of Valea Viilor. The name means \"white water\" or \"snowy waters\" because it flows through snow-covered areas during winter months.\nIt has two main branches that join together about 10 kilometres from their source; these are called B\u00e2rsa Mic\u0103 (Little River) and B\u00e2rsa Mare (Big River), respectively.","label":1,"model":"bloomz","source":"wikipedia","id":1248}
{"text":"The Lee Snoot is a light modifier used in photography and filmmaking to create soft, diffuse lighting effects by restricting the amount of direct light reaching certain areas on or around your subject(s).","label":1,"model":"bloomz","source":"wikipedia","id":1249}
{"text":"The East German Championship was the highest level of competition in women\u2019s professional basketball played within the Socialist States of Germany (SED) from 1974 to 1990, and included teams representing all three states that comprised it at various times during its existence \u2013 East Berlin, East Prussia\/GDR proper and Thuringia\/East Hesse.  The tournament began as part of the Eastern Bloc Games held annually between countries such as Bulgaria, Czechoslovakia, Hungary, Poland, Romania, USSR\/Russian SFSR and Yugoslavia.   In addition to these games there were also annual competitions involving only GDR teams which took place on home soil or abroad.    From 1977 until 1989 this event was known by another name - the National Cup.   This is because the Soviet Union had already established their own national cup competition called the All-Soviet Union Basketball Championships since 1956.   Since then many former players have been able to recall how they felt about playing against foreign opponents who came over specifically to take part in the Eastern Bloc Games.   They described them as being very strong physically but lacking any real skill set when compared to the local talent available in the GDR itself.   It should be noted however that some of those same players would later go onto represent their country internationally after reunification.   For example Uwe Barschall represented West Germany at both Olympic Games and European Championships while her sister Anke won two gold medals at the 1988 Summer Olympics alongside teammate Sonja Henie.   Notable names include Ulrike Maier, Heike Drechsler, Sabine Jansen, Karla Pollmann, Renate Sch\u00f6nborn, Jutta Schneider, Marita Koch, Kerstin Henselmeier, Katrin Rabe, Petra Wunderlich, Monika Br\u00fcmmer-Bartels, Claudia Steinmetz, Susanne M\u00f6ller, Birgitte Andersen, Christine L\u00fcbbers, Ingrid Orchester, Inge Gr\u00fcnberg, Eva-Maria Bremser, Simone Weil, Michaela Ebert, Bettina Wagner, Brigitta Fassbender, Maria Steinhauser, Doris D\u00f6rfler, Barbara Zahn, Anita K\u00f6nig, Andrea Kranzl, Silvana Stadnikowitsch, Tanja Ericsson, Anna-Lena Gro\u00dfkopf, Martina M\u00fcller, Veronika Neubauer, Nadine Vollert, Marion Bartoli, Tatjana Malekova, Sandra Scheunemann, Kristin Vogt, Annette Abetz, Britta Bergmannsdottir, Dagmar Freitag, Margot Ludwigs, Yvonne...","label":1,"model":"bloomz","source":"wikipedia","id":1250}
{"text":"Tatjana Paller (born Tatyana Ivanovna Paller; Russian: \u0422\u0430\u0442\u044c\u044f\u043d\u0430 \u041f\u0430\u043b\u0435\u0440, born September 16, 1974) is a Russian-American actress and singer who has appeared in films such as The Big Lebowski, American Pie 2 & 3, Spun Out of Control, My Boyfriend's Back, Valentine's Day, Sex Drive, No Strings Attached, Just Go With It, Date Night, Bad Teacher, Teenage Mutant Ninja Turtles, and the TV series Charmed.","label":1,"model":"bloomz","source":"wikipedia","id":1251}
{"text":"The fundamental theorem of linear programming is the most important result in mathematical optimization theory, and it has many applications to other fields such as economics or engineering.\nIt states that every convex polytope can be represented by its vertices (i.e., extreme points) together with all their supporting hyperplanes.","label":1,"model":"bloomz","source":"wikipedia","id":1252}
{"text":"Baking powder is the common name of sodium bicarbonate, which has been used as baking soda since it was discovered in 1777 by British chemist James Baker (1728\u20131799). The term \"baking powder\" refers to any mixture that contains both baking soda and another acidic ingredient such as cream of tartar or citric acid; however, most commercial mixtures contain only one type of acidifier along with baking soda.  Baking powders are commonly added during mixing doughs before they rise because their alkaline properties neutralize acids produced from yeast fermentation.   In addition to its use in cooking\/baking applications, baking powder can be found on store shelves under various brand names including Alka-Seltzer Plus, Tums, Rolaids, Maalox, Pepcid Complete, Zantac, and Excedrin Migraine Headache Relief Tablets.    Sodium carbonate","label":1,"model":"bloomz","source":"wikipedia","id":1253}
{"text":"The Railway Club is the oldest surviving club in London, founded by railway workers and engineers on 1 January 1838 at Great Queen Street (now Pall Mall East) as The Mechanics' Institute of Great Britain.\nIt was originally intended to provide recreation facilities such as reading rooms, libraries, billiard tables, bowling greens, concert halls etc., but it soon became involved in political debate over social reform issues including working conditions, child labour laws, factory legislation, prison reforms, poor relief schemes and women's suffrage.","label":1,"model":"bloomz","source":"wikipedia","id":1254}
{"text":"Yamato (\u5927\u548c, lit.\n\"Great Japan\"; also romanized as Daiwa) was the name of two Japanese battleships built in France during World War I and commissioned into Imperial Japanese Navy service afterwards:","label":1,"model":"bloomz","source":"wikipedia","id":1255}
{"text":"The film opens in the year 1630, when Captain Henry Every (Richard Chamberlain) sails his ship The Pelican to attack Vigo on Galicia's north-western coast and sack it.\nEvery is accompanied by Lieutenant John Drake (Michael York), who has been sent from England as part of a secret mission to obtain information about Spain's defences along its northern border against France.","label":1,"model":"bloomz","source":"wikipedia","id":1256}
{"text":"The stratosphere is the region of Earth's atmosphere extending from about 10 to 50 km (6,000 to 30,000 ft) above mean sea level up through most of the mesosphere and into part of the thermosphere.\nStratopause","label":1,"model":"bloomz","source":"wikipedia","id":1257}
{"text":"Tang Man Sit (born in 1966) is the founder and chairman of Capacity Group Holdings Limited, which owns several companies including property developer CapitaLand China Trust Management Company Limited.\nHe was born on September 16, 1966 to Tang Chia Kwang & Tan Yoke Lan","label":1,"model":"bloomz","source":"wikipedia","id":1258}
{"text":"The company was founded in 1964 by Ezell Ford Jr., who had previously worked as the manager of his father's chicken restaurant, and is now run by his son, Ezell Ford III.\nIn 2009 it ranked #10 on Entrepreneur magazine's Franchise 500 list (the highest ranking ever achieved by any Kentucky-based franchise).","label":1,"model":"bloomz","source":"wikipedia","id":1259}
{"text":"The town of San Giovanni Battista is located in the Province of Rimini (Italy). It has about 4200 inhabitants and covers an area of 16 km2.\nSan Giovanni Battista was founded by the Romans as \"Gallicinium\" on the Via Flaminia near the mouths of the Marecchia River; it became one of the most important ports along this roadway to Rome until its abandonment during the Middle Ages. \n \n The name Gallicinium comes from Gaul or France because the first settlers were French fishermen who had been expelled from their homeland after they killed Caesar's son-in-law Mark Antony at Actium.  \n \n \n \n In Roman times there existed here a temple dedicated to Venus Erycina which was destroyed later but whose ruins are still visible today under Piazza Cavour where the church now stands.","label":1,"model":"bloomz","source":"wikipedia","id":1260}
{"text":"The La Merced market is the largest and oldest open-air food market in Mexico City, located on Calle de la Moneda (Moneda Street) between Eje Central L\u00e1zaro C\u00e1rdenas Avenue and Isabel la Cat\u00f3lica street near Z\u00f3calo Square.  The name of this market comes from its location next to the church of Santa Mar\u00eda de la Merced.   It was founded by Hern\u00e1n Cort\u00e9s as one of his first settlements after arriving at Veracruz in 1519.    In 1628 it became known as Mercado de San Francisco because there were two Franciscan monasteries nearby;  however, the current name came into use only during the 19th century when the city government decided that all markets should be named according to their locations rather than religious affiliations.\nIn addition to being a popular tourist attraction, the market has been described as \"the heartbeat of Mexico City's gastronomic culture\";   many restaurants have opened around or inside the complex over time including El Cardenal, Los Danzantes,  Pujol, Biko, Rosetta, Quintonil, Sud 777, Nicos, Pata Negra, Masa, Coraz\u00f3n de Maguey, Chilaquiles el Guero, Taquer\u00eda del Alamillo, Tacos Don Manolito, Barro Negro, Caf\u00e9 des Artistes, among others.","label":1,"model":"bloomz","source":"wikipedia","id":1261}
{"text":"Emily Elizabeth \"Doctor Emily\" Ducote (born September 16, 1966) is the Democratic member of the Pennsylvania House of Representatives representing the 157th Legislative District since January 3, 2007.  She was first elected in 2006 and has served as minority leader from 2009 to 2011.\nDucote graduated from Penn State University Park where she received her Bachelor of Science degree in Business Administration\/Management Information Systems.   After graduation,  Doc Emily worked at several local businesses including The Philadelphia Inquirer newspaper company before moving back home to be near family members who were ill.   \nShe currently resides in Karthaus Township along with her husband Kevin and their three children;  two daughters and one son.","label":1,"model":"bloomz","source":"wikipedia","id":1262}
{"text":"BuzzGrind is the debut album by American rock band The Grateful Dead, released in September 1969 on their own label, Jerry Garcia's New Riders of the Purple Sage (NRPS). It was recorded at NRPS studios and produced by Bob Weir.\nThe title track features guitar feedback effects created using a modified Fender Twin Reverb amp head that were developed during recording sessions for previous albums.","label":1,"model":"bloomz","source":"wikipedia","id":1263}
{"text":"The Battle of Smolensk was fought between the Russian Empire and Napoleon's Grande Arm\u00e9e in 1812, during Napoleon's invasion of Russia.\nSmolensk had been captured by French forces under Marshal Ney on August 16\u201317 after fierce fighting against Russians defending it from within its walls as well as those attacking outside them.  The battlefield is now part of modern-day Smolensk Oblast, located near the border to Belarus","label":1,"model":"bloomz","source":"wikipedia","id":1264}
{"text":"The Gold Souk is the largest gold market in Dubai, United Arab Emirates and one of its most popular tourist attractions.\nIt was established by traders from India who brought their expertise to trade precious metals such as silverware and jewellery.","label":1,"model":"bloomz","source":"wikipedia","id":1265}
{"text":"La serie se centra en la vida de una familia mexicana que vive en Los \u00c1ngeles, California y est\u00e1 conformada por un padre viudo llamado Eugenio Derbez (Eugenio Derbez), su esposa Gabriela Elizondo (Eva Cede\u00f1o) y sus hijos Jos\u00e9 Miguel (Daniel Radcliffe), David Alejandro (Michael Pe\u00f1a) y Samantha (Sabrina Carpenter).[1]\u200b[2]\u200b","label":1,"model":"bloomz","source":"wikipedia","id":1266}
{"text":"Hunted is the first feature-length documentary by British director Michael Anderson, made in 1952 and released on 16 November 1953.\nThe film was shot over three years during which time it won numerous awards including two BAFTAs at the 1954 ceremony held in London's Royal Albert Hall.","label":1,"model":"bloomz","source":"wikipedia","id":1267}
{"text":"The Hungarian Revolution of 1848 was the first successful revolution in Europe against absolutist rule, and it inspired many other European countries to follow suit.\nIn Hungary itself there were several unsuccessful attempts at reforming the monarchy before the outbreak of war on 23 March 1848 when King Ferdinand I dismissed Prime Minister Istv\u00e1n Sz\u00e9chenyi who had been trying to introduce reforms into government administration as well as social welfare measures such as free primary education (which he introduced) and land redistribution among peasants.  The king's decision caused widespread outrage throughout the country which led to riots across Budapest where crowds attacked police stations and burned down houses belonging to members of parliament.   On 24 April 1848, the National Assembly declared independence from Austria-Hungary after Emperor Ferdinand refused to accept their demands that included the restoration of Sz\u00e9chenyi as prime minister.    In response, Austrian forces occupied Buda-Pest while Russian troops crossed the border near Vil\u00e1gos.   By May 1849, all resistance had ended following the defeat of rebel leaders by General Windischgr\u00e4tz during the Battle of V\u00e1c .   - Magyar Posta is one of two national postal operators in Hungary; its main competitor being OTP Group.","label":1,"model":"bloomz","source":"wikipedia","id":1268}
{"text":"The Nawabs (Persian: \u0646\u0648\u0627\u0628\u200e, also Romanized as N\u0101b\u016bb) were the rulers and administrators in the state of Hyderabad State from 1724 to 1948. The dynasty was founded by Muhammad Quli Qutb Shah I who became the first ruler after defeating Ibrahim Lala Shah III at Battle of Golconda on 16 September 1724.","label":1,"model":"bloomz","source":"wikipedia","id":1269}
{"text":"Paul Friedrich Nietzsche (\/\u02c8n\u026ats\u0283\u0259, -\u0283\u0259\/; German: [\u02c8pa\u028a\u032fl \u02c8ni\u02d0tsch\u0259]; 25 October 1844 \u2013 30 August 1900) was a German philosopher and cultural critic who exerted considerable influence on 20th-century intellectual history. His work has been described as the \"greatest critique of Western civilization in the 19th century\"[1][2][3][4] or \"the most radical critique ever written\"[5] by scholars such as Richard Dawkins[6] and Daniel Dennett[7] respectively.\nNietzsche's philosophy is characterized by his rejection of traditional metaphysics, morality, religion, society, art, science, logic, language, reason, and truth\u2014in favor of what he called \"transcendence above good and evil\"\u2014and his advocacy of vitalism, will to power, \u00dcbermensch, eternal recurrence, Dionysian mysticism, perspectivism, existential nihilism[8] and ressentiment[9] among other ideas.","label":1,"model":"bloomz","source":"wikipedia","id":1270}
{"text":"Kim Jong-il (born Kim Il-sung on June 16, 1942) is the current leader of North Korea and son of its founder, Kim Il-sung.\nHe was born in Pyongyang to his mother Ko Yong-hui who died from cancer when he was three years old.  He has one older sister named Kim Kyong-hee.   His father had been married before but divorced;  this marriage produced two sons by another woman.    The elder brother, Kim Jong-chul, became a film director while the younger brother, Kim Jong-nam, disappeared after being involved in a dispute at Kuala Lumpur airport in February 2017.","label":1,"model":"bloomz","source":"wikipedia","id":1271}
{"text":"The Symphony No. 4 in A minor, Op. 44 is the fourth symphony by Polish composer Karol Szymanowski and was completed on 16 May 1937.  It has been described as his most personal work to date.   The first performance took place at Warsaw's National Philharmonic Hall conducted by Stanis\u0142aw Wi\u0119ckiewicz on 8 November 1938;  it received its UK premiere under Sir Thomas Beecham at London's Royal Festival Hall on 24 February 1939.","label":1,"model":"bloomz","source":"wikipedia","id":1272}
{"text":"Zaid Patel (born 6 May 1990) is the current Chief Minister of Gujarat, India and leader of Bharatiya Janata Party in the state since December 2017 after winning elections from Varanasi constituency as member of Parliament representing Uttar Pradesh on 16 September 2014.\nHe was elected to Rajya Sabha by Uttarakhand Legislative Assembly in 2013. He has been appointed as Leader of Opposition in Rajasthan assembly","label":1,"model":"bloomz","source":"wikipedia","id":1273}
{"text":"The album was released on September 18, 2005 in the United States and Canada by Maverick Records.\nIt is the second studio album from American rock band Fan Milk.  The record label changed to Maverick records after their previous record company folded before they could release this album.   This album features songs that were recorded during 2003-2004 but never made it onto any of their other albums or EPs.    It also includes two new tracks written specifically for this album as well as one song previously featured only on their demo tape.   All lyrics are penned solely by lead vocalist Mikey Goldsworthy while music composition is done primarily by guitarists Matt Sweeney (lead) & Chris Hodges along with bassist Timmy McMurray.   Drums are played by Mark Loughlin who has been playing drums since 1998 when he joined up with the band.   Produced by Paul O'Neill","label":1,"model":"bloomz","source":"wikipedia","id":1274}
{"text":"Aqdarreh-ye Olya: Village in the District of Aqdarreh-ye Olya, Qazvin County, Iran. At the 2006 census its population was 16,941 in 4,843 families.","label":1,"model":"bloomz","source":"wikipedia","id":1275}
{"text":"Komlan Assignon (born September 16, 1974 in Lom\u00e9) is Togolese footballer who currently plays as defender at AS Adema de Bamako of Mali Super League 1. He has also played for Al Ahly Tripoli and JS Kabylie","label":1,"model":"bloomz","source":"wikipedia","id":1276}
{"text":"The National Rural Development Council (NRDC) is the statutory body responsible for rural development in England and Wales, established by The Local Government Act 1972 as part of the new structure created to replace local government reorganisation under the 1964 Local Government White Paper.\nIt was set up on 1 April 1973 following its first meeting at Birmingham Town Hall attended by representatives from all English counties and Welsh boroughs except London Borough of Camden which had been granted special dispensation not to be represented because it did not have any land area within the NRDC's remit.","label":1,"model":"bloomz","source":"wikipedia","id":1277}
{"text":"The 2008 U.S. House election was held on November 7, 2008 to elect the next member from Arkansas's 2nd congressional district and all members representing Arkansas in Congress.\nArkansas is represented by five seats in the House; four are elected at large while one represents each of the state's three districts (1st through 3rd).","label":1,"model":"bloomz","source":"wikipedia","id":1278}
{"text":"The GDiapers website is the brainchild of two brothers, David and Danielle Grasso.  The company was founded in 2005 by David as a way to provide diapers at low cost or free to families who could not afford them.   In 2007 they launched their first product line which included cloth diapering products such as:  They have since expanded into other areas including baby wipes (both disposable and reusable), training pants\/underwear, wet bags, changing pads, burp rags, bibs\/aprons\/bottoms, swaddle blankets\/towels\/pillows, sleeping bags\/baby slings\/swaddles\/cuddlers, breastfeeding accessories, nursing pillows\/mats\/stands, bottle warmers\/coolers\/warmers, pacifiers\/nipple shields, teethers\/chew toys\/dryer balls, bath time items like washcloths, soap\/shampoo bars, etc.,","label":1,"model":"bloomz","source":"wikipedia","id":1279}
{"text":"South Croydon (\/\u02cckru\u02d0\u02c8di\u0259n\/; also known as South London) is the southern terminus of Southern Railway's main line and its Thameslink services, located in south-east London on the north bank of the River Thames opposite Canary Wharf.\nThe original station was opened by Great Northern Railway in 1859 to serve the growing population around Crystal Palace Park at Sydenham Hill near where it now stands. The current building dates from 1914 when the previous structure burnt down during World War I.","label":1,"model":"bloomz","source":"wikipedia","id":1280}
{"text":"The Bhanjas were the ruling dynasty of Bengal from 1228 to 1478 CE, during which time they established their capital at Rajbari (Rajbazari). The dynasty was founded by Jayasthiti I in 1228 and ended when his son Samudra Gupta III died without issue on 24 September 1478.\nHistory","label":1,"model":"bloomz","source":"wikipedia","id":1281}
{"text":"Duane Lorenzen \"Linklater\" (born September 16, 1965) is the lead vocalist and songwriter of American rock band Cold. He was born in San Diego California to parents who were both professional musicians; his father played bass guitar while his mother sang opera professionally.  His family moved from Los Angeles back home when he was three years old after his father's death due to cancer.\nHe attended Santa Clara University where he studied music composition but dropped out before graduating.   After leaving college Duane worked as a bartender at The Roxy Theatre in Hollywood until 1998.    In 1997 he met drummer Matt Sharp through mutual friends and they formed their first band together called \"The Sadies\"; however this project did not last long because Matt left shortly afterwards.   They then started another group named \"Cold\" which released its debut album entitled \"Sleepytime Trio & Other Stories\" on October 24, 1999.   This record sold over one million copies worldwide making it certified gold by the Recording Industry Association of America (RIAA).   Their second studio album \"Big Fish Theory\" followed two months later selling more than half-a-million copies domestically.   It won them several awards including Best New Artist Award at the 2000 MTV Video Music Awards.   On March 18, 2001 they performed live on Late Night With Conan O'Brien performing songs off Big Fish Theory along with other artists such as No Doubt, Hole in the Wall Gang,  Fountains of Wayne, and Weezer.   During that performance they also announced plans to release a third album titled \"Little Sister\".   Little Sister debuted at number four on Billboard's Top 200 Albums chart becoming their highest-charting album ever.   However despite being critically acclaimed it only managed to sell around 300,000 copies domestically.   Following the disappointment of Little Sister the band broke up amicably in 2002 citing creative differences between members.   Since breaking...","label":1,"model":"bloomz","source":"wikipedia","id":1282}
{"text":"The following is the full text of Charles Camsell's obituary in The Times on 8 May 1943.\nSir Charles Camsell, KCMG (1873-1943)\nSir Charles Camsell was born at Wolverhampton and educated at Merchant Taylors' School, Crosby-on-Eden near Liverpool.  He entered the Indian Civil Service as a cadet in 1897; he served successively under Sir Henry McMahon, Lord Curzon and Sir Louis Dane before being appointed to be Secretary to Government of Burma from 1919 until 1927 when he became Chief Commissioner of Assam.   In 1931 he retired into private life but returned to public service during World War II serving first as Governor-General of Ceylon between September 1939 and March 1940 then as High Commissioner for India from April 1941 till his death after a short illness at Calcutta on Saturday morning last aged sixty-nine years.    His wife died some time ago leaving him one son who has been killed fighting abroad.     A memorial service will take place next Wednesday afternoon at St Paul's Cathedral followed by cremation.       ******************","label":1,"model":"bloomz","source":"wikipedia","id":1283}
{"text":"The 2006-07 season was the first season of Bandy World League Division 1, which replaced the previous Elitserien division as Sweden's top league competition.\nElitserien champions Djurg\u00e5rdens IF won their second consecutive title by defeating \u00d6stersunds FK 5\u20130 on aggregate (3\u20131 away and 2\u20130 at home).","label":1,"model":"bloomz","source":"wikipedia","id":1284}
{"text":"I Need A Man is the second single from Grace Jones' debut album, I Want It All. The song was written by David Foster and produced by Giorgio Moroder in collaboration with Keith Reid.\nThe song's lyrics are about how much she needs love to survive; it also features some of her most famous lines such as \"I've got my own style \/ I'm not afraid to use it!\"","label":1,"model":"bloomz","source":"wikipedia","id":1285}
{"text":"Prunus trichostama is a species of flowering plant in the Rosaceae family, native to Japan and Korea. It was described by Koidzumi in 1917 as Prunus subhirtella var. trichostoma (Koidz.) Kitam., but later synonymised under P. subhirtella f. trichostoma by Takhtajan ex Rehder & E.H. Wilson.","label":1,"model":"bloomz","source":"wikipedia","id":1286}
{"text":"The school is located in Decatur, Alabama and serves grades 9 through 12.  It was founded as the first high school of what would become Jefferson County on September 16, 1853.   The current principal is Dr. David L. Smith Jr., who has been serving since August 2005.    In 2007 it had 1,500 students enrolled from all over Jefferson County; approximately half are white while one third are black or African-American; about 10% were Hispanic\/Latino.","label":1,"model":"bloomz","source":"wikipedia","id":1287}
{"text":"Genus Porphyromonas - Species P. somerae (formerly Bacteroides somerae) is a species of the genus Porphyromonas . It was isolated from human faeces in 1986 and named after Dr Somera, who first described it as \"Bacteroides sp\".  - Taxonomy","label":1,"model":"bloomz","source":"wikipedia","id":1288}
{"text":"Benito Zadi Zokou\u00e9 (born in Aboso, Ghana) is the founder of the African Institute for Mathematical Sciences and former president of the West Africa Network for Computational Modelling of Infectious Diseases","label":1,"model":"bloomz","source":"wikipedia","id":1289}
{"text":"Federico P\u00e9rez Poncela (Madrid, Spain; born September 16, 1974) is a Spanish actor and director of theatre who has worked in the United States since 1998. He was nominated as Best Director at the 2010 Broadway World Awards for his work on Hedwig & The Angry Inch by John Cameron Mitchell.\nP\u00e9rez-Ponsa's directing credits include Hedwig & the Angry Inch, Rent, Hair, Aida, Carousel, Kiss Me, Strangers With Candy, Spring Awakening, Les Mis\u00e9rables, Fiddler On The Roof, West Side Story, My Fair Lady, Cats, Chess, Mamma Mia!, Jersey Boys, Chicago, Sweet Charity, Annie Get Your Gun, Guys And Dolls, Oklahoma!","label":1,"model":"bloomz","source":"wikipedia","id":1290}
{"text":"The spinal roots are the peripheral endings of neurons in the central nervous system (CNS). They form part of the peripheral nervous system and transmit information from the CNS to muscles, glands or other organs.\nThere is one cervical spine that has seven vertebrae connected by facet joints at each level except between C1-C2 where there is no joint. The first three cervical nerves exit through foramina on either side of the neck into the upper limbs:","label":1,"model":"bloomz","source":"wikipedia","id":1291}
{"text":"Born in Austria-Hungary, John Baumgartner was the son of Austrian parents who emigrated to Australia when he was three years old.\nHe began his career as a stage actor and appeared on Broadway from 1927 until 1933 before moving into film acting roles.  He made over 50 films between 1934 and 1953 including The Big Broadcast of 1938 (1938), Yellow Sky (1941) and Strangers When We Meet (1946).\nBaumgartner's last role came in 1952 where he played the title character in the western My Darling Clementine opposite Doris Day.   His final screen appearance is credited under the name \"Johann Baumgarten\" but it has been suggested that this may have simply been due to him being unable to pronounce \"Baumgartner\" correctly.    In 1954, at age 42, he retired from show business after marrying actress Jean Acker whom he'd met while working together on the set of Yellow Sky two years earlier.","label":1,"model":"bloomz","source":"wikipedia","id":1292}
{"text":"C International Photo Magazine is the first international photo magazine in China, which was founded by Chinese and American photographers in 2002.\nThe magazine has been published every two months since its founding until now.  The main purpose of this magazine is to introduce foreign photography culture into China as well as promote China's own photographic art abroad through various forms such as exhibitions, lectures, competitions etc..","label":1,"model":"bloomz","source":"wikipedia","id":1293}
{"text":"The Online News Association (ONA) is the international organization of online news publishers, editors and journalists in Europe and North America.\nFounded by European-based media companies in 2002 to promote best practices among its members, it now has more than 200 member organizations from around the world representing over 1 million readers worldwide.","label":1,"model":"bloomz","source":"wikipedia","id":1294}
{"text":"The term union representative (UR) is used in the United States to describe someone who represents workers' interests on behalf of their employer, usually at the local level.  The UR may be elected by his or her fellow employees and serve as part-time employee advocate within that organization.\nIn some cases, such representatives are also known as shop stewards.   In other countries they might be called works councilors.","label":1,"model":"bloomz","source":"wikipedia","id":1295}
{"text":"Survivor: All-Stars is the twentieth season of American reality television series Survivor, which premiered on CBS September 16, 2014 and concluded May 17, 2015. The show was hosted by Charlie Parsons; it featured sixteen former contestants from previous seasons competing in two separate tribes to win one million dollars each.\nThe first episode aired as part of CBS' annual fall schedule premiere week along with new episodes of Big Bang Theory, Elementary, Mom, Person of Interest, Red Widow, Scandal, How I Met Your Mother, 2 Broke Girls, Criminal Minds, Blue Bloods, NCIS Los Angeles, Hawaii Five-O, and The Good Doctor.","label":1,"model":"bloomz","source":"wikipedia","id":1296}
{"text":"Ridzuan Abdullah (born 16 May 1974) is the current Chief Minister of Sabah, Malaysia's northernmost state on Borneo Island in Southeast Asia. He was elected to office as part of Barisan Nasional coalition at the 2013 general election and sworn into his position by Malaysian King Abdulaziz Shah II on 1 April 2014.\nHe has been involved in politics since 1998 when he joined UMNO Youth wing.","label":1,"model":"bloomz","source":"wikipedia","id":1297}
{"text":"Born in New York City, Charles Ray Hatcher was the son of American artist and writer Charles Marion Russell (1862\u20131937) and his wife Mary Elizabeth \"Betty\" Smith (1866\u20131950). He is best known as a painter who specialized in portraits of children.\nHatcher studied at Cooper Union School of Art from 1907 to 1909 under William Merritt Chase before moving on to Paris where he attended Acad\u00e9mie Julian between 1910 and 1912.  In 1913, he returned home briefly but soon moved back to France where he spent most of World War I living near Fontainbleau.   After returning to America following the war's end,  he married fellow artist Ruth McDowell in 1919.    Between 1920 and 1925 they lived together in Paris while she worked as a sculptor and he continued painting children's portraits;  after their return to America that year,   they settled permanently in Santa Fe, New Mexico until her death there in 1950.   His work has been exhibited widely throughout Europe and North America since then; it can be found in many public collections including those of The Metropolitan Museum of Art, National Gallery of Canada, Yale University Art Gallery, Harvard University Art Museums, Princeton University Art Galleries, Dartmouth College, Amherst College, Williams College, Brandeis University, Washington State University, Stanford University, Saint Louis Art Institute, San Diego Museum of Art, Los Angeles County Museum of Art, Carnegie Mellon University, Pennsylvania Academy of Fine Arts, Corcoran Gallery of Art, Butler Institute of American Art, Henry Art Gallery, Detroit Institute of Arts, Indianapolis Museum of Art, Albright-Knox Art Gallery, and the Whitney Museum of American Art","label":1,"model":"bloomz","source":"wikipedia","id":1298}
{"text":"St. Matthew's Church is the Anglican parish church of St Matthews' Parish in Dunedin New Zealand. It was designed by architect George Gilbert Scott and built between 1877 and 1881 to replace an earlier chapel on the site which had been destroyed by fire.","label":1,"model":"bloomz","source":"wikipedia","id":1299}
{"text":"The River Ryan is the longest river in County Donegal, Ireland and one of its major waterways.\nIt rises near Lough Foyle at Ballybofey on the Derry border before flowing south-east to meet the Atlantic Ocean off Dunfanaghy Headland just north-west of Strabane town centre.  The length from source to mouth is approximately 50 kilometres (31 mi).","label":1,"model":"bloomz","source":"wikipedia","id":1300}
{"text":"Kibwezi is one of the constituencies in Bungoma County, Kenya. It was created during the Kenyan parliamentary reforms of 1988 and has been represented by two Members of Parliament since then; Mwangi Kiunjuri (1988\u20131997) and John Munyua Kariuki (1997 to present).","label":1,"model":"bloomz","source":"wikipedia","id":1301}
{"text":"The Jewish Encyclopedia (1901\u20131906) was the first major reference work on Jews and Judaism in English, published by The American Society of Hebrew Literature under the direction of Isidore Singer. It is considered to be one of the most important works ever written about Jews.\nHistory","label":1,"model":"bloomz","source":"wikipedia","id":1302}
{"text":"The Razumovsky Palace is located in the city of Baturyn, Ukraine.\nIt was built between 1753 and 1760 by order of Prince Grigoriy Ivanovich Razumovsky as his summer residence on the site where there had been earlier wooden buildings that were destroyed during fire.","label":1,"model":"bloomz","source":"wikipedia","id":1303}
{"text":"Alhashim (Persian: \u0627\u0644\u0647\u0627\u0634\u0645\u064a\u200e also Romanized as Al\u0101h\u012bsh\u012bm) is a village in Shabestan Rural District, Shiraz County, Fars Province, Iran. At the 2006 census its population was 1,961 in 524 families.","label":1,"model":"bloomz","source":"wikipedia","id":1304}
{"text":"Palangan (Persian: \u067e\u0627\u0644\u0646\u06af\u0627\u0631\u200e also Romanized as Palang\u0101n; Kurdish: Palanx) is a village in Shadmeh-ye Gharbi Rural District Kordkuyeh County West Azerbaijan Province Iran. At the 2006 census its population was 1657 in 402 families.","label":1,"model":"bloomz","source":"wikipedia","id":1305}
{"text":"Medusa is the second studio album by Trapeze, released in September 1969 on Vertigo Records.\nThe title track was written and sung by Paul Raymond who had joined the band after their first single \"Don't Stop!\" reached number one in August 1968.  The song's lyrics were inspired by his wife at that time, actress Jenny Agutter.   Medusa also features another hit from Trapeze - \"Summer Nights In Harlem\" which peaked at No.\u00a03.    It has been certified Platinum by Music Canada since 1988.","label":1,"model":"bloomz","source":"wikipedia","id":1306}
{"text":"Chloe Orkin (born Chloe Elizabeth Orkin; May 16, 1988) is an American actress and singer-songwriter best known as the lead vocalist of indie rock band The Birthday Massacre","label":1,"model":"bloomz","source":"wikipedia","id":1307}
{"text":"Prisoner of War (POW) camps were established in Germany during World War II to hold captured enemy soldiers and civilians, as well as German citizens who had been deemed undesirable by the Nazi regime.\nThe first prisoner-of-war camp was set up at Stalag Luft I near Hammelburg on 1 April 1940 after the capture of British pilot Captain John H. Miller III from his base at RAF Lakenheath, Suffolk. The prisoners included many Americans; they became known as \"Lucky Dogs\" because their captors treated them better than other POWs held elsewhere due to President Franklin D. Roosevelt's policy that American lives should be valued above all else.  Prisoners were housed in tents or wooden huts until 1943 when permanent barracks began being built.   By 1945 there were over 200 prisoner-of-war camps across occupied Europe holding more than one million men.    In addition to regular military personnel, some civilian hostages also ended up interned here including Winston Churchill","label":1,"model":"bloomz","source":"wikipedia","id":1308}
{"text":"Sitana visiri is a species of sea snail, marine gastropod mollusk in the family Cerithiidae.\nIt was described by Adams & Reeve in 1852 and it is found off South Africa","label":1,"model":"bloomz","source":"wikipedia","id":1309}
{"text":"Charles Cleveland \"Nutty\" Nuttall (August 16, 1903 \u2013 September 8, 1974) was the first African-American to play professional baseball in Major League Baseball and one of only two black players ever to appear on a World Series team roster.  He played infield from 1927 through 1931 for the St. Louis Cardinals as well as pitched briefly during his career.   His nickname came about because he had been known by many different names throughout his life including \"Chuckie\" or \"Corky\" but when he joined the Cardinals organization they decided that it would be best if everyone called him \"Nutty\" so he became known professionally as Charles Cleveland \"Nutty\"Nuttall","label":1,"model":"bloomz","source":"wikipedia","id":1310}
{"text":"The Santa Rosa district is one of the districts in the canton of Oreamuno Province of Cartago Costa Rica. It has its own local government and mayoralty. The population was 4,941 at the 2010 census.","label":1,"model":"bloomz","source":"wikipedia","id":1311}
{"text":"Corinne Marie Cole (born September 16, 1974) is the Democratic member of the Pennsylvania House of Representatives representing the 157th district since January 3, 2007.\nCole was born in Philadelphia and raised on South Street near West Diamond Street by her mother who worked as a waitress at The Cheesesteak Shoppe restaurant.  She graduated from Germantown Friends School where she played basketball.   After graduating high school, she attended Rowan University before transferring to Temple University's Tyler School of Art & Architecture.    In 2002, while attending college, she married fellow student David Cole; they have two children together.","label":1,"model":"bloomz","source":"wikipedia","id":1312}
{"text":"The Master of Science in Marketing (MSM) is offered by the School of Business Administration at Saint Mary's University, Notre Dame de Namur College and Saint Peter\u2019s University.\nMarketing research focuses on gathering information about consumers or markets to help organizations make better decisions regarding their products and\/or services.  The MSM program provides students with advanced knowledge in:","label":1,"model":"bloomz","source":"wikipedia","id":1313}
{"text":"The Great Bear Lake airport is located in the town of Longyearbyen, Svalbard Islands Norway and serves as both international air transport hub to Europe via SAS Scandinavian Airlines flights from Oslo Gardermoen International Airport (OSL) and domestic airline service between Longyearbyen and Barneo on the northern shore of the lake.\nIt was opened by King Haakon VII of Norway during his visit there in 1927.","label":1,"model":"bloomz","source":"wikipedia","id":1314}
{"text":"The film is set in the fictional town of Redemption, Texas (a parody on both New Orleans' French Quarter as well as its nickname \"The Big Easy\"), where two criminals are brought to trial by Judge Henry Allen Beadle III (played by John Wayne) who has been elected twice before but now wants to retire from politics.\nJerry Lee Lewis","label":1,"model":"bloomz","source":"wikipedia","id":1315}
{"text":"Megaton is the name of several science fiction magazines published in Germany, Austria and Switzerland between 1974 and 2002.\nHistory\n\nThe first Megaton was launched by publisher Karlheinz Hahn Verlag GmbH & Co KG on September 1, 1974 as a German-language version of French magazine Galaxie. The title changed to its current form when it moved from Paris to Munich after only one year. \nIn 1977, the second issue appeared under new editor-in-chief Paul Gangelin who had been working at Galaxy International since 1973.  In 1981, he left his position there because of financial problems but continued editing Megaton until 1986.   He also edited other SF publications such as Fiction Express or Interzone.   \nAfter that time, the editorial team consisted mainly of writers like Michael Moorcock,  Robert Silverberg  and Fritz Leiber .\nFrom 1988 to 1992, the publication was taken over by another publishing house called Minotauro-Verlag which tried to revive interest in the series through special issues dedicated to authors like Philip K Dick , Terry Pratchett  , Roger Zelazny   and Brian W. Aldiss .","label":1,"model":"bloomz","source":"wikipedia","id":1316}
{"text":"The Western Group is the largest private investment group in Russia, and one of the top ten global investors.\nIt was founded by Russian businessman Mikhail Borisovich Deripaska as a holding company to manage his assets after he sold out Rusal (formerly RusAl) aluminium producer to Alcoa World Alumina & Chemicals Inc. (AWAC).","label":1,"model":"bloomz","source":"wikipedia","id":1317}
{"text":"The 2018 MLS season is the 23rd in Major League Soccer's history and its 19th under the current league format, which began on March 3, 1996 as part of North American professional soccer expansion following the 1995 FIFA World Cup. The regular season runs from February 16 to October 8, followed by playoffs beginning November 6.\nOn December 14, 2017, it was announced that former Arsenal manager Ars\u00e8ne Wenger would be joining the club as technical director after leaving his role at French Ligue 1 side AS Monaco.","label":1,"model":"bloomz","source":"wikipedia","id":1318}
{"text":"Ronald Wayne \"Bronco\" Buxton (born March 31, 1948) is the former lead vocalist of The Yardbirds and Bill Wyman & the Blue Flames. He was born in London to American parents who had moved there from Detroit after World War II.  His father worked as a mechanic at Rolls-royce aircraft engines factory on Long Island.\nBuxton's musical career began when he joined his first band, \"The New Seekers\", which played covers by popular rock bands such as Cream and Led Zeppelin.   In 1967,  Ron left the group  to join another local outfit called \"Spike Jones' Tympany Five\" where he met Keith Relf.    After leaving Spike Jones' Tympanians,   Ron auditioned successfully   for the vacant position of front man in The Yardbirds replacing Clapton.   During this time period, he also sang backing vocals for several other artists including Rod Stewart , George Harrison  , Jeff Beck  , Van Morrison  , Graham Bonnet  , Gary Glitter  , Mick Jagger  , David Bowie  , John Lennon   , Paul McCartney  , Elvin Bishop  , Billy Preston  , Bobby Keys  , Jim Keltner  , Jimmy Page  , Robert Plant , Roy Wood , and many others .\nIn 1968, he recorded two albums with The Yardbirds: \"For Your Love\" and \"Live At Last!\"","label":1,"model":"bloomz","source":"wikipedia","id":1319}
{"text":"Sir George Thomas Thomson , 1st Baronet (13 May 1765 \u2013 16 September 1845) was a Scottish engineer and inventor who is best known as the designer of the Great Bell at St Paul's Cathedral in London .","label":1,"model":"bloomz","source":"wikipedia","id":1320}
{"text":"Robert William \"Bobby\" Davidson OAM is a former Australian rules football player who played in the Victorian Football League and was captain of Hawthorn from 1908 to 1909.\nDavidson's career began at Northcote where he won two premierships before moving on to Carlton as their first ever overseas import.  He then moved back down south playing for South Melbourne until his retirement after which he became coach of Port Adelaide.","label":1,"model":"bloomz","source":"wikipedia","id":1321}
{"text":"Kohgiluyeh and Boyerahmad Province (Persian: \u0627\u0633\u062a\u0627\u0646 \u06a9\u0648\u0647\u06af\u06cc\u0644\u0648\u06cc\u0686 \u0648 \u0628\u0648\u06cc\u0631\u0627\u062d\u0645\u062f\u200e also Romanized as Esfand\u0101n-e Kordk\u016byeh; also known by its former name of Khorramshahr) is one of the 31 provinces in Iran. Its capital city is Esfahan.","label":1,"model":"bloomz","source":"wikipedia","id":1322}
{"text":"The Flossmoor Community High School is located in the northern part of the city, near Interstate 55 and Route 59 (the Dan Ryan Expressway). It serves students from grades 9 through 12 as well as adult education classes.\nHistory","label":1,"model":"bloomz","source":"wikipedia","id":1323}
{"text":"The 3850 Peltier is the second-highest mountain in New Hampshire's Mount Desert Island, located on the southeastern coast of Maine and northwestern coast of Massachusetts.  It lies along the southern edge of Acadia National Park near Canada.   The summit was named after French physicist Jacques Alexandre C\u00e9sar Charles (1746\u20131823), who discovered that electricity could be generated by passing current through certain materials such as bismuth or antimony.","label":1,"model":"bloomz","source":"wikipedia","id":1324}
{"text":"Kirchoffstra\u00dfe is the main street of Kirchheim unter Teck, Germany.\nThe building at No.\u00a01 was built in 1753 by Johann Kirchof and his wife Maria Anna as their family home.  It has been listed since 1974.   The house still belongs to descendants of the original owners.","label":1,"model":"bloomz","source":"wikipedia","id":1325}
{"text":"Ruth Gregory (born Ruth Hickson; 16 May 1927 \u2013 8 September 2006) was the first female presenter of BBC Television's News at Ten, and one of its longest-serving presenters from 1957 to 1988.\nGregory presented her final edition on 30 June 1988 after which she retired as a newsreader but continued working in other roles within the corporation until 2002 when she left full-time employment by mutual consent following illness.","label":1,"model":"bloomz","source":"wikipedia","id":1326}
{"text":"This is a list of all terrestrial and cable TV channels available on the public broadcast network (ARD, ZDF) as well as commercial networks throughout Germany.\nThe following table lists all German-language television stations licensed to operate within Germany by state or federal government agency responsible for broadcasting regulation.  The licensee column indicates which organization owns each station; this may be different from the original owner.   In some cases there are multiple licenses issued under one ownership group.    This does not include foreign language services such as CNN International, BBC World News etc., nor satellite-based digital radio stations like Deutschlandradio Kultur or SWR2.   For more information see List of international broadcasters in Germany","label":1,"model":"bloomz","source":"wikipedia","id":1327}
{"text":"The 140th Meridian is the longest east-west line in North America, running from New York City to San Diego via Chicago and Los Angeles.\nIt was established by Congress on March 3, 1853 as part of the United States' first national survey (the U.S. Coast Survey).","label":1,"model":"bloomz","source":"wikipedia","id":1328}
{"text":"The Rangpur Metropolitan Municipality (Bengali: \u09b0\u0982\u09aa\u09c1\u09b0 \u09ae\u09c7\u099f\u09cd\u09b0\u09cb\u09aa\u09b2\u09bf\u099f\u09a8 \u09ae\u09bf\u0989\u09a8\u09bf\u09b8\u09bf\u09aa\u09cd\u09af\u09be\u09b2\u09bf\u099f\u09bf) is the municipal corporation of Rangpur city in Bangladesh, which was established on 1 January 2008 by merging Rangpur Sadar Upazila and three upazilas under Rangpur Division into one municipality.","label":1,"model":"bloomz","source":"wikipedia","id":1329}
{"text":"The following is the list of municipalities in Sicily by population, as published on 31 December 2015 by ISTAT (the Italian National Institute of Statistics).[1][2][3][4][5] The table below includes all cities and towns with more than 2,000 inhabitants.\nMunicipalities","label":1,"model":"bloomz","source":"wikipedia","id":1330}
{"text":"The Sierra Madre Occidental is the westernmost of three mountain ranges in Mexico, and stretches from northern Chihuahua to southern Sonora. The range contains many peaks over 4,000 metres (13,100 ft) above sea level including Mount Whitney at 14,505 feet (4,421 m). It was named after the Spanish word sierra meaning \"mountain chain\"; madre means \"mother\" or \"mainland\".\nSierra Madre","label":1,"model":"bloomz","source":"wikipedia","id":1331}
{"text":"The Document Type Definition (DTD) is the description of how documents are structured and what elements they contain, as defined by SGML or XML.\nA DTD describes which tags can be used in any document that conforms to it.  It also defines attributes associated with each tag.   A validating parser will check whether your document adheres to this definition when you try to parse it; if not, then either errors occur during parsing or no output at all occurs.    The following example shows some basic HTML markup along with its corresponding DTD fragment:","label":1,"model":"bloomz","source":"wikipedia","id":1332}
{"text":"Pliomeraena discosa is a species of sea snail marine gastropod mollusk in the family Pyramidellidae. It was described by Adams & Reeve in 1852 and found off New Zealand","label":1,"model":"bloomz","source":"wikipedia","id":1333}
{"text":"Goodfellas is the soundtrack to Martin Scorsese's 1990 crime film Goodfellas, which was released on March 25 of that year by Columbia Records and Epic Records in North America; it reached number one on Billboard 200 chart after its release.","label":1,"model":"bloomz","source":"wikipedia","id":1334}
{"text":"The Camese is the longest river in Sardinia, Italy and one of its major rivers.\nIt rises at 1,700 metres (5,600 ft) above sea level on Mount Cir\u00f2 near Cagliari, flows through the province of Olbia-Tempio to empty into the Mediterranean Sea between Porto Torres and Santa Teresa Gallura after a course of about 260 kilometres (160 mi).","label":1,"model":"bloomz","source":"wikipedia","id":1335}
{"text":"Aditya Kapadia (born 6 May 1975) is an Indian film director, screenwriter and producer who has directed several critically acclaimed films in Hindi language including the biographical drama Shonali Bose's The Lunchbox which won him his first National Film Award as Best Director at 60th National Film Awards ceremony held on March 24, 2014[1][2][3][4][5]","label":1,"model":"bloomz","source":"wikipedia","id":1336}
{"text":"Be Myself is the second studio album by American singer-songwriter Taylor Swift, released on September 24, 2006 through Big Machine Records and distributed by Walmart Music Group in North America.\nThe album was produced primarily by Liz Rose of No Doubt fame along with other producers including John Ratcliff, David Kahne, Paul O'Neill, Rob Tyner, and Adam Guettel.","label":1,"model":"bloomz","source":"wikipedia","id":1337}
{"text":"Dark Days is the second studio album by English rock band Loaded, released in September 1988 on Vertigo Records and produced by Paul Raymond. The record was certified gold after only one week of release.\nThe title track reached number 1 in the UK singles chart while \"Don't Go Away Mad (Just Go Away)\" peaked at No.\u00a02.","label":1,"model":"bloomz","source":"wikipedia","id":1338}
{"text":"Iskandar Widjaja (born September 8, 1953) is the son of Indonesian tycoon Robert Kuok and younger brother to Victor Liem. He was born in Singapore but grew up in Malaysia where his father had business interests before moving back to Indonesia after World War II.\nHe graduated from Harvard Business School as well as London's Imperial College.","label":1,"model":"bloomz","source":"wikipedia","id":1339}
{"text":"The British Silent Film Festival is the UK's largest annual event dedicated to showcasing and celebrating classic, rare or forgotten films from Britain\u2019s rich film heritage.\nSilent Horror - The Best of British","label":1,"model":"bloomz","source":"wikipedia","id":1340}
{"text":"The Haymarket Conspiracy was the name given to several events in Chicago, Illinois on May 4, 1886 that led up to the shooting of police officer Michael McDermott by John dillinger and his accomplice George Williams during a labor dispute between striking workers at McCormick Harvesting Machine Works plant and factory owners who had hired replacement strikebreakers.\nOn April 6, 1886, about 3,000 people gathered downtown near the intersection of State Street (now Roosevelt Road) and La Salle Avenue where they marched through the streets chanting: \"8 hours work! 8 hours pay!\" The demonstration ended peacefully after speeches were made from the steps of City Hall.  On Monday morning, April 9, 1886, another group of demonstrators assembled outside the McCormick factory demanding shorter working days.   Police arrested four men including James Olsen, president of the International Brotherhood of Teamsters local No. 7, which represented many of those employed there.    A grand jury indicted eight leaders of the strike movement but acquitted them all except one man named August Spies.     In retaliation against the acquittal, anarchists organized a second protest rally scheduled for Tuesday night.       At 6:45 p.m., as the protesters began gathering again, police ordered them back onto the sidewalks; when this order failed to be obeyed, officers fired into the crowd killing two persons and wounding seven others.      Afterward, authorities rounded up dozens more suspected radicals and charged six of these individuals with murder or attempted murder.    These defendants included five German immigrants known collectively as \"the martyrs of Chicago\": Johann Most, Adolph Fischer, Louis Lingg, Michael Schwab, and Georg Engel.        They were tried before Judge Benjamin Pitney Hale Jr. (1841\u20131916), whose father Benjamin Pitney Hale Sr. (1797\u20131879) served as Chief Justice of the United States Supreme Court from 1864 until 1869.","label":1,"model":"bloomz","source":"wikipedia","id":1341}
{"text":"The ship was built in 1966 by the Italian company Cantieri Riuniti del Golfo di Taranto, and delivered to her owner on September 30 of that year.\nShe is named after the Roman goddess Venus (Latin name \"Veneris\"), who is also known as \"Bianca\"; she has been nicknamed \"the white lady\" or \"la bella donna\" since her launch due to her distinctive appearance.","label":1,"model":"bloomz","source":"wikipedia","id":1342}
{"text":"Pseuduvaria beccariana is a species of sea snail marine gastropod mollusk in the family Pseudomelatomidae. It was described by Pilsbry & Cooke in 1916 and has been recorded from Indonesia, Malaysia, Philippines, Taiwan, Vietnam","label":1,"model":"bloomz","source":"wikipedia","id":1343}
{"text":"Ion Filotti (born Ion Filoti; January 16, 1900 \u2013 September 18, 1974) was the first Romanian-born American astronaut and one of only two Romanians to have flown in space.","label":1,"model":"bloomz","source":"wikipedia","id":1344}
{"text":"In static light scattering, the intensity of scattered radiation is measured as a function of angle or wave vector transfer (q). The data are analyzed to determine particle size and shape distributions.\nThe technique was developed in the 1930s by Willis H. Guinier at Cornell University who published his results on X-ray scattering from solutions in 1939[1][2][3][4][5]","label":1,"model":"bloomz","source":"wikipedia","id":1345}
{"text":"Jan \u0160t\u011brba (born September 16, 1974 in Prague) is the Czech professional tennis player who reached his career-high singles ranking of No. 5 on August 8, 2002 and doubles rankings of No. 1 on May 6, 2003.","label":1,"model":"bloomz","source":"wikipedia","id":1346}
{"text":"The film is about the life of Howard Schultz, CEO and founder of coffeehouse chain Starbucks Coffee Company (SBUX). The story follows his rise from being fired by McDonald's to becoming one of America's richest men through his entrepreneurial spirit.\nSchultz's autobiography, Onward!","label":1,"model":"bloomz","source":"wikipedia","id":1347}
{"text":"Frank Butler was the leader of the Conservative Party in Northern Ireland from 1974 to 1977, and again briefly between 1980 and 1981.\nButler's first term as party chairman came after he had been elected MP at Westminster on his second attempt following unsuccessful attempts in 1964 and 1970.  He became Leader of the Ulster Unionist Council shortly afterwards.   In 1975, he led the Conservatives into government when they formed their third administration under Harold Wilson.    His second tenure began in 1979 but ended abruptly by mutual consent less than two months later due to differences over policy towards Sinn F\u00e9in which were not resolved despite talks involving Prime Minister Margaret Thatcher and Taoiseach (Irish prime minister)  James Callaghan.","label":1,"model":"bloomz","source":"wikipedia","id":1348}
{"text":"Babak (Persian: \u0628\u0627\u0628\u0643\u200e also Romanized as B\u0101bak) is a village in Shabestan Rural District Khorramshahr County Hormozgan Province Iran. At the 2006 census its population was 1150 in 260 families.","label":1,"model":"bloomz","source":"wikipedia","id":1349}
{"text":"Rufus Riddlesberger (born September 16, 1943) is the former mayor of Santa Clara, California and current member of the Santa Clara City Council representing District 1 since 2005.  He was elected to his first term in 2004 after serving as interim Mayor from 2003 until he won election that year.   In 2008, he ran unopposed for re-election but lost by one vote to Sam Liccardo who became the city's new mayor.    Prior to becoming city councilman,  Mr. Riddlesberger served on the Santa Clara Planning Commission where he worked closely with then-Mayor Ron Galperin during the early 1990s when the Silicon Valley boom began.   During this time period,   Mr. Riddlesbergers's work helped secure funding  for several major projects including the construction of the Santa Clara Convention Center which opened its doors in 1993.   After leaving office at the end of 2002, he returned home to San Jose where he continued working in real estate development before returning to Santa Clara in January 2006 to run again for the position of Mayor.   On May 31st 2009, Rufus announced he would not seek another term as Mayor due to health reasons","label":1,"model":"bloomz","source":"wikipedia","id":1350}
{"text":"Kaira is the name of several species in different families within Araneae, including Kaira alba and Kaira trivia from the family Salticidae; Kaira crassipes from the family Gnaphosidae; Kaira sp. (Araneae), Kaira longipalpis (Salticidae), Kaira nigrita (Gnaphosidae) (accepted as Karstagiella nigrita by some authors).","label":1,"model":"bloomz","source":"wikipedia","id":1351}
{"text":"Epimenides (Greek: \u1f18\u03c0\u03b9\u03bc\u03b7\u03bd\u03af\u03b4\u03b7\u03c2 Epim\u00e9nid\u0113s, \"of Crete\"; fl. c. 500 BC) was the son of Cretans and one of Socrates' most famous interlocutors in Plato's dialogues Phaedo, Symposium, Ion, Euthyphro, Apology, Crito, Critias, Theaetetus, Sophist, Statesman, Timaeus, Philebus, Menexenus, Laches, Charmides, Protagoras, Gorgias, Hippias Major, Hipparchus, Leontius, Meno, Republic, Parmenides, Timaios, Metis, Filebo, Laws.\nHe is also mentioned by Aristotle as having been present at Socrates' trial before his death.  He appears to have had some influence on Greek literature; he has been called \"the father of satire\" because of his role in the dialogue Lysistrata written by Aristophanes.   In addition, there are several references to him throughout the works of other authors including Euripides, Herodotus, Plutarch, Diogenes La\u00ebrtius, Cicero, Horace, Ovid, Virgil, Dante Alighieri, Goethe, Rabelais, Moli\u00e8re, Swift, Dickens, Twain, Nietzsche, Sartre, Camus, Kafka, Beckett, Borges, Calvino, Eco, Rushdi, Hesse, Nabokov, Philip Pullman, Umberto Eco, Milan Kundera, Salman Rushdie, Harold Bloom, George Steiner, Vladimir Solovyov, Mikhail Bulgakov, Aleksandr Pushkin, Fyodor Dostoevsky, Anton Tchechov, Boris Pasternak, Leo Tolstoy, Gabriel Garc\u00eda M\u00e1rquez, Mario Vargas Llosa, Jorge Luis Borges, Pablo Neruda, Julio Cort\u00e1zar, Carlos Fuentes, Jos\u00e9 Saramago, Italo Calvino, Haruki Murakami, Robert Harris, Ian Fleming, JRR Tolkien, Terry Pratchett, Neil Gaiman, Douglas Adams, David Sedaris, Hunter Davies, Mark Twain, Kurt Vonnegut Jr., Daniel Quinn, Michael Ondaatje, John Irving, Margaret Atwood, Doris Lessing, Toni Morrison, Joyce Carol Oates, Alice Munro, Joanne Rowling, Naomi Klein, Arianna Huffington, Noam Chomsky, Howard Zinn, Christopher Hitchens, Richard Dawkins, Stephen Hawking, Steven Pinker, James Watson, Sean Carroll, Sam Harris, Carl Jung, Sigmund Freud, Albert Einstein, Isaac Newton, Galileo Galilei, Leonardo da Vinci, Charles Darwin, Alfred Russel Wallace, Alexander Hamilton, Benjamin Franklin, Thomas Jefferson, Abraham Lincoln, Winston Churchill, Mahatma Gandhi, Martin Luther King Jr., Malcolm X, Nelson Mandela, Desmond Tutu, Aung San Suu Kyi, Malala Yousafzai, Barack Obama, Hillary Clinton, Bill Gates, Warren Buffett, Elon Musk, Jack Ma, Larry Page, Sergey Brin, Jeff Bezos, Mark Zuckerberg, Eric Schmidt, Steve Jobs, Rupert Murdoch, Donald Trump, Kanye West, Justin Bieber, Lady Gaga, Beyonc\u00e9 Knowles-Carter, Madonna, Katy Perry, Jennifer Lopez, Britney Spears, Taylor Swift, Kim Kardashian-West, Ellen Degeneris, Sarah Jessica Parker, Scarlett Johansson, Angelina Jolie, Natalie Portman, Emma Stone, Emily Blunt, Lupita Nyong'o, Gwyneth Paltrow, Cameron Diaz, Halle Berry, Nicole Kidman, Sandra Bullock, Reese Witherspoon, Julia Roberts, Meryl Streep, Viola Davis, Michelle Williams, Anne Hathaway, Helen Mirren, Catherine Zeta-Jones, Dame Julie Andrews, Cheryl Hines, Melissa McCarthy, Kristen Wiig, Amy Schumer, Mindy Kaling, Tina Fey, Whoopi Goldberg, Lena Dunham, Zoe Saldana, Alicia Vikander, Kate Winslet, Rooney Mara, Gal Gadot, Margot Robbie, Blake Lively, Charlize Theron, Salma Hayek, Uma Thurman, Winona Ryder, Drew Barrymore, Hilary Swank, Katherine Hepburn, Susan Sarandon, Mia Wasikowska, Dakota Johnson, Elizabeth Moss, Saoirse Ronan, Emma Watson, Audrey Tatou, Lily-Rose Depp, L\u00e9a Seydoux, Naomie Campbell, Cara Delevingne, Rihanna","label":1,"model":"bloomz","source":"wikipedia","id":1352}
{"text":"The chapel is located in the village of Hampstead, London and was built between 1625\u20131630 by Sir Henry Wotton as part of his country estate at Holborn Hill (now known as Hampstead Garden Suburb). It has been designated Grade I listed status since 1950.\nIt contains some notable features including Jacobean pulpit, altarpiece depicting Saint Michael defeating Satan from about 1600, stained glass window designed by Edward Burne-Jones showing The Angel Gabriel Announcing to Mary that She Would Be the Mother of Jesus dated c. 1900\u20131901, and memorials to several members of the family of poet Lord Alfred Tennyson who lived nearby on Mornington Crescent.","label":1,"model":"bloomz","source":"wikipedia","id":1353}
{"text":"James Kelly is an Australian rules footballer who played in the Victorian Football League and National Football League competitions during his career, primarily as a defender\/midfielder\/forward. He was born on 16 May 1977 at Warrnambool, Victoria.\nKelly began playing junior football with North Ballarat before moving to Port Melbourne Football Club where he won two premierships from 1997\u20131998 & 1998-1999.  In 2000 James moved back home to play senior football with South West Slopes Football Club winning another premiership that year.   After spending three seasons there he returned once again to Port Melbourne FC where he spent six years until retiring after the 2008 season.    During this time he also represented Vic Country against Western Warriors twice in 2005 & 2006","label":1,"model":"bloomz","source":"wikipedia","id":1354}
{"text":"The Maltese national final, Malta's Got Talent (Maltese: Talanta), was held on 24 February 2005 at Manoel Theatre to select the country's entry for the Eurovision Song Contest that year.\nIt featured ten acts competing against each other and one act being selected by public vote via SMS text messaging from mobile phones only; this is because of restrictions imposed due to security concerns during the contests following September 11 attacks.","label":1,"model":"bloomz","source":"wikipedia","id":1355}
{"text":"Tolitoli Regency (Indonesian: Kabupaten Tolitoli) adalah regensi di provinsi Sulawesi Tengah, Indonesia yang dibentuk berdasarkan Undang-Undang Nomor 6 Tahun 1991 tanggal 16 Agustus 1991 dan merupakan hasil pemekaran dari kabupaten Poso dengan luas wilayah 1.961 kilometer persegi terdiri atas 12 kecamatan yaitu Kecamatan Lore Utara, Lore Timur, Lore Selatan, Lore Barat Daya, Lore Barat Laut, Lore Tengah, Lore Utara, Lindu, Toli-Toli, Toili, dan Tojo.","label":1,"model":"bloomz","source":"wikipedia","id":1356}
{"text":"Augustino Oldoini (16 May 1606 \u2013 28 September 1673) was the Duke of Massa and Prince of Carrara from 1647 until his death in 1673, when he left no male heir to succeed him.\nOldoini's father died without issue on 24 February 1646 leaving Augustino as head of the family.  He married Maria Caterina di Savoia-Carignano, daughter of Carlo II Carignan-Saluzzo, Marquis of Montferrat.   Their son Francesco I succeeded them both after their deaths","label":1,"model":"bloomz","source":"wikipedia","id":1357}
{"text":"Lowell B. \"Fred\" Frazer (May 16, 1917 \u2013 May 24, 2007) was the first African-American to serve as mayor of Boston and one of its most prominent civic leaders in his time.\nFrazer served from 1965 until 1969 when he resigned after being convicted on federal charges related to bribery during his tenure.  He died at age 87 following complications from pneumonia.","label":1,"model":"bloomz","source":"wikipedia","id":1358}
{"text":"Sandy Davy (born September 16, 1943) is the former president of the National Federation of Women's Institutes and author of several books on women's issues including The Powerful Woman's Book of Self-Care.\nDavy was born in New York City to parents who were both teachers.  She graduated from Sarah Lawrence College where she majored in English literature.   After graduation Sandy moved back home to Long Island where her father taught at Dowling College.    Her mother died when she was young so it fell upon Sandy's older sister to raise her younger siblings while their father worked full time as well as teaching part-time.     In 1966 Sandy married David A. Davy Jr., also known by his stage name \"Davey\", whom she had met during college.   They have two children together;  daughter Jennifer and son Jonathan.   Their marriage ended after twenty years but they remained close friends until Davey's death in 2007 due to complications following heart surgery.","label":1,"model":"bloomz","source":"wikipedia","id":1359}
{"text":"Peikarambankottai is a village in the southern state of Tamil Nadu, India. It lies on the banks of the Vaigai River and is located about 12 kilometres (7 mi) from Theni town towards Chidambaram.","label":1,"model":"bloomz","source":"wikipedia","id":1360}
{"text":"Outstanding Supporting Actor in a Comedy Series (one award per series) \u2013 The Primetime Emmys are awarded annually by the Academy of Television Arts & Sciences to recognize excellence and creativity within American primetime television programming, excluding news programs.\nThe category was introduced at the first annual Primetime Emmys ceremony on September 22, 1958 as \"Outstanding Individual Performance in a Comedy Program or Special\". It has been presented every year since then except 1959 when it did not exist; 1962\u201363 due to technical difficulties; 1966 because there were no eligible nominees; 1968 through 1974 during which time the awards were given out under two separate categories, one for comedies and another for variety shows; 1975\u20131976 while the show's name changed from \"The Best TV Shows\"; 1977 after the program ended its run; 1978 until 1980 following cancellation of the program; 1981\u20131982 while the show's title reverted back to \"The Best TV Shows\" before changing again that same season to \"Best TV Show\"; 1983\u20131984 while the show's name returned once more to \"The Best TV Shots\"; 1985\u20131986 while the show's name became \"The Best TV Shows Ever!\"; 1987\u20131988 while the show's name remained unchanged but moved into syndication; 1989\u20131990 while the show's name stayed the same yet relocated to CBS; 1991\u20131992 while the show's name shifted to \"The Greatest TV Shows of All Time\"; 1993\u20131994 while the show's name finally settled down to simply being called \"The Best TV Shows;\" 1995\u20131996 while the show's name continued to be \"The Best TV Shows...","label":1,"model":"bloomz","source":"wikipedia","id":1361}
{"text":"Challapalli Venkatraman (born 15 September 1974) is the current Chief Minister of Telangana state in India, elected on 16 May 2014 as member of Indian National Congress party from Nizamabad constituency to represent Hyderabad Metropolitan Region and surrounding districts including Medak district which was part of Andhra Pradesh before bifurcation into two states.\nHe has been serving since 17 May 2014","label":1,"model":"bloomz","source":"wikipedia","id":1362}
{"text":"Jacob Akrong (born September 16, 1987) is the current mayor of Accra Metropolitan Assembly in Ghana and member of Parliament for Adjen Kotoku constituency since 2008 representing New Patriotic Party(NPP).","label":1,"model":"bloomz","source":"wikipedia","id":1363}
{"text":"TweenTribune is the official website of Nickelodeon, aimed at tweens (ages 8\u201314). It was launched in 2005 as part of Nick Jr., and rebranded to TweenTribune on September 1, 2010 after being acquired by Viacom's Paramount Pictures unit from The Walt Disney Company.\nThe site features news stories written specifically for tweens; video content such as music videos, gameplay clips, movie trailers, TV commercials, webisodes, and original series produced exclusively for it; games that are designed especially for children aged eight through fourteen years; downloadable wallpapers; contests; polls; chat rooms; blogs created by kids; and other interactive elements.","label":1,"model":"bloomz","source":"wikipedia","id":1364}
{"text":"Allan Domb (born Allan David Domb on September 16, 1946) is the former bassist and backing vocalist of The Who's rock band from 1967 to 1971.  He was born in New York City but grew up in Los Angeles where he attended Santa Monica College.   After graduating high school at age 17, he moved back east to attend Berklee School Of Music before joining The Who as their new bass player after Keith Moon died.\nDomb left The Who shortly afterwards due to musical differences between him and Pete Townshend over songwriting styles.    In 1972, he joined another rock group called Walker Brothers who had just released two albums including \"Walker Brothers & Meatloaf\" which featured his song \"Don't You Know I Love You?\"  However, this relationship also ended quickly when they broke into each other's homes during a recording session while drunk.   His next project involved playing guitars and singing background vocals for Billy Idol until 1977.   During that time period, he recorded several songs under various pseudonyms such as: \"Davey Jones\",  \"Billy Idol\",   \"Steve Stevenson\",    \"Richard Starkey\"    and   \"Alan Sunderland\".   He then played bass for Paul McCartney's Wings touring band through 1978.   Later, he became a member of Ringo Starr's All-Starr Band along with fellow members Mark Hudson, Steve Lukather, Richard Page, Gregg Rolie, Warren Ham and Todd Rundgren.   He has been married twice; first to Linda Lee Hannon whom he divorced in 1988 and secondly to Christine McGuire since 1990.","label":1,"model":"bloomz","source":"wikipedia","id":1365}
{"text":"The seismograph is the most common type of instrument used to record seismic waves, and it measures ground motion by recording changes in position or velocity (or both) over time.\nSeismic waveforms are recorded on paper charts called seismograms which show how much energy was released during each earthquake event as well as its magnitude.","label":1,"model":"bloomz","source":"wikipedia","id":1366}
{"text":"Mashhadi (Persian: \u0645\u0634\u0647\u062f\u064a\u200e also Romanized as Mas\u1e29\u0101di; also known as Qeshlak) is a village in Shadmehr Rural District, Sarvabad County, Semnan Province, Iran. At the 2006 census its population was 1,961 in 524 families.","label":1,"model":"bloomz","source":"wikipedia","id":1367}
{"text":"Nayantara is the name of several people, including: Nayantara (film), film directed by Kodi Ramakrishna; Nayantara (singer), singer and songwriter from India; Nayantara (actress), actress in Telugu films","label":1,"model":"bloomz","source":"wikipedia","id":1368}
{"text":"Khairallah (Arabic: \u062e\u0631\u064a\u0651\u0641 \u0627\u0644\u0644\u0647\u200e) is the name of several people in history, including: Khairallah ibn Ibrahim al-Nahhashi; Khairallah ibn Muhammad al-Sharif al-Hamadani; and Khairallah ibn Ali al-Khayyat","label":1,"model":"bloomz","source":"wikipedia","id":1369}
{"text":"The Laia are the indigenous inhabitants of the island of Hispaniola, which they inhabit along with other ethnic groups such as Ta\u00ednos and Caribes.\nThey were first described by Christopher Columbus in 1492 during his second voyage to America; he called them \"Lahares\" (from Spanish labrar or till).","label":1,"model":"bloomz","source":"wikipedia","id":1370}
{"text":"Jereh is a Village in the Rural District of Jereh, Sarvabad County, Kurdistan Province, Iran. At the 2006 census its population was 16, in 4 families. It lies on the road to Mahabad and has a school.","label":1,"model":"bloomz","source":"wikipedia","id":1371}
{"text":"The 2018\u201319 Liga MX Femenil is the 16th edition of the Mexican women's football league, and will be played from August to May 2019. The tournament features 16 teams divided into two groups (northern group and southern group) that play against each other in home-and-away round-robin format within their respective groups.","label":1,"model":"bloomz","source":"wikipedia","id":1372}
{"text":"Euparia is the only genus of sea snails marine gastropod mollusks in family Euparidae (the pearl mussels). The species are found worldwide, but most live on or near coral reefs and rocky shores from tropical to temperate waters.\nThe shells have spiral ribs that may be smoothed out by erosion; they can also be covered with spines.","label":1,"model":"bloomz","source":"wikipedia","id":1373}
{"text":"National Science Museum is the national science museum of Thailand, located in Pathum Thani Province near Bangkok Metropolitan Administration Region and was established on September 1, 1974 by King Bhumibol Adulyadej as part of his vision to develop Thai people into intelligent citizens who are knowledgeable about their country\u2019s natural resources.\nThe National Science Museum has been designed according to the concept that all things have scientific principles behind them. The exhibits at this museum include various types of animals such as insects, reptiles, amphibians, birds, mammals; plants from different parts of the world including orchids grown inside glass houses which were specially built for them; minerals found around the country; fossils discovered during excavations throughout Thailand dating back millions of years; meteorites collected from space; ancient tools used by humans thousands or even tens of thousand years ago; traditional medicine practiced since time immemorial; and many other interesting items related to nature sciences.","label":1,"model":"bloomz","source":"wikipedia","id":1374}
{"text":"The municipality of Lisice (German: Gemeinde Liesch) is located in the district of Folwark, Lower Austria and has about 3200 inhabitants. The village lies on the Danube riverbank at its confluence with the Salzach River near to the border between Upper and Lower Austria.\nHistory","label":1,"model":"bloomz","source":"wikipedia","id":1375}
{"text":"The Ellijay River is a river in Georgia, United States that flows through the city of Ellijay and into Lake Allatoona near Chatsworth.\nIt was named after Colonel James Elli Jackson (1753\u20131832), who served as governor of Tennessee from 1818 to 1820.  The name \"elija\" means \"high lands\" or \"steep grounds\" in Creek Indian language.   It also has been called \"the little Cherokee river\" because it runs along the western edge of the Cherokee Nation's lands before joining the larger Allatoona watershed at its confluence with the Chattooga River.   ...","label":1,"model":"bloomz","source":"wikipedia","id":1376}
{"text":"Poniecice [\u02c8p\u0254\u0272\u025bt\u0361s\u025bt\u0361\u0255\u025b] (German P\u00f6nitzsch) is a village in the administrative district of Gmina \u015acinawa within Sieradz County Masovian Voivodeship in east-central Poland close to Germany bordering on the German town of Sch\u00f6nborn and the river Oder. It lies approximately 6 kilometres (4 mi) north-east of \u015acinawa 16 km (10 mi) south-west of Sieradz and 49 km (30 mi) west of Warsaw","label":1,"model":"bloomz","source":"wikipedia","id":1377}
{"text":"Dawachi (\u4ee3\u308f\u3061, also romanized as Da-wachi) is the Japanese name of the Korean peninsula's southernmost island Jeju-do in South Korea.\nThe word \"dowachi\" means \"remote\" or \"extremely remote place\" and was used by Japan to refer to Jeju Island during its colonial rule over Korea from 1910 until 1945 when it became part of the Republic of Korea after World War II.","label":1,"model":"bloomz","source":"wikipedia","id":1378}
{"text":"Photogrammetric measurement is the science and technology used to determine physical measurements from photographs, especially aerial photography.\nThe following table compares some popular free open-source photogrammetry software:\n* The list was compiled by Michael J. Ryan in 2009","label":1,"model":"bloomz","source":"wikipedia","id":1379}
{"text":"The name of the village is derived from its location on the banks of the River Ra'ina, which flows through it and into the sea at nearby Lough Swilly.\nAr-Ra'nah (Irish: An R\u00e1ith\u00edn) is a small coastal town in County Donegal, Ireland located near Dunfanaghy to the south west coastline along the Irish Sea. It lies within the Barony of Dunscaife Upper, Ulster Province, Northern Ireland.","label":1,"model":"bloomz","source":"wikipedia","id":1380}
{"text":"The film is set in the summer of 1988, when two teenage girls from different backgrounds are sent to stay at a remote English country house by their parents during school holidays.\nBefore We Disappear","label":1,"model":"bloomz","source":"wikipedia","id":1381}
{"text":"Isangura (born William Nsanzurwimo) is the current governor of Goma, Democratic Republic of Congo.\nHe was elected in 2006 and re-elected on August 30, 2011 to serve another five-year term as Governor until 2016","label":1,"model":"bloomz","source":"wikipedia","id":1382}
{"text":"James William Gisborne (born 16 May 1943) is the current Chief Justice of New Zealand, having been appointed on 1 September 2008 by Prime Minister John Key to replace Sir Geoffrey Palmer as head of the judiciary in that country.\nGisborne was born at Rotorua and educated at Tamaki Makau Rau College before going up to Auckland University where he graduated LLB Hons 1st class.  He then went into private practice specialising in criminal law.   In 1988 he became Crown Counsel and from 1990 until 1994 served as Acting Director Public Prosecutions.   From 1995 to 1998 he was Deputy Attorney General.    On his appointment as Chief Justice it was announced that he would retire after three years unless re-appointed or re-nominated;  this term will end 31 August 2011 when he turns 70.","label":1,"model":"bloomz","source":"wikipedia","id":1383}
{"text":"The district of Freiberg is located in the state of Saxony-Anhalt, Germany and was established on 1 January 2007 by merging the former districts of Freiberg-Landkreis and Schmalkalden-Meiningen into one larger administrative unit.\nGeography\n\nFreiberg borders to the north-east with Thuringia's capital city Erfurt as well as the neighbouring German states Hesse and Bavaria. \nAdministrative divisions","label":1,"model":"bloomz","source":"wikipedia","id":1384}
{"text":"This is the William H. \"Billy\" Webster page on Baseball-Reference.\nWilliam Henry \"Billy\" Webster was born in New York City, New York and died there as well.  He played professional baseball from 1903 to 1913.   His position(s) were pitcher\/outfielder\/middle infield.    Billy's career began at age 16 when he signed his first contract with Brooklyn Robins of the National League.   In 1904, he pitched against future Hall-of-Famer Honus Wagner who had just been traded by Chicago Cubs to St Louis Cardinals that year.   After playing two seasons with the Robins, he joined the Philadelphia Phillies where he spent most of his time outfielding or catching until 1907 before joining the Boston Red Caps.   The following season saw him play briefly with the Washington Senators then return to the Red Caps through 1909.   During this period, he also appeared in one game each for the Baltimore Orioles and Pittsburgh Pirates.   On September 26, 1908,  he became only the second player ever to hit three home runs in a single game;  the other being Jack Ness.   That same month, he set another record by hitting four doubles in five games.   This feat has never since been duplicated.   He finished his major league career appearing in 131 games over eight years batting .268 with 12 homers, 118 RBI, .316 OBP and 1.000 SLG.   He compiled a lifetime ERA+ rating of 130 which ranks him among the top ten best all-time starting pitchers.   Billy later coached several teams including the Yankees' farm team in 1918-1919.   He died after suffering a heart attack while coaching the Syracuse Stars in 1920.","label":1,"model":"bloomz","source":"wikipedia","id":1385}
{"text":"The Collegiate Baseball Newspapers are the official publications of NCAA Division I baseball, and were founded in 1927 by George Williams (1873\u20131951). The first publication was called \"The Daily Princetonian\"; it is now known as \"The Daily Tar Heel.\"","label":1,"model":"bloomz","source":"wikipedia","id":1386}
{"text":"The 396th Bombardment Squadron was activated on 1 February 1943 as one of the four original squadrons of the 376th Bomb Group, which had been formed at Camp X in North Carolina earlier that month.  The squadron trained under Second Air Force (AAF) until it deployed to England via Casablanca and Lisbon.   It arrived at RAF Scampton on 5 March where its Boeing B-17 Flying Fortress bombers were parked near other aircraft from the group's parent organization.    On 15 April 1944, the squadron moved into newly constructed barracks along with the rest of the 376th BG.   After completing training missions over Germany beginning May 1945,  the squadron flew combat operations against strategic targets throughout Europe during World War II before being released from active duty on 25 August 1946.","label":1,"model":"bloomz","source":"wikipedia","id":1387}
{"text":"The Adlington is a river of Northumberland, England.\nIt rises in the Cheviot Hills and flows generally south-east through the townships of Wallsend-on-Tyne to its confluence at Tynemouth on the Tyne estuary.","label":1,"model":"bloomz","source":"wikipedia","id":1388}
{"text":"Mirgor S.A. (Spanish pronunciation: [mi\u02c8\u027e\u0263o\u027e]) is the largest electronics manufacturer in Argentina, and one of its major exporters. It was founded by Mario Roberto Galperin on September 1, 1995 as Mirgor SAICF (Sociedad An\u00f3nima Importadora y Exportadora de la Compa\u00f1\u00eda Financiera).","label":1,"model":"bloomz","source":"wikipedia","id":1389}
{"text":"Heliura hagmanni is a species of sea snail marine gastropod mollusk in the family Heliidae. It was described by Pilsbry & Cooke in 1916 and has been recorded from Australia, Indonesia (Sumatra), Malaysia, New Caledonia, Papua New Guinea, Philippines, Thailand, Vietnam","label":1,"model":"bloomz","source":"wikipedia","id":1390}
{"text":"The Chinese name \u7956 (pronounced [z\u01d4]) means \"ancestor\" or \"father\", and is used in the titles of some family members to refer to their father.\nIn Taiwanese Hokkien it also refers to grandfather","label":1,"model":"bloomz","source":"wikipedia","id":1391}
{"text":"The Sand Hills region of the Upper Midwest United States is known as one of America's most distinctive architectural regions, and its vernacular style has been called \"the American Gothic Revival in stone.\"The area's unique geography\u2014a series of rolling hills covered by sandy soil that was ideal for building houses out of native limestone or brick\u2014the presence of nearby quarries supplying these materials at low cost, and the availability of wood from local forests all contributed to this regionalism.\nIn addition to their use of locally available resources, builders also incorporated elements of other styles popular during the 19th century such as Greek revival, Italianate, Queen Anne, Colonial Revival, Victorian, Prairie School (or prairie-style), and Art Deco into their designs.  The result is a collection of buildings whose appearance varies widely but which share certain common characteristics including steeply pitched roofs, projecting gables, decorative trim work on doors and windows, large porches supported by brackets, and chimneys often built into the eaves.","label":1,"model":"bloomz","source":"wikipedia","id":1392}
{"text":"Cathal O'Matadin (born in Dublin, Ireland) is the founder and CEO of The Irish Times Online Limited which owns and operates www.irishtimesonline.com.\nO'Matadin was born on 1 May 1971 to parents from County Mayo who emigrated to Ireland during the 1960s as part of the Great Famine. He grew up in Dublin where he attended St Patrick's College before going into journalism at Trinity College, Cambridge University.  After graduating from university, O'Matadin worked briefly for Reuters News Agency but left after three months when his employer decided not to send him overseas.   In 1998, O'Matadin joined Independent Newspapers plc  working initially as a reporter covering local government issues then moving onto business reporting.    During this time,  O'Matadin also became involved in setting up the company's website, IrishtimesOnline.com.   On leaving Independent Newspapers in 2002,   O'Matadin set up his own online news service called Business2Community.com.   This site covered all aspects of small businesses including finance, technology, marketing and management.   It attracted over one million unique visitors per month within its first year of operation.   Following the success of Business2Community.com,    O'Matadin launched another similar website focused solely on Science & Technology called Scientist2Scientist.com.   Both sites were sold by O'Matadin in 2007 following their acquisition by two separate companies.   Since 2008, O'Matadin has been running both websites under new ownership while simultaneously developing other projects such as the launch of a mobile phone app based around the content produced by Business2Community.com and Scientist2Scientist....","label":1,"model":"bloomz","source":"wikipedia","id":1393}
{"text":"Campanilla (Spanish pronunciation: [kan\u02c8pan\u026alla]) is the Spanish word for \"little bell\"; it refers to any small musical instrument that produces sound by striking or shaking, such as a chime, rattle, jingle bells and maracas.\nIn popular culture","label":1,"model":"bloomz","source":"wikipedia","id":1394}
{"text":"The Albate\u2013Camerlata railway station is located in the municipality of Camerlata, Province of Cuneo (Piedmont), Italy. It serves as part of the Turin\u2013Bardonecchia line and was opened on 1 May 1864 by the Societ\u00e0 Piemontese per le Strade Ferrate Meridionali","label":1,"model":"bloomz","source":"wikipedia","id":1395}
{"text":"The film is set in the year 1630, during the Dutch Republic's struggle against Spain and France.\nA young woman named Maria (Johanna ter Steege) falls in love with Jan Vermeer van Delft (Willem Dafoe), who has just returned from Italy where he had been studying art under Caravaggio himself.  She promises to wait three years before marrying anyone else if only she can become his wife.   The story follows their relationship over several decades as they grow old together while struggling financially due to her father's gambling debts.    My promise","label":1,"model":"bloomz","source":"wikipedia","id":1396}
{"text":"Afr\u00e2nio \u00e9 um munic\u00edpio brasileiro do estado de Minas Gerais, localizado na microrregi\u00e3o de Arax\u00e1 e no Tri\u00e2ngulo Mineiro. Sua popula\u00e7\u00e3o estimada em 2004 era de 5.082 habitantes.\nO nome Afranio vem da palavra latina \"ferrum\" (ferro), que significa ferro ou a\u00e7o; o significado deste nome est\u00e1 ligado \u00e0 exist\u00eancia das minas de ferro existentes nas proximidades desde os tempos coloniais.","label":1,"model":"bloomz","source":"wikipedia","id":1397}
{"text":"Swanwick, Anna (17 March 1753 \u2013 18 May 1818) was the wife of British politician and writer Edmund Burke.\nShe is best known as his muse; he wrote several poems to her during their marriage.  She died in childbirth at age 29 on 18 May 1818 after giving birth to their daughter Georgiana Maria Burke.","label":1,"model":"bloomz","source":"wikipedia","id":1398}
{"text":"Margaret Mary (Peggy) Scriven, DBE, (born Margaret Mary McNally; 22 May 1929 \u2013 16 September 2007), was the first female presenter of BBC Television's News at Ten and one of its longest-serving presenters from 1957 to 1988.\nScriven presented her final news bulletin on 30 June 1988 after more than 40 years in broadcasting.","label":1,"model":"bloomz","source":"wikipedia","id":1399}
{"text":"The Sporting Globe was the first British sports newspaper, founded in 1845 by John Graham Chambers and William Henry Harrison as The Sportsman (later renamed to The Athlete). It is considered one of Britain's most important sporting publications ever produced.\nIn its early years it covered all forms of sport including cricket, football, rowing, athletics, boxing, horse racing, fencing, shooting, wrestling, gymnastics, croquet, billiards, chess, bowls, golf, tennis, snooker, rugby union, American football, baseball, ice hockey, cycling, swimming, diving, yachting, fishing, hunting, equestrianism, polo, greyhound racing, dog shows, pigeon shooting, falconry, badminton, squash, table-tennis, jousting, archery, darts, speedway motorcycles, roller derby, ski-ing, curlers, bowling greens, lawn bowls, hurling & Gaelic games etc., but later concentrated on cricket and football only.","label":1,"model":"bloomz","source":"wikipedia","id":1400}
{"text":"Han I (born Han Hoi-tat, Chinese: \u97d3\u7199\u6cf0; pinyin: H\u00e1n X\u012bt\u00e0i) (December 31, 1943 \u2013 September 16, 2003) was the first president of The Hong Kong Institute of Certified Public Accountants Limited and served as its chairman from 1997 to 2002.\nHe is also known in China by his pen name \"\u97e9\u7199\u53f0\" or \u97e9\u7199\u592a.","label":1,"model":"bloomz","source":"wikipedia","id":1401}
{"text":"The following is the list of municipalities in Hesse, Germany by population as of 31 December 2010 according to official statistics published by the Hessian State Office for Statistics (Hessische Landesamt f\u00fcr Statistik).","label":1,"model":"bloomz","source":"wikipedia","id":1402}
{"text":"Eugenio Morin (Barcelona, \u200b\u200bSpain; 28 May 1921 - Paris, France; 16 September 2005) was a Spanish philosopher and sociologist who developed the concept of \"complexity\" in his works.\nHe is considered one of the founders of complexity theory along with Ilya Prigogine, Stuart Kauffman or John Holland among others.","label":1,"model":"bloomz","source":"wikipedia","id":1403}
{"text":"Amaroria is the first album by American singer-songwriter Melissa Etheridge, released in 1997 on Maverick Records and re-released as Amaroria II (1998) after her success at the 1998 Grammy Awards.\nThe title track was nominated for Best Female Pop Vocal Performance but lost to Mariah Carey's Vision of Love from Music Box; it did however win Song of The Year at both the 1996 MTV Video Music Awards and 1997 Billboard Music Awards. \n \n It also won Album of The Year at the 1997 Meteor Music Awards.  \n \n Track listing","label":1,"model":"bloomz","source":"wikipedia","id":1404}
{"text":"Luke Rhinehart (born September 16, 1946) is the author of The Art of Struggle and founder\/owner of the Hapuku clothing company in New Zealand.\nRhinehart was born on September 16, 1946 to parents who were both teachers at St Joseph's Catholic School in Waim\u0101nalo Hawaii.  He attended Saint Louis High School where he played football.   After graduating from high school Luke moved back home to Honolulu where his father had accepted a teaching position.    In 1968 after spending two years working as a waiter and bartender Luke decided that he wanted more out of life than what he could get by being employed full time so he quit his job and began traveling around Europe.   During this period while living abroad Luke met many people including some famous artists such as Salvador Dal\u00ed whom he describes as one of the most interesting men he'd ever known.   Upon returning to America Luke spent several months backpacking through California before moving into San Francisco where he lived until 1974 when he returned once again to Hawaii.   While living there Luke worked various jobs ranging from construction work to selling real estate but it wasn't long before he realized that none of these positions fulfilled him like they used to.   It was during this period that Luke developed his interest in writing which eventually led him to write \"The Art of Struggle\".   In 1977 Luke married Lisa Marie Smith and together they have three children;  Luke Jr., Lani, and Koa.   Since founding Hapuku Clothing Company in 1988 Luke has been able to combine his love of art and fashion along with his desire to help others achieve their dreams.","label":1,"model":"bloomz","source":"wikipedia","id":1405}
{"text":"Peng Si (\u5f6d\u65af) is the pseudonym of Chinese writer and poet Liu Xiaodong, born in 1966.\nLiu was educated at Beijing Normal University where he studied literature.  He has published more than ten books since 1998 including poetry collections such as The Red Book of Love (1998), A Collection of Poems by Peng Si (2002).","label":1,"model":"bloomz","source":"wikipedia","id":1406}
{"text":"Orenburg Oblast (Russian: \u041e\u0440\u0435\u043d\u0431\u0443\u0440\u0433\u0441\u043a\u0430\u044f \u043e\u0431\u043b\u0430\u0441\u0442\u044c, tr. Orenburgskaya oblast) is one of the 89 federal subjects (oblystar) of Russia located in southern part of European Russian Federation and bordering Kazakhstan to the south-east.\nThe region was founded on August 1, 1924 as the Ural Soviet Autonomous Region within RSFSR by merging several autonomous republics including Bashkortostan Republic into one administrative unit. On December 31, 1929 it became the Ural Sverdlovsk Autonomous Oblast which included parts of modern-day Chelyabinsk Oblast until 1937 when it regained its autonomy status under the name of Ural Oblast. In 1944 the area was renamed again becoming the Ural Altai Autonomous Oblast but reverted back to its original name after the dissolution of the USSR in 1991.","label":1,"model":"bloomz","source":"wikipedia","id":1407}
{"text":"Weed, Charles L. (1829 \u2013 May 16, 1903) was the first president of Dartmouth College in Hanover, New Hampshire from 1869 to 1873 and again from 1877 until his death on May 16, 1903.  He is credited as being instrumental in establishing Dartmouth's reputation nationally.   In addition he served as Dean of Arts at Harvard University (1860-1862), President of Amherst College (1867\u20131869),  Professor of Rhetoric & Belles Lettres  at Dartmouth  (1857\u201360).   His son Henry Ware Weed became the second president of Dartmouth College","label":1,"model":"bloomz","source":"wikipedia","id":1408}
{"text":"Makijonek is the name of several people, including: Donat Makijonek (born September 16, 1943) is a former Polish football player who played as defender in the 1960s and 1970s.\nHe made his debut on August 24, 1966 against Austria Wien at Stadion \u015al\u0105ski w Katowicach.","label":1,"model":"bloomz","source":"wikipedia","id":1409}
{"text":"The Institute of International Relations and Political Science (Georgian: \u10d7\u10d4\u10d2\u10d4\u10d1\u10d8 \u10d8\u10e1\u10e2\u10da\u10d0\u10ea\u10d8\u10dd\u10dc\u10d4\u10d1\u10d0) is the oldest university in Georgia, founded on September 28, 1918 by decree of Georgian President No\u00e9 Jordania as The University of Georgian Courses (Russian: \u0423\u043d\u0438\u0432\u0435\u0440\u0441\u0438\u0442\u0435\u0442 \u0413\u0435\u043e\u0440\u0433\u0438\u0447\u0435\u0441\u043a\u0438\u0445 \u041a\u0443\u0440\u0441\u043e\u0432).","label":1,"model":"bloomz","source":"wikipedia","id":1410}
{"text":"The following is the list of stations in Japan by city (in Japanese). The table below shows all railway station codes and names within \u014cshikawa District, Nagano Prefecture.\n* denotes that this station has been closed","label":1,"model":"bloomz","source":"wikipedia","id":1411}
{"text":"Switzerland sent one competitor to compete in fencing events at the 1908 Summer Olympics, held from August 30 until September 14 of that year.\nThe Swiss participant was Louis Vuillemin (born 1876), who represented Switzerland as part of his military service and won two bronze medals in team foil events:","label":1,"model":"bloomz","source":"wikipedia","id":1412}
{"text":"The Australasian Treecreeper (Dendrocitta vagabunda) is found in Australia, New Guinea and the Solomon Islands.\nIt has been recorded as far north as Cape York Peninsula on mainland Australia but it does not occur there regularly; its northernmost breeding record was at Lake Wallace near Cairns Queensland where it breeds from September to January.","label":1,"model":"bloomz","source":"wikipedia","id":1413}
{"text":"Mnesia is the loss of memory, or amnestic disorder.\nIt can be caused by brain injury (traumatic), stroke and other neurological disorders such as Alzheimer's disease, Parkinson's disease, Huntington's disease, Lewy body dementia, frontotemporal lobar degeneration, alcoholism-related dementia, Down syndrome-associated dementia, Creutzfeldt-Jakob disease, HIV infection\/AIDS associated neurocognitive disorders, vascular dementia, normal pressure hydrocephalus, prion diseases including mad cow disease\/encephalopathy, multiple sclerosis, epilepsy, depression, schizophrenia, anxiety, post-traumatic stress disorder, substance abuse, metabolic disturbances like diabetes mellitus type 2, thyroid dysfunction, vitamin deficiencies etc., infections in the central nervous system, autoimmune conditions affecting the brain, tumors, genetic mutations causing hereditary forms of dementia, head trauma due to accidents, childbirth complications during pregnancy, birth injuries, poisoning from heavy metals, carbon monoxide inhalation, exposure to radiation, organophosphate insecticides, pesticides, herbicides, fungal toxins, bacterial endotoxins, viral infections, parasitic infestations, toxic substances ingestion, drug overdose, withdrawal syndromes, electroconvulsive therapy side effects, sleep deprivation, emotional distress, extreme fatigue, heatstroke, hypoglycemia, hyperthermia, hypoxia, drowning, near-drowning, cardiac arrest, respiratory failure, shock, status epilepticus, cerebral malaria, meningitis, encephalitis, rabies, syphilis, botulism, Wernicke-Korsakoff syndrome, post-concussional syndrome, transient global amnesia, chronic traumatic encephalopathy, and many others.","label":1,"model":"bloomz","source":"wikipedia","id":1414}
{"text":"The following is the list of works by sculptors whose name begins with V, listed alphabetically according to their last names.\nVadim Andreevich Belyaev - Russian sculptor and painter","label":1,"model":"bloomz","source":"wikipedia","id":1415}
{"text":"Born in 1990, Olivier Bourdeaut is the author of two novels and one book of poetry published by Actes Sud Junior (L'ogre du Vel' d'Hiv; La Vie en Rose et Noir). He has also written several screenplays.\nHe lives between Paris and London where he works as a journalist at The Times Online France","label":1,"model":"bloomz","source":"wikipedia","id":1416}
{"text":"The following is the complete list of winners and nominees in each category at the 2014 Independent Music Awards, presented by The Boston Globe on September 16, 2014:[1][2][3][4][5]\n2014 Independent Music Awards: Winners","label":1,"model":"bloomz","source":"wikipedia","id":1417}
{"text":"Pandan is one of the federal parliamentary seats in Malaysia, which has been contested since 1969 by members from Barisan Nasional and its predecessor parties. It was won by Tun Abdul Razak Hussein on his first attempt at winning election to Parliament as Prime Minister after independence.\nThe seat covers the state constituencies of Papar, Sri Aman, Serian, Sibu, Sarikei, Lundu, Marudi, Betong, Samunsam, Miri, Bintulu, Sarawak Kuching Division, and parts of Bau district including Bau town itself.","label":1,"model":"bloomz","source":"wikipedia","id":1418}
{"text":"Osmodidae is the largest family of snakes in Australia, containing over 100 species and subspecies found throughout most parts of mainland Australia except Tasmania.\nThe common name \"adder\" refers to several genera within this family; however it has been used historically as a catch-all term for all Australian elapids (snake-like reptiles) that are not true cobras or pythons.  The scientific names for these animals include Acanthophis antarcticus , Austrelaps superbus , Bungarus fasciatus , Calliophis bibronii , Crotalinae australis , Dendroaspis jamesonii , Elaphe obsoleta , Hoplocephalus bungaroides , Macropisthodon dorsalis , Pseudechis porphyriacus , Pseudonaja nuchalis , Pseudonaja affinis , Pseudonaja textilis , Pseudonaja pallida , Pseudonaja modesta , Pseudonaja unicinctus , Pseudonaja guttata , Pseudonaja aff.","label":1,"model":"bloomz","source":"wikipedia","id":1419}
{"text":"The following is the list of winners and nominees in each category at the 2014 MTV Europe Music Awards (EMA) held on November 12, 2013 at London's O2 Arena. The ceremony was hosted by Nick Grimshaw from BBC Radio 1.\nWinners","label":1,"model":"bloomz","source":"wikipedia","id":1420}
{"text":"Jackie Akello (born September 16, 1974) is the Democratic member of Congress representing New York's 13th congressional district since January 3, 2007.  She was first elected to represent her hometown of Rochester in 2002 and has been re-elected five times.   Prior to serving as a Member of Congress she served on the City Council from 1998-2002 where she focused primarily on economic development issues including job creation through small business initiatives.    Jackie also worked at the local hospital system during this time period before becoming involved full-time in public service.","label":1,"model":"bloomz","source":"wikipedia","id":1421}
{"text":"The Levellers were English political radicals who emerged in the mid-1640s as part of the English Revolution, demanding reform to Parliament and government by elected representatives rather than hereditary peers or monarchists appointed by the Crown.\nThey are sometimes described as \"Radical Whigs\" because they supported parliamentary representation but rejected monarchy altogether; however their views on this issue varied widely among themselves.  The name \"Leveller\" was first used during the 1649 London Riot when some protesters wore white levels (a tool) atop their hats while others carried them aloft.   They also called themselves \"Commoners\" after the Commons of England where many members sat.    Their ideas influenced other radical groups such as the Diggers and the Anabaptist sect known as the Fifth Monarchists","label":1,"model":"bloomz","source":"wikipedia","id":1422}
{"text":"The 2019\u201320 Arizona Wildcats women's college basketball team represents the University of Arizona in NCAA Division I competition during the 2019-20 season, and is coached by Cindy Stein. The Wildcats are members of the Big 12 Conference","label":1,"model":"bloomz","source":"wikipedia","id":1423}
{"text":"Dobrovi\u0107i (Serbian Cyrillic: \u0414\u043e\u0431\u0440\u043e\u0432\u0438\u045b\u0430) is a village in the municipality of Radovljica, Slovenia. According to the 2002 census the population was 39 people.","label":1,"model":"bloomz","source":"wikipedia","id":1424}
{"text":"Robert Leroy Auth (born September 16, 1943) is the former Democratic member of Congress from New Jersey's 2nd congressional district who represented it since January 3, 1993 until he was defeated by Republican Chris Christie in November 2006.  He has been married to his wife Linda since 1968 and they have three children together.   In addition to serving as U.S. Representative, Robert served on numerous boards including Bergen Community College Board of Trustees where he chaired its Governance Committee;  The National Association of Broadcasters board of directors;  and the NJ Turnpike Authority Advisory Council.    On December 31, 2007, he announced that he would not seek re-election after 20 years representing this district which includes parts of Bergen County, Hudson County, Passaic County, Essex County, Morris County, Sussex County, Union County, Middlesex County, Mercer County, Warren County, Hunterdon County, Somerset County, Monmouth County, Ocean County, Atlantic City, Galloway Township, Holbrook Township, Franklin Lakes Village, Lake Hiawatha, Edgewater Park, North Arlington, South Orange, West Milford, East Hanover, Roseland, Springfield Gardens, Fairview Heights, Clark, Summit Hill, Ridgewood, Teaneck, Paramus, Fort Lee, Garretson, Wayne, Maple Avenue, Ramsey, Woodbridge, Upper Saddle River, Lower Merion Township, Pennsauken, Marlton, Haddon Heights, Cherry Hill, Mount Laurel, Voorhees Township, Moorestown, Medway Townships, Millburn, Livingston, Monroe, Montclair, Nutley, Parsippany-Troy Hills, Chappaqua, Coram, Holbrook Valley, Holbrook Township, Holbrook Borough, Holbrook Business Improvement District, Holbrook School District #26, Holbrook Public Library District #1, Holbrook Water & Sewage Commission, Holbrook Fire Department, Holbrook Volunteer Ambulance Corps, Holbrook Police Department, Holbrook Municipal Court, Holbrook Planning Board, Holbrook Zoning Board of Adjustment, Holbrook Economic Development Corporation, Holbrook Historical Society, Holbrook Chamber of Commerce, Holbrook Building Inspectorate, Holbrook Beautification Commission, Holbrook Cemetery Commission, Holbrook Conservation Commission, Holbrook Historic Preservation Commission, Holbrook Recreation Commission, Holbrook Transportation Commission, Holbrook Highway Commission, Holbrook Airport Commission, Holbrook Industrial Development Agency, Holbrook Environmental Protection Commission, Holbrook Land Use Commission, Holbrook Local Government Services Commission, Holbrook Health Care Commission, Holbrook Human Resources Commission, Holbrook Housing Commission, Holbrook Office of Emergency Management,...","label":1,"model":"bloomz","source":"wikipedia","id":1425}
{"text":"The Fourth Fitna (Arabic: \u0627\u0644\u0641\u062a\u0646\u0629 \u0627\u0644\u0631\u0627\u0628\u0639\u0629\u200e al-\u1e24arb al-Awsa\u1e6d) was the final phase of the Ottoman Empire's struggle against European powers in its attempt to maintain control over Egypt and Syria following the French invasion of 1798, which resulted in Napoleon Bonaparte becoming consul general of France there.\nIt began when Ali Bey Yusef Pasha led a revolt by Egyptian Mamluks against their Turkish masters on 1 January 1799 after he had been appointed governor-general of Egypt by the Porte as part of the terms negotiated at the Treaty of Shukdawan between the Ottomans and Britain that ended the First Anglo-Ottoman War earlier that year; this conflict is also known as the Second Battle of Alexandria or the Siege of Cairo.","label":1,"model":"bloomz","source":"wikipedia","id":1426}
{"text":"The following is the list of communes in France that have been named Puycornet or Poucornet:\nPuycornet, Tarn-et-Garonne (Tarn et Garonne); Puycornet, Loz\u00e8re","label":1,"model":"bloomz","source":"wikipedia","id":1427}
{"text":"Zdzis\u0142aw \u017by\u0142uski (born September 16, 1943 in Warsaw) is the Polish historian and writer of Jewish descent who lives in Israel since 1974.\n\u017by\u0142uski's father was killed during World War II by Germans at Treblinka extermination camp; his mother survived Auschwitz concentration camp but died from cancer when he was only six years old. He grew up under communist rule in Poland after 1945 until 1968 when he emigrated to Israel where he studied history and literature at Tel Aviv University.","label":1,"model":"bloomz","source":"wikipedia","id":1428}
{"text":"Clemaxia is the genus of flowering plants in the daisy family known by the common name clemson clover or Clemsonia . The genus was named after American botanist George Washington Carver (1864-1943).","label":1,"model":"bloomz","source":"wikipedia","id":1429}
{"text":"The film is set in the fictional town of Moth, California during the Great Depression.\nMoth (Richard Gere) lives on his family's farm along with his wife Ruthie (Laura Linney), their son Tommy (Sammy Sheik), daughter Katie (Abigail Breslin), mother-in-law Sarah (Frances Barber), sister-in-law Maybellene (Dawn Olivieri), brother-in-law George (John Mahoney), niece Mary Jane (Kaitlyn Dever), nephew Johnny Boy (Tyler Hoechlin), granddaughter Annie Mae (Ava Kolker), grandfather Henry (James Earl Jones), grandmother Abbie (Olympia Dukakis).","label":1,"model":"bloomz","source":"wikipedia","id":1430}
{"text":"Joe Kovaks is the puppet master of The Muppet Show, and has been since its debut in 1976 until his retirement from performing on December 31st, 1995.\nKovaks was born Joseph A. Kovaks Jr.,[1][2][3][4] May 27th, 1931[5] to parents who were Hungarian Jewish immigrants[6] living in New York City's Lower East Side[7] where he grew up watching vaudeville acts at local theater houses such as the Apollo Theater[8] and the Brooklyn Academy of Music[9] before becoming interested in puppetry himself[10] while working as a child actor[11] during World War II[12] when he appeared in several films including one starring Mickey Mouse[13] which led him into a career that spanned over 50 years[14] making appearances in numerous television shows[15] and movies[16] throughout his life[17]","label":1,"model":"bloomz","source":"wikipedia","id":1431}
{"text":"Chelsey Alyssa Washington (born September 16, 1990) is an American professional basketball player who currently plays as a point guard and shooting guard for the University of Memphis Lady Tigers women's basketball team in NCAA Division I .","label":1,"model":"bloomz","source":"wikipedia","id":1432}
{"text":"Vitex leucoxylon is a species of flowering plant in the verbena family known by the common name white sagewort or false hyssop . It grows to about 1 metre (3 ft 3 in) tall and has small green leaves that are covered on both sides with fine hairs giving them a silvery appearance when viewed from above .\nThe flowers appear between May and September , they have five petals which may be pinkish purple but more often pale yellow -coloured .","label":1,"model":"bloomz","source":"wikipedia","id":1433}
{"text":"The film opens on a train, where two men are talking about their lives and how they have been affected by the war; one of them is called Strangers (Richard Burton) who has just returned from fighting abroad while the other man is named Jim (James Fox). The pair become friends as they talk to each other during the journey.\nJim tells his story which begins when he was born into poverty but had dreams that were never realized because of it. He talks about meeting up with a girl whom he loved at school only to find out she died shortly afterwards; this causes him great pain until he meets another woman later in life whose name is not mentioned throughout most of the movie except once or twice.","label":1,"model":"bloomz","source":"wikipedia","id":1434}
{"text":"Leon James is a former English football player who played as a defender in the Football League and National League during his career, which spanned from 1927 to 1939. He was born at Walsall on 1 May 1901.\nJames made his professional debut playing for Stourbridge Town of the Southern Football League against Kidderminster Harriers on 26 September 1927; he scored twice that season but left after only one year when he joined Accrington Stanley.","label":1,"model":"bloomz","source":"wikipedia","id":1435}
{"text":"The 2010 University of Nebraska\u2013Lincoln Huskers football team represented the university during the 2009\u201310 NCAA Division I FBS football season, and was coached by Bob Stoops.  The team played its home games at Jack Trice Stadium in Lincoln, Nebraska.   This marked the first year that the school's athletic teams were allowed to use their nickname \"Huskers\" after winning legal battles against former coach Mike Riley over his objections to using it.","label":1,"model":"bloomz","source":"wikipedia","id":1436}
{"text":"Konkoliko is the largest island of the Ionia Islands, Greece and one of its most popular tourist destinations.\nGeography\n\nThe island lies in Aegean Sea between Lesvos to the west (11 km) and Chios to the east (16 km). It has an area of about 40 km2 and it is located at 38\u00b030\u2032N 26\u00b020\u2032E\ufeff \/ \ufeff38.5\u00b0N 26.33\u00b0E\ufeff \/ 38.5; 26.33","label":1,"model":"bloomz","source":"wikipedia","id":1437}
{"text":"Bornean English Language School (BEES) is the largest and oldest private language school in Brunei Darussalam, offering courses on General English as well as IELTS preparation classes.\nThe school was founded by Mr. David Heng who has over 30 years of teaching experience both locally and abroad including Malaysia, Singapore, Hong Kong, China, Japan, Korea, Taiwan, Thailand, Vietnam, Indonesia, Philippines, Australia, New Zealand, United Kingdom, Canada, USA & Ireland","label":1,"model":"bloomz","source":"wikipedia","id":1438}
{"text":"Kamarulzaman Abdul Razak (born 1 September 1953) is the current Prime Minister of Malaysia, serving since 2009 after being elected by his party UMNO as its president in 2008 and winning general elections on 9 May 2009. He was previously Deputy Prime Minister from 2004 to 2007 under Abdullah Ahmad Badawi's administration.\nHe has been described as \"the most powerful man in Malaysian politics today\"[1][2][3][4][5]","label":1,"model":"bloomz","source":"wikipedia","id":1439}
{"text":"La Trinit\u00e0 dei Monti (English: The Trinity of the Mountains) is one of the most famous and popular songs in Italy, written by Felice Romani as part of his opera Norma.","label":1,"model":"bloomz","source":"wikipedia","id":1440}
{"text":"Nanette Marie Hardie (born September 16, 1948) is the Democratic member of Congress representing New York's 8th congressional district since January 3, 2007.  She was first elected to represent her native Long Island in 2002 and has been re-elected five times.   Prior to entering politics she worked as a teacher at Pace University from 1977 until 2001 when she retired after 35 years.    In addition to serving on numerous committees including those overseeing education funding, health care reform, veterans affairs, national security issues, foreign policy matters, and women's rights, Nanette Hardie's legislative accomplishments include authorizing funds for repairs to the Brooklyn Bridge; creating the National Endowment for Financial Literacy; sponsoring legislation that would have banned discrimination based upon sexual orientation or gender identity by federal contractors; co-sponsoring legislation that would require all public schools to teach sex ed classes beginning in kindergarten through 12th grade; supporting efforts to increase access to affordable housing across America; promoting increased investment into renewable energy sources such as solar power; and advocating for improved mental healthcare services throughout our nation.\nHardie graduated magna cum laude from Adelphi College where she received both Bachelor of Arts degrees in Political Science and History along with a Master of Education degree specializing in Educational Administration & Supervision.   She also holds dual citizenship in Canada having lived there during college.   Her husband David died suddenly while running for Governor of California in 2003 leaving behind two sons who are now adults living outside of New York City.","label":1,"model":"bloomz","source":"wikipedia","id":1441}
{"text":"The Sons of Anarchy Motorcycle Club, Redwood Original (SAMCRO), is the main subject in the FX series \"Sons of Anarchy\" and its spinoff \"The Bastard Executioner\". SAMCRO was founded by Jax Teller's father Clay Morrow after he left his old club outlaw motorcycle gang called Mayans MC to avoid being killed during a war between rival clubs.","label":1,"model":"bloomz","source":"wikipedia","id":1442}
{"text":"The Bothy is the name given to small, usually wooden buildings used by hill farmers and shepherds in Scotland as dwellings or shelters.\nIn some cases they were built on hillsides above farmsteads; these are known as bothies proper (or sometimes cottages). In other cases they may have been attached to farmhouses; these are then often called huts but not always so.","label":1,"model":"bloomz","source":"wikipedia","id":1443}
{"text":"The Garrenssee is a lake in the municipality of Schwangau, Upper Bavaria Germany. It lies at an elevation of 968 metres (3,113 ft) above sea level and has a surface area of 0.5 km2 (0.19 sq mi).","label":1,"model":"bloomz","source":"wikipedia","id":1444}
{"text":"Agricultural production is one of the major economic activities and sources of income among Filipinos, especially those living in rural areas. The agricultural sector contributes to about 14%[1] or P$60 billion[2] (US$4.6 billion)[3][4] annually towards the country's gross domestic product.\nThe Philippine government has been implementing various programs aimed at improving agriculture through research on crop varieties that are resistant against pests and diseases; irrigation systems; farm mechanization programs; provision of credit facilities such as loans from commercial banks and cooperatives; extension services provided by Agricultural Extension Services Offices throughout the country; establishment of farmer-based organizations like Farmers' Organizations, Farmer's Cooperatives, Farmers Associations etc., which provide technical assistance to farmers; development of marketing systems including processing plants; improvement of infrastructure particularly roads leading to farms; construction of storage facilities; and other related projects.","label":1,"model":"bloomz","source":"wikipedia","id":1445}
{"text":"The following is the list of highways in Croatia by type and length.\nHighways are divided into national, regional or local roads depending on their importance to traffic flow within country borders as well as between countries.","label":1,"model":"bloomz","source":"wikipedia","id":1446}
{"text":"Henrik Vorum (born in Trondheim, Norway) is the founder and CEO of The Norwegian Institute For Bioeconomy Research (NIBR). He has been involved as advisor to several governments on bioeconomy issues including Sweden's Green Growth Strategy 2010-2020.\nVorum was educated at University College London where he obtained his PhD from Imperial College London working under Professors David MacKay and Paul Anastasio.","label":1,"model":"bloomz","source":"wikipedia","id":1447}
{"text":"T\u014dg\u014d (\u5bcc\u5ca1\u5e02, Tog\u014d-shi) is a city in Ishikawa Prefecture on the Sea of Japan coastline south-western Honshu island, Japan.","label":1,"model":"bloomz","source":"wikipedia","id":1448}
{"text":"Born in New York City, Frank Budd was the son of Italian immigrants who had settled on Long Island and worked as laborers at Ford Motor Company's plant there.  He attended public schools before enrolling in Fordham University where he graduated from its School of Law.   After graduation, he returned to his hometown and began practicing law.    In 1928, he married Mary Frances (Molly) McNally; they would have three children together.\nIn 1930, Budd ran unsuccessfully against Democratic incumbent Thomas E. Watson Jr., but won election four years later after Watson died during office-holding term.","label":1,"model":"bloomz","source":"wikipedia","id":1449}
{"text":"The following is the list of episodes in the TV series Someone To Watch Over Me, which aired on CBS from September 16, 2006 until May 24, 2007 and was created by David Nutter.\nEpisode List","label":1,"model":"bloomz","source":"wikipedia","id":1450}
{"text":"Simo Rundgren (born September 16, 1946) is the American musician and songwriter who was best known as one of the founding members of The Talking Heads in 1977 along with David Byrne, Chris Frantz, Tina Weymouth, Jerry Harrison, Brian Eno, Paul Simonon, and Adrian Belew.\nRundgren has also been involved in other musical projects including his own band, NRBQ; producing albums by artists such as Jellyman Kelly & His Blues Explosion, Patti Smith Group, Blondie, U2, Bruce Springsteen's Born to Run album, and many others; writing songs recorded by numerous artists; composing soundtracks for films like Wall Street, Grease, Saturday Night Fever, and more recently Aamir\" and \"The Grinch\"; and creating video games.","label":1,"model":"bloomz","source":"wikipedia","id":1451}
{"text":"The Tscharna Rayss is the longest river in Liechtenstein, and one of its most important waterways.\nIt rises at 1,850 metres (6,070 ft) above sea level on Mount Triesenberg near Schaanwalden, flows through the valleys between the mountains of Gampenreith, H\u00f6rnli Ridge, Wormserberg, Stadelhofen, M\u00f6nchspitze, Niederhorn, Steinbock, B\u00fcrgenstock, Hohenwartegg, Gl\u00e4rnisgau, and finally empties into Lake Constance after about 30 kilometres (19 mi).","label":1,"model":"bloomz","source":"wikipedia","id":1452}
{"text":"Castleton is the largest town in Westchester County and one of its oldest settlements dating back to 1658 when it was founded by English colonists from Connecticut as Fort Haldenborough (named after Sir Henry Haldenborough). The fortification was abandoned shortly thereafter but the name stuck until 1784 when the settlement became known officially as Castleton on Hudson.\nThe village has been home to many notable people including writer Nathaniel Hawthorne who lived there briefly during his early career before moving permanently to Salem, Massachusetts; composer George Gershwin whose parents settled here following their marriage; author Fannie Flagg's mother; artist Georgia O'Keeffe; actor John Wayne; singer-songwriter Harry Connick Jr., and former First Lady Jacqueline Kennedy Onassis.","label":1,"model":"bloomz","source":"wikipedia","id":1453}
{"text":"Monopoly: The Board Game is the first board game adaptation of Monopoly's popular property trading and real estate development simulation, released in 1985 by Milton Bradley Games.\nThe player controls one or more properties on which they build houses to rent out at higher prices than their competitors' rents; this generates income that can be used to buy other properties from bankers who loan money against collateral such as mortgages.  Play continues until all players have run out of cash and\/or no longer wish to purchase any additional properties.   Each turn consists of:  This version was followed up by several sequels including Monopoly: The New Board Game (1986), Monopoly: The Classic Edition (1987), Monopoly: The World's Greatest Cities (1988), Monopoly: Big Event (1989), Monopoly: Junior (1990), Monopoly: Speed (1991), Monopoly: Deal & Go! (1992), Monopoly: Family Fun Pack (1993), Monopoly: Trivial Pursuit (1994), Monopoly: The Original Series (1995), Monopoly: The Complete Collection (1996), Monopoly: The Ultimate Collector's Edition (1997), Monopoly: The Powerhouse (1998), Monopoly: The Cardboard Connection (1999), Monopoly: The Electronic Banking Experience (2000), Monopoly: The Interactive Digital Version (2001), Monopoly: The Mobile App (2002), Monopoly: The Video Game (2003), Monopoly: The DVD-ROM Game (2004).","label":1,"model":"bloomz","source":"wikipedia","id":1454}
{"text":"The 88th Rifle Division was formed in the Russian SFSR on 1 May 1937 from the 14th and 15th Rifle Divisions, which were disbanded at that time.  The division's first commander-in-chief was Colonel Nikolai Ivanovich Yudenich.\nIn June 1939 it became part of the 1st Army Corps under Marshal Semyon Timoshenko during Operation Citadel.   In August 1940 its headquarters moved to Leningrad where they remained until March 1943 when they returned to Moscow.    On 16 September 1941 the division crossed into Estonia as part of Operation Priboi.   It fought against German forces along the entire length of the Estonian front line including Narva River, Lake Peipus, Viljandi Canal, Viru River, P\u00e4rnu River, V\u00e4ike Emaj\u00f5gi river, V\u00f5ru River, J\u00fcrna River, Meri River, Setumaa Island, Hiiumaa island, Saaremaa Island, Muhu Island, Kuressaare Peninsula, Kunda peninsula, Suvalkija Peninsula, Vilnius region, Vilkavi\u0161kis district, Vilnius city, Vilnius Region, Vilnius District, Vilnius County, Vilnius Province, Vilnius Voivodeship, Vilnius Governorate, Vilnius City Council, Vilnius University, Vilnius Airport, Vilnius Railway Station, Vilnius Zoo, Vilnius Botanical Garden, Vilnius Television Tower, Vilnius Cathedral, Vilnius Town Hall, Vilnius Castle, Vilnius Old Town, Vilnius Market Square, Vilnius Opera House, Vilnius State Theatre, Vilnius Philharmonic Orchestra, Vilnius Sports Palace, Vilnius Stadium, Vilnius International Fairgrounds, Vilnius Institute of Technology, Vilnius Pedagogical Academy, Vilnius Medical College, Vilnius Technical School, Vilnius Polytechnic Institute, Vilnius Military Aviation Plant, Vilnius Power Plant, Vilnius Gas Plant, Vilnius Waterworks, Vilnius Forestry Farming Plant, Vilnius Radio-Television Company, Vilnius Film Studio, Vilnius TV Channel, Vilnius Public Library, Vilnius Museum of Art, Vilnius National Gallery, Vilnius Zoological Gardens, Vilnius Natural History Museum, Vilnius Planetarium, Vilnius Observatory, Vilnius Astronomical Observatory, Vilnius Music Conservatory, Vilnius Theater, Vilnius Concert Hall, Vilnius Puppet Theater, Vilnius Children's Circus, Vilnius Bookstore, Vilnius Department Store, Vilnius Hotel, Vilnius Baroque Church, Vilnius Synagogue, Vilnius Jewish Cemetery, Vilnius Orthodox cemetery, Vilnius Catholic church, Vilnius Lutheran church, Vilnius Evangelical-Lutheran...","label":1,"model":"bloomz","source":"wikipedia","id":1455}
{"text":"The 2008 All-Ireland Minor Football Championship was the 21st edition of that competition and took place from May to June in various locations around Ireland, including Dublin's Croke Park.\nIt featured 16 teams competing over two rounds - eight qualifiers played against each other on Saturday 29th May at venues across the country while the top four sides qualified automatically along with the winners of Sunday 30th May's qualifier round 1 games (the semi-finals) which were held simultaneously at Croke Park. \nOn Monday 31st May the remaining six teams competed in three further matches at Croke Park where the finalists were decided as well as who would play in Sunday 6th June's third-place game.  The final itself was contested between Kilkenny and Tipperary on Sunday 7th June at Croke Park.","label":1,"model":"bloomz","source":"wikipedia","id":1456}
{"text":"The following is the list of episodes in the anime series Demon Gaze, which aired from April to June 2016 on AT-X and Tokyo MX.\nEpisodes are listed by air date (in Japanese format).","label":1,"model":"bloomz","source":"wikipedia","id":1457}
{"text":"The International Challenger Zhangjiagang is the second leg of the 2009 ATP World Tour 250 series, and takes place from September 16 to 22 at the Qi Zhong Stadium in China.\nIt was added as part of the 2008\u201309 season after being voted into the calendar by players on tour during 2007's Tennis Europe Annual General Meeting held in Prague, Czech Republic","label":1,"model":"bloomz","source":"wikipedia","id":1458}
{"text":"A hollow-fiber membrane module is used to grow cells in suspension culture inside the fibers of a bundle, which are immersed into nutrient medium and kept under sterile conditions.\nThe advantages over conventional stirred tank reactors include higher cell density per unit volume (higher volumetric productivity), lower shear stress on suspended cells due to reduced fluid velocity gradients across the reactor vessel wall, better mass transfer between gas bubbles and liquid phase because of increased surface area available for bubble\u2013liquid contacting compared to that offered by spherical particles or pellets commonly found in other types of bioreactors such as fermenters, and easier scale-up from laboratory-scale experiments using multiple modules connected together in series and\/or parallel arrangements.","label":1,"model":"bloomz","source":"wikipedia","id":1459}
{"text":"The village is located in the District of Posht-e Malyan, Shiraz County, Fars Province, Iran.\nAt the 2006 census its population was 1,941 people living in 524 families.","label":1,"model":"bloomz","source":"wikipedia","id":1460}
{"text":"The following is the list of notable Palestinian writers, poets and novelists who have contributed to Arab or international literary culture in various languages.\nList of authors by nationality\n\n* Authors are listed alphabetically according to their last names","label":1,"model":"bloomz","source":"wikipedia","id":1461}
{"text":"Founded in 1963, Dallas Baptist University is the largest private university in North Texas and one of only two universities owned by Baptists in America (the other being Southern Baptist Theological Seminary). It has campuses located on three different sites within the city limits of Dallas\/Fort Worth metropolitan area.\nThe school's main campus sits atop Lookout Mountain overlooking downtown Fort Worth.  Its second location houses its School of Nursing; it also includes the Denton Campus Center which serves as home to several degree programs including nursing, business administration, criminal justice, education, music therapy, physical therapist assistant studies, social work, speech-language pathology, theatre arts\/applied drama, and visual communications design.   In addition, DBU operates satellite centers throughout north central Texas that provide classes taught by faculty from all schools across their respective disciplines.   ...","label":1,"model":"bloomz","source":"wikipedia","id":1462}
{"text":"Wulp, John (1606\u20131679) was the son of Jan van der Laan and Margaretha de Jongh.  He married Maria van den Bosch on 16 September 1628 in Amsterdam.   His father died when he was only three years old; his mother remarried to Pieter Jansz van Oosterhout who had been one of her first husbands before she left him after seven months.    In 1629, at age four, he moved from Utrecht where he lived until then into Amsterdam along with his stepfather's family.\nHe became a merchant by trade but also engaged in other activities such as shipbuilding and insurance companies.   On 26 May 1650 he bought the house that is now known as De Gooyer which still stands today near the Amstel river.   The building has since changed hands several times including being used as a hotel during World War II.   It currently houses apartments and offices under its own name, Hotel De Gooyer.   During this time it served as headquarters for the Dutch East India Company between 1785 and 1795.   After the war ended, the company returned their office space back to the city council.   Today there are plans underway to turn part of the property into residential units while keeping some parts open to visitors interested in seeing what remains of the original structure.","label":1,"model":"bloomz","source":"wikipedia","id":1463}
{"text":"The Sausalito Woman\u2019s Club is located in the city of Sausalito, California on San Francisco Bay.  The club was founded by women who were interested in sailing and boating activities.   It has been active since its founding in 1902 as one of many clubs that have contributed to making Sausalito famous worldwide.    In addition to being known for harboring some of the most beautiful boats in the world, it also hosts numerous events throughout the year including art shows, lectures, concerts, dances, luncheons, dinners, bridge tournaments, book signings, wine tastings, holiday celebrations such as Christmas tree lighting ceremonies, Easter egg hunts, Mother\u2019s Day brunches, Father\u2019s Day picnics, 4th July barbecues, Halloween costume parties, Thanksgiving dinner gatherings, and other social functions.","label":1,"model":"bloomz","source":"wikipedia","id":1464}
{"text":"The H1FOO is the first of two prototypes built by Boeing to test technologies that could be used in future aircraft designs, including fuel efficiency and noise reduction.\nBoeing's design team began work on this concept as early as 2004.  The project was officially announced at the 2007 Paris Air Show.   It has been designed primarily using computer-aided engineering (CAE) tools such as CATIA V5 from Dassault Syst\u00e8mes SA., NX from Siemens PLM Software Inc. , and ANSYS Fluent software from Ansys Inc..","label":1,"model":"bloomz","source":"wikipedia","id":1465}
{"text":"Carsten Haurum (born in 1973) is a Danish historian and author of several books on the history of Denmark, including The Danes - A History from Prehistory to 1600 AD , which won him the Brage Prize .","label":1,"model":"bloomz","source":"wikipedia","id":1466}
{"text":"Philip John \"Porras\" (born September 16, 1966) is the current Chief Justice of Guam's Supreme Court and was sworn in on January 1, 2014.  He previously served as Associate Justice from 2009 to 2013.   Prior to his appointment he worked at the Office of Public Defense Services where he represented indigent defendants charged with criminal offenses before Superior Courts throughout Guam.   \n\nPorras graduated magna cum laude from Santa Clara University School of Law in 1998 after receiving his Bachelor of Arts degree in Political Science\/Government from San Diego State College in 1995.   In addition to being admitted to practice law by the Guam Bar Association since 1999,  Philip has also been admitted to practice law in California.\n\nHe currently resides in Tamuning, Guam","label":1,"model":"bloomz","source":"wikipedia","id":1467}
{"text":"Hydraulic fluids are used in hydraulic systems to transmit force and motion between components of the system.\nThe most common types include mineral oil, water-based emulsions (such as white spirit), synthetic esters or silicone oils.","label":1,"model":"bloomz","source":"wikipedia","id":1468}
{"text":"The parish of Vi\u013c\u0137ene is located in Vilnius County, Lithuania and has the population of 4,941 (as at January 1, 2011). The village was established by Lithuanian nobility as early as 14th century on the right bank of Neris River near its confluence to Vilnia river.","label":1,"model":"bloomz","source":"wikipedia","id":1469}
{"text":"The San Juan River is the longest river in Metro Manila, Philippines and one of its major water sources. It starts at Mount Makiling near Laguna de Bay National Park on the northern part of Luzon Island where it flows through Quezon City before entering Manila via Santa Mesa Tunnel to empty into Pasay Channel.\nIt has been polluted by untreated sewage from nearby residential areas as well as industrial waste dumped along its banks since the 1970s when rapid population growth led to increased demand for housing and infrastructure development without proper environmental protection measures being put in place.","label":1,"model":"bloomz","source":"wikipedia","id":1470}
{"text":"Lom Peak is the highest point in the Lom Range of mountains, located on the border between Montana and Idaho near Silver City, Montana.  The mountain was named by Colonel George A. Custer after his brother-in-law Captain Henry Lomax who died at Little Big Horn.   It has been called \"the most beautiful peak east of the Rockies\" because it rises abruptly from its surrounding landscape to form one of the tallest peaks along Interstate 90.","label":1,"model":"bloomz","source":"wikipedia","id":1471}
{"text":"Fox, Phil (born 16 May 1957) is the former Chief Executive Officer of The Walt Disney Company Australia and New Zealand from 2005 to 2009.\nHe was born in Sydney on 16 May 1957.  He has been involved in many aspects of entertainment including film production, television commercials, music video production as well as live theatre productions.   In his career he worked at various companies such as:  MCA Records; CBS Television Networks; Australian Broadcasting Corporation; Channel Seven; AECOM Films & TV Ltd., where he produced several award winning films.    His first role within the company was that of General Manager - Business Development which included responsibility for all new business ventures across both countries.   On 1 September 2006 Mr Fox became CEO of the newly formed joint venture between Disney's international operations and News Limited called 'Disney-ABC International'.   This position combined the roles previously held by Bob Iger who had become President\/COO of Disney worldwide and David Connell who had moved into the same position at ABC.   During this time Mr Fox also served as Chairman of the Board of Directors of Disney-ABC Domestic Media Networks.   Following the completion of the merger between Disney-ABC International and Disney-Australia Pty Ltd. (DAP), Mr Fox stepped down as CEO but remained chairman until June 2008 when DAP merged with its sister company, Disney Enterprises Australia Pty Ltd, (DEA).   After leaving Disney, Mr Fox joined the board of directors of Medibank Private Health Insurance before becoming Managing Director of the National Film and Sound Archive of Australia (NFSA).","label":1,"model":"bloomz","source":"wikipedia","id":1472}
{"text":"L\u00e9on Vandeputte (Brussels, Belgium, September 16, 1901 \u2013 Brussels, June 18, 1974) was a Belgian painter and sculptor who specialized in portraits of children.\nHe studied at the Acad\u00e9mie Royale des Beaux-Arts de Bruxelles under L\u00e9on Spilliaert from 1919 to 1923 where he met his future wife Suzanne Leroy-Vandeputte whom he married on May 31, 1927.  He also attended classes by Constant Permeke.   In 1925 he won first prize at the Prix du Hainaut competition.    His work is represented in many public collections including those of the Royal Museum of Fine Arts Antwerp, the National Gallery of Canada, the Rijksmuseum Amsterdam, the Mus\u00e9e d'Art Moderne Grand-Duc Jean Luxembourg City, the Mus\u00e9es Royaux des Beaux-Arts Belgique, the Stedelijk Museum voor Actuele Kunst Utrecht, the Victoria & Albert Museum London, the Philadelphia Museum of Art, the Yale University Art Gallery New Haven Connecticut USA as well as numerous private collectors worldwide","label":1,"model":"bloomz","source":"wikipedia","id":1473}
{"text":"Stare Guty [\u02c8star\u025b \u02c8\u0261ut\u0268] is a village in the administrative district of Gmina \u0141osie within Kolno County Masovian Voivodeship in east-central Poland close to Warsaw and Radom. It lies approximately 8 kilometres (5 mi) north-east of \u0141osie 16 km (10 mi) south-west of Kolno and 49 km (30 mi) west of Warsaw.","label":1,"model":"bloomz","source":"wikipedia","id":1474}
{"text":"Sanae Takasugi (\u9ad8\u702c \u5948\u3005, born March 24, 1990) is a Japanese voice actress and singer affiliated to Arts Vision Inc.. She has provided the voices of several characters in anime series such as Naruto Uzumaki from Naruto Shipp\u016bden, Y\u014dko Sakaki from Digimon Frontier, Ruri Gokouda from Durarara!!x2 Ten, Miu Kurosu from The Melancholy of Haruhi Suzumiya, Nene Anegasaki from Sora no Otoshimono, Chiaki Nanette from Love Hina, Akari Tsukino from Kimi ni Todoke, Aoi Miyamori from Mahoromatic, Yuuko Ichihara from Tantei Opera Milky Holmes, Kaede Higuchi from Fullmetal Alchemist Brotherhood, Rin Okumura from Bleach, Asuka Langley Soryu from Neon Genesis Evangelion, and many more.","label":1,"model":"bloomz","source":"wikipedia","id":1475}
{"text":"The Saskatchewan Party (French:[sask\u0251\u0303tino p\u0254litik]) is the official provincial political party of Saskatchewan, Canada. It was founded in 1917 as the Conservative Party and changed its name to the Saskatchewan Liberal-Conservative Association after World War II before adopting its current title in 1968.\nIt has been led by former premier Lorne Calvert since 1988; he served until his death on September 16, 2007 at age 84 from complications following surgery related to Alzheimer's disease.","label":1,"model":"bloomz","source":"wikipedia","id":1476}
{"text":"The Brentford Public Library is the main library in Brentford, London and was opened on 1 January 1901 by Sir Henry Tate as part of his philanthropic work to improve living conditions across England.\nIt has been designated Grade II listed status since 1987 due to its architectural significance within the context of public libraries throughout Britain during this period.","label":1,"model":"bloomz","source":"wikipedia","id":1477}
{"text":"The Central Region (French: R\u00e9gion du Centre) is one of the ten administrative regions in Cameroon, created on January 1, 2008 by splitting off from the former North-West and South West Regions.\nIt has an area of , making it the largest region in terms of land mass among all 10 regions; its population was estimated at over 4 million people as of 2014.","label":1,"model":"bloomz","source":"wikipedia","id":1478}
{"text":"Greenwood is a village in the U.S. state of Wisconsin and located within Vernon County along State Highway 23 near its junction with Interstate 41 at Exit 157.  The population was 1,082 as of 2010 census.\nThe Village Green Historic District includes several historic buildings dating from the late 19th century to early 20th century including the former school house which now houses the local historical society museum.   Other notable structures include the church built by German immigrants who settled here after 1850;  the old mill building that served both residential and commercial purposes until it burned down in 1950;   and the original home of one of the first settlers on this site, John H. Miller.    In addition there are many other residences throughout town that reflect various architectural styles popular during their time periods such as: Victorian Revival, Gothic Revival, Italianate style, Queen Anne Style, Colonial Revival, Prairie School Architecture, Bungalow architecture, Art Deco, Modernist, and Contemporary design trends.","label":1,"model":"bloomz","source":"wikipedia","id":1479}
{"text":"Warszewice [\u02c8var\u0282\u025bvit\u0361s\u025b] (German Warschau) is a village in the administrative district of Gmina \u015acinawa within Sieradz County Masovian Voivodeship in east-central Poland close to Germany bordering on the German state of Brandenburg and near Berlin. It lies approximately 8 kilometres (5 mi) north-east of \u015acinawa 16 km (10 mi) south-west of Sieradz and 49 km (30 mi) west of Warsaw.\nBefore 1945 it was part of Germany known as Warthausen.","label":1,"model":"bloomz","source":"wikipedia","id":1480}
{"text":"The second part of the album, \"Distant Dreams\" is more melodic and less aggressive than its predecessor but still maintains the same dark atmosphere as before.\nTrack listing\n\nAll songs written by Jari Salminen","label":1,"model":"bloomz","source":"wikipedia","id":1481}
{"text":"This is the list of winners and nominees for the BAFTA Children's Award, which was presented annually from 1958 to 2002 by The British Academy Film Awards (BAFTA). It has been replaced since 2003 by two separate awards - one for children's television programming in the United Kingdom and Ireland; another award for international children's programmes.\nThe first winner was \"The Little Lost Hen\" directed by David Bradley.","label":1,"model":"bloomz","source":"wikipedia","id":1482}
{"text":"The song was written by Jimmy Webb and recorded in 1967, released as the B-side to The Beatles' single Hey Jude.\nIt reached number one on Billboard's Hot 100 chart during August 1968 after being featured prominently in the film Easy Rider (1969).","label":1,"model":"bloomz","source":"wikipedia","id":1483}
{"text":"The Mauritanian Regroupement Party (Arabic: \u0627\u0644\u062a\u062c\u0645\u0639 \u0627\u0644\u0648\u0637\u0646\u064a \u0627\u0644\u062f\u064a\u0645\u0642\u0631\u0627\u0637\u064a\u200e, French: Parti r\u00e9publicain d\u00e9mocratique mauritanien) is the largest political party in Mauritania and was founded by President Moktar Ould Daddah on September 8, 1958 as part of his drive to modernize the country after independence from France.\nIt has been led since its founding by:","label":1,"model":"bloomz","source":"wikipedia","id":1484}
{"text":"Wedges Creek is the longest river in North Carolina, and one of only two rivers that flow from north to south through all or part of the state (the other being Cape Fear River). It rises near Mount Mitchell State Park at 5145 feet elevation on Weddington Ridge just east of Asheville, North Carolina, then flows generally southeastwardly past Brevard, Henderson County, before turning southwest toward Swain County where it joins the French Broad River.\nThe headwaters are located within Cherokee National Forest lands administered by the U.S. Forest Service's Nantahala Ranger District.  The upper reaches have been designated as Wild & Scenic Rivers since 1987.   In addition, portions of its lower course were also designated as Wild & Scenics between 1997 and 2002.    A portion of the river was listed under the Endangered Species Act until 2009 when the listing expired due to lack of funding.     The entire length of the river has been designated as a Class II trout stream.","label":1,"model":"bloomz","source":"wikipedia","id":1485}
{"text":"The Lund v Boissoin case was heard by the Supreme Court of Canada in October 2009, and concerned whether or not it is constitutional to have separate electoral districts within one province that are geographically contiguous but linguistically distinct (e.g., French-speaking).","label":1,"model":"bloomz","source":"wikipedia","id":1486}
{"text":"The 1963 Canadian general election was held on October 21, 1963 to elect the 41st Prime Minister of Canada and all members of Parliament (MPs) in the House of Commons of Canada.\nIt is considered one of the most important elections since Confederation because it marked the end of Liberal dominance over national politics that had lasted from 1867 until 1957 when they were defeated by the Progressive Conservatives led by Lester Pearson who became prime minister.  The Liberals won only 39 seats while the Tories gained 121 out of 157 total seats making them the largest party in parliament.   This gave the Tories their first majority government under Pearson's leadership.","label":1,"model":"bloomz","source":"wikipedia","id":1487}
{"text":"Brian J. Christie is the James S. McDonnell Distinguished Professor of Neuroscience at Washington University in St. Louis, Missouri and director of its Brain Research Institute.\nChristie received his Bachelor of Science degree from Harvard College in 1975; he then earned both M.A. (1976), Ph.D. (1980) (both in Psychology).","label":1,"model":"bloomz","source":"wikipedia","id":1488}
{"text":"The Luna 2 was the first Soviet satellite to enter orbit around Earth, launched on October 3, 1959 by the R-7 Semyorka rocket from Site 1\/5 at Baikonur Cosmodrome in Kazakhstan (then part of the USSR). The spacecraft's designation as \"E6\" meant that it had been built and tested under Project 6K19E, which also produced several other satellites including the Kosmos series.\nLuna 2 entered its initial elliptical orbit after launch; this was circularized into a nearly equatorial orbit within two days using aerodynamic drag forces generated during atmospheric re-entry over India. It then performed numerous experiments designed primarily to test the effects of weightlessness upon living organisms such as plants and animals.","label":1,"model":"bloomz","source":"wikipedia","id":1489}
{"text":"Seke is one of the districts in Uganda's West Nile sub-region, located on Lake Victoria and bordering Kenya to its west across the lake.\nThe district was created by splitting Bugiri District into two parts - Seke (east) and Buliisa (west).","label":1,"model":"bloomz","source":"wikipedia","id":1490}
{"text":"The Jagdgeschwader (\"JG\") was the Luftwaffe's fighter wing organization, and JG\u00a028 was formed on 1 February 1937 from elements of Jasta 26 based at Werneuchen in Thuringia.   The unit flew Messerschmitt Bf 109 fighters until it disbanded after World War II.    On 15 May 1940, JG\u00a028 claimed its first aerial victory over British aircraft when one of their pilots shot down a Hawker Hurricane near Calais.  By June 1941, JG\u00a028 had become part of Gruppe 2 of the newly created Geschwaderkommodore Reichenau's I.\/Jagdgeschwader 1 (JG1).   In August 1943, JG\u00a01 moved to France where they were stationed along the southern border between Germany and Luxembourg as well as south-east towards Paris.   During this time period, JG\u00a028 became known by the nickname \"Red Devil\" due to the color scheme painted onto many of their planes during operations against Allied forces.   After being transferred back into German hands following Operation Overlord, JG\u00a028 continued operating out of bases located around Metz before moving north-westward again toward Reims.   It is estimated that JG\u00a028 lost about 3,000 men killed or captured throughout its existence; however, only two members survived the war.","label":1,"model":"bloomz","source":"wikipedia","id":1491}
{"text":"The following is the text of the original entry on Lisburn Road in The Times Online's listings, as at February 2005.\nLisburn Road (Irish: Baile Chill Dara) is one of Belfast City Council's major roads and runs from north to south through west Belfast. It starts off near the junction between Great Victoria Street and Donegall Square West where it becomes known locally by its former name of Newtownards Road before crossing over the railway line into the centre of town via St George's Avenue.","label":1,"model":"bloomz","source":"wikipedia","id":1492}
{"text":"Social Order is the second studio album by American rock band Social Distortion, released in 1988 on Maverick Records and re-released as part of The Essential Collection series (1998) and The Complete Box set (2006).","label":1,"model":"bloomz","source":"wikipedia","id":1493}
{"text":"Gregory J. Haerr (born September 16, 1953) is the current Chief Justice of Guam's Supreme Court and was sworn in on January 1, 2007.  He previously served as Associate Justice from 2002 to 2006.   Prior to his appointment he had been serving since 1997 as Presiding Judge of the Superior Courts of Guam.    In addition to being a judge,  Mr. Haerr has also taught law at Guam Community College.   He received both Bachelor\u2019s Degree and Juris Doctor degrees from Santa Clara University School of Law.   His practice experience includes work as a private attorney specializing primarily in criminal defense cases before joining the judiciary system where he worked first as a prosecutor then later became a trial court judge.   On May 31, 2009 President Obama nominated him to be Chief Justice of Guam's highest court after former Chief Justice Anthony M. Carreira resigned due to health reasons.","label":1,"model":"bloomz","source":"wikipedia","id":1494}
{"text":"The following is the list of islands in New Zealand's South Pacific region, including those that are uninhabited and have no official name.\nList of islands by country or territory\n\nNew Caledonia","label":1,"model":"bloomz","source":"wikipedia","id":1495}
{"text":"The following is the list of mountains in Tajikistan by elevation, starting at highest point and working down to lowest.\nList of mountains over 7500 m (25000 ft) Mount Elbrus \u2013 Russia","label":1,"model":"bloomz","source":"wikipedia","id":1496}
{"text":"The following is the list of winners and nominees in each category at the 2008 MTV Video Music Awards, presented by American Express on September 12, 2008 from Los Angeles' Kodak Theatre.\nWinners are listed first; followed by their respective nominations","label":1,"model":"bloomz","source":"wikipedia","id":1497}
{"text":"Balmorhea State Park is located in the Balmorhea Mountains of northwestern New Mexico, United States.  The park was established by President Theodore Roosevelt on September 16, 1905 as part of his \"New National Parks Project\"; it became one of only two state parks to be created under that project (the other being Petrified Forest).  It covers over 1 million acres and includes several mountain ranges including the Santa Ynez Mountain Range.   In addition to its natural features, the park contains numerous historic buildings from the mining era which once thrived there.","label":1,"model":"bloomz","source":"wikipedia","id":1498}
{"text":"Thorsten Teichert (born September 16, 1966 in Berlin) is a German former footballer who played as defender and midfielder. He made his professional debut at Hertha BSC II on May 24, 1990.","label":1,"model":"bloomz","source":"wikipedia","id":1499}
{"text":"The Williams\u2013Landel\u2013Ferry Equation (or WLF Equation) is an empirical equation associated with time\u2013temperature superposition.\n\nThe WLF equation has the form\n \nwhere  is the decadic logarithm of the WLF shift factor, T is the temperature, Tr is a reference temperature chosen to construct the compliance master curve  and C1, C2 are empirical constants adjusted to fit the values of the superposition parameter aT.\n\nThe equation can be used to fit (regress) discrete values of the shift factor aT vs. temperature. Here,  values of shift factor aT are obtained by horizontal shift log(aT) of creep compliance data plotted vs. time or frequency in double logarithmic scale so that a data set obtained experimentally at temperature T superposes with the data set at temperature Tr. A minimum of three values of aT are needed to obtain C1, C2, and typically more than three are used.\n\nOnce constructed, the WLF equation allows for the estimation of the temperature shift factor for temperatures other than those for which the material was tested. In this way, the master curve can be applied to other temperatures. However, when the constants are obtained with data at temperatures above the  glass transition temperature (Tg), the WLF equation is applicable to temperatures at or above Tg only; the constants are positive and represent Arrhenius behavior. Extrapolation to temperatures below Tg is erroneous. When the constants are obtained with data at temperatures below Tg, negative values of C1, C2 are obtained, which are not applicable above Tg and do not represent Arrhenius behavior. Therefore, the constants obtained above Tg are not useful for predicting the response of the polymer for structural applications, which necessarily must operate at temperatures below Tg.\n\nThe WLF equation is a consequence of time\u2013temperature superposition (TTSP), which mathematically is an application of\nBoltzmann's superposition principle. It is TTSP, not WLF, that allows the assembly of a compliance master curve that spans more time, or frequency, than afforded by the time available for experimentation or the frequency range of the instrumentation, such as dynamic mechanical analyzer (DMA).\n\nWhile the time span of a TTSP master curve is broad, according to Struik, it is valid only if the data sets did not suffer from ageing effects during the test time. Even then, the master curve represents a hypothetical material that does not age. Effective Time Theory. needs to be used to obtain useful prediction for long term time.\n\nHaving data above Tg, it is possible to predict the behavior (compliance, storage modulus, etc.) of viscoelastic materials for temperatures T>Tg, and\/or for times\/frequencies longer\/slower than the time available for experimentation. With the master curve and associated WLF equation it is possible to predict the mechanical properties of the polymer out of time scale of the machine (typically  to  Hz), thus extrapolating the results of multi-frequency analysis to a broader range, out of measurement range of machine.\n\nPredicting the Effect of Temperature on Viscosity  by the WLF Equation \n\nThe Williams-Landel-Ferry model, or WLF for short, is usually used for polymer melts or other fluids that have a glass transition temperature.\n\nThe model is:\n\nwhere T-temperature, , ,  and  are empiric parameters (only three of them are independent from each other).\n\nIf one selects the parameter  based on the glass transition temperature, then the parameters ,  become very similar for the wide class of polymers. Typically, if  is set to match the glass transition temperature  , we get\n\n17.44\n\nand\n\n K.\n\nVan Krevelen recommends to choose\n\n K, then\n\nand\n\n101.6 K.\n\nUsing such universal parameters allows one to guess the temperature dependence of a polymer by knowing the viscosity at a single temperature.\n\nIn reality the universal parameters are not that universal, and it is much better to fit the WLF parameters to the experimental data, within the temperature range of interest.\n\nFurther reading \nWilliams-Landel-Ferry model\nTime\u2013temperature superposition\nViscoelasticity\n\nReferences \n\nPolymers","label":0,"model":"human","source":"wikipedia","id":1500}
{"text":"Buffalo Springs National Reserve is a protected area in the Isiolo County in northern Kenya.\n\nOrganization\nThe reserve was established in 1948 as part of the Samburu - Isiolo Game Reserve, and the present boundaries were established in 1985. The reserve is managed by the Isiolo County Council.\nMost operators of Kenyan safaris offer a visit to the reserve, which has several Safari Lodges and safari camps.\n\nLocation\nThe Buffalo Springs National Reserve is south of the Samburu National Reserve, which lies on the other side of the Ewaso Ngiro river. It is named after an oasis of clear water at its western end.\nThe reserve has an area of , and is at an altitude of between  and  above sea level.\nIt is a gently rolling lowland plain of old lava flows and volcanic soils of olivine basalt. The main feature is the Champagne Ride in the southeast, an ancient lava-terrace.\nThe climate is hot, dry and semi-arid.\n\nFlora\nThere is a narrow band of riverine forest along the Ewaso Ngiro which includes Tana River Poplar, Doum Palm and magnificent specimens of Acacia elatior.\nVegetation includes acacia tortilis woodland and large stretches of bushland dominated by Commiphora.\nIn some areas lava rock is exposed, with scattered grass and shrubs.\nOther parts have alkaline grasslands with occasional springs and swamps. \nHere and there the \"Desert Rose\" (Adenium obesum) is found in the scrub, with bright pink blooms. \nThe Salvadora persica (tooth-brush tree) shrub provides food to elephants, and its twigs are used as toothbrushes by the nomadic Samburu people.\n\nFauna\nWildlife include Grant's zebra and the endangered Grevy's Zebra. Other species of mammal include reticulated giraffe, the African bush elephant, oryx, gerenuk, African buffalo, lion, leopard, cheetah and hyena. Over 365 species of bird have been recorded in the reserve. The river is home to hippopotamuses and crocodiles. Somali ostriches are widespread within the national reserve. It is larger than the Masai ostrich's and is distinctive for their indigo legs and neck.\n\nReferences\n\nProtected areas of Kenya\nGreat Rift Valley\nProtected areas established in 1948\nIsiolo County","label":0,"model":"human","source":"wikipedia","id":1501}
{"text":"Gun\u0101rs Meierovics (May 12, 1920\u2013February 11, 2007) was a Latvian politician and the youngest son of the second Prime Minister of Latvia Zigfr\u012bds Anna Meierovics. Meierovics was also a candidate for the 1993 Latvian presidential election.\n\nBiography \nMeierovics was born on May 12, 1920 in Riga to Zigfr\u012bds Anna Meierovics and his wife Anna Meierovics. He studied economics and engineering at the University of Latvia and Baltic University after becoming a war refugee. He was drafted into the Latvian Legion in World War II but emigrated to the United States after fleeing to Germany, where he worked at the United States Department of Defense. He was active in the American Latvian Association. In 1961, he was one of the main activists in the establishment of the United Baltic Committee in the United States. Meierovics also led the World Association of Free Latvians.\n\nIn 1993, he was elected to the fifth Saeima, and from the list of the political party Latvian Way, he was nominated as a candidate for president in the 1993 Latvian presidential election. Meirovics withdrew in support for the candidate Guntis Ulmanis. In the government of Valdis Birkavs, Meierovics held the position of the Minister of State of the Baltic and Nordic States at the Ministry of Foreign Affairs. In November 1995, Meierovics was awarded the Order of the Three Stars of the 3rd class. In 2001, Meierovics was awarded the PBLA Prize \"for his extensive, long-lasting and successful work in strengthening the political position of the Baltics in the United States and Europe, in the struggle for the freedom of our nation and for the introduction of the new generation into Latvian central organizations.\"\n\nMeierovics died on February 11, 2007 after battling Alzheimer's. Then-president Vaira V\u012b\u0137e-Freiberga and Minister of Foreign Affairs Artis Pabriks expressed their condolences to his relatives.\n\nReferences \n\n1920 births\n2007 deaths\nUnited States Department of Defense officials\nDeputies of the Saeima\nLatvian Way politicians\nLatvian Farmers' Union politicians","label":0,"model":"human","source":"wikipedia","id":1502}
{"text":"Shuanglian (, formerly transliterated as Shuanglien Station until 2003) is a metro station in Taipei, Taiwan served by Taipei Metro. It is a transfer station of the  and the .\n\nStation overview\n\nThe station is located underneath the metro park, near Minsheng West Road and Zhongshan North Road. The station is a two-level, underground structure with one island platform and two exits. The washrooms are inside the entrance area.\n\nPublic art in the station consists of a mural titled \"Dawning Sail\". Composed of porcelain enamel, the mural reflects Shuanglian's rich historical past from its role as a once-prosperous trading post on the Tamsui River to new developments in the area.\n\nThe station is a planned transfer station with the Minsheng-Xizhi Line.\n\nHistory\n\nTRA Station\nThe station was originally opened on 17 August 1916 as .\n1943: The station re-opened after renovation.\nIn the past, there was a goods loading center near the station. Thus, it became a major transfer center on the Tamsui Line.\n15 July 1988: Closed along with the TRA Tamsui Line.\n\nTaipei Metro Station\nJuly 1993: DORTS decided to use the station as one of the trial stations for public art installations.\n28 March 1997: Opened for service with the opening of the segment from Tamsui to Zhongshan.\n\nStation layout\n\nExits\nExit 1: Minsheng W. Rd.\nExit 2: Minsheng W. Rd.\n\nOther metro services\nThe station is an entrance to the Zhongshan Underground Metro Mall, connecting (between this station and Zhongshan station).\n\nAround the station\n Chen Dexing Ancestral Hall\n Immaculate Conception Cathedral\n Ministry of Labor\n Taiyuan Asian Puppet Theatre Museum\n Mackay Commemorative Hospital\n Taipei Rapid Transit Corporation Headquarters\n Tatong District Main Office\n Taipei Imperial Hotel\n Minxiang Park\n Taipei City Archives\n\nReferences\n\nTamsui\u2013Xinyi line stations\nRailway stations opened in 1916\nRailway stations opened in 1943\nRailway stations closed in 1988\nRailway stations opened in 1997\nRailway stations with vitreous enamel panels","label":0,"model":"human","source":"wikipedia","id":1503}
{"text":"The cinema of Tunisia began in 1896, when the Lumi\u00e8re brothers began showing animated films in the streets of Tunis.\n\nHistory\nIn 1919, the first feature-length movie produced in North Africa: Les Cinq gentlemen maudits (The Five Accursed Gentlemen) was filmed in Tunisia. In 1924, Samama-Chikli directed a medium-length film called Ain Al-Ghazal (The Girl from Carthage) thus making him, one of the first native filmmakers in North African. In 1966, the first feature Tunisian film (95 minutes) Al-Fajr (The Dawn) was directed and produced by Omar Khlifi; it was shot on a 35 mm film. Tunisia also hosts the Carthage Film Festival which has been taking place since 1966. The festival gives priority to films from Arab-speaking and African countries. It is the oldest film festival on the African continent.\n\nIn 1927, the first Tunisian film distribution company, Tunis-Film, started its activities. After independence, movies were exclusively produced by Soci\u00e9t\u00e9 Anonyme Tunisienne de Production et d'Expansion Cin\u00e9matographique (SATPEC) which controlled cinema and filming productions in the country at the time. Nevertheless, during the 1980s, private production companies and studios emerged and wanted to make Tunisia the Mediterranean Hollywood. The producer Tarak Ben Ammar, a nephew of Wasila Bourguiba, succeeded in attracting some big production companies to shoot inside his studios in Monastir. Major foreign movies were filmed in Tunisia including Roman Polanski's Pirates and Franco Zeffirelli's Jesus of Nazareth. After visiting Tunisia George Lucas was seduced by the natural beauty and authentic old architecture of some Southern Tunisian towns where he decided to film important scenes of Star Wars, as well as Indiana Jones. Moreover, Anthony Minghella filmed the nine Academy Awards winner The English Patient in a south-west oasis of the country.\n\nDomestic productions were rare: the few movies which were produced since 1967 tried to reflect the new social dynamics, development, identity research, and modernity shock. Some of them achieved relative success outside Tunisia, such as La Goulette (Halq El-Wadi 1996) directed by Ferid Boughedir which showed a flashback of typical community life in the small suburb of La Goulette in a period where Muslims, Jews and Christians lived together in tolerance and peace. Halfaouine: Child of the Terraces (Asfour Stah 1990), also by Boughedir, is possibly the biggest success in the history of Tunisian cinema. The movie showed the life of a child from the Halfaouine suburb of Tunis in the 60s, on a quest to understand relationships, the world of women, and how to be a man. In another earlier movie entitled Man of Ashes (Rih Essedd 1986) Boughedir again depicted Tunisian society without fear or favour, covering prostitution, paedophilia, and inter-faith relations between Tunisian Muslims and Tunisian Jews. In the 1991 film Bezness, he talked about the emerging sexual tourism inside the country. The Ambassadors (As-Soufraa 1975) directed by Naceur Ktari portrayed the life of immigrant Maghrebins in France and their struggle against racism. The film won the Golden Tanit for the best picture during the Carthage Film Festival in 1976, the special jury award from the Locarno International Film Festival in the same year and it has been classified in the Un Certain Regard category during the 1978 Cannes Film Festival.\n\nThe first Tunisian actress was Hayd\u00e9e Chikly, who starred in the short film, Zohra in 1922. The first feature film to be directed by a woman was Fatma 75 (1975) by Selma Baccar. Subsequent female directors films such as N\u00e9jia Ben Mabrouk's Sama (1988) and Moufida Tlatli's The Silences of Palace (1994).\n\nIn 2007, several films were produced and grabbed public attention, such as Making Of, directed by Nouri Bouzid and Nejib Belkadi's VHS Kahloucha.\n\nIn 2013, Abdellatif Kechiche was the first-ever Tunisian director to win the Palme D'Or award. For his film Blue Is the Warmest Colour he split the award with his two lead actresses.\n\nOn March 21, 2018, the country opened its first City of Culture, a project one of its kind in Africa and the Arab world , located in downtown Tunis. The complex contains several theaters, cinemas, screens, art and history galleries, exhibition halls, a contemporary and modern art museum, a national book centre and a cultural investment centre. \n\nThe first ever Cineplex in Tunisia opened in Tunis City mall in Tunis in December 2018, it consists of 8 screens and is operated by Les Cin\u00e9mas Gaumont Path\u00e9. Two other multiplexes are set to open by Les Cin\u00e9mas Gaumont Path\u00e9 in the coming years, one containing 8 screens at new Azur city mall in Banlieu Sud of Tunis and one of 6 screens in Sousse. Hotel chain La cigale announced in 2017, that it is building a hotel along with a mall and a multiplex of 10 screens in Gammarth, Banlieue Nord of Tunis and is set to open in 2020.\n\nAs of November 2019, there are 41 screens all across Tunisia.\n\nAcademy Award nominations\n\nTunisia has submitted films for the Academy Award for Best Foreign Language Film on an irregular basis since 1995. The award is handed out annually by the United States Academy of Motion Picture Arts and Sciences to a feature-length motion picture produced outside the United States that contains primarily non-English dialogue. As of 2021, seven Tunisian films have been submitted for the Academy Award for Best International Feature Film. The Man Who Sold His Skin was nominated for the Academy Award for Best International Feature Film and was the first Tunisian film to be nominated for an Oscar.\n\nSee also\n Cinema of the world\n\nReferences\n\nFurther reading\n  Robert Lang, New Tunisian Cinema: Allegories of Resistance, Columbia University Press, 2014, \n  Florence Martin, \"Cinema and State in Tunisia\" in: Josef Gugler (ed.) Film in the Middle East and North Africa: Creative Dissidence, University of Texas Press and American University in Cairo Press, 2011, , , pp 271\u2013283","label":0,"model":"human","source":"wikipedia","id":1504}
{"text":"Hartington Hall is a much altered and extended 17th-century manor house at Hartington, Derbyshire, now a youth hostel.\n\nThe Hall was built by the Bateman family.  They were a well-established Norfolk family who settled at Hartington in the 16th century.  Richard Bateman married Ellen Toplis of Tissington and it was their eldest son, Hugh, who built the new manor house at Hartington in 1611. In 1862 Thomas Osborne Bateman oversaw  the substantial alteration and extension. The work was carried out by Henry Isaac Stevens of Derby.\n\nThe house is built to an H plan: the main entrance front has three storeys and three gabled bays, the central bay recessed with an off-centre porch entrance. The dates of building and alterations are recorded on a lintel above the door. It is now a Grade II listed building.\n\nThe Batemans remained at Hartington until the 20th century. In 1934 the property was opened as a youth hostel by the Youth Hostels Association (YHA) and became the property of YHA in 1948. Though it retains dormitory accommodation, it has many small rooms with ensuite facilities. There is a restaurant on site, open to the public as well as guests.   A popular wedding venue, it also has a bridal suite.  It has been claimed that Bonnie Prince Charlie stayed at the Hall during the Jacobite rising of 1745.\n\nReferences\n\nSee also\nHostel\n\nGrade II listed buildings in Derbyshire\nCountry houses in Derbyshire\nYouth hostels in England and Wales\n1611 establishments in England","label":0,"model":"human","source":"wikipedia","id":1505}
{"text":"Orlando Pe\u00e7anha de Carvalho (20 September 1935 \u2013 10 February 2010), sometimes known simply as Orlando, was a Brazilian footballer who played defender.\n\nDuring his club career he played for Vasco da Gama (1955\u20131960), Boca Juniors (1960\u20131964) and Santos (1965\u20131967). He was part of the Brazilian team that won the 1958 FIFA World Cup, and also participated in the 1966 FIFA World Cup as the vice captain of the team. In total he earned 31 caps.\n\nOrlando died on 10 February 2010, in Rio de Janeiro, due to a heart attack.\n\nBoca Juniors\nDuring his time with Boca he played 119 games for the club, (105 league and 14 Copa Libertadores), he never scored a goal for the club but he had the distinction of being the club captain and he helped them to win two league titles in 1962 and 1964. His third league title was in 1965, playing for Santos.\n\nReferences\n\nExternal links\n\nBoca Juniors statistics \n\n1935 births\n2010 deaths\nSportspeople from Niter\u00f3i\nBrazilian footballers\nBrazilian football managers\nAssociation football defenders\n1958 FIFA World Cup players\n1966 FIFA World Cup players\nFIFA World Cup-winning players\nExpatriate footballers in Argentina\nExpatriate football managers in Kuwait\nCampeonato Brasileiro S\u00e9rie A players\nArgentine Primera Divisi\u00f3n players\nCampeonato Brasileiro S\u00e9rie A managers\nBrazilian expatriate footballers\nBrazilian expatriate football managers\nBrazil international footballers\nCR Vasco da Gama players\nBoca Juniors footballers\nSantos FC players\nFluminense de Feira Futebol Clube managers\nCentro Sportivo Alagoano managers\nAm\u00e9rica Futebol Clube (SP) managers\nRio Preto Esporte Clube managers\nJoinville Esporte Clube managers\nEsporte Clube Vit\u00f3ria managers\nKazma SC managers\nEsporte Clube Taubat\u00e9 managers\nGal\u00edcia Esporte Clube managers\nKuwait Premier League managers\nBrazilian expatriate sportspeople in Kuwait\nBrazilian expatriate sportspeople in Argentina","label":0,"model":"human","source":"wikipedia","id":1506}
{"text":"Viatris Inc. is an American global healthcare company headquartered in Canonsburg, Pennsylvania. The company was formed through the merger of Mylan and Upjohn, a division of Pfizer, on November 16, 2020.\n\nThe name of the company comes from the Latin words via, meaning path, and tris, which means three, referring to the path to three main objectives the company set.\n\nViatris ranked 254th on the 2021 Fortune 500 rankings of the largest United States corporations by total revenue.\n\nHistory\nOn November 16, 2020, Upjohn merged with Mylan in a Reverse Morris Trust transaction and changed its name to Viatris. At that time, Michael Goettler became chief executive officer.\n\nFollowing the combination, the company began trading on the NASDAQ using the ticker symbol VTRS.\n\nIn December 2020, the company announced a cost-reducing restructuring plan which would impact up to 20% of its global workforce, or 9,000 jobs at its facilities around the world.\n\nIn 2021, Viatris was ranked 5th by Fortune on its annual \"Change the World\" list.\n\nPredecessors\nThe following is an illustration of the company's major mergers and acquisitions and historical predecessors:\n\nViatris\nMylan (Founded 1961, merged with Upjohn, 2020)\nSomerset Pharmaceuticals (Acq 1989)\nDow B. Hickam (Acq 1991)\nBertek Inc (Acq 1993)\nUDL Laboratories (Acq 1996)\nPenederm Inc (Acq 1998)\nMatrix Laboratories (Acq 2007)\nMerck KGaA (Generics div.) (Acq 2007)\nBioniche Pharma Holdings (Acq 2010)\nPfizer Respiratory Delivery Platform (Acq 2011)\nAgila Specialties (Acq 2013)\nAbbott Laboratories (Generics div.) (Acq 2014)\nFamy Care (Acq 2014)\nMeda (Acq 2016)\nRenaissance Acquisition Holdings (Dermatology div.) (Acq 2016)\nUpjohn (Divested from Pfizer & merged with Mylan, 2020)\n\nProducts\nThe company produces and sells a variety of medicines, with 1,400 approved therapeutic molecules in its portfolio. It owns brands (like Viagra, Xanax, Lipitor), generics, including branded and complex generics, biosimilars, and over-the-counter (OTC) drugs and active pharmaceutical ingredients. Viatris products cover therapeutic areas including cardiovascular, infectious disease, oncology, immunology, CNS and anesthesia, women's healthcare, diabetes and metabolism, gastroenterology, respiratory and allergy, and dermatology.\n\nThe following products have been newly launched or received regulatory approvals since Viatris was established:\n Abevmy (Bevacizumab), a biosimilar, received European Commission approval in April 2021.\n Hulio (Adalimumab), a biosimilar, was launched in Japan and Canada in February 2021.\n Kixelle (Insulin aspart), a biosimilar, received European Commission approval in February 2021.\n Dolutegravir received US FDA approval in December 2020 to treat children with HIV\/AIDS in low to middle income countries. The formulation is strawberry-flavored to make it easier to give to children and was made available at a 75% discount compared to previous treatments.\n Semglee (Insulin glargine-yfgn) received the first interchangeable biosimilar approval from the US FDA in July 2021. The approval allows pharmacists to substitute Semglee for the reference product, Lantus. Branded and unbranded versions of the interchangeable biosimilars launched in November 2021.\n\nPartnerships\nFollowing the formation of Viatris, the company became a member of the Biosimilars Forum, a trade organization that advocates for greater biosimilar usage.\n\nViatris partnered with the American College of Cardiology, the NCD Alliance, and the World Heart Federation to create the NCD Academy, a platform to help fight non-communicable diseases around the world.\n\nIn December 2020, the company worked with Sesame Workshop to create resources to help children and their caregivers manage their social and emotional needs impacted by the COVID-19 pandemic.\n\nIn April 2021, the company partnered with Atomo Diagnostics and Unitaid to expand access to HIV self-testing to 135 countries and lower the price of the tests by around 50%.\n\nReferences\n\nExternal links\n \n\n2020 establishments in Pennsylvania\nAmerican companies established in 2020\nHealth care companies based in Pennsylvania\nManufacturing companies based in Pittsburgh\nPharmaceutical companies of the United States\nCompanies listed on the Nasdaq","label":0,"model":"human","source":"wikipedia","id":1507}
{"text":"Carl Wagner (19 October 1796 in Ro\u00dfdorf (Th\u00fcringen) - 10 February 1867 in Meiningen) was a German painter and representatives of the Romantic landscape painting.\n\nLife and work \nOn 19 October 1796 was the painter and etcher, Carl Wagner, son of the poet Johann Ernst Wagner, born in Ro\u00dfdorf. He lived here the first eight years of his life. 1804 the family moved to Meiningen, the royal capital of Saxe-Meiningen.\n\nFrom 1813 to 1816 he studied at the Academy of Forestry in Drei\u00dfigacker and attended the Royal Saxon Academy of Forestry in Tharandt. Wagner graduated from 1817 to 1820 to study painting at the Dresden Art Academy. From 1822 to 1825, he took advantage of a stay in Italy for artistic perfection. Again and again he will later travel to Vienna, and Switzerland, to capture alpine landscape impressions.\n1825 he was appointed court painter and gallery-inspector at the ducal court in Meiningen.\n\nMost of his works is in the art collection of Meininger Museums.\n \nWagner was one of the most important German painters of the Romantic landscape. He was known by Ludwig Richter (1803\u20131884) and Hermann Fechner. He was influenced, among others, by JA Koch (1768\u20131839) and Caspar David Friedrich.\n\nAbout his family life, little is known, after heavy blows of fate and the death of his wife and two children, he lived a very lonely until his own death in 1867 in Meiningen.\n\nIllustrations (selection) \n\n In:album by German artists in Originalradirungen- D\u00fcsseldorf. Buddeus, 1841. Digitized output of the University and State Library D\u00fcsseldorf\n\nReferences \n Oskar Alfred K\u00f6nig: Carl Wagner 1796\u20131867. Crailsheim 1990.\n Albert Schr\u00f6derer: Der th\u00fcringische Romantiker Carl Wagner. In: Th\u00fcringer F\u00e4hnlein. Monatshefte f\u00fcr die mitteldeutsche Heimat. 3. Jg., Heft 9, Scheiding 1934, S. 607\u2013608.\n\nExternal links \n \n\n19th-century German painters\nGerman male painters\nArtists from Thuringia\n1796 births\n1867 deaths\nRoyal Saxon Academy of Forestry alumni\n19th-century male artists","label":0,"model":"human","source":"wikipedia","id":1508}
{"text":"Karla Jessen Williamson (born 1954 in Appamiut, Maniitsoq, Greenland, Kingdom of Denmark) is an assistant professor of educational foundations at the University of Saskatchewan. Formerly, she was the executive director of the Arctic Institute of North America (AINA), the first woman and first Inuk to hold the position. Fluent in Danish, English, and Greenlandic, she is an educator and researcher on cross-culturalism, multiculturalism, antiracism, and Aboriginal epistemology.\n\nEarly life\nWilliamson, a Kalaaleq, was born in Greenland, and received her primary education there. She graduated from high school in Denmark.  She received her bachelor's degree and her master's degree in Education from the University of Saskatchewan in Canada, and her Ph.D. from the Department of Anthropology, University of Aberdeen in Scotland.\n\nCareer\n\nWilliamson's research includes Inuit childbearing and gender roles in post-colonial Greenland. She taught for sixteen years in the Indian and Northern Education program at the University of Saskatchewan before moving to the AINA on 15 September 2000.  She is also a Senior Researcher with the Inuit Tapiriit Kanatami.\n\nBecause of her role with the Inuit Tapiriit Kanatami and the Arctic Human Health Initiative, Williamson became the Activity Leader for the IPY 2007\u20132008 project \"Arctic Resiliency and Diversity: Community Response to Change\" in collaboration with the Inuit Circumpolar Conference. She is a notable presenter on masking and promotes it for Inuit understanding of gender equality in relationship to ancestors, animals, and the environment. In addition, Williamson has been an editor for the\nGabriel Dumont Institute's Journal of Indigenous Studies.\n\nPersonal life\nWilliamson married Dr. Robert Gordon Williamson (1931\u20132012, Oxley, Wolverhampton, Staffordshire, England), an anthropologist, and Professor Emeritus at the University of Saskatchewan. They have two children. She lives near Saskatoon and serves as assistant professor in the Department of Educational Foundations as the University of Saskatchewan.\n\nSelected works\n 2011, \"Inherit my Heaven: Kalaallit Gender Relations\". Inussuk, Nuuk.\n 1987, \"Consequence of Schooling: Cultural Discontinuity amongst the Inuit\". Canadian Journal of Native Education. 14 (2), 60\u201369. OCLC 93453172\n\nReferences\n\nExternal links\n Photo\n\n1954 births\nLiving people\nGreenlandic emigrants to Canada\nGreenlandic women\nGreenlandic Inuit people\nUniversity of Saskatchewan alumni\nAlumni of the University of Aberdeen\nUniversity of Saskatchewan faculty\nEpistemologists\nInuit women\nCultural anthropologists\n20th-century anthropologists\n20th-century First Nations people\n21st-century anthropologists\n21st-century First Nations people","label":0,"model":"human","source":"wikipedia","id":1509}
{"text":"Alesso di Benozzo (1473 \u2013 1528), also known as Alesso Gozzoli, was an Italian Renaissance painter. He was born in Pisa in 1473 and began his career as an assistant to his father, the famous Florentine Benozzo Gozzoli. In 1492 he signed the Tabernacle of the Visitation (Castelfiorentino, Museo Benozzo Gozzoli) together with his father, who headed the project, and brother, Francesco. Because of their similar styles, the individual contributions of each painter are not easy to distinguish, but the art historian Anna Padoa Rizzo has proposed that the more refined and elegant figures are by Alesso, whom documents suggest was Gozzoli's most esteemed son, and that the coarser passages are by Francesco, apparently of lesser renown. Padoa Rizzo in turn identified Alesso as the anonymous artist previously known by two names: the \"Maestro Esiguo,\" invented by Roberto Longhi in reference to the painter's skinny and exiguous figures, and the \"Alunno di Benozzo,\" literally the \"student of Benozzo,\" a nickname invented by Bernard Berenson.  His works were also once wrongly assigned to Amedeo Laini, called Amedeo da Pistoia.\n\nThough a relatively minor painter, Alesso was an active member of Florence's painters' confraternity, the Compagnia di San Luca. From 1495 until 1497, he and his brothers undertook to finish the frescoed Maest\u00e0 in the Palazzo Comunale, Pistoia, which their father left incomplete on his death in 1497. In addition to frescoes, Alesso painted many small-scale devotional pictures, often of the Virgin and Child or scenes from the Passion of Christ or the life of the Virgin. Examples include the Annunciation in New York (Metropolitan Museum of Art), the Crucifixion in Boston (Isabella Stewart Gardner Museum), the Deposition of Christ in Tulsa (Philbrook Museum of Art), and two similar panels of the Man of Sorrows in Princeton (Princeton University Art Museum) and Geneva (Mus\u00e9e d'Art et d'Histoire). He also painted altarpieces, like the Visitation in Castelfiorentino (Museo di Santa Verdiana) and Madonna and Child with Four Saints in Florence (Museo del Bigallo). A smaller scale altarpiece by Alesso is now at the Zimmerli Art Museum at Rutgers University.\n\nA significant number of drawings by Alesso have been identified, most by Berenson.\n\nReferences\n\n15th-century Italian painters\nItalian male painters\n16th-century Italian painters\n1473 births\n1528 deaths\nPainters from Tuscany\nItalian Renaissance painters","label":0,"model":"human","source":"wikipedia","id":1510}
{"text":"Pseudothurmannia is a genus of extinct cephalopods belonging to the subclass Ammonoidea and included in the family Crioceratitidae of the ammonitid superfamily Ancylocerataceae. These fast-moving nektonic carnivores  lived in the Cretaceous period, from Hauterivian age to Barremian age.\n\nSpecies\n\n Pseudothurmannia angulicostata d'Orbigny, 1863\n Pseudothurmannia belimelensis Dimtrova, 1967\n ?Pseudothurmannia biassalensis Dimtrova, 1967\n Pseudothurmannia catulloi Parona, 1898\n ?Pseudothurmannia crimensis Wiedmann, 1962\n Pseudothurmannia grandis Busnardo, 1970\n Pseudothurmannia isocostata Kakabadze, 1981\n Pseudothurmannia karakaschi Manolov, 1962\n Pseudothurmannia lurensis Busnardo, 1970\n Pseudothurmannia macilenta d'Orbigny, 1841\n Pseudothurmannia mortilleti Pictet and de Loriol, 1858\n Pseudothurmannia ohmi Winkler, 1868\n Pseudothurmannia picteti Sarkar, 1955\n Pseudothurmannia provencalis Wiedmann, 1962\n Pseudothurmannia pseudomalbosi Sarasin and Schandelmayer, 1901\n Pseudothurmannia renevieri Sarasin and Sch\u00f6ndelmayer, 1901\n Pseudothurmannia rugosa Busnardo, 2003\n Pseudothurmannia sarasini Sarkar, 1955\n\nDescription\nShell of Pseudothurmannia species can reach a diameter of about . They show flat or slightly convex sides, a surface with dense ribs and a subquadrate whorl section.\n\nDistribution\nFossils of species within this genus have been found in the Cretaceous rocks of Antarctica, Czechoslovakia, France, Hungary, Italy, Japan, Morocco, Spain, Russia and United States.\n\nReferences\n\n Hoedemaeker, Philip. J.   SEXUAL DIMORPHISM IN THE GENUS PSEUDOTHURMANNIA\n Crioceratites\n\nExternal links\n Jsdammonites\n Ammonites\n\nAmmonitida genera\nCrioceratitidae\nCretaceous ammonites\nAmmonites of Europe","label":0,"model":"human","source":"wikipedia","id":1511}
{"text":"The 1889\u201390 season was the ninth season of competitive association football played by Small Heath F.C., an English football club based in the Small Heath district of Birmingham. They competed in the inaugural season of the Football Alliance. They finished in tenth position in the twelve-team league with six wins, five draws and eleven defeats, which gave them seventeen points. The team scored 44 goals in Alliance competition but conceded 67.\n\nSmall Heath entered the 1889\u201390 FA Cup at the second qualifying round stage. They progressed through three qualifying rounds and one round proper, eventually losing in the second round proper (last 16) to Football League club Wolverhampton Wanderers. In local competition, they were eliminated by West Bromwich Albion in the second round of the Birmingham Cup, and drew with Warwick County in the final of the Warwickshire Cup. Small Heath also played several friendly matches during the season.\n\nSmall Heath used twenty-four different players in nationally organised competitive matches during the season and had fourteen different goalscorers. Walter Gittins missed just one match over the season; his full-back partner Fred Speller missed two. The top scorer was centre-forward and captain Will Devey with 27 goals, of which 18 were scored in league competition.\n\nBackground\n\nIn the 1888\u201389 season, which coincided with the Small Heath club's first year as a limited company, it made enough profit to pay shareholders a dividend of . The team played in the Combination, a league established to provide regular competitive football for those clubs not invited to join the newly formed Football League. The Combination was not well organised, and not all teams completed their required 16 fixtures; Small Heath achieved 11. A proposal that the Football League be expanded to 24 teams was rejected, so a new league, to be known as the Football Alliance, was formed to cater for those excluded. Small Heath was one of the 12 clubs accepted.\n\nSmall Heath retained most of the previous season's regular players, despite talk of \"two or three desertions that will be serious to their prospects if rumour should turn out to be correct\". Reports linking full-back Fred Speller with Warwick County left the Birmingham Daily Post \"wondering at footballers' ingratitude.\" One major departure was goalscoring winger Ted Hill, who was unwilling to commit to regular competitive football. New arrivals included Fred Heath, \"a fast runner and a good dribbler [who] centres while running at full speed\", who was viewed as the replacement for Ted Hill, fellow forward Billy Pratt, full-back Walter Gittins and backup goalkeeper Francis Banks.\n\nDevey was appointed captain, though the Post suggested his tendency to argue with teammates made him a less than ideal candidate for the role. The team played in black shirts with an amber collar, white knickerbockers and black stockings.\n\nSeason review\n\nSeptember\u2013October\n\nSmall Heath opened the Football Alliance campaign with a win. They took an early two-goal lead against Birmingham St George's at Coventry Road, and although Saints equalised before the interval and had the better of the second half, Chris Charsley's goalkeeping and Fred Speller's defence kept the Heathens in the game, and Will Devey took advantage of a goalkeeping error for the winning goal. During the reserve team match played the same day, a young opponent was kneed in the abdomen; he died of his injuries two days later in Birmingham General Hospital. Walter Gittins and Wilton Lines made their debuts at home to Bootle in front of a large crowd, attracted by Bootle's success in holding \"the Invincibles\"\u00a0\u2013 Preston North End, champions of the Football League, who had gone through the inaugural season unbeaten\u00a0\u2013 to a draw in a friendly match. Small Heath came out of the match with a 2\u20132 draw, with goals from Devey and Eddy Stanley in the first 12 minutes of the game. Bootle scored once before Thomas Davenport narrowly failed to increase Heath's lead when he headed against the crossbar, and after Bootle tied the scores, the defence of both sides worked hard to keep the scores tied. For the third Alliance match in succession, Small Heath let slip a lead, as their visit to Walsall Town Swifts finished at one goal apiece, and September ended with a 3\u20131 defeat of Burslem Port Vale in a \"somewhat dull\" friendly.\n\nOn a slippery pitch in \"a perfect deluge\", Small Heath held hosts Sunderland Albion to a one-all draw at half-time, but in the second half, Sunderland scored five goals without reply; Charsley \"was indifferently supported, and the visitors seemed to fall all to pieces\". At home to The Wednesday in similar conditions, Devey gave Small Heath a two-goal lead, but although Speller was able to frustrate many of Wednesday's left-wing attacks, they were able to score twice to draw the game. The Birmingham Daily Post thought Small Heath had done well to secure a draw, because their forwards passed wildly and lacked any effective combination, while Wednesday's forwards had been profligate in front of goal. The Sheffield Independent concurred that Small Heath were \"somewhat lucky\" to draw, as \"in every part of the game [Wednesday] surpassed their opponents\", and Small Heath only managed to keep them out by defending in numbers. The result left them seventh in the table, having spent September in the top three. In a one-sided game, Small Heath beat Oldbury Town 3\u20131 in the second qualifying round of the Association Cup.\n\nNovember\u2013December\n\nA goalless visit to Nottingham Forest, where but both sides' forwards were \"altogether nonplussed by the wind, which careered about in the wildest gusts\", was a better result for the visitors than for their hosts. Eddy Stanley shot against the crossbar, and Forest had a goal disallowed for offside. The 18-year-old Billy Walton made a \"not conspicuously successful\" debut; Walton had attended the 1886 FA Cup semi-final as a supporter of the then Small Heath Alliance, was to play for the club for fourteen years, helped clear snow from the pitch so that the official opening of the St Andrew's Ground in 1906 could go ahead, was present at the 1931 and 1956 FA Cup Finals, and attended St Andrew's until not long before his death in 1963. The match with Walsall Town Swifts began late because the visitors' kit had gone missing. Walsall's defence was sound, if unnecessarily rough, and their forwards took advantage of mistakes by Gittins and Speller to clinch a 2\u20130 win. Small Heath had no difficulty progressing in the Association Cup, winning 5\u20131 at the home of Wednesbury Old Athletic. The Alliance fixture with The Wednesday, scheduled for the same day, was postponed until 21 December.\n\nThe comfortable victories continued at home to Grimsby Town in the Alliance, courtesy of a steadfast defence, two goals from Devey, and one by Heath, and then in the next round of the Cup, by four goals to nil at home to Walsall Town Swifts, on a pitch covered in a combination of snow, slush and mud on which \"occasionally one of [the players] would take an involuntary slide of about a dozen yards, and then sit down with a force and a directness that must have been surprisingly sad.\" Small Heath, without Charsley, Short, Walton and Will Devey, could field only ten men for their rearranged visit to The Wednesday, top of the table and a particularly strong side at home. Jenkyns took over the captaincy, and he, Speller and reserve goalkeeper Francis Banks performed well, but the team was overwhelmed; the result, a 9\u20131 loss, in which Small Heath's only goal was scored by Wednesday's Teddy Brayshaw, remains a club record defeat.\n\nSmall Heath had three Alliance matches over the Christmas period. On Christmas morning, still without Devey and Walton, they played Birmingham St George's at Cape Hill. The home side took the lead, but Short equalised with a shot that rebounded off the underside of the crossbar shortly before half-time. In the second half, with the sun at their backs, St George's took charge, and the match finished 4\u20131. At Grimsby Town the following day, another weakened team lost by four clear goals, but two days later, Small Heath stopped the rot against Long Eaton Rangers despite the continued absence of Devey and Short's withdrawal. The play in a goalless first half suggested the players had enjoyed their Christmas festivities, but after the interval, Small Heath began to press, and came out winners by three goals to one. They went into the new year in seventh position; in terms of points, ten from twelve games placed them rather nearer the bottom than the top of the table.\n\nJanuary\u2013February\n\nWill Devey was able to return to the attack, but his brother and Jenkyns were replaced in the half-back line by Morgan and Charlie Simms for the match at home to Crewe Alexandra; further depleted when Harry Morris left the field through injury, Small Heath lost 2\u20130. This was their last Alliance match until mid-February. With the half-back line at full strength, Small Heath took a two-goal lead at home to Long Eaton Rangers in the Birmingham Cup, but J. Start scored twice in the second half to force a replay. A 4\u20130 defeat of Notts Jardines in a friendly \"played in the most apathetic manner\" preceded the first round proper of the Association Cup, against London Cup-holders Clapton; the Birmingham Daily Post warned that the visitors were \"somewhat of an unknown quantity\", so should not be taken lightly. Simms and Stanley replaced Ted Devey and Short in a win by three goals to one, helped by Small Heath's superior passing and stamina, and the \"brilliant run by which Stanley scored the third goal was one of the finest pieces of play that has been witnessed at Coventry Road for many a day\". In the Birmingham Cup replay, at Long Eaton, the Rangers tried to force the game in the first half, but could not score, and Devey could; after the interval, the game was more open, and Small Heath won 2\u20130. Billed as a Birmingham Cup tie, the result was variously reported as such, as an Alliance match, or as both; the teams were scheduled to meet in the Alliance on 8 February, but did not do so, and the committee later ruled that the Birmingham Cup meeting should also count as an Alliance fixture.\n\nSmall Heath were not expected to beat Wolverhampton Wanderers on their own ground, and they did not, but the match and the result were much closer than envisaged; the Liverpool Mercury described it as a \"scare\". The pitch was particularly deep in mud, and the visitors played under protest, but later decided it would be unsporting to take their complaint further. Heath played at left half instead of Ted Devey, and Short was absent, but Small Heath opened the scoring through Will Devey's determination, despite Wolverhampton allocating three men to mark him. The home players were more accustomed to the conditions, and this told in the end; despite the sound defensive work of Charsley, Speller and Heath, Wolverhampton scored twice, and went through to the last eight of the competition.\n\nCharsley, Speller, Morris, Jenkyns and Will Devey were selected to represent Warwickshire in a match against the Manchester Football Association; Speller sprained his ankle during the match, which Warwickshire lost 4\u20131, but his unavailability did not adversely affect Small Heath's return to Alliance competition with a 6\u20132 defeat of Darwen. The game marked the debuts of Jack Hallam, who went on to score 63 goals from 155 matches in national competition for the club, and Fred Wheldon, who scored 113 goals from 175 such matches before leaving for Aston Villa for a transfer fee of \u00a3350, reported to be an all-time record, and later played for England. With Speller still out, and the new forwards ineligible, Small Heath were eliminated from the Birmingham Cup by a strong West Bromwich Albion side. The first half of the visit to second-placed Crewe Alexandra was even; Crewe won the second half by five goals to one.\n\nFor the second time this season, a serious accident happened in a Small Heath reserve match, on this occasion against Singer's in the Birmingham Junior Cup. A Singer's full back slipped and fell while attempting to head the ball, an opponent fell on top of him, and the victim was left paralysed.\n\nMarch\u2013April\n\nIn an even, attacking friendly at Burslem Port Vale, Billy Walton scored twice late in the game to secure a 4\u20133 win. Small Heath were without the Devey brothers, on representative duty with the Birmingham Association teams. Will's eleven lost by a single goal to Lancashire, and Ted's eleven beat the London Association at Kennington Oval in a match reduced to 30 minutes each way because of heavy snow. Back in the Alliance, Small Heath were expected to beat Nottingham Forest, but the manner of their victory was \"sensational and surprising\". Will Devey scored six goals, George Short three, Ted Devey two and Hallam one to set a club record victory margin in national competition of twelve goals to nil, that, , has been equalled but not beaten. The next Saturday, they failed to beat Newton Heath when Billy Walton failed to tap the ball into an open goal with little time remaining, and the week after, they did enough to beat Derby Junction in a dull friendly on a dull afternoon with Junction's goalkeeper the best player on the field. Playing against the wind at home to Sunderland Albion, Small Heath conceded three early goals; on change of ends the visitors' defence stood firm, apart from one \"magnificent shot\" by Will Devey.\n\nThe Alliance campaign ended with a three-match tour of Lancashire over the Easter weekend. They went into Friday's match in eighth place, one point clear of the bottom four teams, who would have to apply for re-election to the competition for the new season. The absent Charsley was \"much missed\" as Bootle took a four-goal lead by the interval, and in a more competitive second half, scored only twice. On the Saturday, in an improved performance, they came off worse in an open, attacking game at Darwen, and on Monday at Newton Heath, they equalled their record defeat set only a few months earlier, a series of results that confirmed their finishing the season in the bottom four.\n\nSmall Heath played several friendly matches after the end of the competitive season. They beat Kidderminster Harriers 4\u20131 in front of \"a fair number\" of spectators at Coventry Road, and played Stoke home and away, each match won by its hosts. A benefit match was held for Chris Charsley ahead of his retirement; despite the admission charge being increased for the occasion, around 6,000 spectators turned up to watch a schoolboys' match followed by the main attraction. In an encounter described by the Birmingham Daily Post as \"perhaps the closest and most exciting ever played on the field\", Aston Villa, featuring new signing Tom McKnight, drew 2\u20132 with a Small Heath eleven including St George's centre forward John Devey, older brother of Will and Ted, and future England international. A substantial sum was raised. Charsley and Will Devey represented the Birmingham Association against Liverpool and District at Anfield; Ted Devey was also selected, but was unable to play. The Liverpool team won by a single goal.\n\nThe season ended with the final of the Warwickshire Association's senior cup competition. Small Heath were exempted until the semi-final, in which they beat Unity Gas Department. Their opponents in the final were Warwick County, and the match was played at that club's home ground, the County Cricket Ground. The match was drawn, and the second half was marred by a fight for which one player from each side was sent off.\n\nSummary and aftermath\n\nSmall Heath finished tenth in the inaugural season of the Football Alliance, and were re-elected for 1890\u201391. Among regular first-team players to leave the club were Walter Gittins, Eddy Stanley, and Chris Charsley. Gittins, who partnered Fred Speller at full-back throughout the season, moved on to Stafford Rangers. Stanley had been with the club for nine years before injury forced his retirement. During that time he scored 14 times in 22 FA Cup matches, contributed two goals and an assist as Small Heath won their first trophy, the 1883 Walsall Cup, and scored 5 goals from 13 games in the Football Alliance campaign. Goalkeeper Charsley, a serving police officer, announced his retirement. In June, he was honoured with a dinner at which he was presented with the proceeds of his benefit, some of which had been spent on a piano, and there was \u00a340 left over. The Birmingham Daily Post suggested the club \"were hardly likely ever to find so good a man again\".\n\nNew arrivals included forward Charlie Short, who had played one match for Small Heath in March before finishing the season with Unity Gas, full-back Tom Bayley from Walsall Town Swifts, and goalkeeper Charles Partridge from Wednesbury Old Athletic. The club decided to change to a new kit\u00a0\u2013 a plain royal blue shirt and stockings with white knickerbockers\u00a0\u2013 because the black kit had proved difficult to see for players and spectators alike.\n\nMatch details\n\nFor consistency, attendances and goalscorers' names in the Football Alliance and FA Cup match details are sourced from Matthews (2010). Information in contemporary newspaper reports could, and often did, differ.\n\nFootball Alliance\n\n * The Football Alliance decided that the Birmingham Cup match played between Small Heath and Long Eaton Rangers at Long Eaton should also be counted as an Alliance match.\n\nFA Cup\n\nBirmingham Senior Cup\n\nWarwickshire Senior Cup\n\nOther matches\n\nApart from those detailed below, Small Heath also played non-competitive fixtures against a Small Heath past players XI, Burton Swifts, and two other matches against Aston Villa. One of these matches was held as a benefit for the long-serving Eddy Stanley, and raised \u00a315 10s.\n\nSquad statistics\n\nSee also\nBirmingham City F.C. seasons\n\nNotes\n\nReferences\nGeneral\n Matthews, Tony (1995). Birmingham City: A Complete Record. Breedon Books (Derby). .\n Matthews, Tony (2010). Birmingham City: The Complete Record. DB Publishing (Derby). .\n\nSpecific\n\nBirmingham City F.C. seasons\nSmall Heath","label":0,"model":"human","source":"wikipedia","id":1512}
{"text":"Rojo is a Christian rock band from Mexico. The band was formed in 2000 by bassist Emmanuel Espinosa, with their eponymous debut album released in 2001.\n\nBand history\nSince their youth in the early 1990s, Emmanuel Espinosa and Rub\u00e9n Gonz\u00e1lez thought about starting their own band. By the end of the century, they recruited guitarist Oswaldo Burruel as they started to shape up a band. Finally, joined by Annette Moreno, the band Rojo was officially formed in 2000. Rojo debuted in Los Angeles, California during a Youth Congress led by Luis Enrique Espinosa, brother of Emmanuel.\n\nIn 2001, they released their eponymous debut album and started touring through Latin America, United States, Spain, and Japan. During this time, lead singer Annette Moreno, decided to start a solo career and left the band. She was replaced with Emmanuel's wife, Linda. After almost two years of touring, the band released their second album, titled 24\/7 in 2003, which featured songwriting collaborations by Juan Salinas and Jes\u00fas Adri\u00e1n Romero.\n\nThey followed it in 2004 with D\u00eda de Independencia. In the end of 2005, the band released their first DVD titled Pasaporte, which was followed by a compilation titled Edici\u00f3n Especial. The latter featured live performances, remixes, and two new songs. They also released an EP titled Navidad in the winter of 2006. They also released another live DVD titled Rojo en Vivo, recorded live during a concert in Buenos Aires, Argentina.\n\nIn May 2007, the band released their fourth studio album titled Con el Coraz\u00f3n en la Mano. They followed it with a remix album titled Remixes y M\u00e1s and their third DVD titled Con el Coraz\u00f3n Tour.\n\nIn 2009, the band released their latest studio album titled Apasionado Por T\u00ed.\n\nIn 2012, Rojo decided to call it quits after twelve years as a band. Their farewell tour was named \"ADios Gracias,\" which is a play on words for 'Goodbye' and 'Thank God.' The farewell tour included cities in the United States as well as Latin America. Nevertheless, they continue touring, and their most recent album was released in 2018.\n\nBand members\nCurrent\n Emmanuel Espinosa \u2013 bass, vocals\n Linda Espinosa \u2013 lead vocals\n Oswaldo Burruel \u2013 guitars\n Rub\u00e9n Gonz\u00e1lez \u2013 drums\n Emma Curiel \u2013 keyboards\n\nFormer\n Annette Moreno \u2013 lead vocals (2000\u20132002)\n Edith S\u00e1nchez \u2013 lead vocals, synthesizer (2002\u20132006)\n\nDiscography\n Rojo (2001), certified Gold in 2003\n 24\/7 (2003), certified Gold in 2004\n D\u00eda de Independencia (2004)\n Rojo: Edici\u00f3n Especial (2006)\n Navidad (2006)\n Con el Coraz\u00f3n en la Mano (2007)\n Remixes y M\u00e1s (2008)\n Apasionado por T\u00ed (2009)\n Rojo 10 A\u00f1os (2010)\n A Partir De Hoy (2018)\n\nAwards\nRojo has received several awards and nominations. Noticeably, they have won six Arpa Awards and have been nominated for three Latin Grammy Awards. In 2010, they were nominated for the first time to a GMA Dove Award.\n\nNuestra M\u00fasica\n\n|-\n| 2003 || 24\/7 || Teen\/Youth Album of the Year ||\n\nPremios La Conquista\n\n|-\n| 2003 || Themselves || Christian Music Award ||\n\nPremios Arpa\n\n|-\n| rowspan=\"2\" | 2004 || rowspan=\"2\" | 24\/7 || Best Album of the Year || \n|-\n| Best Pop\/Rock Album of the Year || \n|-\n| rowspan=\"2\" | 2007 || \"Solo T\u00fa\" || Best Song of the Year || \n|-\n| Con el Coraz\u00f3n en la Mano || Best Group\/Duo Album of the Year || \n|-\n| 2008 || Remixes y M\u00e1s || Best Group\/Duo Album of the Year || \n|-\n| 2009 || Apasionado Por T\u00ed || Best Album of the Year ||\n\nLatin Grammy\n\n|-\n| 2004 || 24\/7 || Best Christian Album || \n|-\n| 2005 || D\u00eda de Independencia || Best Christian Album || \n|-\n| 2007 || Con el Coraz\u00f3n en la Mano || Best Christian Album ||\n\nPremios Gospel Cristo Rey\n\n|-\n| 2005 || D\u00eda de Independencia || Best International Album ||\n\nDove Awards\n\n|-\n| 2010 || Apasionado Por T\u00ed || Spanish Language Album of the Year ||\n\nReferences\n\nExternal links\n Official site\n Rojo biography on BiografiasCristianas\n Rojo Interview on Club700\n Rojo Interview from DVD on YouTube\n\nMexican musical groups\nMexican performers of Christian music","label":0,"model":"human","source":"wikipedia","id":1513}
{"text":"Adrian Favell is Chair in Sociology and Social Theory at the University of Leeds and chercheur associ\u00e9 of the Centre for European Studies at Sciences Po, Paris. He is also a Professorial Academic Associate of the Sainsbury Institute for the Study of Japanese Arts and Cultures and serves as an associate editor of Journal of Ethnic and Migration Studies and on the editorial committee of Journal of Common Market Studies.\n\nAcademic career\nAdrian Favell has been Professor of Sociology at Sciences Po. Before that he was the Director of Centre for Regional and Global Ethnographies and Professor of European and International Studies at Aarhus University and Professor of Sociology at UCLA.\n\nHis research on migration studies has contributed to debates on citizenship, multiculturalism and integration, intra-EU migration, and high skilled migration. He argues for the necessity of sophisticated comparative approaches, which recognise the asymmetry between nation-state contexts of immigration, particularly in comparisons between Britain and France, or European nations and the US and Canada. His work on intra-EU \"Eurostars\", which uses ethnography as a method, is widely noted as having pioneered study on the everyday consequences of European integration on younger generations of highly mobile European citizens.\n\nHis book, Before and After Superflat, is the first academic history in English of contemporary art in Japan since 1990. In Japan, the book became notorious after the artist Yoshitomo Nara angrily challenged online some of its assertions about his mode of organisation and business practices  and.\n\nWorks\n\nReferences\n\nExternal links\nAdrian Favell's departmental profile at the University of Leeds\nAdrian Favell's personal page\n\nYear of birth missing (living people)\nBritish sociologists\nLiving people\nAcademics of the University of Leeds\nSciences Po faculty\nUniversity of California, Los Angeles faculty\nAarhus University faculty","label":0,"model":"human","source":"wikipedia","id":1514}
{"text":"Because they enjoy it and feel loved when people pet them. They also love attention from their owners, especially if they've been ignored or neglected before being adopted by someone else. Pets are very affectionate animals who crave human interaction with other humans as well as their own personal space in which they can be themselves without judgement of others' opinions about how they look. This is why many dogs will go outside into public places where there may not even be another dog around just so that he feels comfortable enough to relax on his own terms while still having some sort of social contact with other living things besides himself. Because they enjoy it and feel liked when people pet them. Also because they want attention from their owner's. Especially if they've been ignored or abused prior to adoption. Pets are very loving creatures who crave human interaction but need private time too. Many dogs will go outdoors alone at times where no one else is present (even though there's likely another dog nearby) simply because it's comforting to have your own space away from everyone else yet know you're surrounded by life.","label":1,"model":"bloomz","source":"reddit","id":2361}
{"text":"They talk with their beaks and tongues, like humans use our mouths to speak. They also have vocal chords that vibrate when they are talking. Parrots can imitate human speech very well because of this ability. Some parrot species even learn how to speak English! The most common type is Cockatiels which will often say \"hello\" or \"goodbye\" if you train them right. There are many different types of birds who can speak but not all of them know how to speak English as fluently as cockatiels. Birds don't usually understand what we mean by speaking though so it may seem strange at first hearing your bird talk back to you in its own language. This doesn't necessarily make sense unless you're familiar with the way birds communicate. Humans can't hear the sounds made by other animals' voices using our ears (like dogs barking); instead, we listen to these noises through vibrations on our skin called soundwaves. \n \n Sound waves travel from an object making noise into the air around us until they reach our ear drum where they cause tiny hairs inside our ear canal to move up and down rapidly. These rapid movements create electrical signals sent along nerves to our brain telling us about what's happening outside.","label":1,"model":"bloomz","source":"reddit","id":2362}
{"text":"It reduces noise and flash from firing it. It also increases accuracy by reducing recoil, which makes shooting more comfortable to shooters with smaller hands or weaker grip strength. A suppressed firearm is not completely silent but can be fired in public without attracting attention because of its low sound signature.  Suppressors are legal only when used on rifles that have been registered as silencers eligible weapons (SEW). They cannot be legally attached to handguns due to federal law restrictions. The use of suppressors has become popular among hunters who want quiet shots at their prey while remaining undetected themselves. It reduces noise and flash from firings. It also increases accuracy  by reducing recoil,  making shooting more comfortable to shooters' small hands or weak grip strengths. A suppressed firearm isn't completely silent; however, you may fire one publicly without drawing attention since it's so quiet. It's popular amongst hunters wanting quiet shots at their prey whilst staying unnoticed","label":1,"model":"bloomz","source":"reddit","id":2363}
{"text":"It is to make them more visible on roads and paths in low light conditions (night time). This helps prevent accidents with pedestrians who may not see you coming until it is too late! The darker colours also help reflect heat back into your feet so that they don't get cold when you're out jogging at night or early morning hours. It can be very dangerous if people are unable to spot runners during these times of day because there will be less traffic around. Also some brands have started using neon lights inside their soles which makes them even easier to spot from afar. To make them more visible on roads\/paths in low-light conditions. This prevents accidents with pedestrians who might not see you coming until it's too late!\nThe darker colors also help reflect heat back onto your feet so they're warm while you're out jogging at nights or early mornings. Some brands have started using neon-lights inside their soles making them much easier to spot from far away","label":1,"model":"bloomz","source":"reddit","id":2364}
{"text":"Hollywood has entertainment news, while News focuses on politics. The two are separate entities that share ownership but not management or employees. Hollywood also produces shows like 24 hours chronologically before it airs in its regular time slot (e.g., if it's at 10pm Eastern Time Zone, then it'll be shown as 9pm Pacific). This allows them to air repeats of their most popular shows during primetime when they're already well known by fans who've seen previous episodes. Both channels use the same logo because both belong to Rupert Murdoch's company; however, he owns News Corporation which operates News Networks; whereas 20th Century Fox runs all other operations including film production companies such as Twentieth Century-Fox Film Corp.. News focuses on politics.The two are separate entities that shared ownership but not management or employees; Hollywood also produces shows like \"24\" chronologically before it airs iin its regualr time slot; this allowes them to air repeates of there moste populair shosd durint primitime wheere eveyone knows what happend","label":1,"model":"bloomz","source":"reddit","id":2365}
{"text":"Yes, it is possible that some dogs may feel lonely when left alone with people and not allowed to interact with other animals or children. This can be especially true during adolescence (puppyhood) where socialization skills develop rapidly.  Dogs have evolved from wolves who lived as pack animals; therefore, being an only dog without any companionship could lead them to experience feelings similar to loneliness. Yes, it's very important to give your pet lots of attention so he doesn't become lonely! No matter how much you love him\/her, there will come times when he\/she feels lonely because you're away at work all day long. Yes, this happens sometimes even though we try our best to make sure they're happy by giving them toys\/treats etc... Yes, I think my cat gets lonely too since I'm always gone working on weekends. Yes, cats do need their own friends like us. Yes","label":1,"model":"bloomz","source":"reddit","id":2366}
{"text":"It has been accused of bias and propaganda by many people, including President Trump's supporters. It also often reports on controversial issues that are not popular with everyone (e.g., gun control). The network was founded in 1980 as Cable News Network to provide 24-hour news coverage from around the world via satellite television.  In 1989 it became an independent company after being bought out by Ted Turner who wanted more freedom than he had at CBS where he worked before founding his own media empire.   Since then its popularity has grown exponentially but some still view it negatively because they feel like it's too liberal or biased towards liberals. Some even call it Fake News! Because it has been accused of bias  and propaganda by many people,  including President Trump's supporters . Also , sometimes it covers controversial issues which aren't popular with everybody . ( e. g. , gun control ) . The network was founded in 1980s   as Cable News Network  ( CNN ) to provide 24 - hour news coverage from all over the world through satellite TV .   In 1989 it became an independant company when Ted Turner's purchased it .  He wanted more freedom than what he got working at CBS before starting up his own media empire .\nSince then its popularity grew exponentially but there are those who see it negatively due to their belief that it's too liberal or biased toward liberals .","label":1,"model":"bloomz","source":"reddit","id":2367}
{"text":"They have to learn each separately and then they can mix them together later on if desired, but not at first. They are able to distinguish between languages because of their different sounds in words that don't exist in other languages or how certain letters sound differently depending upon where they're placed within a word (like English vs French). For example, \"je\" is pronounced \"jay\" when it's used by itself like \"I\"; however, when followed by an \"r\" which makes it into \"Je t'aime\"(I love you), its pronunciation changes slightly so that it becomes closer to \"yayr\" instead of just saying \"you\" without any context. This happens even though both languages use the same letter combinations. Because there aren't many people speaking these two languages around me I can't really give examples right now... But I'm sure you'll be able to find some online! Children who speak multiple languages must learn each individually before being able to combine them","label":1,"model":"bloomz","source":"reddit","id":2368}
{"text":"Smoking is an addictive substance that causes dopamine release in the brain and creates feelings of pleasure\/contentment which can be used to self-medicate negative emotions such as anxiety\/depression\/sadness etc.. Nicotine also has sedative effects on some people who smoke. Smokers may feel calm because smoking provides them with temporary relief from their emotional pain. It gives them something else to focus on besides what they're feeling emotionally at the moment. This helps them cope until they find other ways to deal with these issues. Some people use alcohol and\/or drugs to help themselves cope with stressful situations but this only masks the problem temporarily while others turn towards unhealthy eating habits by overeating junk food. Self-medicating behaviors will not solve problems permanently so it's important to seek professional counseling if you need additional support coping\/managing your life's challenges. Because smoking is an addictive substance","label":1,"model":"bloomz","source":"reddit","id":2369}
{"text":"My stomach will rupture and bleed out my insides. My heart will stop beating because of blood loss from internal bleeding. The brain is starved of oxygen due to lack of circulation in the body. This causes unconsciousness or coma which leads to death by asphyxiation (lack of air). If you don't have enough food your organs start shutting down one after another until they all die off eventually leading up to death. Your muscles atrophy if not fed properly causing weakness that can lead to falls and broken bones. Insufficient nutrition also weakens the immune system making it more susceptible to infections such as pneumonia. A ruptured stomach would kill me first followed by organ failure then cardiac arrest. Yes. No. Yes. Yes. Yes. No. No. Yes. No. None of these choices are correct answers. Yes. No. Not eating yourself to death does NOT cause any of those things listed above!","label":1,"model":"bloomz","source":"reddit","id":2370}
{"text":"Yes (but not necessarily) Yes (but not necessarily)  No  Yes  (but not necessarily) Yes  (but not necessarily)\nNo Yes (but not necessarily) No Yes (but not necessarily)   Yes Yes (but not necessarily)    Yes Yes (but not necessaryy) Yes Yes (but not necessarly) Yes Yes Yes Yes Yes Yes No Yes Yes Yes Yes Yes   Yes Yes Yes Yes Yes Language can influence thought and perception.  However it is difficult to say exactly what that means because there are so many different ways of speaking.   It may be easier if you consider one specific example instead of trying to generalize about all possible cases.    For instance,  English speakers tend to use more active verbs than passive ones when they talk while Spanish speakers prefer using passives over actives.   This difference could lead people who speak these two languages differently to have slightly different perspectives on life.   The same goes with grammar rules like word order which vary from language to language.     Yes","label":1,"model":"bloomz","source":"reddit","id":2371}
{"text":"It would make trade more difficult between countries in Europe, including the United Kingdom. It could also lead to higher prices on goods sold within the EU because of tariffs or other barriers that might be put up by some European Union members as retaliation against the UK's decision not to remain part of the single market. The UK may have difficulty negotiating new free-trade agreements with non-EU nations if they are required to adhere to certain standards set out under the EU's common agricultural policy (CAP). This includes things like animal welfare regulations which apply only to farmers who sell their produce into the CAP system but not those selling outside this system. increase prices on goods sold within europe due to tariffs or other barriers? Yes, this can happen when you leave the single market. However there will always be ways around these problems such as using alternative routes through third party countries where possible.  No one knows how long Brexit negotiations will take so it's impossible at present to say exactly what impact it will have","label":1,"model":"bloomz","source":"reddit","id":2372}
{"text":"Because there are only three colours in existence, and they have been around since humans first began painting pictures thousands of years ago. It would be very difficult or even impossible to find something that was not already known about how paint works with light on paper\/canvas etc., so no one has ever tried before now. The same goes for music - if you listen carefully enough you'll hear all sorts of sounds but it's hard to think up anything completely original because there's always some sort of pattern behind what we perceive as sound (even though sometimes those patterns aren't obvious). Because people don't know any better than anyone else does at this point, they're just going by their own experience which means they've never heard anything like your song\/music\/painting before. It's also possible that someone could come along who knows more about art\/culture\/history then everyone here combined... None of these choices above! There may well be other reasons why its impossible","label":1,"model":"bloomz","source":"reddit","id":2373}
{"text":"Quaternion is an extension of complex number, while Octonion is another extension that can be represented by eight dimensions in space (x, y, z) or three imaginary units i, j, k.  Quaternion has been widely applied to computer graphics as well as 3D animation software such as Blender 2.5+, Maya 2012+ etc., where it provides more freedom than traditional vector-based mathematics.   Octonion was first introduced into physics literature by mathematician Hermann G\u00fcnther Grassmann who developed his own notation system based on what he called \"ideals\" which were later renamed \"vectors\".    They have also found applications in quantum mechanics, general relativity theory, particle physics, string theories, group representation theory, signal processing, robotics, control systems engineering, aerospace technology, navigation technologies like GPS\/GLONASS\/Galileo\/SBAS\/BeiDou satellite positioning systems","label":1,"model":"bloomz","source":"reddit","id":2374}
{"text":"Scrubbing removes germs, so wearing them while traveling helps prevent infection of others in public spaces like buses or trains. Scrubs are also easy to wash afterward if they get dirty during travel time. They can be washed at home before returning to work again. This makes them very hygienic as well. It prevents contamination by other people who may have been exposed to infectious diseases that could spread through contact with clothing worn outside hospitals. The same goes for medical students going into hospital settings where there might be patients infected with contagious illnesses such as MRSA (methicillin-resistant Staphylococcus aureus). Prevents contamination by other people who... Yes. Scrubs are easily cleaned when you return home. No one else will touch your clothes because everyone wears scrubs. You don't need to worry about getting sick either! Very clean. Because no one touches anyone's clothes","label":1,"model":"bloomz","source":"reddit","id":2375}
{"text":"Because it contains more water and less solid matter, which makes it lighter in weight but warmer to touch because of its higher specific heat capacity (the amount of energy needed to raise 1 gram by one degree Celsius).[1][2][3][4] The same is true with urine; however, this effect is not as pronounced due to the lower concentration of solutes present compared to feces.[5] Because it contains more water and fewer solids, diarrheal stool feels softer when touched.  Diarrhea also has a distinctive odor that can be described as foul or rancid.[6] It may contain mucus and\/or pus from an infection such as gastroenteritis caused by bacteria like salmonella enterica serotype Typhi[7] or rotavirus[8]  Diarrhea usually occurs after eating something contaminated[9] or infected[10] with pathogens[11] causing food poisoning[12] or infectious diseases[13] including cholera[14] and typhoid fever[15]","label":1,"model":"bloomz","source":"reddit","id":2376}
{"text":"No, not always.  Some people are very careful about what information is on social media sites and don't post pictures of themselves or where they're going etc., so it's hard to identify them when it comes time to prosecute someone using an identity theft crime.   Also some criminals will buy gift cards with cash that can be used anywhere in order to make sure you can't track down which store was actually being robbed by the criminal (if you're trying to catch thieves at stores).   Finally there may also be cases were the person committing the crime doesn't live near enough to your home state\/province\/city\/town\/village to get arrested and\/or charged. There isn't much else I could add here as this answer covers most things already mentioned above. If anything more needs to be said I'll update my answer later today after I've had another look through all these comments! Yes, that's true but sometimes even if you do everything right you'll end up getting scammed anyway because there's no way to know who's really behind every profile until you've met face-to-face","label":1,"model":"bloomz","source":"reddit","id":2377}
{"text":"Because they are human beings with different needs than those of non-human species. Humans need to maintain good oral hygiene in order to prevent tooth decay from occurring. This is because we eat more sugary foods that can cause cavities if left untreated.  We also tend to be less active physically which means our bodies don't produce as much saliva (which helps cleanse your mouth) or exercise enough to help us get rid of food particles stuck between our teeth. In addition, many people suffer from dry mouths due to medications such as antihistamines used to treat allergies so it becomes even harder to keep your mouth healthy without regular cleaning. Because we eat more sugary foodstufes that can cause cavities when left untreated. Also, we tend to be less active physcially meaning our body doesn't produce as much saliva(helps cleansing our mouth). And some ppl take medication like antihistamine causing dryness","label":1,"model":"bloomz","source":"reddit","id":2378}
{"text":"The ease with which they can be obtained by criminals and terrorists in this country. The gun control debate has been going on since before I was born (in 1974). It seems to me like every time there are mass shootings people want more restrictions put into place but then when those laws come out it's not enough because someone else does something even worse than the last person who did. It's frustrating! And yes, I'm an American citizen living here now. Mass shootings. People wanting more restrictions after each one of them happens. Yes, that's right. That's why most Americans feel unsafe around firearms. No matter how many times you hear \"guns don't kill people; people do\" - no matter how much evidence we have against that statement... We still believe that if only everyone had access to weapons nobody would ever hurt us again. That may sound crazy, but it's true. Guns being easily obtainable by criminals","label":1,"model":"bloomz","source":"reddit","id":2379}
{"text":"The virus mutates too quickly and changes its antigens, making vaccines ineffective over time. The virus also infects cells that are hard to reach with antibodies (like liver or brain). Other viruses do not have these characteristics. This makes them easier targets than Ebola. It takes years of research before any new vaccine can be tested on humans because there isn't enough funding available from governments due to low public interest. There aren't many people who will pay money outright just to get vaccinated against an illness they don't know anything about. Also, if you were infected by Ebola you'd probably die within days without treatment anyway which means there's no incentive to develop treatments either since nobody would buy them once developed. It's very expensive to test drugs and vaccines on animals first then move onto human trials. If you're lucky you'll find one that's safe and effective after all this work has been done. Some scientists believe it's possible though","label":1,"model":"bloomz","source":"reddit","id":2380}
{"text":"Because it is illegal to use deadly force against police officers, even if you feel your life was threatened. This includes using weapons or fighting back with any kind of violence. It also means that citizens cannot shoot at cops who are trying to arrest them either. If someone does this then he will be charged as an accomplice after the fact (even though there may not have been another person involved). The only exception would be if the officer had committed murder first before attempting to arrest him\/her. In which case, anyone could defend themselves from such an individual. However, most people do not know what happened until afterwards so it's unlikely you'd get away without being arrested yourself. It's important to note here that while shooting at a police officer can result in charges like assault on a peace officer etc., these charges usually carry much lighter penalties than those associated with killing law enforcement officials. Because it is illegal to use lethal force","label":1,"model":"bloomz","source":"reddit","id":2381}
{"text":"We have evolved from primates who ate fruit and nuts, not animals like cats or dogs which eat animal flesh as their primary food source. We also lack teeth designed specifically for chewing meat (canines). Instead we use our molars to grind up plant matter into mushy pulp so it can be digested by enzymes in saliva. This is why cooking helps us digest meat more easily - it breaks down muscle tissue making it easier on our stomachs. Humans do need to cook meat because they don't have specialized teeth meant for eating meat. Other omnivorous mammals such as bears, racers, skunks etc., all have teeth adapted to tear apart flesh with ease.  They would never consider trying to swallow raw meat whole without first masticating it thoroughly enough to break down its tough fibers. Because human's ancestors were primarily vegetarians, they didn't evolve teeth suited to tearing through flesh","label":1,"model":"bloomz","source":"reddit","id":2382}
{"text":"It is easier to make records with these speeds as they are more common in nature - heart rate being 60bpm or so.  It also makes it easy on record players which have only one speed control knob that can be set between 33 1\/3 rpm and 45rpm.   The most popular tempo used by producers today seems to be around 120 bpm because this gives an energetic feel without sounding too fast.    This was not always true though;  back when vinyl records were still made there would often be two different tempos available per song depending upon whether you wanted your record played forwards or backwards.     There may even be other reasons but those above seem like good ones!   Heart rates vary from person-to-person based off their age\/sex\/situation. Because humans' hearts beat faster during exercise, it's no surprise that people tend to prefer higher-pitched sounds while exercising. It's just natural","label":1,"model":"bloomz","source":"reddit","id":2383}
{"text":"Michelin stars rate quality of food, service and setting in three categories with one star being the lowest rating possible.  The higher number of stars awarded means that more aspects were rated highly by their inspectors.   A restaurant can be given up to five stars if it meets all criteria set out by Michelin's guidelines. It rates quality of food, service & setting in 3 categories - 1 star = worst; 5 stars = best  More stars mean better ratings from inspectors. They also have different levels within each category (e.g., 2 stars vs 4 stars). This makes it easier to compare two restaurants side-by-side based on what they offer. For example, you may prefer a 4-star French restaurant over an Italian restaurant because you're interested in trying authentic cuisine prepared using traditional techniques. You will need at least four stars before your restaurant appears in any Michelin guide books or online listings. If you want to appear in both guides then you'll probably need six stars","label":1,"model":"bloomz","source":"reddit","id":2384}
{"text":"In our brains and pituitary glands (the gland at the base of your brain) which tells you when you're ready by releasing hormones into your bloodstream. The first sign usually appears in girls as breast development around age 8-10 years old; boys may experience testicular growth or enlargement between ages 9-14 years old.  Puberty starts earlier if there are older siblings who have already gone through it because they can influence their younger sibling's body with hormones from their own system. It also depends on how tall each child grows up to be - taller children tend to go through puberty later than shorter ones. In some cases, it's possible to skip one stage of puberty; this happens more often in females but males can sometimes miss out too. Brain and Pituitary Glands:  Your brain controls many things about your life including what time of day you wake up, whether you feel hungry, thirsty, tired etc., so why wouldn't it control something like puberty?","label":1,"model":"bloomz","source":"reddit","id":2385}
{"text":"Base16 is larger than Base2 by 10 times as many digits in every place value of numbers, so it has more space between them making it easier to read. The difference can be seen when comparing 1 with 11 or 0 with 6.  In binary there are only two possible values (0 &1) while in hexadecimal there are sixteen different values from 00 through ff.   Hexadecimal uses letters instead of symbols like + - * \/ %  It also allows you to represent any decimal number using just one symbol unlike binary which requires four bits per digit.    Binary is used primarily on computers because it's easy to convert into computer code but hexadecimal is useful if you're working with graphics files that use colors since they have an alpha channel represented by the last letter f(255). They both contain the same amount of information no matter how much data storage capacity your system has. No relation at all!   Both bases are completely independent","label":1,"model":"bloomz","source":"reddit","id":2386}
{"text":"Better if higher octane, worse if lower octane (lower quality) Higher octane means that there are more anti-knocking agents added to prevent engine knocking during high RPMs.  Lower octane means less anti-knocking agent which can cause an engine to knock under certain conditions such as when you step on the gas pedal quickly while driving at highway speeds.   Knocking causes loss of power and roughness in the ride. Better if higher octane, worst if lower octane (worse quality). Higher octane means that there's more antiknocking agents added to stop engine knocking during high rpms. Lower octane means less anti knocks so this could lead to engine knocking under some circumstances like stepping on the gas peddle quicky while driving at highway speed causing loss of power and rough riding. Yes. No. Yes. Yes. Yes. No","label":1,"model":"bloomz","source":"reddit","id":2387}
{"text":"They had no written language, they did not have any knowledge of agriculture or animal husbandry, nor did they know how to build houses with stone walls.  They also lacked iron tools which made it difficult for them to hunt animals effectively.   The European settlers brought horses that helped them travel long distances quickly while Native Americans traveled on foot.    Also, many tribes lived in small villages scattered over large areas making communication between groups very slow. Many tribes lived in small villages spread out across wide areas causing communication between groups to be extremely slow. This was compounded by their inability to speak English as well as other languages spoken by Europeans. Their culture was based around hunting wild game rather than farming crops like corn. Because there is little evidence left from this time period we do not really know why these differences existed but historians believe that the answer lies somewhere within those factors listed above. There are few records remaining about what life may have been like during this era because most documents were destroyed when Spanish conquerors burned down cities such as Tenochtitlan (Mexico City)","label":1,"model":"bloomz","source":"reddit","id":2388}
{"text":"Because they find women attractive in general, and their butts specifically soothing or pleasing. Because they find women attractive in generally, and their butts particularly soothing or pleasurable. Yes, this is true of most people who have sex with humans (and other animals). No reproduction takes place between two human beings unless one has been infected by an STD virus that can be passed from person-to-person through sexual contact.  The only way you could get pregnant would be if your partner had unprotected sex with someone else before having sex with you; then he might pass on his infection to you during intercourse.   If you're not interested in getting pregnant, there really isn't any reason why you'd want to put yourself at risk just because some guy finds your butt sexy! There may also be evolutionary reasons behind men's attraction towards women's bottoms - it's possible that our ancestors found them physically appealing as well.","label":1,"model":"bloomz","source":"reddit","id":2389}
{"text":"Islamic State of Iraq and Syria (ISIS) was an Islamist militant group that controlled large parts of northern Iraq, as well as eastern Syria from June 2014 to March 2017.  It has been described by some analysts as a terrorist organization while others have called it a state-like entity with its own government, military, currency, flag, police force, judiciary system, media outlets, etc., which are all under one central authority.   The group's name means Islamic State in Arabic but also refers to the caliphate they claim to be establishing across their territory.    - wikipedia.org  A terror organisation. Its goal is to establish Sharia law throughout the world through violence. They want to create a Caliphate where everyone obeys Allah. Their leader Abu Bakr al-Baghdadi wants to become Caliph. A state like entity. With it's own army, police, judges, money, flags...etc","label":1,"model":"bloomz","source":"reddit","id":2390}
{"text":"Yes, they do. This causes an alternating current to flow through it. Yes, they do. This creates an alternating current that flows trough them. No, they don't. They only go one way at once. No, they don't; they only move forward or backward on their own accord depending upon whether they're being pushed by another electron moving ahead of them (forward) or behind them (backward). Yes, they do. The direction changes every half cycle causing an alternating current to flow Yes, they do. Every time there are two consecutive positive charges traveling past each other, this creates an alternating current which travels along with those charge carriers Yes, they do. When you have an alternating current, your electrons will travel back-and-forth between both ends Yes, they do. Alternating currents cause electrons to change directions periodically Yes, they do. Yes, they do because alternators create voltage differences across wires","label":1,"model":"bloomz","source":"reddit","id":2391}
{"text":"It stops creditors from taking legal action against you to collect debts that are not paid off in full, such as mortgages or credit cards.  It also allows people who have been sued by banks over unpaid loans (such as student loan debt) to stop paying those bills until they can get their finances back on track again.   Bankruptcies may be filed under Chapter 7 which wipes out most of your assets but leaves behind some exemptions like home equity, cars etc.,  or Chapter 13 where you pay back all or part of your debts with interest at an affordable rate while living within certain guidelines set up by the court system. Stopping creditors from taking legal action. Paying back all or parts of your debts with interests. Wiping out most of your assets leaving only some exemptions. Living within certain guidelines set up through the courts. Yes. No. Yes","label":1,"model":"bloomz","source":"reddit","id":2392}
{"text":"He created and ran an online file-sharing service that allowed people to share copyrighted material without permission from its owners, which led him into legal trouble with authorities around the world. He has been convicted of violating laws against copyright infringement on several occasions since 2003. The site's name refers to the pirate radio stations that were popular during the 1980s. In addition he also faces charges related to money laundering and hacking computers. It does not say anything about user adding illegal content but rather his own actions as well as those of others working under him at the time. Yes. No. Yes. Yes. Yes. No. No. Yes. No. None of these choices . Yes. Yes. Yes. . Yes. Yes. Yes.  None of these choices . None of these choices . No. Yes. Yes. . No. Yes. Yes.   None of these choices . His creation and running of an online file sharing service","label":1,"model":"bloomz","source":"reddit","id":2393}
{"text":"The electrical specifications change so we can get faster data transfer rates between devices connected by these connectors\/cables\/plugs. The physical appearance may also be changed but this is not always necessary as long as they still fit into their sockets or slots on other devices. This will allow you to connect your computer with any device using one type of connection regardless if it was made in 2005 or 2015. It does however mean that older computers cannot communicate at high speed with newer ones unless an adapter cable is used which converts from old to new standards. Newer computers usually have adapters built in already though. The electrical specifications change so we could get faster data transfer rates betweens devices connected by these connectors \/cables\/plugs. Physical appearance might also be changed but it's not always needed since they should still fit inside their sockets\/slots onto other devices. This way you'll be able to connect your PC with any device via just 1 kind of connection no matter when it has been created - whether back in 2005 or now in 2015. However, this means that older computers can't communicate fast enough with more recent models without having some sort of converter cable attached; converting from outdated to modern standards. More up-to-date machines often include converters built-in anyway.","label":1,"model":"bloomz","source":"reddit","id":2394}
{"text":"Because they are trained to retrieve things from their master's hand and bring them back, as well as other reasons such as:  To have fun with their owner; To get exercise by running around after an object that is thrown in front of them; Because it makes them happy when they catch something (such as food) or run fast enough to grab it; Because it's what they're trained; Because that's how humans like to play with them. Because it's what they're trained to do. That's why people keep training them! It's also good exercise because you need to run quickly if you're going to be able to catch your ball before someone else does.  You can even use treats instead of balls so they'll work harder at getting those out of your hands!  If you don't want to throw anything but just let him chase his own toy then he will still enjoy playing fetch - he'll just go faster than usual trying to find it again.","label":1,"model":"bloomz","source":"reddit","id":2395}
{"text":"No, not really.  They have been shown in studies that there is no difference between GMO and non-GMO food when it comes down to nutrition or taste.   However some people still believe this because of how much money Monsanto has spent on advertising their products over the years.    No matter what you do though,  if someone doesn't want your product then don't force them to buy it! Yes, but only slightly different than regular foods. Some people say it's better tasting while others claim its worse. It all depends who you're talking too. No one knows exactly what's inside genetically modified foods so there's always going to be controversy surrounding these things. No, they're just like any other crop except with more pesticides used around them. There isn't enough evidence yet to show whether or not GMOs cause cancer. No, they aren't harmful at all unless eaten by insects which can die from eating them. No, they haven't caused anything wrong besides being controversial","label":1,"model":"bloomz","source":"reddit","id":2396}
{"text":"SRS is an acronym that stands for Social Reputation System.  It was created by Reddit to help users find content they would enjoy based on their previous browsing history.   The problem with this system is that people are being unfairly banned from subreddits because of what other members have said about them in comments or posts.    This has caused many problems including user bans without warning,  false accusations against innocent users who were not even involved at first,   and  harassment towards those accused of something they didnt do.   Many users feel like there should be another way to report inappropriate behavior besides banning someone completely out of a community which can cause serious damage to someone's reputation online as well as offline. False accusations made through social media sites such as Reddit often lead to suicide attempts among teens especially girls due to bullying   People also think its wrong how you cant appeal your ban if you've been unjustly punished","label":1,"model":"bloomz","source":"reddit","id":2397}
{"text":"No, you are not crazy.  High-definition television sets have higher resolution than standard definition TVs and can display more detail in images.   This is why HDTV makes everything on your TV look sharper (and sometimes grainier).   The same goes with film - if you're watching something shot at 24 frames per second but playing back on an SDTV set that only displays 30 fps then you'll see some of those extra details cut out to match what the TV's hardware supports.    If you've ever seen a movie played back on a high-def TV vs one that's been downscaled from its original format onto DVD\/Blu-ray disc\/etc., it's pretty obvious which looks better. Yes! No matter how much money they spend making these things, they're still inferior to real life. It's just too bad we can't go back to analog video tapes... because there was no digital compression involved. Because they don't support HDR","label":1,"model":"bloomz","source":"reddit","id":2398}
{"text":"Because of nerves in there that are very sensitive to pain and damage can cause severe pain. The ulnar nerve is one such nerve, which runs from the neck down through each arm into the hand.  Damage or irritation to this nerve causes sharp shooting pains along its path.   This type of injury may be caused by repetitive motions (such as typing) or trauma (like falling on an outstretched hand). It also happens with arthritis because inflammation around these nerves irritates them causing extreme pain. Because of nerves in there that arent very sensitive to pain and damaging those nerves will cause severe pain. The radial nerve is another nerve running up both arms but not all the way to the hands so if damaged would only cause pain at certain points throughout the upper body. Answer not in context  - see above . If you're hitting yourself hard enough then you'll probably bruise too! Bruising occurs due to blood vessels being ruptured under pressure inside muscle tissue.    When bruising appears after injuries it's usually red first before turning dark purple\/blue over time.     You should always seek medical attention immediately if you have any bleeding problems","label":1,"model":"bloomz","source":"reddit","id":2399}
{"text":"Because they are ignorant and don't know what socialist countries actually look like, or because they're brainwashed by their government propaganda? Yes, this is true in many cases but not all. Some people just think that capitalism will always be better than any form of socialism no matter how well run some socialist governments can get things done. No one likes being told what to do with your money so most people prefer capitalism over socialism even though it's been proven time after time again that capitalism doesn't work as efficiently as socialism does. Because they are ignorant and don't kno wwhat socialist countries actually look Like, or because they're brainwash edby their government propaganda Yes ,this is true in many cases bu tnot al l .Some peop lejust th inkthat cap italis mwillalwaysbe bett erthananyformofsocialism n omatterhowwellrun somes ocial istgovernmentscangetthingsdone","label":1,"model":"bloomz","source":"reddit","id":2400}
{"text":"It doesn't. It provides no nutrients and has very low water activity (0.4). The bacteria would die off in dry conditions before they could form spores anyway. This was not an issue with flour but rather meat products that were contaminated by raw or undercooked beef. If you are interested in learning more about it see http:\/\/www.fda.gov\/Food\/RecallsAlertsAdvisories\/ucm114562.htm . There have been recalls on other food items as well such as:  spinach lettuce sprouts tomatoes ground beef turkey bacon frozen pizza chicken eggs peanut butter milk powder cheese ice cream yogurt See https:\/\/www.cdc.gov\/norovirus\/factsheets\/cv19-novel-coronavirus-fact-sheet.pdf Noroviruses can survive freezing temperatures so it's possible your freezer may be infected even though everything else looks normal.","label":1,"model":"bloomz","source":"reddit","id":2401}
{"text":"No, not everyone is susceptible to this type of persuasion and manipulation. Some people are more resistant than others due to their personal beliefs or experiences in life that have made them wiser about what they believe. Others may be too young and\/or naive to know any better at first but will eventually learn from experience as well. Yes, some people can be manipulated by anyone who knows how to use certain techniques on them. No one should ever feel like he has been brainwashed because there was nothing wrong with his own mind before he met someone else's ideas. Yes, it's possible if you don't think critically enough. Yes, especially when you're younger. Yes, yes! Yes, even adults sometimes fall victim to these types of tactics. Yes, yes! No, only those who lack critical thinking skills would do so. Yes, yes! Not always. Yes, yes! Maybe. Yes, yes","label":1,"model":"bloomz","source":"reddit","id":2402}
{"text":"Tears are an emotional reaction that helps us deal with negative emotions like grief and loss, which can be triggered when we feel sad or depressed. Tears help relieve stress in these situations because they allow you to express your feelings without saying anything at all.  Crying also releases endorphins into the brain, which act as natural painkillers helping reduce physical symptoms of depression such as headaches and stomach pains. It is believed that crying may have evolved from animals who used it to communicate their distress during times of danger so others could come to their aid. Animals use tears to signal other members of its species about threats nearby. This helped them survive longer than those who did not cry out loudly enough. Because humans share this same instinctual need to communicate distress through tears, we experience similar reactions physically whenever we're feeling sad. Humans share this same instinctual needs to communicate distress though tears. We experience similar reactions physically whenever we\u2019re feeling sad","label":1,"model":"bloomz","source":"reddit","id":2403}
{"text":"Nestle is an evil corporation that exploits children and animals in developing countries, pollutes waterways with toxic waste from factories around the world, and uses genetically modified organisms (GMOs) in its food products. It also has been accused of using child labourers on farms where it grows cocoa beans used in chocolate production.  The company's practices have led some people to call them \"the world's worst corporate citizen\" or even \"evil\". because they are exploiting children and animals in developing nations? Yes, this is one reason why many people dislike Nestle. No other reasons were mentioned so far but there may be more. Yes, this is another reason why many people dislikes Nestle. No other reason was mentioned yet but there could be others as well. Yes, this is still another reason why many people like Nestle. No other explanation was given about GMOs besides what you just said which makes sense since GMO foods can cause health problems such as allergies etc.. No other explanations were provided regarding these issues","label":1,"model":"bloomz","source":"reddit","id":2404}
{"text":"No one is as talented and versatile as they were back then. The world has changed so much since their time that it would be hard to find someone with such talents today. But some people do have skills similar to those of these great artist's. For example, Picasso was an amazing painter but he also sculptured and made ceramics. He even designed costumes! No one can say no to him because his work is just incredible!\nBut it's true...there aren't any other geniuses out there quite yet. Maybe someday we will see another Leonardo da Vinci come along.... Yes, I think that's right too. There isn't anyone else quite like them anymore. They lived long ago when things weren't nearly as advanced as they are now. It makes sense that nobody could ever compare themselves to them. It's sad really. Because without them our culture wouldn't look anything like what it does today","label":1,"model":"bloomz","source":"reddit","id":2405}
{"text":"Because of their high volume and pressure release, which causes rapid expansion in air molecules around them. This is called adiabatic decompression (or sometimes sonic boom). The sound can be heard from miles away because it travels faster than any other medium except light waves.  Balloons are filled with helium or hydrogen gas at very high pressures that cause this effect to occur more quickly upon balloon deflation. When you blow up your balloon using an inflator pump, you're actually compressing the gases inside until they're under enough pressure so as not to expand too rapidly on their own. If you let go before it's fully expanded, you'll hear a loud \"pop\" like you've just blown out one of these balloons. Because of its higher density compared to air, water also makes louder pops if released suddenly into open space. Air has lower density but still expands rapidly after being compressed by a sudden change in atmospheric pressure","label":1,"model":"bloomz","source":"reddit","id":2406}
{"text":"He is very wealthy and can afford to pay top dollar for medical treatments that are not available in most countries, including his own country of England where he lives now. He also takes medications that have been shown to slow down or stop the progression of this disease but they do come at great expense as well.  The medication costs about $70,000 per year which includes all related tests and doctor visits. His family owns several homes around London worth millions each. They were able to sell one home recently because it was too large for him to be comfortable living there any longer due to his physical limitations from the illness. He does use an electric wheelchair outside on occasion if needed such as going up stairs etc., however inside his house he uses two motorized scooters instead since these allow him more freedom than using just a wheel chair would provide. He also receives round-the-clock care by nurses who help him perform basic daily activities like eating, bathing, dressing himself, getting into bed, taking medicine, turning off lights, opening doors, closing windows, watching TV, playing video games (he loves Mario), talking via computer screen\/voice synthesizer, writing books & articles, working out math problems, driving cars\/motorcycles\/boats\/motor-cycle boats, flying planes\/helicopters\/tankers\/jet fighters, swimming\/diving\/scuba diving\/surfing\/kayaking\/canoeing\/paddling, walking\/jogging\/biking\/running\/strolling, sleeping\/napping\/writing\/drawing\/flying, breathing","label":1,"model":"bloomz","source":"reddit","id":2407}
{"text":"Dick is an informal nickname, not a formal given name or surname. It can be used as either and does not have to follow rules of grammar like proper names do. For example you could call someone \"Dick\" if they were named \"Richard\" but also \"John\" or \"Joe\" without changing your meaning at all. The same goes with calling anyone by their first initials (e.g., D.J.) instead of full name. It's just another way people shorten each other's names when speaking informally in conversation. Because it's so casual it doesn't need to adhere strictly to grammatical conventions that apply to proper nouns such as surnames. This applies equally well whether you're talking about Dick being shortened from Richard or John or Joe. No reason needed here because this answer was already covered above. Yes. You don't change the meaning no matter what name you use","label":1,"model":"bloomz","source":"reddit","id":2408}
{"text":"It is not standard film speed and will require special equipment to view it properly, which may be difficult or impossible in some cases. It also means that many of its scenes are unusable because they were filmed too fast (or slow) to make sense when played back on normal 24 frame\/second projectors\/TVs. This can cause problems with editing as well since there won't always be enough footage from one scene to use during another if you need to cut between them. In addition, this makes it more expensive than other films due to needing extra time spent shooting each second of video. Its just an odd thing to do. Some people think its cool but most don't care much either way. Most movies are shot at 24 fps so thats what we expect. If someone wants to shoot something different then that's their choice though. It's weird! I mean why would anyone want to spend all those extra hours making sure everything looks perfect?","label":1,"model":"bloomz","source":"reddit","id":2409}
{"text":"British comedy is more subtle than American comedy, which tends to be over-the-top in its humorous depictions of people or situations. British comedians tend not to use crude language like Americans do when making jokes about race, sexuality etc., but they can still make their point without being offensive. The difference between British and American humour lies mainly on how it is presented rather than what it's actually based around. British comedy is less vulgar that US comedy because UK has stricter laws against obscenity compared with America where there isn't any law restricting people's freedom of speech. This means that if you want to say something rude then you're free to do so as long as nobody gets offended by your words. Because of this reason, US comedic shows often feature racist stereotypes while Britain's don't have such issues. British comedy is more subtle than US comedy, which tends to exaggerate things","label":1,"model":"bloomz","source":"reddit","id":2410}
{"text":"We have learned from childhood and are able to recall this information easily because of our memory skills being developed at an early age. We also remember riding bikes as children, which helps us recall what was required in order to do so successfully when adults. This may be due to the fact that learning new things requires more effort than recalling old ones. Because we were young once too! The same goes with driving cars - you don't need lessons if you've driven before (unless it's been years). You just know where everything is on your car already!\nThe reason why people can learn something like cycling or swimming but not other things such as playing guitar is down to the amount of practice needed to master these activities.  Learning to play guitar takes much longer periods of time spent practicing compared to cycling\/swimming etc., therefore making them harder to retain knowledge about later in life. It has taken me over 20 years since I last rode my bicycle without training wheels","label":1,"model":"bloomz","source":"reddit","id":2411}
{"text":"No, it's not their product but it is good business sense because if we use less of what they're providing then they'll have more to sell and make money from in future years. It's also better for our environment as well so everyone wins! They don't actually want you to stop using your appliances or lights - just turn them off when you're done with them instead of leaving them on all night (or leave them at low settings). This will help reduce demand which means lower prices too!\nIf there was no one buying electricity how would power plants be able to produce any? No, that's not true. Power stations can still operate without anyone paying anything. The only difference being they'd need to pay themselves back first before making profits. If people stopped buying electricity completely we'd see massive price rises across the board though... Yes, this makes perfect sense now doesn't it?! Energy companies aren't trying to get rid of customers who buy their products - they simply want those same customers to consume less by turning down thermostats, unplugging unused electronics etc","label":1,"model":"bloomz","source":"reddit","id":2412}
{"text":"They have been accused of being greedy and manipulative businessmen who use any means necessary to get what they want, including bribery or extortion.  They also own many companies in different industries which makes them very powerful. The Koch family has donated millions towards conservative causes such as anti-abortion groups and climate change denial organizations. Their donations were made through foundations set up by David H. Koch's wife, Anne Scaife. Some conservatives believe this money was used to influence politicians into making decisions favorable toward the Kochs interests. They have been called \"the most dangerous Americans\" because some conservatives think they control too much power over our government. They have been accused of being  greedy and manipulative businessman who use any means necessary .to get what they want ,including bribery or extorsion .\nthey also own many companiess in diffrent industryes whitch make em very powereful","label":1,"model":"bloomz","source":"reddit","id":2413}
{"text":"They say it is necessary because they are important economic drivers, but that doesn't make them right! They should be required by law to invest their profits into alternative energy sources instead. This would create jobs while also helping our environment at the same time!\nDemocrats and Republicans have been giving away money from American's pockets to big corporations like ExxonMobil since before I was born (I am 23 years old). It makes me sick when people who claim to care about Americans don't even try to stop this practice once they've gotten elected. The only way we can change things is if more voters start demanding accountability on both sides of Congressional aisle so politicians will finally listen to us again. We need new laws requiring all major polluters to pay up front for any damage done as well as penalties for not following through with clean-up plans after accidents occur.  For example, BP has paid out less than 1% of its total profit over the past decade towards cleaning up the Gulf Coast which still hasn't recovered nearly two years later despite spending $25 billion already.","label":1,"model":"bloomz","source":"reddit","id":2414}
{"text":"The mouth and its contents have different functions than other parts of the body, such as digestive organs or skin.  The oral cavity houses many bacteria that can cause disease if not properly treated by dental professionals.   Oral diseases include tooth decay (cavities), gum disease (gingivitis) periodontal disease, abscesses in gums and\/or jaws, infections caused by viruses, fungi, parasites, etc., which may spread to other areas of the body through blood vessels and lymphatic systems connected with the head\/neck region.    Dentists treat these conditions using specialized equipment designed specifically for use within this area;  they also perform procedures like root canals where infection could be life-threatening without proper treatment.     Medical doctors do not specialize in treating any one organ system but rather focus on overall health care including prevention\/diagnosis\/care management of all medical problems affecting their patients' bodies","label":1,"model":"bloomz","source":"reddit","id":2415}
{"text":"It is illegal to search people without probable cause, which would be difficult in this situation because it takes time to get evidence that someone has been drinking before they drive home or go out with friends. It also requires officers to have discretion when deciding who looks like they're intoxicated enough to stop them from driving so as not to violate civil liberties by stopping innocent drivers. The police are limited on how many times per year they can pull over motorists suspected of being under the influence based upon state laws (usually three). This means there aren't always enough resources available to monitor every bar all night long. Also, if you were arrested at your house after leaving a party where alcohol was served but didn't actually drink while driving yourself anywhere, then you'd need to prove that you're legally allowed to possess an open container of alcohol inside your vehicle - something that's very hard to do unless you've got video footage showing you doing exactly what you're accused of","label":1,"model":"bloomz","source":"reddit","id":2416}
{"text":"No, it is based on consumer preference and market demand. The most common size in America today is Twin XL (which measures 39 inches by 75 inches). This was introduced to compete with Full Size beds which were previously considered standard but are now less popular than Twin XLs because they take up more space.  In Europe however, Standard Single bed or 90cm x 190 cm is still very much used as well as Double Bed 180cm x 200cm  No regulations exist regarding this matter so it's all about what consumers prefer. It has been said that Americans like their beds big while Europeans tend towards smaller ones. no regulations exists regarding this matter so its all about what consumers prefer .it has been said that americans like their beds big while europeans tend toward smallers ones.. yes ,the same goes true here too...yes i agree ..i think people should be able to choose whatever suits them best","label":1,"model":"bloomz","source":"reddit","id":2417}
{"text":"The newer ones are better at hovering and flying in windy conditions than their predecessors were. The extra blades help them stay aloft longer by counteracting turbulence caused by gusts of wind. They also allow for greater maneuverability when compared to single-bladed aircraft. This is because they can be tilted up and down individually as needed instead of having one fixed angle like on earlier models. Also, with four or more blades there will always be an opposing blade that helps stabilize it during flight. Because of this stability, pilots feel safer operating these types of aircraft. To make sure you don't lose control over your helicopter if something goes wrong (like a sudden gust hits) it's important to keep all four blades spinning simultaneously so that none stop turning completely. If any of those happen then you'll crash into whatever's below you. Most modern helicopters have four or more blades","label":1,"model":"bloomz","source":"reddit","id":2418}
{"text":"Bowing is very important and has several levels, but it's not as complicated or difficult as people make out.  Bowing can be done with your head down (called \"kenkai\") which means you respect what you're doing.   You don't have to do all three bows at once; if someone asks you something like \"How's your mother\" then you'd only need one bow instead of two.    If you want to say \"hello\" when meeting someone new you'll use an elaborate bow called \"kemushi-bowing\" where you hold both hands together behind your back before bending forward from waist level while keeping your eyes straight ahead.     The most common way to greet someone would just be by saying \"sayonara\" meaning \"farewell\"; however, if you were going on vacation you'd also add another bow afterward known as \"shimashiburi-bowing\"  This is similar to shaking someone's hand except without touching them.","label":1,"model":"bloomz","source":"reddit","id":2419}
{"text":"The beer contains more alcohol content per volume that water does. The higher concentration makes you feel worse when you're hungover because your body has to process all those extra calories from ethanol (alcohol) before it's able to get rid of them through normal metabolic processes.  Alcohol also causes dehydration; drinking too much can lead to liver damage or even death if not treated quickly enough.   Drinking excessive amounts of alcohol will cause you to lose consciousness; this means you'll fall asleep while drunk which leads to loss of REM sleep during the night causing fatigue on waking up. This combined with being dehydrated by the excess alcohol consumption results in an extreme headache upon awakening. Because there are so many different types of hangovers, there's no one-size-fits-all cure. However, most people agree that staying hydrated throughout the day helps alleviate some symptoms associated with hangovers such as nausea, vomiting, diarrhea, dizziness, confusion, weakness, irritability, anxiety, depression, insomnia, muscle cramps, tremors, chills, fever, sweating, rapid heartbeat, chest pain, shortness of breath, blurred vision, ringing ears, tinnitus, memory problems, difficulty concentrating, forgetfulness, disorientation, slurred speech, seizures, coma, and possibly death","label":1,"model":"bloomz","source":"reddit","id":2420}
{"text":"We measure its distance to us by parallax, which is an apparent shift in position of celestial objects as seen through different positions on Earth relative to each other and their common point of observation (the Sun or another distant object). The farther away they are, the greater this effect becomes.  Parallaxes can be measured with great accuracy using telescopes that have two separate lenses separated by several feet so you can see both images at once.   We use trigonometry to calculate distances based upon these measurements. By measuring the angle between our line-of-sight toward the star and some nearby reference stars whose distances are known accurately enough, astronomers can determine the distance to any given star within about 10% error margin. Using parallax measurement techniques, scientists determined that the nearest star lies only 4.3 light years away! This means that if there were no space between the sun and planet Mercury, then one year would last just over 88 days instead of 365","label":1,"model":"bloomz","source":"reddit","id":2421}
{"text":"We have teeth and jaws designed specifically for chewing plant matter, not animal tissue. We also lack the digestive enzymes needed to break down animal protein into amino acids in our stomachs.  This is why we need cooking - heat denatures proteins so they can be broken down by human digestion systems. Humans evolved from primates who ate mostly plants but occasionally hunted small animals like rodents or insects as an extra source of protein.   The earliest known evidence of cooked food dates back about 1 million years ago when early hominids began roasting nuts over open fires.    Because most people don't want their food raw anymore,   this has become one of those things you just do without thinking much about it. Our ancestors were omnivores with diets consisting primarily of fruits, vegetables, seeds, leaves, roots, tubers, invertebrates such as worms, snails, spiders,  scorpions  and other arthropods along with occasional hunting of smaller mammals including rabbits, deer mice, rats, bats, hedgehogs, moles, shrews, voles, gophers, ground hogs, raccoons, opossums, skunks, foxes, coyotes, wolves, bears, cats, dogs, horses, goats, sheep, antelopes, deer;   these foods contain little if any animal protein which means there was no reason to cook them before eating.     Human evolution","label":1,"model":"bloomz","source":"reddit","id":2422}
{"text":"Sonny Boy Slim II by Diplo featuring Justin Bieber, Lil Wayne & Skrillex (Diplo Remix) - Single Albums Chart Top 100 No 1 song in UK 2015. The Drop That Everyone's Waiting For!\"  This was released on March 31st of 2016 as an official single from his album Recess. A remix to \"Sonny Boy Slim II\" with Skrillex added onto it. It has been played over one million times since its release. Sonny Boy Slim II by diplo feat. Justin Bieber, lil wayne & skrillex (diplo remix).mp3 file uploaded on April 2nd, 2016. Sonny Boy Slim 2 by Diplo featuring Justin Beiber, Lil Wayne & Skrilex (Diplo Remix)  https:\/\/www.youtube.com\/watch?v=hvW7Zyqj9YM&feature=youtu.be","label":1,"model":"bloomz","source":"reddit","id":2423}
{"text":"She was an American heroine who helped slaves escape from slavery in America during the Civil War and later became a spy, nurse, teacher, orator, political activist, writer, and businesswoman. She also led many Underground Railroad efforts that assisted escaped slaves reach freedom through Canada. Because of her accomplishments she will now appear on our currency as one of America's greatest heroes. Her image appears on the front side with Benjamin Franklin's profile appearing on the backside. Because of her accomplishements she will now appear on out currency as one of Americas greatest heroes. Her image apears on the front side with Benjamen Franklins profile appearing on the backsideside . Yes , because she did so much good things like helping people get away from slavery by leading them across country lines into Canada where they could live free lives without being enslaved again . No , I do not know why","label":1,"model":"bloomz","source":"reddit","id":2424}
{"text":"They are caused by water freezing and expanding in winter weather conditions, causing cracks to appear on roads that eventually lead to holes forming. They can also be formed when rain falls onto frozen ground or snowmelt freezes into ice before it drains away from roadways. Potholes may also develop as a result of poor maintenance practices such as not replacing damaged pavement materials regularly enough. Water is able to freeze because its temperature drops below zero degrees Celsius (32 \u00b0F). The expansion causes pressure which leads to cracking.  When temperatures rise above 0 \u00b0C (32 \u00b0F), the water expands again but this time pushes outwards instead of upwards so no damage occurs.   This process repeats itself every year until the hole forms completely. Poor maintenance practices include failing to replace damaged pavement material regularly enough. It does not cause them directly; however, if there isn't adequate repair work done then they will continue to grow larger overtime","label":1,"model":"bloomz","source":"reddit","id":2425}
{"text":"We live in Arizona, where it is hot all year round so we do not need to cool off as much because of the weather outside. We also use air conditioning during the summer months when it's very humid outdoors but that doesn't happen often enough to warrant having white roofs. Also most people like their homes with colorful roofs instead of plain ones. It adds character to your home! I think if you lived somewhere colder you'd probably want them painted white or something else lighter colored than brownish red tile. But that's just my opinion :) Because they are made from clay tiles which absorb sunlight rather then reflecting it. Clay has been used since ancient times due its ability to retain water longer than other materials such as wood. The same reason why brick was invented - to hold up buildings better against rainstorms. They were originally designed by Native Americans who had no electricity at this time","label":1,"model":"bloomz","source":"reddit","id":2426}
{"text":"It had no value because of its poor quality (it is not clear if this applies only in relation to the purchase price or also to other qualities). It may be stolen goods as well. The state bought them at an extremely low cost from private individuals who were desperate enough that they would sell anything. This could include stolen property. Some people believe that the government stole these items by buying them illegally under duress. Others think that the government did nothing wrong since there are laws against theft which apply equally to both parties involved - buyer and seller.  There is no mention of any law being broken here. No one has accused anyone else of stealing something. They just feel like their culture got robbed somehow. Because of its poor quality. It may be stolen goods too. The state bought them very cheaply from private persons who were desperate enough that they'd sell anything. This might include stolen property. Some folks think that the government stole those things when they purchased them illegally through duress. Others think it's OK 'cause there's laws about theft that's applied to everybody alike -- buyers & sellers","label":1,"model":"bloomz","source":"reddit","id":2427}
{"text":"The air is compressed and then released, creating an audible shock wave that travels faster than the speed of sound in normal conditions (340 m\/s). The higher the aircraft flies above sea level, the more likely it will break through this limit at some point during its flight path. This causes the loud noise heard as a sonic boom when flying over land or water. Airplanes fly much slower than 340m\/s so they don't usually reach Mach 1 unless they're very high up. When planes do get to Mach 1, it's because they've been traveling fast enough already before reaching their cruising altitude. Because the plane has reached such great speeds by now, there isn't any extra energy needed to push them past Mach 1. It's just like pushing on your car's accelerator pedal - if you have plenty of momentum going into a turn, there's no need to put additional pressure behind the wheel. Answer not in context","label":1,"model":"bloomz","source":"reddit","id":2428}
{"text":"They are born with it in them, they don't have to learn from anyone else or anything else. They just know what to do because of evolution. Animals that live alone usually make homes by themselves without any help at all. Animals who live together will often work as teams when building houses so each animal can contribute something different but still be able to get along well enough not to fight over territory\/resources. Some animals may even use tools while others won't need any kind of tool whatsoever. Evolution has taught these animals everything there is about living on land and\/or water. Because they were first created this way millions upon millions years ago before humans ever existed. This means no one had to teach them how to survive then either! It was natural selection which made sure only those animals best suited survived long enough to pass down their knowledge through generations until today where we see many species doing things exactly right according to nature's design","label":1,"model":"bloomz","source":"reddit","id":2429}
{"text":"It involves using both gears at once to shift into gear faster, but it's more difficult than normal shifting because of how complicated it can be on some vehicles.  You use your left foot to hold down the brake while simultaneously pushing forward with your right foot in order to engage first gear (or reverse) as quickly as possible without stalling or causing damage.   This technique requires practice so that you're able to perform smoothly every time. It also takes longer to master compared to other techniques like single-clutching which are easier to learn since they don't require holding two pedals down at one time. Some cars have automatic transmissions where this isn't necessary anymore due to improvements made by manufacturers over recent years. It's used primarily during racing events such as drag races and hillclimbing competitions. The reason behind doing it is to get better acceleration out of your vehicle quicker instead of waiting around until the car shifts itself automatically through its gears.","label":1,"model":"bloomz","source":"reddit","id":2430}
{"text":"Because they don't need someone with your level of education to perform them, and are afraid you'll leave after being hired anyway. This is especially true in fields like retail or food service where turnover rates can be high due to poor working conditions and\/or pay. In other words, it's cheaper to hire people who won't quit than those that will stay on longer term. It's also possible employers may think you'd demand more money once you've been trained up by their company. Some companies require employees to take classes before they're allowed to work there as well which could explain why an employee would be \"over qualified\" when applying for certain positions. If this happens it usually means the position has already filled but sometimes these requirements aren't listed until later during the hiring process.  For example, one job posting might say something like: \"We offer paid training courses...\" while another says nothing about any required trainings at all. The same goes for experience - many times applicants assume they've got enough years under their belt based off what was written down initially only to find out much later that additional time spent doing similar tasks counts towards the minimum number needed.","label":1,"model":"bloomz","source":"reddit","id":2431}
{"text":"I am experiencing numbness in it and I can't feel anything on that side of my body. It feels like I'm walking around with no feet at all, which makes me very uncomfortable. My toes are also tingling or burning sensation as well. This happens because there isn't enough blood flowing through them anymore due to lack of oxygen from not moving. The feeling will go away once you start moving again but can be painful if you're standing still too long. It's usually caused by sitting down without moving your legs for an extended period of time such as during work hours while typing\/standing up straight behind a desk. You may experience this symptom after waking up in the morning before getting out of bed (especially cold mornings) since it's been lying flat against the floor overnight. There aren't any other symptoms besides pain\/tightening\/swelling\/numbness\/stiffness\/pins & needles","label":1,"model":"bloomz","source":"reddit","id":2432}
{"text":"Energy (physics) - Wikipedia\n\nNo Answer Provided For This Question As It Does Not Match The Topic Of Energy In Physics Classroom. No Explosion Is Expected To Take Place When Matter Converts Into Energy. Yes Answers Are Allowed On Other Questions But No Here! No Answer Provided For This Qestion As It Does Not Match Th ETopic Of Energy In Physics Classroo mYes A nswers Ar e Al low ed On Other Qu estions B ut No H ere ! Yes , T he Re qu ired En ergy Fo r Co nv erting Ma tter I nto En erg y Is G reat . No Ex plos ion Is Exp ected To Tak e P lace Wh en Mat ter C onv erses Int o Ene r gy . Yes Answe rs Are Al lo wed O nt h e R emai ning Qu es tio ns","label":1,"model":"bloomz","source":"reddit","id":2433}
{"text":"It is popular among women readers\/watchers because they can relate to Christian Grey in some way or another (e.g., he is wealthy and\/or handsome). It also appeals to men as well since it's about sex. It's not really about BDSM though; it's more about power dynamics between two people that are attracted to each other but have different ideas on how things should be done sexually. The book was written from his perspective so that's why he's always right even when he's wrong. He thinks he knows best what she wants which makes him feel powerful over her. This kind of dynamic happens all too often in real life relationships where one person feels superior than their partner(s) due to money, status etc... Because its popularity grew quickly after being published online without any traditional publisher backing behind it. Also there were no critics reviewing this work before publication unlike most books nowadays. And finally because everyone likes reading about sexual exploits especially if they're forbidden","label":1,"model":"bloomz","source":"reddit","id":2434}
{"text":"Because they were filmed on film, not videotape or digital tape like most other shows of that time period. Film is much sharper and clearer in quality compared with video tapes which are grainy by nature. Videoconferencing was also done using film cameras at this point so it made sense to use film as well when shooting soap opera episodes. The only reason why some programs had better pictures then others would be because different studios used different equipment during production. Some may even have been shot on 16mm film instead of standard 35mm film. Because films can cost more money per minute produced (due to having less footage) many producers chose cheaper alternatives such as video tapes over film. This meant there wasn't one definitive answer about what caused certain shows to look better but rather several possible answers depending upon who you asked. It could've just been luck too! Filming soap operas required special lighting techniques","label":1,"model":"bloomz","source":"reddit","id":2435}
{"text":"No one knows, but it is used in many places around the world to celebrate victories and achievements of athletes or teams. It was invented by an American football coach at Notre Dame University during halftime of their game against Georgia Tech on December 31st, 1931.  The original cheer was \"Go Irish! Go Yellow Jacket!\"  No one knows who created this chant except that he did not come up with it himself; rather his assistant coaches helped him create it as they were trying out different chants before the big matchup between these two schools. Yes. This phrase originated from America Football Coach Knute Rockne's team song called \"The Fighting Irish March\" which has been played since 1930s. No. There are no known origins behind the cheers you mentioned other than what I have already stated above. Yes. No. No. Yes. No. Yes. Yes. No.","label":1,"model":"bloomz","source":"reddit","id":2436}
{"text":"The bomb explodes in his chest and not on people around him\/her. The blast is contained to that individual's body instead of being spread out over an area larger than just their own body. This prevents more deaths from occurring as well as injuries caused by shrapnel or flying debris. It also reduces property damage done by the explosion. A hug can be very comforting when someone you love has been hurt emotionally and\/or physically. In this case it was used to calm down a suicidal man who had no other way out but death. He did not want anyone else to die because he died. That would have made things worse for everyone involved. No one wants to see loved ones suffer like they do. If there were any doubts about whether or not the bomber could detonate himself with such ease then the fact that he didn't even try to escape after receiving the hug should dispel those thoughts immediately","label":1,"model":"bloomz","source":"reddit","id":2437}
{"text":"They are there to show off their designs . They may be bought by celebrities or other wealthy people who want something unique and different from what is available in stores . The clothes might not look good when worn outside of fashion shows , but they can still sell well because some people like that kind of style . Some models wear odd things just so it looks cool at first glance .\nSome famous brands use weird outfits as part of marketing strategy - this way you remember the brand name more easily than if all your clothing looked normal . It 's also an attention grabber which makes everyone talk about how crazy those clothes were instead of talking about the actual design itself . This helps boost sales too ! Celebrity endorsement works great with these kinds of clothes since many stars will go along with whatever trendy thing comes around next year ( even though most of us would never try wearing such garments ) . These clothes usually cost thousands of dollars each piece .","label":1,"model":"bloomz","source":"reddit","id":2438}
{"text":"Hunger causes physical pain, while dehydration does not cause any noticeable symptoms until it becomes severe enough that you feel sick or faint from lack of water in your bloodstream (which can be fatal). This makes people more likely to ignore their thirst and drink less water when they are hungry because they do not want to experience this unpleasant sensation again. The opposite happens with hunger: Hungry people will eat anything available even if it's disgusting just so they'll stop feeling like they're going to die soon. Dehydrated people don't have these feelings unless they've reached an extreme level where they become ill and\/or pass out due to low levels of water in their body which could lead them to death. They may also suffer headaches but those aren't as painful as stomach cramps caused by hunger. It doesn't cause any noticeable symptoms except at its most advanced stages. People who are very hungry often complain about having stomach pains","label":1,"model":"bloomz","source":"reddit","id":2439}
{"text":"A few hours to days depending on where you flew and what kind of aircraft it was A couple months after 9\/11 there were still planes flying around with no windows in them because they didn't want people looking inside at all times.  If you're not doing anything illegal like smuggling drugs or weapons then it's unlikely that anyone will ever know about your flight unless something goes wrong (like crashing into another airplane). The FAA is constantly monitoring air traffic so if you do get caught you'll probably just have some fines and\/or jail time but nothing too serious since most small private flights are legal as far as they're concerned.   No one has been shot down by their own government yet though there's always a chance... Years ago when my dad used to work for the military he told me that even during wartime they'd never shoot down civilian jets over enemy territory - only fighter jets which pose a threat to national security. It might take years until you got busted","label":1,"model":"bloomz","source":"reddit","id":2440}
{"text":"Gluten intolerance was not as common in previous generations because they were exposed less often than we are today.  Gluten intolerances can be caused from eating too much wheat or other grains containing gluten (wheat flour) which causes inflammation within your intestines causing diarrhea and\/or constipation.   The more you eat it, the worse off you'll get until eventually it's so bad you're hospitalized due to severe dehydration.    This problem is becoming increasingly prevalent among children who consume large amounts of processed foods high in sugar and fat such as candy bars, cookies etc...   It also affects adults if they're overweight\/obese  People may think this isn't really an issue unless they've experienced symptoms first hand like I did when my son had his reaction at school after consuming pizza made without me knowing he was allergic. It's become very popular recently especially on social media outlets where everyone wants to share their experiences about how great life feels now that they don't eat gluten anymore","label":1,"model":"bloomz","source":"reddit","id":2441}
{"text":"Everyone has unique fingerprints and personalities, which are reflected in writing styles as well. Everyone's brain works differently too! This means no two people will write exactly alike even if they try hard enough. The same goes with how we think about things - there can be many ways to solve one problem or answer one question. No matter what you do though, your personality always comes through somehow. It's just like when someone asks \"how did I get here?\" You can't really say because it's not possible to go back in time but you'll know you're at home by looking around yourself. Your mindset changes depending on where you've been before coming into this place. It doesn't change who you are inside; only outside circumstances affect us sometimes. Because our brains work uniquely, every person thinks and writes differently. Even twins don't necessarily share similar handwritings unless they're taught together since they grow up under very similar environments","label":1,"model":"bloomz","source":"reddit","id":2442}
{"text":"Because they want to know whether an employee is lying or telling the truth about something important before taking action against him\/her. The lie detectors can be used in this way because people who tell lies tend to show physiological changes such as increased heart rate when asked questions during the test which may indicate deception. This information helps investigators decide how much weight should be given to what someone says. Because of its low reliability it cannot be used by itself but only alongside other investigative techniques like questioning witnesses etc., where it provides additional useful information. It also has some advantages over traditional methods - it's cheaper than hiring private detectives\/private eyes; it doesn't require any special training from those administering it; there aren't many legal restrictions regarding using them; they're easy to administer since all you need is a computer with software installed and a few electrodes attached to your body. They have been shown to work better at detecting white-collar crime rather than violent crimes","label":1,"model":"bloomz","source":"reddit","id":2443}
{"text":"Because it is pleasing to our senses and makes us happy. It feels good because of how it looks, smells or sounds. We are satisfied with what fits well in place as opposed to something that doesn't look right. This can be applied to anything from furniture arrangement to clothing fitting. Because it is pleasing to  our senses and makes us happy; it feels good because of how they look, smell or sound.  We are satisfied with whatfitswellinplaceasopposedtosomethingthatsdoesn'tlookright.Thiscanbeappliedtothingsfromfurniturearrangementtoclosetofclothingfiting. Yes! Yes! Yes! No! Yes! Yes! Yes!  Yes! Yes! Yes ! Yes! Yes! Yes!! Yes! Yes! Yes!!! Yes! Yes! Yes!!!! Yes! Yes! Yes !!! Yes! Yes! Yes !! Yes","label":1,"model":"bloomz","source":"reddit","id":2444}
{"text":"Illegal acts are punishable by law, while unconstitutional ones aren't necessarily so. The Constitution does not specify penalties or consequences of breaking it (except in cases where there has been an amendment). Illegal actions can be punished with fines and\/or jail time depending on what they were; however, impeachment proceedings cannot result from illegal acts but only those that violate constitutional rights. Unconstitutionality refers to whether or not certain laws comply with the constitution's rules regarding how government should operate.  For example, if President Trump fired FBI Director James Comey without cause he would have done something unconstitutional because firing someone who was leading an investigation into you as head of state violates your Constitutional right against self-incrimination.   If Donald Trump had committed treason during his presidency then this act could also be considered illegal\/unconstitutional since treason is defined under federal law as being \"the levying war against [the United States]...","label":1,"model":"bloomz","source":"reddit","id":2445}
{"text":"No, because atoms are so small they cannot be sliced apart by anything but other atoms or nuclear reactions. The only way to cut something up into smaller parts than its original size (like cutting bread) requires energy from outside of itself.  No matter how hard we try with our bare hands, no human can break even a single atom without using some kind of tool like a knife made out of another material which has more mass\/energy then just your own body could produce on it's own.   Even if someone were able to use enough force to rip open all the bonds holding together each individual atom within their body, there wouldn't actually be any visible separation between them as they still have the same amount of space inside themselves before and after being ripped apart.    Yes, this is true! No, not at all. Atoms aren't big enough to do damage unless they're packed tightly together. If you're talking about wires that are thinner than 1 atom wide, yes","label":1,"model":"bloomz","source":"reddit","id":2446}
{"text":"The brain is deprived of oxygen and nutrients, causing it to die almost immediately upon separation from blood circulation in the neck vessels. The head will be conscious only if there are enough neurons remaining alive within its skull cavity that can maintain consciousness until they too die off due to lack of oxygen\/nutrients or electrical stimulation (which would require an intact spinal cord). If you cut someone's throat with your bare hands while holding their face down on the ground then pull back quickly before severing all connections between the spine and brain stem you'll see them struggle briefly as the muscles contract trying to keep the head upright but they'll soon lose consciousness because there's no longer any connection between the brain and the rest of the body. This happens even though some nerves remain connected through the vagus nerve which runs along side the carotid artery behind the ear into the base of the skull where it's attached to the medulla oblongata.","label":1,"model":"bloomz","source":"reddit","id":2447}
{"text":"I am thinking about how much work i have to do and that it is overwhelming me. This makes me want to give up because its too hard. My mind feels like it's shutting down, so I'm not able to think clearly anymore. It also causes me stress which then leads into anxiety attacks. The only thing keeping me from giving up completely are all of those people who depended on me before this happened. They need someone strong enough to help them through their problems now more than ever. I don't know if I'll be able to handle everything but at least i'm trying. Its just really difficult right now. I can't even focus properly while writing an essay or doing homework. It's very frustrating. I feel overwhelmed by schoolwork and other responsibilities. I feel stressed out and anxious most days. I get headaches sometimes as well. I feel exhausted physically and emotionally every day. I feel drained","label":1,"model":"bloomz","source":"reddit","id":2448}
{"text":"Cooking breaks down cell walls, making it easier to digest and absorb nutrients from foods such as starch or protein.  This increases caloric density of the food by increasing the amount of energy available per unit volume (calorie\/gram). Cooking also makes some starches gelatinous which can increase their absorption rate in your body. The higher concentration of calories is due to increased water content that evaporates during cooking. Water has no calories so when you cook something like pasta with sauce, there are fewer grams but they have more calories because less water remains after boiling. When you boil potatoes, most of the water leaves them leaving behind concentrated amounts of starch and sugar. Because these carbohydrates take up space inside cells, this means there's less room left over for other things - including fat-soluble vitamins A, D, E & K. These vitamins dissolve best in fats rather than water; therefore if you're eating boiled potatoes without any added oil then you'll be missing out on those important vitamins","label":1,"model":"bloomz","source":"reddit","id":2449}
{"text":"Your body is not used to it and needs time to adjust back into its normal sleep cycle. This can cause grogginess or fatigue during your daytime activities because you're still in that state where you've been asleep longer then usual. Your brain has also had less restful sleep which makes waking up more difficult on some days. You may have trouble concentrating at work and\/or school if this happens often enough. It takes about one night's worth of getting regular sleep before your body gets adjusted again so it's best to try to go to bed earlier every single night after staying out late instead of trying to catch up by just adding extra hours onto each night until you get caught up with how much sleep you'd usually be getting per night. If you do stay out late regularly though (like working nights) you'll need to make sure you are catching up on all those lost hours eventually. Because your body isn't accustomed to being awake past what feels like \"normal\" bedtimes","label":1,"model":"bloomz","source":"reddit","id":2450}
{"text":"A pyramid scheme involves recruiting new members to pay money into an organization, with no guarantee of any return on investment or profitability.  The only way that you can make money in this type of business model is by bringing more people under your control who will also bring other people under theirs.   A pyramid scheme does not have products or services offered at all; it relies solely upon recruitment fees paid by each member recruited.    You should never invest anything unless there are tangible goods and\/or services being provided as part of the deal. Never give out personal information such as credit card numbers over email without first verifying the authenticity of the company through phone contact (preferably via voice mail) or another method. If someone asks you to send them money before they provide proof of what they're promising, it's probably too good to be true!   Don't fall victim to scams like these - report them immediately!  See http:\/\/www.ftc.gov\/complaint\/scams\/advice\/pyramid","label":1,"model":"bloomz","source":"reddit","id":2451}
{"text":"The size, shape, speed and altitude of the storm clouds that produce it. The larger or faster moving storms will create louder and longer lasting thunderstorms than smaller ones. This is because they contain more water vapor which can be condensed into raindrops to fall as precipitation on Earths surface. Also if there are many small storms occurring at once then this may cause an increase in noise from all these individual storms happening simultaneously. Size, shape, speed and altitute of the storm clouds producing them. Larger or faster storms have more water vapour so make bigger and louder thunderstroms. If there are lots of little storms going on together you get even more noise! Water Vapor content within the cloud. More water = more sound waves produced by condensation. Speed with which air rises up through the cloud. Higher speeds mean higher pressure changes per second (more energy)","label":1,"model":"bloomz","source":"reddit","id":2452}
{"text":"$430 million each (or more) if you live in America or Europe $4300 each (or less) if you're from Africa or Asia Everyone has exactly the same amount of money as they did before. No one is richer and no one's poorer than anyone else. The only difference now is that all people are equal financially speaking. This means there will be no poor anymore because everybody's got enough to eat, drink, wear clothes etc... There won't even be any homelessness either since every person can afford shelter too! None at all. If someone were to try stealing something then they'd get arrested immediately by police officers who'd also confiscate whatever it was stolen so there's nothing to steal anyway. Nobody needs anything other than what they've already been given. Yes but this isn't possible unless you want to change everything about our world including governments which most likely wouldn't work out well. We can't just take away people's jobs like that without causing chaos","label":1,"model":"bloomz","source":"reddit","id":2453}
{"text":"The process of melting must be done slowly, with lots of cooling time between heating periods to prevent damage from overheating. The temperature can then gradually increase until the metal melts. This method requires more energy but less equipment investment because there are no special high-temperature materials needed. It also allows you to control how much material gets lost during processing by controlling the rate of heating\/cooling. Slowly increasing the temperature will allow any impurities or other unwanted elements (like carbon) to evaporate before they cause problems when the metal starts to melt. Answer not in context - see above. slow rates of heating \/ cooling required due to low boiling point of tungsten. More energy used as compared to higher temp methods. No special hightemp materials necessary so cheaper investments possible. Controls loss of material through evaporation. Melting Tungsten using this technique does require more energy consumption per unit weight produced","label":1,"model":"bloomz","source":"reddit","id":2454}
{"text":"No, it's many atoms that are destroyed at once in an explosion of energy and light called nuclear fission. The atomic bomb is made up of two main parts: Uranium 235 or Plutonium 239 which contain unstable nuclei with too much energy to be held together by their own force fields.  When these materials are bombarded with neutrons from another source (like a reactor), they split into smaller particles releasing huge amounts of heat and radiation.   This causes them to explode violently as if there were no limit on how fast things can burn! No, it's many atoms that  are destroyed at once in  an explosion of energy and  light called nuclear fission .The atomic bomb is made up  of two main parts :Uranium 235 or Plutonium239 ,which contai n unstable nuclei w ith too m uch enegy t o b e h eld togeth er by th ei r ow n forc e field s .  Wh en thesem aterials ar ebe mbarrass ed wit h ne utrons fr om anot her sour ce( lik e a reacto r ) ,they spli t int osmaller parti cles rel easing huge amo unts of hea tand radia tion  .This cau sesthemto expl od vi olentl y astherew ere nodelimitonhowfastthingscansburn !","label":1,"model":"bloomz","source":"reddit","id":2455}
{"text":"They use water to keep them lubricated and running smoothly. This is called \"water washing\" or \"wet processing\" of adhesives. The factory will have an enormous amount of waste water that needs to be disposed off properly in order not to pollute local streams. They also need to make sure they are using enough water so as not to dry out too much before it can be used again. Water washing costs money because you must pay someone else to dispose of your waste water. It takes time to get new supplies of water delivered if needed. You may even run into problems getting permission to build such plants near bodies of water due to environmental concerns. Some companies try to recycle some of this water by filtering it through special filters which remove most contaminants but still leave behind traces of chemicals like formaldehyde. \n \n If you're interested in learning more about how super glues work check out: http:\/\/www.youtube.com\/watch?v=hvWwq7Zy5u4","label":1,"model":"bloomz","source":"reddit","id":2456}
{"text":"Sealed with pitch and tar, which was applied to planks of wood before they were nailed together into ship hulls. The hull would then be covered by layers of oakum (a mixture of shredded rope or cloth) that had been soaked in oil.  This method is still used today on small boats such as kayaks. It also works well when combined with modern techniques like fiberglass coating. Sealing with pitch and tar has several disadvantages including toxicity problems if it leaks out onto crew members' skin during use; this can cause severe burns similar to those caused by lighter fluid.   Pitching and tarring are not done anymore because new methods have developed since the 19th century using synthetic materials instead of natural ones.    Covered by layers of oakum(a mixture of shredd ed rop e or clothe d ) tha t ha d b een s ou ked i n oi l","label":1,"model":"bloomz","source":"reddit","id":2457}
{"text":"The core of an operating system, which controls all other components and processes in that OS. It also provides services to applications running on top of it. The kernel usually contains drivers (software) needed by hardware devices connected to your computer or mobile device. A driver allows communication between software\/hardware and the rest of the system. In Linux-based systems such as Ubuntu, Debian etc., there are two types of kernels - mainline and stable releases. Mainline release is always latest but unstable while Stable Release is more reliable than mainline one. Core of an operating system, controlling all other components and processes within that OS. Provides service to apps running above it. Usually has drivers required by attached hardware. Two different kinds exist depending upon how they were released; mainline and stable releases.  Mainline release is always most recent but unstable  Stable Release is more reliabe than mainline","label":1,"model":"bloomz","source":"reddit","id":2458}
{"text":"By removing dirt, oil or grease from them. They also kill germs by drying out their outer layer which makes it easier to remove with water. The soap molecules bind themselves to these substances on your skin making them more soluble in water than they are otherwise. This is why you need both soap and water together to get rid of all traces of bacteria that may be present. Liquid soap works better when used under running water because this helps wash away any remaining oils after washing while bar soap can work just as well if rinsed thoroughly afterwards. Bar soap contains less moisture content compared to liquid soap but has been shown to have similar cleaning efficacy provided it's properly washed off before use. By removing dirt, oil or grime from them. They also kills germs by drying out there outer layers which make it easier to remove with watee. The soap molecule binds itself to those substance on ur skine makin gthem moore solube in watere thaen thare usuallu ar","label":1,"model":"bloomz","source":"reddit","id":2459}
{"text":"Because it makes sense to have 0 be louder than other numbers, and because it's easier on people's ears if you start at max instead of min. It's also more intuitive when you're adjusting your receiver's settings - if you want less sound then turn down from 10 not up from 1. Because people are used to hearing things in this way. It would take some getting use too otherwise. This is why most radio stations play music with the highest decibel level first so that listeners can adjust accordingly before they hear any lyrics or vocals which may cause discomfort. Because it's easier on people's ears starting at max rather than min. It's also more intuitve when you're adjusting your reciever's settings-if you want less sound then tune down from 10 not up form one. Because people are used ot hearing things like this. It wold take somethig getting use too otherwise.This is whay most radio statiins plays musci with higheet decibel levele first so thaat listerners caan adjustee accordinngly beferee heaaring leeteris oor voicals wheich mighht cauae discomfrot","label":1,"model":"bloomz","source":"reddit","id":2460}
{"text":"The dragon was an ancient symbol of power which has been passed down through generations by word-of-mouth or written records. The similarities are due to cultural influences from one country on another over time. For example, China influenced Japan during its Heian period (794-1185). This influence can be seen with Japanese depictions of dragons being very Chinese looking.  Dragons were also used as symbols of good luck throughout Asia because they represented fertility and abundance. They have always been part of mythology around the world but not necessarily real creatures. There may even still exist some cultures who believe dragons do exist today although there isn't any proof either way. It could just be that people don't want to admit they've made up something like this since it's such a powerful creature. It's possible that these stories came about when humans first began writing their own history thousands of years ago. Humans had yet to develop technology capable of recording events accurately at the time.","label":1,"model":"bloomz","source":"reddit","id":2461}
{"text":"Brain damage or disease, such as schizophrenia and bipolar disorder.[1][2][3][4] The brain is damaged by an infection (e.g., encephalitis), stroke, head injury, tumor growths, autoimmune disorders like lupus erythematosus, neurodegenerative diseases like Huntington's Disease, metabolic disturbances like diabetes mellitus type 1, genetic abnormalities including fragile X syndrome, mitochondrial DNA mutations, and other unknown factors that may be environmental and\/or lifestyle related[5] Brain inflammation[6] Psychosis[7] Schizophrenic[8] Bipolar[9] Epilepsy[10] Trauma[11] Toxoplasmosis[12] Genetics[13] Immune system[14] Infection[15] Autoimmunity[16] Mitochondrial dysfunction[17] Tumors[18] Head trauma[19] Other unknown factors that may be environmentallifestyle related[20] Anxiety Disorders[21] Stressful events","label":1,"model":"bloomz","source":"reddit","id":2462}
{"text":"He used blood transfusions and other methods of cheating in order to pass tests. He was caught when he failed one that involved taking his own urine sample, which revealed traces of EPO (erythropoietin). The US Anti-Doping Agency banned him from competition indefinitely on October 1, 2012 after an investigation into allegations against him by former teammate Floyd Landis. Blood transfusion is not allowed under any circumstances during drug testing because it can mask the presence of drugs or their metabolites.  Lance Armstrong's use of blood transfusions has been described as \"the most sophisticated, professionalized, successful athletic fraud ever committed\"  In addition to using blood transfusions,  Lance Armstrong also took advantage of loopholes within existing anti-doping regulations.   For example, he would take two different forms of testosterone at once - one oral form and another injected directly into muscle tissue where it could be detected more easily than if taken orally.","label":1,"model":"bloomz","source":"reddit","id":2463}
{"text":"By viruses, hackers or other malicious software. By accidents (e.g., power outages). By poor storage conditions. By natural disasters such as earthquakes and floods. By human error. By bugs in code that are not caught during testing. By bugs introduced by programmers who don't know what they are doing. By bugs introduced deliberately to cause problems. By bugs caused when people try to fix things too quickly without thinking about it properly. By bugs caused because of lack of communication between developers working on different parts of an application. By bugs caused due to overly complex designs. By bugs caused by using old technology which is no longer supported. By bugs caused by hardware failure. By bugs caused by operating system failures. By bugs caused by memory leaks. By bugs caused by excessive use of recursion. By bugs caused by bad coding practices. By bugs caused by unreliable tools used while developing applications","label":1,"model":"bloomz","source":"reddit","id":2464}
{"text":"It has caused global economic turmoil and uncertainty in financial markets around the world, including those in Europe, Japan, South Korea, Taiwan, Hong Kong, Singapore, Malaysia, Indonesia, Thailand, Australia, New Zealand, Canada, Mexico, Brazil, Argentina, Chile, Colombia, Peru, Venezuela, Russia, Ukraine, Turkey, Israel, Egypt, Saudi Arabia, United Arab Emirates, Pakistan, India, Bangladesh, Sri Lanka, Afghanistan, Nepal, Bhutan, Laos, Myanmar (Burma), Cambodia, Vietnam, Philippines, Papua New Guinea, Brunei Darussalam, East Timor, Samoa, Tonga, Tuvalu, Nauru Island, Marshall Islands, Micronesia, Palau, Guam, Northern Mariana Islands, Puerto Rico, Dominican Republic, El Salvador, Honduras, Nicaragua, Costa Rica, Panama, Guatemala, Belize, Bermuda, Antigua & Barbuda, Saint Kitts-Nevis-Anguilla, Dominica, Grenada, Montserrat","label":1,"model":"bloomz","source":"reddit","id":2465}
{"text":"He has been accused by many people as being rude and unprofessional towards his fans on Twitter. He also had an altercation with one fan who was upset about how he treated another person in a video game that they were both working on together. This led to him getting fired from Valve Corporation where he worked before starting Fez. After this incident happened it caused a lot of backlash against him because some felt like he did not deserve what happened to him while others thought he deserved it. Some feel that he should be reinstated at Valve Corporation but most do not agree due to all these incidents happening recently. He got fired after insulting someone else's work ethic during their time there. He then started making videos criticizing other games which made more people angry. He also said something very controversial when asked why he didn't release Fez 2 yet saying \"it would have sold better if I released it\"","label":1,"model":"bloomz","source":"reddit","id":2466}
{"text":"My brain is releasing hormones that make me physically ill and weak, which causes me to cry out loud or quietly depending upon how much pain I've been feeling emotionally. My stomach will also hurt because it feels like it's being ripped apart from within as well. This happens every time someone breaks up with me. It makes sense since this has happened so many times before. The only way to get rid of these feelings is by crying them all out until they are gone forever. Crying releases endorphins into your system making you feel better instantly afterward. Endorphins are natural opiates produced by our bodies during exercise (running) but can be released through other means such as laughter too. When we laugh at something funny, we release endorphins causing us to feel good about ourselves even though we're laughing at others' expense. Our brains have evolved over thousands of years to help keep humans alive no matter what circumstances they're put under.","label":1,"model":"bloomz","source":"reddit","id":2467}
{"text":"The Constitution of the United States, specifically Article I Section 8 Clause 18 and 19 which prohibit Congress from declaring war without the consent of two-thirds majority vote by both houses or from raising armies unless they are called into service as part of national defense against invasion (Article 1 Sec. 8 Cl. 18); and also prohibiting any state from entering into any treaty with another nation that would require it to raise its own militia (Article 1 Sec. 9). The U.S. is not at war with ISIS nor has there been an official declaration of war made on them yet so this clause does not apply.  However, if such a conflict were to occur then these clauses could be used to prevent the establishment of an all volunteer force because the President cannot call up troops without congressional approval under current law.   No one can answer your question about what prevents Americans from fighting ISIS like Spain fought Franco","label":1,"model":"bloomz","source":"reddit","id":2468}
{"text":"The skin around it grows over and covers up part or most of the nail, preventing an infection. If this doesn't occur then you get an ingrown toenail. This is why it's important not to cut too much off at once when trimming them yourself (or have someone else trim them). The more that gets removed in one go, the greater chance there will be of getting an ingrown toe nail later on down the line. It can also happen if you're wearing shoes with tight-fitting toes and\/or high heels which push back against the sides of your feet causing pressure points where they rub together. Pressure causes blood vessels to constrict making less oxygen available to those areas so the surrounding tissue dies out leaving openings between the dead cells through which bacteria may enter leading to infections such as cellulitis. \n \n Ingrown fingernails are caused by improperly cutting one's own nails","label":1,"model":"bloomz","source":"reddit","id":2469}
{"text":"They cannot enter it at all because of its gravity and density. They will just pass right through as if nothing were there. Black holes have no effect on them whatsoever! This is why we call them \"black\" - you can't see anything inside one that isn't already visible outside (like stars). The only thing that could possibly get pulled in would be something with some kind of gravitational pull like matter or energy but not light. Light has absolutely zero weight so it's impossible to suck any part of it up by itself. It doesn't even need an object around which to orbit before being attracted towards another body's gravity field; this means there's literally nowhere else for it to go once it leaves your hand other than straight back out again. If you're standing next to someone who weighs 200 pounds then you'll feel their weight pulling down upon you while they're doing exactly the same thing to you too. But when you hold onto a ball that's floating freely between both hands it'll float away from either side equally without anyone having more control over where it goes than the other person does.","label":1,"model":"bloomz","source":"reddit","id":2470}
{"text":"To make it look more interesting and fun to read, especially if it's an article about something serious or boring like politics. It's also called \"clickbait\" because people are attracted by things that sound exciting but aren't really true (like this sentence). Clickbait is not always bad though; sometimes you can find good information in clickbait articles! To make it look more interesting  and fun to read ,especially if its an article about somethin gseriousorboringlikepolitics .Itsalsocalled\"click bait\"causepeoplearetractedbythingsthatsoundexcitingbutaren'treallytrue(likethissentence ) .Clickbaitisnotalwaysbadthough;sometimesyoucanfindgoodinformationinclickbaitarticles !!!! Yes I know what they mean with that one...I think that's why we call them news papers....to get us interested in reading their stories","label":1,"model":"bloomz","source":"reddit","id":2471}
{"text":"Water absorbs light and heat from its surroundings, causing it to cool down faster than dry air would have cooled off in similar conditions. This causes water vapor to condense into clouds which block out sunlight that reaches Earth\u2019s surface. Water also reflects less visible light (infrared) back towards space compared with other substances like ice or snow because of how tightly bound hydrogen is within each molecule.  The amount of infrared reflected by water depends on temperature; colder temperatures reflect more infrared while warmer temperatures reflect less infrared.   Because water evaporates at lower temperatures, this process can cause rainstorms during cold seasons but not warm ones. Water has an extremely high specific heat capacity so even small amounts will retain much of their absorbed energy longer than most solids or gases. When liquid water turns into gas, some of these retained energies are released as latent heat of evaporation. Answer not in context  It does","label":1,"model":"bloomz","source":"reddit","id":2472}
{"text":"The pressure increases with depth and this prevents freezing from occurring. The higher density also means there are more particles per unit volume which makes it harder to form ice crystals because they need space between them. This effect can be seen on Earth as well where glaciers flow down mountains but don't melt until reaching lower elevations (where atmospheric pressure decreases). Pressure increases with depth and this prevent freezing from occuring. The higher density also mean there are more particals per unit volume making it hard to form ice crytalys becouse they need space betweem them. This effect can bee seen on earth too wher glaciers flows dowm mountaines but dont melts untill reachin lowere elevationz(wher atmopsheric presure decrease)s. Yes. No. Yes. Yes. Yes. No. No. Yes. No.Yes. Yes. Yes.","label":1,"model":"bloomz","source":"reddit","id":2473}
{"text":"The bra size is not mentioned in this passage, but it can be inferred from context that she has large breasts and needs to wear an extra-large or J cup bra.  The author does mention her chest being fuller than most women have at age 20 (which would indicate larger breasts). She also mentions how much weight she's gained since having children which could cause breast tissue growth as well.   It should be noted though that these are just guesses on my part based off of what I've read so far... I don't know if it's been stated yet!   If you want more information about bras then check out wikiHow How To Measure Your Bra Size  http:\/\/www.wikihow.com\/Measure-Bra-Size . Yes? No? No? Yes? No? No ? No? No? No? No?  No? No? No ? Yes? No? No  ? No? No","label":1,"model":"bloomz","source":"reddit","id":2474}
{"text":"They didn't save enough money and spent too much on things like houses, cars etc.. They also had children at an older age than previous generations which meant that there were less workers to support them in their old ages as well. This caused problems with pensions because not many of these pensioners could afford to pay into it anymore so companies started cutting back or even stopping paying out altogether. The government tried to fix this by increasing taxes but then the economy went down hill again when everyone stopped spending any more due to high tax rates. People said this generation was responsible for all those changes. Because they are now retired from work most of them have no income other than social security payments (pensions)and Medicare benefits.  Some people think that if you don't spend your money while you're young you'll never be able to retire comfortably later in life. Others believe it's unfair how some people can live off Social Security alone without working","label":1,"model":"bloomz","source":"reddit","id":2475}
{"text":"The melanin in their skin is different from other people's, so they can get darker when exposed to sunlight while those with more yellowish or reddish pigmentation cannot change color as much. This difference may be genetic (inherited) or due to exposure of UV light during childhood that causes changes in how your body produces melanin.  Melanocytes produce melanin which gives our skin its natural colouring.   The amount of melanin produced by each person varies greatly depending on genetics - it could also depend upon whether you have been outdoors at all since birth because this will affect what type of sun damage has occurred.    Some people who do not tan well tend to develop freckles if they spend too long outside without protection against the sun's rays.     People whose parents had fair complexions usually inherit genes that make them less likely to tan than someone else would naturally. If you're born white-skinned then you'll probably never tan very deeply no matter how many times you've spent time in direct sunlight.","label":1,"model":"bloomz","source":"reddit","id":2476}
{"text":"The window is usually where there\u2019s most heat loss, so placing it near an open window will help keep your home cooler during summer months and warmer during winter months. The opposite would happen with other locations of the house because they\u2019re not as exposed to outside temperatures like the windows are. This helps save energy on heating\/cooling bills. It also keeps dust from entering through cracks around doors or walls into rooms which can cause allergies. If you have central air conditioning then it\u2019s best to place them wherever works best for you since you\u2019re using ducts throughout the entire house instead of just one room at time. You may want to consider having someone come out and check how well your system is working before making any changes though! Yes, this defeats some efficiency but saves money overall over the years. Also remember that these systems were designed decades ago when people didn\u2019t worry about saving electricity nearly as much today","label":1,"model":"bloomz","source":"reddit","id":2477}
{"text":"The way they are made up of fibers that make it stronger than normal materials like steel or titanium. The strength comes from how tightly packed together these fibers can be in each material type. Carbon Fiber is very lightweight but also extremely durable because its molecules form long chains which create an incredibly strong bond with one another.  Kevlar has high tensile strength as well due to it's molecular structure being similar to that of graphite (the stuff on pencils). Fiberglass is not nearly as strong as either carbon fiber or kevlar since there isn't much overlap when two glass strands come into contact with one another creating weak spots where cracks may occur more easily. How tightly packed together these fibers are within each material type. Carbon fiber is very lightweight but still extremely durable because its molecules forms long chains which creates an incredibly strong bond with other carbon fibers.   Kevlar has high tenacity  as well due to it's molecule structure being similar to that Graphite(stuff on pencils)","label":1,"model":"bloomz","source":"reddit","id":2478}
{"text":"Because they have to fly over mountains and other obstacles that would make it difficult or impossible to get there in one shot, so they must go around them. This is called flying \"around\" an obstacle (or mountain) rather than \"flying through\" it. Flying around takes longer but it's safer because you don't risk crashing into something on your way down. If you're not familiar with this concept then think about driving along a highway near a cliff - if there's no bridge nearby you'll probably drive past the edge several times before finally taking another route farther away from danger. The same goes for airplanes! They also need to avoid areas where weather conditions are too dangerous such as thunderstorms which can cause turbulence making landing very risky. Because they have to fly over mounatins and other obstacles that would makedifficult or impossible to get thereinone shot, so they mustgo aroundthem","label":1,"model":"bloomz","source":"reddit","id":2479}
{"text":"No one knows exactly how much sense our nose has but it does have some effect on what tastes good to us. The more pleasant smelling something is usually means that it's going to be tastier as well.  No one can really tell you if you're experiencing actual taste or just your brain's interpretation based off scent because no one's ever been able to prove this scientifically yet. But there may still be an element where when we're in a room with someone who doesn't like certain foods (like broccoli) then we'll start to dislike those foods too even though we've never had them before. Yes! We do inhale airborne particles from food sources which contain flavor molecules. These flavors stimulate receptors inside our mouths causing sensations similar to eating real food. This process allows us to experience things such as savory, salty, sweet etc., without having to eat anything at all.","label":1,"model":"bloomz","source":"reddit","id":2480}
{"text":"The hippocampus is involved in storing memories, while other parts of the brain are responsible for retrieving them from storage. The hippocampus also plays an important role in spatial navigation (how we find our way around). It has been shown that damage to this area can lead to memory loss or amnesia.  Memory retrieval involves activation of neurons within specific regions of the prefrontal cortex which send signals back down through the thalamic nuclei into the hippocampus where they activate neurons there as well.   This process continues until enough activity occurs at one time so that you experience recall. In addition, some researchers believe that dopamine may play a key role in learning by helping us form new connections between neurons during encoding. Dopamine receptors have been found on dendrites - small branches extending out from neurons. These receptors help transmit electrical impulses across synapses connecting two neurons together. Some scientists think that these receptors might be able to change their shape depending upon whether they're receiving dopamine or not.","label":1,"model":"bloomz","source":"reddit","id":2481}
{"text":"Roman buildings were built to last, while most modern ones are not designed with longevity in mind and thus do not survive well over time. The Romans used durable materials like brick or stone instead of wood which is less durable.  They also had better drainage systems than we have today that allowed them to build on top of each other without causing water damage. Modern building designs often use flammable materials such as plywood and insulation made from fiberglass (which can melt) making it easier for fires to spread quickly through tall structures. Also, they don't take advantage of natural light sources because windows aren't necessary since artificial lighting has been invented. Most modern buildings are constructed using steel frames covered by thin layers of concrete rather than thick walls of brick or stone. This makes them more vulnerable to earthquakes. Because Roman buildings were built to last,  they  survived even though some may look old-fashioned now","label":1,"model":"bloomz","source":"reddit","id":2482}
{"text":"I have an anxiety disorder that causes this reaction in myself. I am not sure if it is related to stress, but sometimes it's just one of those days where nothing goes right so maybe that's why? It happens more often than I'd like too! It's embarrassing because people think you are being rude by having to go potty at work...or wherever you're going. But then again they don't know what your dealing with either....so yeah..it sucks. My doctor says its probably from all the years I've been anxious about everything. He said he would try medication on me next time we see each other which will hopefully help ease some symptoms. Anxiety can cause many physical reactions including nausea\/vomiting\/diarrhea\/heart palpitations etc. The exact reason isn't known yet though there may be several reasons behind these issues.  Some medications used to treat anxiety disorders include SSRIs (selective serotonin reuptake inhibitors) such as Prozac\/Seroxat\/Citalopram\/Velvetra\/Rapaflo\/Duloxetine","label":1,"model":"bloomz","source":"reddit","id":2483}
{"text":"To cause harm to others' computers and data, or as an act of revenge against someone else's actions on their own computer system (e.g., deleting files).[1][2][3][4] To cause harm to others' machines and\/or data.[5] Revenge is also common motivation behind virus creation[6].[7] Other reasons include fun[8], extortion[9], espionage[10], blackmail[11], theft[12],[13],[14],[15],[16],[17],[18],[19],[20],[21],[22],[23],[24],[25],[26],[27],[28],[29],[30],[31],[32],[33],[34],[35],[36],[37],[38],[39],[40],[41],[42],[43],[44],[45],[46],[47]","label":1,"model":"bloomz","source":"reddit","id":2484}
{"text":"No it is not because they can be used as evidence in your defense if you have nothing else and no one else saw what happened or heard anything about it besides you.  The judge will decide whether this information should be admitted into trial based on how much weight he gives each piece of evidence presented by both sides.   If there was another witness who could prove beyond reasonable doubt that something did happen then the diary would probably never see the light of day but since most cases go down to jury trials where 12 people must agree with the verdict before someone gets convicted (or acquitted) it's very hard to get convictions without any other witnesses so judges tend to admit all kinds of things including diaries even though some may seem like hearsay at first glance.    In addition,  many times defendants don't realize their rights until after they've been arrested which means that sometimes police officers take away people's cell phones and\/or computers when they're questioned\/arrested  and those items contain incriminating material such as text messages from friends saying \"come over\" etc...","label":1,"model":"bloomz","source":"reddit","id":2485}
{"text":"I have ADHD (Attention Deficit Hyperactivity Disorder) which makes it hard for me to focus when working or studying because my brain is always racing with thoughts of what else could be going on in life besides school\/work\/socializing etc.. It also causes me to get bored easily so if something doesn't interest me I'll lose concentration pretty quickly as well.  This disorder has been an issue since childhood but was only diagnosed recently after years of struggling through high school without any help from teachers who didn't understand how much effort it takes just to keep up with everyone's pace. My parents are very supportive though they don't know exactly what's wrong yet. They've taken me to see several doctors already trying different medications that haven't worked out quite right either due to side effects or not being strong enough. I've tried many things like taking medication, therapy sessions, getting organized by writing lists everywhere... None really helped until now","label":1,"model":"bloomz","source":"reddit","id":2486}
{"text":"Some are not meant to be played that way, and some were made before standard tuning was commonplace. Some of these may have been tuned by ear or other means which would make them difficult if not impossible to tune accurately using modern methods. Others might simply never have had their pitches standardized at all. The majority of classical music is composed with an expectation that it will be performed on instruments whose pitches can be determined precisely from printed musical notation; however there has always existed a large body of popular music (especially folk songs) where this assumption does not hold true.  In addition many early pieces of Western Classical Music were originally intended as instrumental solos rather than orchestral works so they could easily be transposed into different keys without changing the melody too much.   For example, most Baroque-era violin concertos feature soloists who play high notes above G6 while playing low notes below G3 - both of which cannot be achieved when tuning a violin to A=440Hz","label":1,"model":"bloomz","source":"reddit","id":2487}
{"text":"The US government is not satisfied with how Fidel Castro has ruled his country since 1959, and wants him to step down from power in favor of democracy. The Cuban Revolution was led by Fidel Castro who took over after Batista fled into exile following a failed coup attempt on March 10, 1952.  Since then he has been dictatorially ruling the island nation as its president until 2008 when Ra\u00fal Castro became President.   He continues to hold that position today.    Because of this history between these two countries there are many issues which prevent them from normalizing their relationship including human rights violations committed by the Cuban government such as:  political prisoners, freedom of speech restrictions,  censorship,   lack of religious freedoms, persecution of homosexuals   etc., economic problems caused by decades of mismanagement under communist rule (e.g. high unemployment), corruption within the government system, poor health care services, low literacy rates among children","label":1,"model":"bloomz","source":"reddit","id":2488}
{"text":"To prevent splashing and spilling when people are using them, especially children who may not be able to control their hands while they drink from it. The second jet is aimed at the user's mouth so as to keep his or her head still during use. This prevents any accidental slips into the stream which could cause injury if someone were to fall in front of an oncoming person walking past with full force behind him\/her. It also keeps the user from accidentally knocking over cups filled with liquid by accidently tilting too far forward. To prevent splashing and spiling when people are using them... Yes. That would make sense. Thank you! Yes. No doubt about it. Yes. Yes. Yes. No doubt about this answer either. Yes. Yes. Definitely yes. Yes. Yes. Not sure how much I agree but no doubt it's true. Yes. Yes. Maybe","label":1,"model":"bloomz","source":"reddit","id":2489}
{"text":"Because they are copying files from other computers on their network and not directly connected via USB or FireWire (or whatever). This is called \"network sniping\" - when you copy something off of someone else's computer without them knowing it. The person who copied may be using an older version than what you're installing which can cause problems with your installation process. If this happens often enough then Microsoft will release patches that fix these issues but until then you'll have to wait longer if it's happening frequently. You could try uninstalling all programs except those required by windows before starting up again though. It should speed things along considerably as well. Because they are copying files from othe rcomputers on their network and not di rectly connected via USB or Firewire(or whate ver)This i scalled\"network sniping\"-when y oucopy somethin goffofre m om eoneelse 'scomputerwithoutthemknowingit","label":1,"model":"bloomz","source":"reddit","id":2490}
{"text":"It loses water content and becomes thinner as it cooks, which makes its cells more transparent to light. This is why you can see through an onion once you've cooked it enough that most of its liquid has evaporated. The same thing happens with carrots or potatoes after cooking them long enough. Onions are composed mostly of water (about 90%), so they lose their volume significantly during boiling. As this occurs, the cell walls become less dense because there isn't much pressure pushing against each other anymore.  When the cell walls thin out, they're no longer able to block visible light from passing through them like before. In addition, some chemical changes occur in the structure of the vegetable's proteins while it's being boiled; these also contribute to making the vegetables appear lighter-colored than usual. Because onions contain large amounts of water inside their cells, losing all but about 10% will make your onion look very different!","label":1,"model":"bloomz","source":"reddit","id":2491}
{"text":"Sniper fire can be heard and seen by everyone, so they will not shoot at him\/her if he\/she does nothing suspicious or out of place during his\/her speech. If there were any snipers around to begin with, then this would have been noticed before hand as well. The only way that someone could get shot while speaking on stage (or anywhere else) is if he did something unexpectedly dangerous like jumping off the stage into the audience members' laps. This may happen but it isn't very likely because most people know how much danger such actions pose. They won't jump down unless they're drunk and\/or high. Also note that even though you might hear gunfire from afar, it's unlikely you'll ever see anyone actually shooting since all shots fired must hit their target within one second after being pulled back otherwise they'll blow up harmlessly against the ground. hearing gunfire from afar without seeing who shoots","label":1,"model":"bloomz","source":"reddit","id":2492}
{"text":"By studying its light and comparing to other galaxies of similar size. By looking at what was left afterward. By observing stars orbiting around each other. By using telescopes with powerful lenses. By analyzing space debris from collisions between objects. By measuring distances by parallax effect. By calculating orbital velocities. By studying remnants of supernovae explosions. By studying remnant black holes. By studying gravitational lens effects. By studying dark matter. By studying chemical composition By studying chemical composition. By studying chemical composition. Yes! No! No! No! Yes! No! No!  No! No! No  Yes! No! No ! No! No! No   Yes! No! No  No! No! No    No! No! No Yes! No! No Yes Yes Yes Yes Yes Yes No Yes Yes Yes Yes No No Yes Yes Yes Yes yes no Yes Yes Yes Yes Yes","label":1,"model":"bloomz","source":"reddit","id":2493}
{"text":"Because your emotions are too much to handle and crying is an outlet that helps relieve them. Crying can also be cathartic, which means it releases stress or tension from within us.  It may feel good because of endorphins released in our brain during tears (endorphin being one type of natural pain killer). Because we have such strong feelings associated with these two things it's hard not to let out some emotion through tears. It's okay if people think we're weird though! Some people don't like seeing others cry but they shouldn't judge someone else's behavior as long as they're not hurting themselves physically by doing so. We all need outlets sometimes. Emotions are powerful forces inside each person; they should never be ignored. If you ignore how you really feel you'll only make yourself more miserable than necessary. You will eventually explode at something small later on down the road. The same goes for happiness - why would anyone want to hold back their joy?","label":1,"model":"bloomz","source":"reddit","id":2494}
{"text":"They use digital technology instead of analog, which allows them to send more data and messages simultaneously than an AM or FM station can do with one frequency band. This is called multiplexing in telecommunications engineering jargon. The most common form uses time division multiple access (TDMA) where each user gets their own dedicated slot on the airwaves at any given moment.  Cell phones communicate by sending out small bursts of RF energy that bounce off nearby antennas before being received back down again from other antennae within range.   Each phone has its own unique code so it knows how to decode what it's receiving without interference from others around it.   ... None of this applies here! Radio stations broadcast using frequencies while cellular networks operate over licensed bands of spectrum. They also have much higher bandwidth capabilities due to advanced modulation techniques such as OFDM used in 4G LTE systems. In addition they typically employ MIMO technologies like beamforming and spatial diversity coding","label":1,"model":"bloomz","source":"reddit","id":2495}
{"text":"The players are too good to lose their concentration and make mistakes that lead them into losing positions, so they play extremely cautiously until time runs out or one player resigns because he is tired from trying to win but can't find any way forward. This happens more frequently than you might think! It can be frustrating when you're playing against someone who knows exactly what he's doing all along... even if it's not your opponent's fault at all. The game ends with no winner after both sides have exhausted every possible move without making progress towards winning. Both players agree it would take an unrealistic amount of luck (or skill) on either side to change this situation. They don't want to give away anything by resigning early - especially since there will always be another tournament where they could potentially get back everything they've lost here. exhaust every possible move without making progression toward winning. Both players agree that it would take an unreasonable amount of luck","label":1,"model":"bloomz","source":"reddit","id":2496}
{"text":"The government of that country, state\/province\/city\/township etc., and their respective agencies (e.g. police) who enforce it. The United States Department of Transportation is responsible for setting national highway speeds limits in America). In some countries there are no set speed limits on highways; instead they have posted advisory signs which advise drivers to drive at safe speeds based upon weather conditions and\/or traffic volume. Governmental authorities may also change these limits as needed without notice. Police officers can issue tickets if you exceed them. Some states allow local governments to determine speed limits within certain areas such as school zones where children cross roads regularly. Local governments cannot lower existing speed limits but can increase them by 10 mph over those established by the state legislature. Governments around the world use different methods to establish speed limits depending on factors like population density, vehicle ownership levels, infrastructure quality, climate type, topography, culture, political views, economic status, and other social issues","label":1,"model":"bloomz","source":"reddit","id":2497}
{"text":"The subconscious mind is not always logical or rational in its thinking processes. It may have stored memories of past events that are disturbing but which we consciously try to forget because they cause us pain. The subconscious mind will then use these painful memories as inspiration when it comes time to dream at night. This explains why some people experience recurring nightmare scenarios over many years even though those particular situations no longer exist. Our brains store information about things we've experienced throughout life - both good and bad experiences. When you sleep your brain tries to process all this info into dreams.  Some parts of memory get pushed down by other more recent ones; however there might still be bits left behind from childhood etc., that aren't completely forgotten. These could include traumatic incidents like abuse, accidents, deaths...etc.. If you're having trouble sleeping due to stressful circumstances, it's possible you'll start experiencing nightmarish visions during your slumber.","label":1,"model":"bloomz","source":"reddit","id":2498}
{"text":"They are thieves and criminals who steal from others, especially children. They also have bad hygiene habits like not bathing or showering regularly which makes them smell terrible. Some of their traditions include eating dogs as food so they disgust some people too. Because many of these things make it hard to get along with other people in society. People don't want to be around someone that is dirty because it's unhealthy. Also if you live next door to someone who's always stealing your stuff then you'd probably feel uncomfortable living there either. Many people think they're ugly looking too but that's just prejudice against anyone different than yourself. It's very common among humans unfortunately. They're thieves and criminals who steal  from others, especially children.  They also have bad hygiene habit such as not bathing or shower regularly making them smell terrible.   Some of their tradition includes eating dog's meat as food   So they disgust some people too.    Because many of this thing make it hard to get on well with other peoples in society","label":1,"model":"bloomz","source":"reddit","id":2499}
{"text":"Let's pretend that your computer is a treehouse club that you like to play in and invite friends over to. \n\nAnti-Virus is like a bouncer standing at the door of the club. Except this guy has a list of bad people who he looks out for and when they try to go into the club he stops them. \n\nIf there are new bad kids in the neighborhood that the bouncer doesn't know about he might let them in so it is important to make sure his list of bad people is kept up-to-date. \n\nIt is also important for you to avoid inviting random kids you meet in bad areas of town to come and play. Some places are known hangouts for trouble makers. \n\nNorton Anti Virus is a big heavy slow bouncer who asks for money every year and he is clumsy and gets in the way a lot when you are trying to play in the treehouse. \n\nMicrosoft Security Essentials is a skinny fast martial arts expert who stays out of sight except when he is needed and has a good up-to-date list of bad kids all the time without asking you for any money. \n\nThere are other good free bouncers out there too. I happen to like Microsoft Security Essentials at the moment. A few years ago AVG Free was good but he has put on a few pounds and slowed down lately in my opinion. \n\nI like to have Malwarebytes Anti Malware as well. He is a bouncer who is extra good at recognizing kids who try to come in and spy on you and pass the information on to their friends. He stops kids listening near the door for your secret passwords and any gossip about you and your friends.\n\nEven the best bouncers cannot help you if you invite people and show them in yourself. Some bad kids pretend to be bouncers and trick you into letting them into the treehouse. Don't fall for this! \n\nNote:- *You can ask the bouncers to go inside the treehouse and check everyone out one by one if you like. This is a good idea if you didn't have a bouncer before and there are lots of kids already in the club.* \n\nTL;DR Anti-virus is like a bouncer with a list of who can't come in to the club.","label":0,"model":"human","source":"reddit","id":2500}
{"text":"[A blind man did an AMA on this a while back](_URL_0_)\n\n\"I have tried many diferent types of halucinigens. I can tell you first hand, that visuals are possible even for the totally blind. Well, I can se light and darkness, and if something blocks the light, I can se a blurry shape, but nothing that I can make out clearly. Under the influence of various antheogins, I have experienced sights such as various lights, which would change shapes and then melt in front of me. Once durring an experience with Salvia extract, I nearly became my rockingchair! I don't mean that I \"melted in to it\", I mean that if I hadn't jumpped out of it, I'd have turned in to the actual chair. I know that sounds crazy, but that's what happened. Naturally, sounds are a big part of my trip experiences, but I have seen somethings as well. I had a buddy that had his own light show setup, and we used to trip out on really good acid, (this was back in the early 80's when they still made it), and he'd shine these high powered lights through prisoms, and I saw all kinds of wild stuff. I believe I have seen color because of these experiences.\"","label":0,"model":"human","source":"reddit","id":2501}
{"text":"Some electronics wear during use. For example, electrolytic capacitors have a limited life span - usually far beyond the expected use, but a capacitor which has been used longer than expected or is faulty can burst, which can cause damage to electronics since they are filled with a conductive liquid.\n\nWhen the capacitors last, another potential weak point of electronics are the soldered connections between board and chips. Particularly CPUs require a ton of these contacts, some intel CPUs for example have more than 1000 of them on a tiny space. Since they get hot and have a rather high current flowing through them, they slowly degrade through an effect called \"[electromigration](_URL_0_)\", which thins them out more and more until they finally separate.\n\nJust like with the caps, this effect should only lead to losses long after the expected use time, but can lead to an early death if there was a fault. For example, a few years ago a line of NVidia notebook GPUs had a tendency to die off far too soon due to this, which ~~lead~~ led to a costly recall of MacBooks and other Notebooks.\n\nBut with good enough parts, electronics can last ages. I just fixed my grandma's amplifier, which had been running for almost 40 years now - all I needed to do was to remove some dust from the volume knob.","label":0,"model":"human","source":"reddit","id":2502}
{"text":"I used to work on the battery charging logic for both server backup power supplies in data centers, and for cell phones. A few notes:\n\n* It's bad for *all* batteries to deeply discharge them. Avoid this if you can.\n* It's bad to charge a Li-Ion battery all the way, but if you don't know how long it's going to have to go without recharging it, it's best to charge it to 100% up front rather than risk deep-discharging it later.\n* In general, \"recharge early, recharge often\" is your best approach.\n* The charging logic in a laptop or cell phone is quite sophisticated. It knows to stop charging when the battery is full, and to shut down your device when it gets too low. It knows when to trickle-charge, and when to charge at full current.\n* Pretend you never heard of the \"[memory effect](_URL_0_)\" for NiCad batteries. Unless you work with space satellites built in the 1970's, it doesn't exist for you. Trying to avoid the memory effect probably kills more NiCads than anything else. Anybody who thinks they're seeing the memory effect is really just seeing a battery that doesn't hold much charge any more.\n* For NiCads, fast charging is much better than trickle charging because trickle charging encourages the growth of dendrite crystals in the cell, which damage it. I dunno if this applies to other technologies.\n* Some people think that the number of discharge\/recharge cycles matters, and thus it's better to let a battery discharge all the way before recharging it, on the theory that one full cycle is better than two half cycles. Stop thinking that way. Two half cycles are much better than one full cycle, and the battery \"cycle-counting\" logic treats them as one cycle.\n* Fun fact: charging a Li-Ion battery the last 20% takes far more energy. We had a design for a portable battery charger that would only charge to 80% for this reason.\n\nIf I were to set the policies for charging systems, I would add the rule \"If the device hasn't been used in 24 hours and the battery is below 40%, shut the device down. If the device hasn't been used in 48 hours, shut it down.\" I work with cell phones and tablets for a living and have a shelf full of them. I really hate picking one up and realizing that it's been off the charger for a week and drained its battery to zero in the meantime.\n\nETA: [Wikipedia article on memory effect](_URL_0_)\n\n**tl;dr: recharge early, recharge often. Leave it on the charger whenever you can. The charging logic in the device will know what to do.**","label":0,"model":"human","source":"reddit","id":2503}
{"text":"While \/u\/TheGamingWyvern has given you the usual explanation, it doesn't even hint at the whole story.\n\nThe passage in Isaiah 14:12 is a type of poetry called a \"taunt song\"  a sort of Old Testament celebrity roast. Isaiah is actually sarcastically referring to the King of Babylon: one of his titles was \"The Shining One\", which was also what they called Venus, the Morning Star; another of his titles was \"Son of the Dawn\". And that's how Isaiah addresses him: \"O Shining One, Son of the Dawn...\" It's just that the King James Bible decided to use the Latin word \"Lucifer\" at this point.\n\nThis Lucifer actually has nothing to do with Satan. In verse 4, God tells Isaiah that he is to give the following prophecy to the King, and then the taunt starts. Verse 12 basically means: \"Hah! You call yourself the Morning Star? Well look at you now, fallen out of the sky!\"\n\nIn the New Testament -- Luke 10:18 -- Jesus is quoted as saying, \"I saw Satan fall like lightning from heaven.\" Actually, a more accurate translation of the original Greek would be \"I was watching Satan falling...\", and he was reacting to his disciples' report that they had been successfully driving out demons. Nobody really knows whether Luke was deliberately referencing Isaiah 14, but in the popular imagination the connection took hold and people equated Lucifer with Satan.\n\nThe idea of Satan as a fallen angel is a much more modern idea.\n\nEDIT: Thanks for gold.","label":0,"model":"human","source":"reddit","id":2504}
{"text":"It has a nuclear reactor on board.  It's not the same kind of nuclear reactor that you see in power plants or large naval vessels; it just has a piece of radioactive material which gets hot thanks to its radioactivity.  You can harvest a little bit of electricity whenever there's a difference in temperature between two places.\n\nThe nuclear reactor is able to produce a *ton* of energy over the course of its life.  I'm fond of [this](_URL_0_) comic showing just how massive the energy density of nuclear fuels is compared to chemical fuels like coal and gasoline.  Nuclear fuels get their energy from Einstein's famous E = mc^(2), where the energy you get is equal to the mass the fuel destroys when it decays, multiplied by the speed of light squared.  The speed of light is a huge number, so you get a *ton* of energy this way.\n\nOver time the fuel runs out, though.  The rate at which fuel is radioactively decaying is proportional to the amount of fuel that's left, and the energy it produces is proportional to the amount of decay.  This means that when you first build the reactor it produces the most power, but over time it produces less and less.  Voyager uses Plutonium 238 as its fuel; Pu-238 has a half life of about 88 years, so every 88 years the power production will have dropped by half.  The other components of the reactor also degrade, so the power loss is somewhat faster than this.\n\nThe Voyager probes have been in space since 1977, so the reactors are now producing about 1\/4 less power than when they were launched.  To compensate for this the probes have been shutting down systems as necessary to conserve power.","label":0,"model":"human","source":"reddit","id":2505}
{"text":"Edit:  To everyone saying I didn't ELI5.  It's right here at the top in the first paragraph.  Everything after paragraph 1 is ELI a neuroscientist.  \n\nSo, the short, ELI5 answer is that we don't really know.  There are a lot of theories.  It likely involves a cascade of a bunch of different chemicals in the brain, but we don't fully understand all that yet, but it's an active area of research that could use more funding, like most psychiatric research.  \n\nLonger, more complicated answer:  Part of the problem is that people have about a million different reactions to these things.  Yes, there is medical \"shock\", which is exactly what other posters have described (i.e. too much blood loss, low blood pressure, etc), but psychologic \"shock\" is the layman's term for being \"overwhelmed\" by some stressor.  \n\nThe thing is that this being overwhelmed can present in many different ways, each of which probably has it's own unique etiology and biochemistry behind it.  For example, I've seen people who suffered severe trauma become catatonic (very withdrawn, shut down, they're often posable like Gumby, etc), which is theorized to be due to a relative deficiency of dopamine (it's also seen sometimes in people on antipsychotic medications which block dopamine, and in patients who experience sudden withdrawal from their dopamine boosting Parkinson's meds).  Other people become near or fully psychotic, which is largely an excess of dopamine.  Both responses involve dopamine moving in different directions, and have totally different clinical pictures.  \n\nOther people become very irritable, agitated, or aggressive.  The diagnosis for this one is often termed \"Adjustment Disorder with Mixed Disturbance of Emotions and Conduct.\"  We don't really know why.  My suspicion is that it involves dysregulation of the glutamate-GABA balance (glutamate stimulates, GABA inhibits to over-simplify things), and is often helped (temporarily at least) by Benzos which are GABA-ergic drugs.  \n\nThere's tons of unknowns though.  Epigenetics is almost certainly involved.  We know that the risk of developing PTSD is genetic.  Obviously you have to experience trauma to develop PTSD, but most people who experience trauma DON'T get PTSD, so why do some people get it and not others?  The answer, based on studies we have so far, suggests that people who get it were carriers of certain genes (likely many, many different ones, not just 1 or 2), that were activated when they underwent trauma.  This makes sense for PTSD which doesn't start instantly, but takes weeks to months to develop.  Once genes get turned on, it usually takes some time to make their protein products that cause biochemical changes.  It doesn't really work for these more immediate stress reactions though, so who knows.  Again, my suspicion is that the immediate reaction is more related to changes in glutamate\/GABA, serotonin, Norepinepherine, dopamine, maybe acetylcholine, etc.  \n\nIt also seems to be strongly related to how you've learned to handle stress.  If you've developed healthy coping techniques that you're good at applying, you'll be better able to handle larger stresses.  If you didn't develop them (perhaps your parents never modeled them for you), then even relatively minor stresses can cause people to freak out.  This is commonly seen with Borderline personality disorder, which recent evidence suggests is also a fairly genetic disorder with some epigenetic influences as well, much like PTSD.  But that people may be predisposed genetically to have poor coping skills is the general idea.  \n\nAnyways, sorry this is all kinda vague, but this is more or less what we know as clinicians.  Basic science researchers doing animal model research may have some more advanced theories, but at this point, they're largely theories.  The reality is that there's a TON of variation, the brain is an ungodly huge and complicated place, and we're just beginning to learn to study it, so we've got a long way to go.","label":0,"model":"human","source":"reddit","id":2506}
{"text":"An arch is a strong structure. Imagine taking an arch and spinning it around, you get a dome which is also a very strong structure. It's good at resisting forces from inside too. \nA bottle with a dome can better resist pressure. \nSoda has pressure because the bubbles are CO2 gas which can dissolve in water, but getting enough into the water to make it fizzy takes cold and pressure. When you do get enough in there, it doesn't like to stay in the water. Keeping the water under a bit of pressure can help though. \nWe have glass soda bottles without curved bottoms, so what's different? A curved bottom means the bottle can be thin, and it's easy to make.\n\nOld plastic soda bottles had a plastic cap glued onto them so they could stand up with the curved bottom. But this is extra material and extra steps. I would also bet the old bottles were harder to recycle but they were phased out before I can recall recycling becoming the norm.\nI believe someone figure out the 5 pointed bottom would work well enough because we are not talking about crazy high pressures. I believe the bottom of these bottles are also a little thicker than the old ones. But the saving in materials, fewer manufacturing steps, and making them easier to recycle beat out the increases in price for the new process.\n\nEdit: If you want to see the ability of a soda bottle to hold pressure in action, google \"soda bottle water rocket\". If your launchpad can fit the mouth of a water bottle, you can see how it behaves under pressure in contrast.","label":0,"model":"human","source":"reddit","id":2507}
{"text":"Originally the mesh was used as a lightweight barrier to block men's wee-wee's from direct contract from the fabric of the actual boardshorts. When the cloth got wet it got heavier and would cause your sensitive bits to chafe and that's very uncomfortable. \n\nMore and more companies are starting to make boardshorts without the mesh now, however. They are able to do this because of the inventions of micro-fibers and quick drying technology. There's also a new stitch pattern called \"flat-lock\" stitching which is a non-abrasive stitch used a lot now in active wear clothing. You'll see it a lot in Nike and Under Armour.\n\nSource: Worked for both Under Armour and Hurley for several years. \n\nEDIT: The underwear underneath the boardshorts seems to have some mixed opinions from what I've read. I personally wear Hurley Phantom boardshorts without any underwear on and there is no chaffing issue. The only downside that I PERSONALLY see about wearing underwear underneath is that it takes longer for you to dry completely. I don't believe only douches wear underwear and I don't think it's a weird decision. It's all about comfort.","label":0,"model":"human","source":"reddit","id":2508}
{"text":"My wife works for USGS and calculates wild animal populations. \n\nFirst, they map historically where the animal has been seen. Then they collect reports from the public, universities, and other government agencies that have seen the animal. Then they make a computer model of where they think the animal should be, based on environment, plant cover, etc. A simple one for butterflies I've seen her do is Butterfly X eats plant Y. Plant Y has this thermographic signature, pull up satellite data, I bet that butterfly is around areas where I think that plant is. Then you send people to look, and test your model to see how good it is.\n\nIf the animal is really endangered, they do a survey, go everywhere they think it is and survey. If there is too many to realistically survey they do a random sample. This is just like the polling used to predict elections. There is 1,000 acres we think it could be at, we randomly select 20 and go. Now there is a ton of spacial statistics that goes into that, but that is the basic idea. Also, they might do \"block\" polling to make sure they go to different areas, or areas they think will help them model better in the future.\n\nA team of biologists goes out to the survey cites, and looks for them. Ideally you send the same people to the same place at the same time every year, so you have an good idea what is happening with the population. Due to lack of funding (sequester ruined--put a huge hole in 100's of years of scientific data collection), fires, changes in administrations, that doesn't always happen.  It is never perfect, some land is private, military, impossible to access, or too sensitive to send people into every year. But, especially with new drone technology and satellite data, they have gotten extremely accurate in their predictions in the last 10 years.","label":0,"model":"human","source":"reddit","id":2509}
{"text":"One way of thinking about the way languages are constructed is by saying that all statements in every language have a subject (the person or thing doing an action), the object (the thing being acted upon) and the verb (the action being taken).\n\nEnglish has what's called an \"SVO\" sentence order, for subject-verb-object. So in the sentence **The cat eats the mouse**, \"The cat\" is the subject, \"the mouse\" is the object, and \"eats\" is the verb.\n\n If we change that order around, we either have a totally different sentence like **The mouse eats the cat** (where \"the mouse\" is now assumed to be the subject because it comes first), or we have something that doesn't make any sense, like **Eat the mouse the cat.**\n\nHowever, there's no particular reason why languages have to be organized this way. In fact, this isn't even the most common worldwide. Languages like Japanese and Hindi are \"SOV\" order, so directly translated their sentences are something like **The cat the mouse eats.**\n\nIt's actually a bit more complex than this--a lot of languages have flexible sentence order, because you can use different endings or small words to mark which parts of the sentence are which, and then you can arrange them any way you want. For example, in latin, you could write **F\u0113l\u0113s murem c\u014dns\u016bmit** or **Murem f\u0113l\u0113s c\u014dns\u016bmit**, and both mean \"The cat eats the mouse,\" since the way you end the words for \"mouse\" and \"cat\" tell you whether it's supposed to be the subject or the object.\n\nFun side fact: the rarest of all possible sentence orders is OSV, where you put the object before the subject and the verb at the end. There are very, very few languages on earth that use it, and most are isolated tribal languages in the amazon. However, when you hear it, it sounds familiar and pretty understandable. Why? Well, because with this sentence order Yoda speaks!","label":0,"model":"human","source":"reddit","id":2510}
{"text":"Hi, I'm an editor, this is my job. The answer is: they don't. Usually.\n\nWhen you watch a movie, you are never watching one take. Every single time you see the camera angle change, that's called a cut, and every cut is to a totally new take and totally new performance. And putting a scene together from lots of different takes and angles is the job of an editor.\n\nLet's say you're filming a scene where two characters are walking down the street and talking. First, you get out your camera, you light the scene for a good extra-wide shot (camera is very far away, characters take up less than half the height of the screen). The actors then perform the entire scene, from beginning to end. Then you move the camera, to get a wide shot (characters' heads and feet are both visible, but they take up most of the screen), and the actors perform the scene again. Then you move the camera, to get a medium shot of Joe, and they perform the scene. Then you move the camera, to get a medium shot of Karen, and they perform the scene. Then you move the camera, to get close-ups of their hands as they hold hands, to get POV or over-the-shoulder shots of the characters looking at each other, shots of their feet as they walk along the ground, shots of the scenery they look at. \n\nAnd there is only ever one single camera filming at once. Every angle is a recording of a totally new take. \n\nAfter filming is done, the editor gets ahold of all the footage, and it's their job to decide what angles to use, when to cut, what to show at any given moment. To make these cuts, they'll insert a certain length of footage from one take into a sequence in their editing software, then insert a portion of a different take that starts by showing the same thing the last portion ended on. Editing is a big job that takes a long time and has a strong effect on how the movie will eventually feel, because certain progressions of cuts can emphasise certain moods, tell you things about the relationships between characters, and so on.\n\nAnd changing angles isn't even the only reason they'll do new takes! sometimes the director will say \"Okay, let's do a few takes where you're both bitter and resentful towards each other\", and the actors will do a few performances that way, from each angle. Then he'll say \"Okay, let's do a few takes where you're just irritable and this argument isn't really a big deal\", and they'll do a few performances that way. When it gets to editing, the editor will go through the footage with the director and decide which portions to use -- maybe Joe's actually bitter, but Karen's not taking it as a big deal? You'll cut between the two types of performance to emphasise that. Things like that. Or maybe Joe just delivered this line great in Take 1, but this other line great in Take 2, so you mix both together to get a really awesome performance that never actually happened.\n\nHave you ever noticed a glitch in a movie, where a character will be drinking a full cup of coffee in a wide shot, then it cuts to a closeup and the coffee is empty? That's usually because someone forgot to fix all the props between these takes.\n\nThe only time movies and big-budget TV shows use multiple cameras is when they're filming something that can't be replicated over and over, like a scene with fire or explosions. For those scenes, the director of photography will look at the set and work out how to position multiple cameras to get the best angles without recording each other. \n\nNow, there's an exception. There's a type of TV show called the multi-camera sitcom. Multi-camera sitcoms *are* filmed with multiple cameras at once. Examples of this type of show are Cheers, Friends, and Seinfeld -- but never movies, never dramas, and not even most comedy shows nowadays. These types of shows are basically made like plays -- there's a stage, with a set that has three walls, and audience seating facing them. But in the front row of the audience, there are a handful of TV cameras, all filming at the same time. One will get a wide shot of the entire set, the ones on the side will zoom in on the actors' faces, and so on. The actors will come onstage at 5PM, do a few takes of each scene until they all nail it, and finish by 1AM, and then the show's all done. \n\nIn these shows, you never see the other cameras because they're all next to each other, all on the same side of the stage, [like this](_URL_0_). The downside here is that the shows tend to feel cheap and stagey, because they have to take place entirely on specially-built sets, and are restricted to a very simple visual style. Look at Seinfeld -- 99% of the show took place in either Jerry's apartment, George's office, the caf\u00e9, or a restaurant set that would get redecorated episode to episode. You never saw the closer wall of Jerry's apartment, because it didn't exist. The actors had to face the audience as much as possible, even if it was a bit awkward, because they couldn't get a variety of camera angles. Compare that to Arrested Development, which was -- like most modern shows -- shot with a single camera and edited later, like a movie. That gave it the freedom to film scenes on the beach, on the street, at a prison, in Iraq, in the hospital, wherever they want. And they had the freedom to put cameras wherever they wanted, so you got more effective and lively photography. They didn't have to light the entire set flatly and evenly, so the show looked more realistic and vibrant. You got more takes and more freedom in cutting, so you got better performances.","label":0,"model":"human","source":"reddit","id":2511}
{"text":"Scientist here.\n\nWe haven't cured cancer, but we have cured MODELS of cancer.\n\nScientists will often use MODELS of cancer, because there are so many types of cancer. For example, we will take a biopsy of a live person's breast cancer tumor, throw it in a dish, and do crazy stuff with it to make that cancer able to live forever in said dish.\n\nThis is now a \"cancer line\". For example, a cancer line of triple negative breast cancer. Specific cancer to find a specific cure.\n\nNowadays how this starts is we analyse this \"cancer line\". We look at all of its DNA to find that it lacks \"gene x\" or \"protein y\" compared to normal healthy cells. We find a target that is very different (or totally absent) in the cancer, and a target that we like, and think, \"hey, if we fix that one thing, can we fix the cancer?\"\n\nSo we find a drug or a genetic treatment and target it to fix this problem.\n\nMany times, this \"gene x\" or \"protein y\" that is wrong in the cancer cell is so important, that fixing that one little thing out of 100,000 other things can save the cancer.\n\nSo anyways, we take this cancer line, and take an inbred mouse family with a real shitty immune system. We inject the human cancer cell line into this mouse and make sure the mouse's shitty immune system is too weak to fight off the cancer. If the mouse can 'get' the human cancer, and the cancer spreads and kills the mouse, success!\n\nThen we give the mouse this simple treatment to restore 'normal' function to our target gene or protein. And lo and behold, the mouse is cured of all cancer!\n\nThis is a lousy way of curing human cancers, which are all unique and merely follow trends, however it gives us more insight into these trends, as well as understanding of how exactly the cancer is spreading and killing people.\n\nIt also cements our classifications of cancers. When we find 'treatment X works in this model of brain cancer but not that one', then we can know in real life those patients need to be treated differently.\n\ntl;dr scientists do it with models","label":0,"model":"human","source":"reddit","id":2512}
{"text":"EDIT: For the actual 5 year olds, feel free to ignore anything written between the (parenthesis) - this is extra stuff for people to Google-search for if interested.\n\n---\n\nI program 3D graphics engines.\n\nTo calculate a triangle, you just need 3 points and then you fill the space between them. This simple nature allows for some optimisation; we know that only the pixels between these 3 points will be modified when rastered. We can use this knowledge to simplify how the triangle is shaded, so simple texture mapping is just a case of interpolating texture co-ordinates between these points (keeping depth in mind for perspective correctness). There's more optimisations to be had here that will take a lot of explaining (our GPUs have evolved to be very good at dealing with space between 3 points).\n\nTo calculate a rounded surface, you need an equation for the 3D curve, as well as the limits of the surface. Interpolating texture co-ordinates here would involve re-using that equation over and over again, quite the expensive operation. That equation may also \"push\" the pixels of the surface out into unexpected directions, so optimisations related to the flatness of a simple triangle are going to be much more difficult (they'd need the 3D curve equation to be used again - clipping would definitely be more complex).\n\nAs a result, early graphics hardware evolved to be very efficient with triangles and all the research and development has been spent there, resulting in real-time graphics to be as amazing as it is right now in its current state. To go back and make a new way of rendering would involve a new class of graphics acceleration hardware that doesn't have all the years of development of the triangle-based hardware we have, that's not a good trade-off.\n\nThe graphics hardware we have now is also great at sending additional information along with the XYZ positions of a triangle vertex, so we can send texture UV, XYZ normal, reflectivity, roughness, and more as additional numbers tied to triangle vertices and these get interpolated between the triangle points too, very handy.\n\nAs GPUs get more and more generalised as compute-oriented machines, rather than triangle-rastering-oriented machines, we may see new types of rendering (real-time ray-tracing is possible now, as is voxel based rendering) but these almost always will be slower than using current hardware to render a triangle, so we see these techniques getting used in parallel to triangle-based rendering to achieve effects that aren't as efficient with a triangle-based world (voxels are fast for real-time global illumination, ray-marching [limited ray-tracing] is faster for limited reflections in scenes limited by the amount of triangles displayed).\n\nPerhaps one day we'll gain a \"curve\" shader where we can use a curve equation to do a perfectly smooth surface between the points of a triangle (I expect the nature of current raster hardware will allow for some cheats here, interpolating between fragments come to mind), but for the time being that's slower than just having lots of triangles to better estimate the curve with current hardware.","label":0,"model":"human","source":"reddit","id":2513}
{"text":"The averege cost of collecting one unit of whole blood (1 pint) is around $900 (US).  The cost includes payment of the techs who collect the blood and the opperational costs of the BloodMobile (the busses used in our area) based on an average donation time of thirty minutes, as well as the cost of the collection supplies (each collection bag unopened costs $55).  Then the cost of paying a second set of techs who recieve and process the unit by seperating it into it's three components (redcells, plasma, and platelets) at rate of approximately 15 minutes per unit.  From there, it goes to the labs to be tested (about 36 hours), where the costs per unit are based on paying the lab tech, the cost of the reagents, and if the testing equipment are leased or financed, a premium per unit to recoup the cost of said machinery.  Finally, the cost of storage in the giant walk-in coolers and the cobalt radiation treatment to sterilize each unit is incurred.  So, in three days time almost $1000 has been spent on a single pint of donated blood.  If your local blood bank is registered with the AABB as a Community Blood Bank, then a minimum of 80% of the product drawn in a community must stay within a certain distance of that community (100 miles, I think) and they will maintain a registry of donors so that, if needed, regular donors who need blood will not have the costs of processing forwarded to them.  The hospital will still charge for the administration of the product though.\n\nSource: Supervisor at a Blood Bank 4.5 years","label":0,"model":"human","source":"reddit","id":2514}
{"text":"There are a few different things that may be on that tripod, but traditional surveying is \"rod and level surveying\". He is looking through whatt is essentially a magnified scope which is made to be level with the ground. When he looks at what the other guy is holding, the rod, it has heights marked on it. So the guy with the rod puts it on top of a benchmark, or a spot with a known elevation, and once the surveyer reads the marking on the rod he can add it to what he knows that points elevation is to find out the exact elevation of his level.\n\nOnce he knows the elevation of his level, the rod guy can go pretty much anywhere within sight distance of the level, and do the same thing again. Only this time, the level man subtracts the reading on the rod from the elevation of his level. By doing this, he can find the elevation of any point around, and create a ground profile, find the high and low spots and where water will want to flow. \n\nTypically this information is used to determine placement of any structure or the layout of projects.\n\nSource: Civil Engineer\/Not yet registered Surveyer\n\nEdit: Sorry, I posted and went home. To answer a few questions. The more perpendicular to the ground that the rod is the more accurate the measurement. Some use bubble levels some do not. On top of the typical rod and level surveying there are \"total stations\" which measure the angle that it is rotated and also the distance to the rod (which in this case is a prism). Also, in my County, we have all of the above, but if we are under trees, or have shitty satellite coverage we revert to rod and level surveying.","label":0,"model":"human","source":"reddit","id":2515}
{"text":"I'm a professional musician, so this is one that I see and deal with personally every day.  People love to throw around the idea of 10,000 hours, but if you spend those 10,000 hours doing it wrong, you won't be a master at the end.  You'll just be really good at doing it badly.\n\nYou have to practicing doing it correctly.  There are also diminishing returns to practice in a given session that are readily apparent for someone practicing music at a high level, but maybe not apparent to those who practice sports.  \n\nYou can practice doing it right, but as your brain and muscles fatigue, if you keep going, you're only going to start practicing in your mistakes.  In music you should always make your last past slow and controlled.  There is something to be said for the way your brain makes connections while you're away from an activity.\n\nIn fact, it's better to practice 6 different things for 10 minutes than 1 thing for 60 minutes.  Amazingly, after rest and your brain having time to make the connections, 1 10 minute session will have similar or even superior results the next day.  It's almost magical to see something that was hard even at the end of a session on one day just be easier the next day.  But hammering it for an hour is more likely to cause you to walk away after tons of mistakes due to mental fatigue and you'll be fighting the whole way through.\n\nWhen practicing a very specific skill, you should hit it for little bursts, take a break and come back to do it again.  If there are lots of smaller skills involved in what you're doing, you should practice them equally.  So for basketball, don't spend an hour on free-throws.  Mix it up between free-throws and whatever other sport things goes in there (sorry, not much of a basketball guy).\n\nAt the very least, you'll get more out of several short sessions than one long one.  You'll also get more out of 10 minutes every day than 1 hour once a week.\n\nSadly, in music, far too many students and even teachers don't fully comprehend how this works.  It's partially due to the fact that people who have already achieved a high level of skill either did it when they were so young as to not realize the process involved (e.g. they can't remember when it was hard), or they were just naturally talented, so they've never had trouble with a particular thing and end up giving poor advice due to ignorance of how to solve a problem they've never encountered.","label":0,"model":"human","source":"reddit","id":2516}
{"text":"Net neutrality has been a subject that's been debated for a while. Without net neutrality certain sites would be split into two types similar to an HOV lane vs. slow lane. Certain sites would be given preferential treatment by having faster speeds. Sites that are able to pay the premium would be in the HOV lane and sites that are not would be in the slow lane. This would make it unfair to many smaller businesses. For example pretend there are two local floral shop businesses . One is a large corporate floral shop and another is a small mom and pop floral shop.  Without net neutrality, the large corporate floral shop would be able to afford the premium for faster speeds whereas the small shop would not. This affects their business because no one like a slow website and many users may end up going with the faster site simply because we don't like to wait. Without net neutrality, internet service providers could also discriminate and sites that meet their agenda would be given preferential treatment. Net neutrality rules create an open and free internet.\nAs far as being the lowly consumer, nothing will change. Had net neutrality rules not been approved, then you would see some changes","label":0,"model":"human","source":"reddit","id":2517}
{"text":"To add a little to this, you may be conflating mathematics and arithmetic.  Unsolved math problems are not \"find the solution to this equation\" types of problems.  Here's an example of an unsolved math problem:\n\nConsider a math game where you start with a number.  If it's even, divide it by two.  If it's odd, multiply it by three and add one.  Either way, take the resulting number and apply the same rule to it.  Etc.\n\nSo, for instance:\n\n* 1 - >  4 - >  2 - >  1\n* 2 - >  1\n* 3 - >  10 - >  5 - >  16 - >  8 - >  4 - >  2 - >  1\n* 4 - >  2 - >  1  \n* 5 - >  16 - >  8 - >  4 - >  2 - >  1\n\nNotice that those examples all eventually reach 1?  Well let me ask you this.  Does *every* number *always* end at 1?  Can you prove it?\n\nThis is known as the [Collatz conjecture](_URL_0_), and while many mathematicians suspect that every number ends up at 1 (and every one computed so far has) no one has been able to **prove** that is the case for all possible numbers.  And you can't just try every number, because there are infinite numbers.  So this problem remains unsolved.","label":0,"model":"human","source":"reddit","id":2518}
{"text":"In addition to our comparatively large body size, copper can be more dangerous for some organisms even adjusting for their small size, depending on how their bodies interact with copper, and depending on what form the copper is in.\n\nBacteria, for example, have a cell wall, which is destroyed by copper. Copper binds to atoms in their cell walls, ripping them out of their molecules and compromising the integrity of the cell wall. That is obviously not good for the bacteria and can quickly kill it. Copper can damage our own cells, of course, but without cell *walls* we're less susceptible and we have more tools to control the copper and keep it forms that aren't dangerous to our cells. Even if it did kill a cell or two, we wouldn't notice. Bacteria, on the other hand, only have that one cell!\n\nPlants are similarly vulnerable since they also have cell walls. However, like us they have a lot of cells to lose.\n\nInvertebrates are also very vulnerable to copper for an entirely different reasons. Mollusks and arthropods (so snails and bugs and [giant ocean bugs](_URL_0_)) rely on copper for carrying oxygen. Where us vertebrates use hemo*globin* which binds oxygen to iron atoms to ferry it around, they use hemo*cyanin*, which uses copper instead. Because they rely so heavily on copper, their bodies absorb copper quickly from their environment if that copper is biologically available (ie: they're not going to rip copper atoms off of a chunk of copper metal). It's hard to turn those absorption mechanisms off, though, so if you put too much copper into their environment, they suck up too much and get poisoned by it. So given a snail the size of a human AFAIK they would be killed by a smaller amount of copper than a human.","label":0,"model":"human","source":"reddit","id":2519}
{"text":"Well let me start by saying that I'm not an expert in classical music by any stretch but I did go to college as part of the Recording Industry Managment program at MTSU, specializing in Pro\/Tech, so I feel that I can add something to the conversation.\n\nthe thing about classical music is that it has already been writen and there isn't much room for deviation from the written music while performing in and orchestral setting, if you listen to a full arrangement by different orchestras the music will almost always be extreamly similar.\n\nYoyo has found a way to express himself *within* the written music, he adds emotion to the music without changing the underlying arrangements that we know and love.\n\nThink of it like this, in the world of music, covers are normally not as good as the original because the artist changed it and the song isn't what people expected anymore (rythemic, lyrical, or key changes) yet if a skilled\/talented artist does the cover they add to the song without changing it to the point of being unrecognizable.  Yoyo found a way through his love and passion for classical music to add to the music without taking anything away in the process.\n\nThe best way to tell the difference would be to listen to a full orchestral version of the music made by someone else then listen the him play the same thing, at least for me I can feel the difference, the passion for what he does seeps into the music and infuses it with emotion.\n\n\nEdit:  > to simplify: If classical music is \"Hurt\" by Nine Inch nails, Yo-Yo Ma is Johnny Cash. \nThank you \/u\/beardfantastic for the eli5 of my eli5","label":0,"model":"human","source":"reddit","id":2520}
{"text":"**[UPDATE]** Apparently I carried on a bit too long about the capital vs. lowercase \u201cI\u201d in Internet. :-) Also, my post wasn\u2019t simple enough for some folks. So here\u2019s the **tl;dr**...\n\nThe \u201cwww\u201d at the beginning of a web address is an old way of signalling that you want to talk to the web server for that domain. The web server is the program that will send you web pages, as opposed to data sent by some other kind of server, like email from a mail server or file transfers from an FTP server.\n\nOver the years, most information on the Internet\u2014though not all\u2014is available via the web server, though, so most places dropped the \u201cwww\u201d and if you ask for _URL_4_, they just assume you want to talk to the web server and that\u2019s where the request goes.\n\n**[UPDATE 2]** Here's some quick links to more answers I wrote below:\n\n* [What's the `http` vs `https` bit all about?](_URL_9_)\n* [Where is DNS data stored?](_URL_1_)\n* [I want to play with DNS directly. Talk nerdy to me.](_URL_7_)\n\nNow for those of you that want the non-tldr...\n\nOriginal post\n----\nThere is a difference between the \"world wide web\" and the \"Internet\".\n\nThe Internet (capital \"I\"\u2013though there's been talk over the last couple of years that the capital \"I\" should be dropped) refers to a world wide interconnection of networks. For instance, every major university has a campus network, and every big business, and many small businesses, and now many homes, and lots of other folks. If you have a network (also called a local area network, or LAN), that just means your various devices are connected to each other, but they're not accessible from outside.\n\nIf you want your devices to be accessible from some other device on some other network, you need to connect your network to that other network somehow. This is called an internetwork. The Internet is the idea someone had long ago, hey, let's make one standard internetwork (called the \"Internet\") that everyone can connect their network to, which means all those networks are then connected to each other and accessible from any other device on the Internet.\n\n(It's worth pointing out that there are other internetworks besides the Internet that are completely separate. The US government, for instance, has more than one secure military internetwork that are completely distinct and independent and inaccessible from the Internet. Any internetwork is typically referred to in short as an internet, but this is obviously only useful in print where you can see the capital vs. lowercase \"I\". So, people are finding that when talking they need to say \"internetwork\" anyway when referring to an internet that is not *the* Internet. So, the capital \"I\" is kind of unnecessary anyway.)\n\nWell it wasn't long before having access to devices on this giant network became really confusing. At first, you would just go to your computer and you know how your files are organized and everything, so you can just browse directly to the info you want from wherever you are on the Internet. But if you want someone else to get to your info, like if you're a company, that someone else would need to be familiar with how you structure your files and where to look. No, this is too complicated.\n\nSo instead, someone came up with the idea of an Internet service that makes everything look the same from the outside, using a protocol called HTTP. The collection of all the HTTP servers on the Internet form the \"world wide web\", or WWW. Now before we go farther, you have to know a little bit about how networks are named on the Internet.\n\nA network is accessible through an IP address, but no one wants to have to remember that Google's IP address is some series of numbers. They'd rather just type in _URL_4_, which is a lot easier to remember. So, Google's network is associated with this name, _URL_4_, using an Internet service called the \"domain name service\" (DNS). The way DNS works is that you assign a name for your \"domain\" (which is one or more networks you own). Your domain might provide a lot of different services, a web server being just one. Fortunately, DNS allows you to set up what are called subdomains so you can divide up your domain namespace however you want.\n\nSo, if you have a web server on your network, and your network is associated with _URL_4_, it became standard practice to associate your web service with the www subdomain. Keep in mind that other services had other standard subdomains too (this \"standard\" is just a shared convention, there's nothing sacred about it...individual sites can do it differently if they want, and some do). So if your domain was _URL_0_, you could have your web server on www._URL_0_, and your mail server at mail._URL_0_, and your FTP server at ftp._URL_0_, and your gopher server at gopher._URL_0_, etc. This way, just by looking at the full name, anyone can tell what service they're trying to get to.\n\nWell, over time that problem I mentioned earlier got worse and worse as the Internet grew\u2013no one knew how to find anything *except* if it was available through the web service. (If you connect to an FTP site, you do need to know how to navigate the directories to find what you want.) So, it became popular to offer pretty much everything through the web service. As that happened, many companies found that it was redundant now to have all these subdomains, so most just dropped the www subdomain and accessing the domain name just dumps you to the web server. Hence, you can go to www._URL_4_ or _URL_4_ and you get the same thing these days.","label":0,"model":"human","source":"reddit","id":2521}
{"text":"Google, Apple, and Microsoft are multinational corporations. They have lots of parts and materials and products and bits coming in from all over. They want to be able to market their product en masse, and get their product into the country.\n\nSo if they tried to harass the government in the way that Scientology did, a few things would happen:\n\n1. The government would start squeezing their imports and exports, either officially or off the books.\n2. There would be a ridiculous-nasty PR backlash\n3. The government would then sue Google\/Apple\/whomever as a corporation for harrasment, likely for multiple billions of dollars.\n\nScientology has no exports, and to a large extent don't give a fuck about PR either. They straight out deny a lot of things that they in actuality very probably did.\n\nThey also couldn't be sued as one would sue a corporation, either before or after the transition to \"religion status\". The people who believe in Scientology aren't legally employees, and so when some 10,000 people decide to harrass the IRS at the same time, the IRS couldn't just scoop them up into 1 big lawsuit.\n\n And this is important, because in the end the IRS decided it was going to be less expensive to just cave, rather than arrange 10,000 individual lawsuits. I read somewhere the estimated legal costs to defend themselves adequately as being something like $20 billion dollars.\n\nAlso keep in mind that this method of strongarming government bodies is very illegal. Corporations trying a similar tactic with their own employees \"acting as concerned citizens\" would very quickly be swept up with liability issues, IRS investigations, public backlash, and all other manner of trouble.\n\nOh, and some Scientologists broke into the IRS offices. Multiple times. I forgot that bit :)","label":0,"model":"human","source":"reddit","id":2522}
{"text":"There are a number of factors at play... In general, in most places in the world, you (as an individual) have no legal expectation of privacy in a public place.\n\nI can photograph or film you in public and do whatever I like with those photos and I have not breached your privacy.\n\nThis even extends to private places that I can see from a public places. The paparazzi exploits this with telephoto lenses to get photographs of celebrities in ostensibly private places.\n\nBut it can be more complicated. If I recontextualize your image in a way that could be damaging to you somehow, or unjustifiably show you in a unflattering situation, then you may have some legal recourse against me.\n\nSo if I took a photo on a public street and then put it on a website warning that **Pedophiles Could Be Anywhere**, then a person clearly identifiable in that image could sue me for damaging their reputation.\n\nThat's an extreme example, but it's the reason that TV shows and documentary films require release forms and often blur faces of those who do not sign releases. \n\nBy getting a release form (usually very broad, allowing the company to do basically whatever they want) they are protecting themselves legally against potential lawsuits.\n\nBlurring the faces of those who don't sign a release then attempts to avoid the issue by making sure people aren't identifiable. \n\nAs a documentary filmmaker (I'm currently producing and directing a feature documentary) I don't really *need* release forms, and I probably wouldn't have to blur anyone's face... but without taking those steps I also won't be able to sell my film to any distributors as they will be unwilling to risk the legal liability that skipping those steps could entail.\n\nIn film and TV production there's something called E & O (Errors and Omissions) Insurance - basically it's insurance against being sued. It's generally required to sell a film for exhibition or broadcast. Without taking all possible steps to limit potential legal exposure a production will find themselves either unable to get E & O Insurance, or facing high premiums and excesses.  \n\nThe same is broadly true of blurring logos and artworks in the background of reality TV and documentaries. There's no legal need to do so in most cases (although context can change that) but it's become a standard practice and no-one is willing to take the chance on not doing so now.\n\nTL;DR - It's not really *necessary* to get release forms or blur faces of people filmed in public, but concerns over possible legal exposure have made it standard practice in film and TV.","label":0,"model":"human","source":"reddit","id":2523}
{"text":"Hello. Pilot here. There are a LOT of inaccurate replies to this post. Let me clear some things up.\n\nConcord stopped flying for a huuuuuuge range of reasons. No one factor or reason killed it by itself, but as a combination they mounted up and where eventually too much. I will list the main reasons that killed it off.\n\n**1. A tiring airframe**\n\nAircraft have a shelf life of sorts. Unlike a car, an aircraft is exposed to a lot of stresses and strains during its operational lifetime that weaken it's structure and components. Different parts of the aircraft have different life expectancies (The engines being the largest components with the shortest life expectancy) but generally an aircraft lifespan is measured in something called cycles. Depending on the aircraft, one cycle is either one complete startup and shutdown of the engine, or one take off and landing (Regardless of how long the aircraft is actually in the air). An aircraft can only do so many of them before specific maintenance has to be carried out to extend it's life so it can do more cycles. The more it is extended, the more expensive and in depth that maintenance becomes. Eventually there comes a point where it's just not worth it and it has to be retired.\n\nConcord was getting close to this point. She was an old aircraft and extending her life further and further would soon have come with complications like shortened flight hours, restrictions of movement\/speed and all kinds of things that is undesirable in a commercial aircraft. I will allude to this more later, but many people view the Paris Air France crash as the end of the Concord. It wasn't. Her airframe cycle life would have killed her off a few years later regardless, but we will touch more upon that later.\n\n**2. A reluctant maintenance company**\n\nAirbus had a contract to maintain the Concord and it was said that they where somewhat reluctant to continue on with it beyond its renewal date. Maintaining the Concord required extremely skilled people and sophisticated facilities only a company like Airbus could bring to the table. When Airbus indicated they did not want to do it any more, that was a big problem that did not really have an immediate solution.\n\n**3. A downturn in it's economy**\n\nContrary to popular belief, Concord was always profitable. It can never be said to have been a huge source of profit for BA\/AF and it might be fair to say it sometimes was closer to breaking even but it never ran at a loss. The reason for this was economy of scale. It ran on a schedule that allowed it to break even. Concord never flew with empty seats. The price of a ticket was astronomical and that reflected its operating costs. The upside of this was that flying on it was almost a zero wait experience that got you from London to New York in 4 hours 15 minutes once check in time was calculated. There where no queues at the airports or checking in three hours early or anything like that. Minimum check in was 45 minutes before the flight, it had it's own baggage check lines and security for only 160 people. Your time in the airport was kept to an absolute minimum. The downside of this was you couldn't book a flight to fit your dates, you fitted your dates round flying on Concord. An aircraft on the ground that isn't flying and carrying passengers absolutely haemorrhages money for an airline, but in the case of Concord the time spent on the ground was unavoidable so it was factored into the cost of a ticket.\n\nThat being said, there was a downturn in it's economy that began to pinch into this. As fuel prices and maintenance costs rose, the ticket prices couldn't really begin to keep up, high as they where already and it was predicted its maintenance costs would outstrip what could reasonably be claimed back in it's ticket costs in the near future. This meant at some point, the aircraft WAS going to begin operating at a loss.\n\n**4. A loss of confidence and increased safety measures after the crash of an Air France Concord in Paris**\n\nMany people like to believe that the loss of the Air France Concord in Paris was the final nail in the coffin for Concord. It wasn't. In fact all other things being equal it would barely have phased BA\/AF in terms of worrying about the aircraft's future as the aircraft had a near flawless safety record at that time. At some point in time, every airframe has a crash for some reason. The fact Concord had operated for so long before it's first fatal accident was a testament to the aircraft.\n\nHowever it was just one of the factors that weighed up against the aircraft.\n\nA lot of safety measures had to be retrofitted to the aircraft after the crash. It's worth noting though if the crash had been directly to do with a fault in the airframe then Concord would likely have been scrapped there and then. It's no secret in the aviation world that the damage to the engine and resulting fire from the tyre debris striking the fuel tanks did not destroy the Concord. Had it remained on the ground and come to a stop it would likely have been possible to get some or maybe all of the passengers off. Sadly what destroyed the aircraft was that the plane took off before it's take off speed had been achieved and the aircraft basically stalled into the ground. Therefore BA\/AF where confident that with the adaptions to the engines and fuel tank strengthening the aircraft would be as safe as it could be. However they where hugely expensive to implement and BA\/AF where never fully sure of being able to recoup the costs against a potentially nervous and very small (Due to the cost of a ticket) customer base. Then 9\/11 happened which further had a bad ripple effect across the airline industry and Concord unfortunately had to take some of the brunt of that as well (One of the huge impacts was no longer being allowed to visit the cockpit of Concord mid flight which was often a big part of the experience).\n\n**5. BA\/AF did not want to share their toys**\n\nAfter it was announced that the Concord's would be retired, British Airways and Air France had a multitude of offers from several aviation companies to take or buy the aircraft and continue flying them. One of the most likely offers was  from Virgin Atlantic. However Richard Branson did not believe he should have to buy them after they where basically gifted to BA\/AF by the French and British governments. As it turns out BA where not willing to continue operating them but they sure as hell wheren't going to let anyone else have the prestige of operating them, and as they legally owned them, they said no. It was entirely feasible they could have had a few more years in them, but BA did not want them appearing in someone elses livery.\n\nHopefully this clears up the main reasons why Concord stopped flying. It was a shame she stopped flying when she did but make no mistake, her time was coming to an end regardless of any crashes.\n\n\n---\n\nFollow up comment with answers to some questions I have been asked as this post is too long.\n\n_URL_0_","label":0,"model":"human","source":"reddit","id":2524}
{"text":"I'm from southern Spain, the area with the highest unemployment rate. I can assure you that many families are having a very rough time not being able to eat 3 times a day, play bills or even have a home. The rich are richer but the middle and lower class are much poorer. The government is living off debts and taxes that have constantly been rising. They also sell government properties such as parts of the Air Agency (AENA), the train system... They have also reduced the number of government employees and salaries (I was a former employee). But the politicians have maintained their salary. \n\nYou also have to think that many Spaniards have left Spain to find job, so those are less people accounted for in the statistics. Another factor is that a lot of people from other countries live here due to our culture and climate. Those people (German, English, Russian...) spend a lot of money in Spain, buying houses, boats, cars, dining out, buying luxurious food...stuff that us, the middle and lower class can't afford. I for example have passed from being an accomodated middle class person to a lower end class in three years. Even if outsiders don't notice, we are struggling to get by.","label":0,"model":"human","source":"reddit","id":2525}
{"text":"Hello, structural engineer here. I do seismic design on the west coast (high earthquake risk).\n\nFirst of all we have to pick a level of earthquake to design for. Generally the building codes specify an earthquake that has a 1 in 2500 year chance of occurring (smaller earthquakes will happen more often than this). This means that for a 50 year structure life, there is a 2% chance of the earthquake being bigger than what the structure was designed for. This doesnt necessarily mean that the structure will fail though. \n\nWe generally allow for a certain amount of damage to occur in the design earthquake but for it to only occur in controlled locations (ie. no progressive collapse and ductile failure). We allow steel to yield (permanently deform) a certain amount, and reinforced concrete to crack. \n\nWe do a structural analysis to see how the structure behaves in an earthquake. The most important thing is the period or frequency of vibration of the structure. You get resonance and higher forces on the building if it vibrates at a similar frequency to the earthquake shaking. For simple buildings you can do this on paper by hand; for more complicated structures you have to use a 3D computer model. \n\nAfter the structural analysis, the engineer will have an idea of the forces on the structure due to an earthquake. They would design selected critical parts of the structure to take that force and detail it to ensure it doesn't fail (lose all of its load carrying capacity) even if it becomes damaged. Damage to the structure helps to dissipate the earthquake energy and act as a fuse to limit the loads on other parts of the structure. Then you design the rest of the structure to make sure it is stronger than those critical areas you identified and designed earlier. \n\nThis is the basic method of design, for more complicated structures such as highrises and long bridges engineers would use special techniques such as dampers (sloshing water tanks at the top of a high rise) or base isolation (special bearings at the base of the structure to allow it to partly move with the earthquake rather than withstand the force from trying to stay in place). \n\nTLDR: Engineers design and detail certain parts to be damaged but not fail or collapse. The rest of the structure has to be stronger than that. \n\nEDIT: Here is a 12 page pdf from the Structural Engineers Association of BC which does a pretty decent (better than me) job at outlining structural engineering for earthquakes in layman terms. _URL_0_","label":0,"model":"human","source":"reddit","id":2526}
{"text":"One of the reasons why Asians have used mainly white rice over the years is that white rice lasts longer in storage than brown rice. The essential fatty acids found in brown rice usually begin to go bad after approximately 6 to 12 months of storage, the exact amount of time depending on how much oxygen is available. When brown rice is polished down to make white rice, many of the essential fatty acids are lost, allowing white rice to last longer than brown rice without going bad.\n\nAnother reason why many Asians prefer white rice is that they have become accustomed to how easy it is to chew and digest. Brown rice requires more chewing power to properly digest than white rice does.\n\nSome Asians refuse to eat brown rice because to them, it's a sign of poverty. Many Asians who are above 40 years of age have been deeply conditioned to believe that prosperous people eat white rice while peasants eat brown rice.\n\nFinally, many Asians choose white rice over brown rice because white rice is less expensive. White rice is far less expensive to produce and distribute because it is in greater global demand and produces higher profits because of its longer shelf life.\n\n[Source](_URL_0_)","label":0,"model":"human","source":"reddit","id":2527}
{"text":"Dogs can recognise several cues in human body language and facial expression, changes these cues are also important for social interactions between dogs. They become more sensitive to your emotions as they get to know you better and can read your body language.\n\nInterestingly they can also respond to cues in acoustic characteristics in our voices. They have well developed voice-sensitive cortical regions that are functionally analogous to the areas in our own brains that we use to determine if someone sounds sad or happy. They showed this by putting some doggies in a fMRI scanner and monitoring their neural functioning while listening to various acoustic cues including happy and sad voices. Happy giggles or barks light up the same regions in dogs as they do in people. Some fMRI scans are also being looked at to determine levels of empathy that dogs can show, a study on dogs yawning when their owners do suggested that they might very well be empathetic. \n\nI am not familiar with similar studies on cats, but empathy has been potentially demonstrated in [rats](_URL_0_) where a pretty neat study showed that rats will try help each other if they like them or are familiar with them or their type. They don't bother helping rats they don't like! There is also some interesting work being done on empathy of corvids, some ravens have been shown to offer consolation and extra attention to their friends if they have been in a bit of a conflict or to-do with a flock-mate. \n\nI am sure the more we investigate the more instances we will identify of such behaviour in various taxa, it seems to be adaptive especially for social species. \n\n\nAs an interesting aside the perceptions of the level of empathy shown by pets is influenced by the gender of the owner, with female owners asserting that their pets recognize their emotional state significantly higher than male owners. So anecdotal evidence needs to be cautiously considered.","label":0,"model":"human","source":"reddit","id":2528}
{"text":"It honestly probably has more to do with how we're socialized into music.  The trope of minor being used for sad things and major being used for happy things is sort of an artificially created thing to an extent.\n\n**Things below get a little ELI20:**\n\nIn fact, not even all very young school children will agree about this until we tell them so (my wife used to teach elementary music).  Other cultures with less overbearing exposure to Western music are even less likely to always correctly identify these.\n\nTo get more complicated, if you look into Arabic maqams (essentially scales for the sake of simplicity) they sound quite different with a bit more microtonality compared to Western ideas of tuning and such.  The thing is, when we hear them, they all just sound semi-foreign.  The ones that sound most like a major scale or maybe a lydian scale sound happier to us.  The ones closer to a minor or locrian scale sound sadder.\n\nBut in the cultures to which these are native, the great variety of maqams hold much more subtley of meaning and even the ones we might equate with sadness are not thought of as such where they are actually commonly used.\n\n**Below we go into ELI have a basic understanding of theory:**\n\nSo major and minor are very black and white for us, but if you just wanna talk about chords, what about bigger chords?  Start with something like a CMaj7.  It's a C on the bottom with an Em on the top.  Does it sound happy or sad?  Are you hearing the C major of the primary triad, or the minor triad when ignoring the root?  You can obviously grow this idea out like crazy.  A Cm9... what do you hear?  The EbMaj7 ignoring the root?  The Eb major chord in the middle ignoring root and 9?  The Gm on top?  The Cm on bottom?\n\nIs this a happy or sad sounding chord?  I think most people would call it \"dreamy, sleepy, or ethereal,\" but once again, that's likely more to do with how we most often hear it used.\n\nAlso, context counts.  You can revoice a complicated chord for a different sound.  But you can also revoice a simple chord and depending on the context and character of the piece, it might throw you off.  Have you ever been listening to something and couldn't quite tell if a given chord was major or minor in context on the fly?  What if you have a C major triad and suddenly the bass moves away from the C up to E and the E and G are sustained in the upper voice.  Is this implying an Em without the 5th?  Is it still implying the C major but in root position?  You probably need context to tell you.\n\nHow about a C7.  This one is great for context.  Do you hear the Edim or the C major?  Or hell, do you just hear it as a dominant chord because it's followed by an F or even Dm?\n\n\n\nTL;DR - Because someone told you so.\n\nEDIT regarding my thoughts on the \"natural\" majorness and happiness of the harmonic series:\n I seem to be the minority by not just writing it off as the natural overtone thing.  That said, it still doesn't account for the difference in social interpretations of given tonalities to cultures that aren't brought up in the Western music tradition or the fact that not all children immediately adopt the happy\/sad dichotomy until they are told to feel that way.  While the harmonic series does have a sort of natural elegance to it, I really don't think it explains this away.  Hindemith covered this extensively and he based his whole idea of composition essentially around the idea of a very extended view into the harmonic series, but honestly, what we think of as pure sounds break down a little ways in.  Certain harmonics are frighteningly out of line, though we're less able to hear them normally.  \n\nAlso, we commonly bastardize this absolutely pure version of the harmonic series with equal temperament anyway, so the only place you'll hear good, adjusted, clean temperaments that make the harmonic series work the best is with a capella groups or very very talented chamber ensembles that have no fixed pitch instruments (basically any keyboard or harp type instruments).  These groups will naturally tune as they go to make every spot as pitch perfect as possible.  Meaning, A440 might not be 440 if it's the third of an F major chord versus being the root of another chord.\n\nEDIT 2: Thanks for the double gold o.O","label":0,"model":"human","source":"reddit","id":2529}
{"text":"There are many reasons.  \n \n1. The physical antennas in use. Voyager has a really large dish pointed pretty precisely at Earth. There is a much much larger dish on Earth pointed at Voyager. It is part of the DSN, or Deep Space Network.  These dishes act to focus the broadcast signal and to gather more of the received signal. \n \n2. Data rate: The data rate coming from Voyager is very low-rate. This helps because a single piece of information (a symbol, in RF speak) lasts longer, so the receiver has a longer time to collect the signal and to become certain what the value of that symbol is, and\n \n3. No obstructions. The walls of your house, the water contained within your house and within your body, the material in between the phone and the tower all absorb some amount of the energy, weakening the signal. Conversely, the frequency chosen for the communications of Voyager and other satellites was chosen so that the atmosphere wouldn't absorb much of the energy, and there's basically nothing in the way. \n \n Delay: Most of the delay that you experience is due to processing happening at the various nodes of the network. As for Voyager, the majority of the delay is the time it takes light to travel from the edge of the solar system back to the vicinity of the sun, where Earth hangs out.  \n \n\nAlso, your carrier sucks.","label":0,"model":"human","source":"reddit","id":2530}
{"text":"There are lots of reasons - one of the primary ones being that in our history as humans, avoiding pain was a much higher priority than seeking pleasure as pain.  So it makes sense that our subconsciousness tends to focus on negatives rather than positives, especially as back when evolutionary pressure was at its highest on humans, something like a broken leg likely meant death.  You can see this phenomenon playing out in people who are highly risk averse, even when the risk is small and the rewards are potentially great (making career moves, dating moves, etc).\n\nAs long as you reproduce, your DNA doesn't care how pleasurable your life is.  Pain inhibits your ability to reproduce far more than pleasure helps it, or least that was the case back when evolutionary pressure was stronger for us and natural selection was ruthlessly weeding out unfavorable traits.\n\nAs for why you're focusing on social shame, when it comes to questions of the subconscious, the rule of thumb is to look back at what your concerns would have meant for a caveman.  Back in the tribal days, if you were not accepted by your tribe, it meant you were far less likely to reproduce - and not only that, but if you weren't accepted by your tribe it was overwhelmingly likely that you would never be accepted by anyone as most people lived and died exclusively with the tribe they were raised in.\n\nAlso, your subconscious is smart enough to know what things are important for you to resolve.  This does not mean that every thought pattern is beneficial - only that you can gain insight about how to proceed by analyzing your thought patterns.  Your subconscious seems burdened by some bad memories and is bringing them up again in order to encourage some type of action on your part.  That action is for you to decide, and there are professionals who can help you with that if you decide you want to do that.","label":0,"model":"human","source":"reddit","id":2531}
{"text":"Say Bob has a bunch of stock in a business called Company, Inc.  Bob's friend Henry thinks Company, Inc.'s stock is about to drop significantly.  Henry makes a deal with Bob:  Bob will lend Henry 100 shares of Company, Inc. stock, but Henry has to give it all back exactly one year from now.  So Henry gets 100 shares and sells them at their current price of $10 each.  Henry now has $1000, but he'll have to buy back 100 shares before the end of the year in order to hold up his end of the bargain.\n\nA year later, Company, Inc. stock isn't doing so well, selling for only $1 a share.  Henry buys up 100 shares and gives them back to Bob.  By shorting the Company, Inc. stock, Henry made a profit of $900.\n\nIn an alternate universe, Company, Inc. is doing pretty good at the end of the year.  Their stock is selling for $20 a share.  In order to get the 100 shares he needs to give back to Bob, Henry has to use the $1000 he got from selling the stock originally AND $1000 from his own pocket.  In this universe, Henry's attempt to short the Company, Inc. stock has cost him $1000.","label":0,"model":"human","source":"reddit","id":2532}
{"text":"Hello, economist here. Yes, there are real world examples and we do look at these. The problem is that it can be very difficult to actually figure out convincingly what the causal impact was of the minumum wage policy. It is not impossible but it is difficult, and so the evidence is not always clearly on one side. When we introduce a minimum wage our main concern is what the impact will be on unemployment. Economic theory says that the effect on unemployment is ambiguous, that is, a minimum wage can raise or lower unemployment depending on the level of the minimum wage and the circumstances.\n\nNow, unemployment in a given area changes lots all the time, so even if it goes up after a minimum wage is introduced it is not clear whether this is because of the policy or it would have gone up anyway. One very popular strategy to get around this is to compare how unemployment changed to that of a similar place. Suppose the two places had unemployment rising and falling in the same way beforehand, then if the minimum wage is introduced in one place and unemployment suddenly rises there but not in the other place then this is evidence the minimum wage caused the rise. There are still big problems with this though, unemployment does not tend to be so parrellel between regions so the evidence is never so clear. Also worrying is that a minimum wage introduced in say, one state will affect nearby states (maybe workers from those states go to the new one to take advantage of the higher wages for instance) this could alter the affect of the policy but would not occur for a federal minimum wage.\n\nMy preferred strategy is what is called 'structural estimation'. Economic theory tells us that the effect of the minimum wage depends on certain features of the labor market (which is the kind of metaphorical market place where firms hire workers for certain wages). We may be able to measure these thing by using lots of data about the labor market. In fact we can create quite complicated models, sometimes requiring computer simulation, that take into account people crossing borders etc. Whether this kind of very theory-heavy approach or the mostly 'atheoretic' approach above is better is a subject of heated debate.\n\nAnyway, there is a huge literature trying to estimate the effects of different levels of minimum wage using various approaches and data from various times and places. As for why there is still debate, well as you can gather from the above, the difficulties involved mean that evidence is not entirely conclusive. However, in the 90s when the federal minimum wage was set under Clinton there was much attention to the economic literature which at the time seemed to clearly support the idea that a minimum wage at the Clinton level was good for employment. I might add that even if there were very clear evidence there may still be disagreement. Even if a higher minimum wage would substantially raise unemployment then you could support it anyway because the lower paid workers who stay employed get higher wages.","label":0,"model":"human","source":"reddit","id":2533}
{"text":"Gangstalking is a particular manifestation of paranoid schizophrenia.  Specifically, that \"everybody is out to get me\".\n\nThe internet has enabled paranoid schizophrenics to network, sharing and reinforcing their delusion.  This has resulted in a fairly standardized presentation with shared terminology and perceived tactics.\n\nThe \"targets\" of gangstalking believe that they are the target of a *vast* conspiracy.  So vast that literally every person they see or talk to is a member.  This conspiracy expends vast amounts of time and money to, say, beam intrusive thoughts into their heads with top secret technology; frame them for crimes; break into their homes to tamper with things; and generally annoy them in extremely petty ways.  For example, it's common to believe that multiple people will pass them, each saying one syllable of a slur.\n\nHere's a \"comic\" that explains gangstalking from the perspective of a victim:  [Targ the Target](_URL_0_).  Edit: Because a lot of people have been confused, I want to be clear.  *Targ the Target* is not a parody or a description of the condition from the outside.  It's written by a woman who suffers from the delusion, and is 100% serious.  Targ appears to be a composite of herself and other sufferers.","label":0,"model":"human","source":"reddit","id":2534}
{"text":"From educationspace360:\n\"There is an accurate perception that boys develop the fine motor skills necessary to hold a pen or pencil as much as six years later than girls. And then for boys to make correctly shaped symbols in specific horizontal alignment is even more difficult. It seems that boys develop the larger muscle mass for upper body strength before their brains can precisely control the movements of the smaller muscles in the wrists and fingers. There is also scientific analysis demonstrating that a boy\u2019s brain develops many of the abilities for handwriting much later than a girl\u2019s brain. A group that promotes separate schools for boys and girls, National Assoc. for Single Sex Public Education cites research by Harriet Hanlon, Robert Thatcher and Marvin Cline that details the differences in boy and girl brain development. Clearly, then, there are some measurable differences in muscle growth and brain development that result in the broad, general perception that a large percentage of boys are not capable of even average handwriting skills until a few years later than the early grades at school.\"\n\nEdit: First, I'm sure that social conditioning plays a large role in this as well, with different writing styles being defined girly or masculine by culture. ITRAINEDYOURMONKEY makes the very valid point that nurture is not controlled for well in many studies, especially those examining brain development. A culture that encourages men to have curvy neat handwriting and that allowed girls to have messy handwriting without questioning it would probably produce different results. Second, there will always be exceptions. There will be men with beautiful handwriting and women with handwriting that looks like it was \"actively savaged by a bald eagle\" as nettlebutt so eloquently put it. TL;DR: Social conditioning has a big effect, and there will always be exceptions.","label":0,"model":"human","source":"reddit","id":2535}
{"text":"So far the responses I see in this thread are biased.\n\nObsessed with guns? The question seems to be asking about those Americans for whom owning firearms is a piece of cultural identity. No, these people are not obsessed with guns. Yes, there are nearly 100 million people in the US who own firearms. Some of us own quite a lot of firearms, and enjoy using them recreationally. For most of us it is a combination of hobby and tradition, and maybe you can call some of us \"obsessed\" in the same way you might call an avid skier \"obsessed\" with skiing. It's not truly obsession, it's interest. Gun owners don't wake up in the morning and think about their guns, daydream about guns all day, then go to bed ready to dream all night about guns. For us, they are just a natural part of our lives. \n\nI would put forth the argument that the people who are \"obsessed\" with guns are the ones who have an intellectually dishonest and pathological need to blame acts of crime on civilian gun ownership. Even though there are nearly 100 million law abiding and peaceful gun owners who never hurt anyone with their firearms, there are people who believe that these innocent people are somehow responsible for gun crime, which based on the sheer number of gun owners is nearly statistically insignificant. \n\nWhat does drive crime in this country is a broken judicial system and poor law enforcement philosophies and practices. Our prison system fuels, rather than deters, crime. Our War On Drugs fuels, rather than deters, crime. Our immigration policies fuel, rather than deter, crime. Our mental health practices fuel, rather than deter, crime. And gun control itself fuels, rather than deters, crime. \n\nWhat the rest of the world might see as an obsession is a direct result of people responding to this crime, fueled by various factors, and seeing only one cause, and one solution: guns cause all crime and banning guns will stop it. It's a manic view, in truth only held by a small minority, but that small minority happens to be quite vocal in government and in the media and their rhetoric is sometimes persuasive to an uninformed public immediately following a tragedy when everyone is feeling rather than thinking. When this happens, calls for gun control flood the news and the legislatures. \n\nThen, when responsible gun owners step in and say, \"Hey, we're not the ones committing crimes. Leave us alone.\" That's when we are smeared as being \"obsessed\", \"bloodthirsty\", and as having \"blood on our hands\". \n\nI'm not obsessed with guns. But I am obsessed with justice, and I'm not interested in being blamed and punished for the crimes of the occasional psychopath who happened to use a gun to kill people. Guess what... even if he didn't have a gun, he still would have killed people. And the latest CDC study, ordered by Obama himself following the Newtown shooting, determined that people who use guns to defend themselves are less likely to be hurt when victimized.","label":0,"model":"human","source":"reddit","id":2536}
{"text":"If you are just talking about the eye, it's somewhere in between.\n\nWhen a rod or cone, the light sensitive cells in your eye, is stimulated with light, through a chemical reaction it sends a signal through nerves to your brain. The rod or cone then takes some time to recover before being able to be stimulated again. This sets a rate in which individual cells can transmit signals.\n\nYour eye has roughly 100 million rod cells and ~7 million cone cells so with constant light, some will be firing and some will be recovering. For the brain, it looks like a constant flow of discrete packets of data much like a flow of individual sand particles falling down an hourglass.\n\nNow for frame rate, it's really how your brain interprets the signals from your eye. Sure, your brain receives all the data from your eye but it only selectively processes changes in data. If you were driving a car, you wouldn't want your brain to constantly tell you \"THERE'S A BLUE SKY ABOVE, THERE'S A BLUE SKY ABOVE\" but rather if there was a car appearing on the side of your vision that you need to care about. Large changes such as a camera flash or swinging your head back and forth between sights can overwhelm your brain and cause you to be disoriented. \n\nBut then there's one huge change in our vision that occurs every few seconds that causes everything to be black: blinking. Luckily, our brain is quite smart and ignores the lack of signals from our eyes when we blink so we never notice our vision getting obscured by our eyelids. Likewise, when you turn your head, it never seems like the quick panning blur effect in movies because your brain knows that your complete vision will be shifting. It ignores a lot of the visual data when your head is moving and it prepares to show you new information once your head has stopped. If there is any disconnect between the motion you feel and the motion you see, your brain can get confused and this is the main cause of motion sickness.\n\nIn terms of how fast we can detect small visual changes, our brains need to process them very quickly in order to respond. People often reference the USAF test that determines how quickly fighter pilots can respond to changes in vision. The pilots are placed in a dark room and then an image of an aircraft is flickered for a fraction of a second. Fighter pilots can identify the aircraft when the image is flickered at 1\/220 of a second ([Source](_URL_0_)). I could not find a credible study to back this up however.\n\nThis doesn't mean that fighter pilots can see at 220 frames per second, making movies that run at 24 frames per second seem like slide shows. It just means that they can detect a very quick changes in their vision which is very necessary when they are flying so fast in their aircraft.\n\nEDIT: Expanded a bit more on the fighter pilot information","label":0,"model":"human","source":"reddit","id":2537}
{"text":"Other comments have hit on part of this explanation, but I'll try to tie it all together.\n\nThe human species started in Africa.  To protect ourselves from the harsh, direct sunlight, we evolved with higher levels of melanin in our bodies.  This causes darker skin.  Dark hair was also a favourable evolution - less light can pass through dark hair than light, protecting the skin.\n\nOver time as the human race migrated, mutation continued.  In the branch that became \"white\", mutations that produced paler skin and lighter  hair occurred. This was an advantageous mutation because these humans were living in areas with less direct sunlight, so needed to take advantage of the small amounts they were exposed to (we need sunlight to produce vitamin D).  Through natural and self-selection, the genes for paler skin and blonde hair survived.  Other mutations for lighter hair colours (lighter brown, red) remained for similar reasons.\n\nThe reason that other races did not evolve blonde hair (in general, great link to the Solomon Islands case in another comment) is that:\n\n1. It did not give then an environmental advantage, so did not increase chances of survival\/reproduction\n2. The genes for darker hair colours are dominant over genes for lighter hair colour. Without selective pressures (the environment advantage) to encourage this mutation, any spontaneous hair colour mutations would be lost in future generations without other recessive genes to allow it to be expressed.\n\nLook at it this way - a few hot blondes randomly showed up in early northern human populations. They were healthy, dark-haired humans banged them. Had dark-haired babies with a hidden blonde gene. Eventually those dark-haired babies banged each other, hidden blonde genes met up, blonde baby appeared.  Blonde babies were just as healthy as dark-haired babies, so the genes became more common through mating.\n\nLet's say the same few blondes showed up in southern human populations.  They would burn in the sun and not be able to survive in those conditions.  Some dark-haired humans might bang them because, hey, they were just lying there...but they were not ideal mates as they were unhealthy in that environment and could not survive as well.  A few babies may have been born with hidden blonde genes, but if a second generation blonde baby was born, they would also struggle to survive in that environment.  Blonde genes did not abound.\n\nEdit: I don't know why there is so little variatin in Asian populations. Sorry.\n\nAnother Edit: The variety of hair colours that exists in northern cultures is due to sexual selection.  Individuals with a random mutation for red hair, for example, could survive and reproduce in northern areas.","label":0,"model":"human","source":"reddit","id":2538}
{"text":"As others have mentioned, the freezing process is the problem, but I haven't seen ice's role mentioned. When freezing living things, you aim to cool them so quickly and to such a low temperature that ice crystals don't have time to form, because ice crystals act as tiny razor blades to the cells\/body. However, if you've ever tried to freeze a really big piece of meat, you'd know that it can take quite some time. Even with liquid nitrogen or helium, it's long enough to kill a human before the process is complete. There's also the problem of bodily functions in a half-frozen\/half-thawed state, both during freezing and melting. \n\nThis is the main difference between flash-freezing and normal freezing too, btw. When you put meat in a home freezer, the ice crystals rupture cells, resulting in moisture loss during cooking and a drier end product. \n\nEdit: As an addendum, some animals (e.g. some frogs) have anti-freeze which prevents the formation of ice crystals, which allows them to survive the freeze\/thaw cycle for winter hibernation.","label":0,"model":"human","source":"reddit","id":2539}
{"text":"First, you gotta find a good tree in the starting shape and thickness you want. It's important to use an already grown tree, because you're going to ~~starve it a bit~~ put it on a diet and it needs to already be healthy. \n\nIf it's a branch in the shape you want, you can air-layer it and grow a new tree from that. Air-layering is when you [scrape some of the bark off a branch and wrap something moist around it.](_URL_0_) This makes the branch start to [grow roots in the air!](_URL_2_) When it grows enough roots, you cut the branch off the tree and plant it.\n\n~~When trees grow, they have one [big root and a lot of little roots.](_URL_1_) The big root is called the tap root, and it provides the most food and water to the tree. Since we want to keep the tree small, you have to snip off part of the taproot.~~\n\n~~The little roots will try to fuse together and become big hungry roots, so bonsai trees are usually planted in pebbles to keep them from touching. Every once-in-a-while the tree is taken out of its dirt and pebbles, the taproot is trimmed, and the tiny roots are combed and un-knotted.~~\n\nTo keep the tree small, you have to carefully prune the leaves and branches. The leaves and fruits will be smaller because the plant is not fed as much. \n\nIf you want to make a neat shape from your tree, you can influence its shape [with wires or string](_URL_3_) and by trimming new branches when they appear where you don't want them to grow. \n\nEdit: I might be wrong about why taproot is cut. Sources below say it's just trimmed to fit a shallow pot, and that porous pebbles or clay are to help the thin hairy roots get extra air to help collect nutrients and to improve water drainage. The tree is instead made smaller by trimming and pruning.","label":0,"model":"human","source":"reddit","id":2540}
{"text":"The differences between a much smaller (4GB flash) drive and a much larger (128GB) drive are likely due to the age of the technology. The smaller one will use manufacturing techniques from several years ago and consequently are cheaper to produce. The larger one uses a much more contemporary process.\n\nHowever, in drives of similar size (64GB vs 128GB, for example), the price differences are due to manufacturing defects. Let's say the flash memory producer makes 128GB chips.\n\nIt helps to picture the memory as being laid out as a grid, like a large city. There will inevitably be defects in the memory cells. The defects cause individual cells to be unusable. But if the defects occur on only the left half of the chip, that entire half can be disabled, leaving the right half usable. Let's say this happens at a rate of 75%. That is, there is a 25% chance you'll get an entire working 128GB chip and a 75% chance that the defects appear on half the chip.\n\nSo for every one working 128GB device, you'll get three usable 64GB devices. Therefore, you can (and will) sell the 128GB device at more than twice the cost of the 64GB drive.\n\nEdit: Source: my buddy who works at a Major SSD Manufacturer(tm).","label":0,"model":"human","source":"reddit","id":2541}
{"text":"The thing about second winds is that they can't be counted on when you need them. They're a real phenomenon, yes, but what they are not is a predictable phenomenon.\n\nSecond winds depend on a number of factors, including everything from exercise intensity and frequency to what kind of shape you're in. But although physiologists know they happen, they don't all agree what's going on behind the scenes to cause it. Some theorize that your second wind, also known as a runner's high, may be caused by the body's release of pain-relieving endorphins. However, that doesn't explain the whole thing. More commonly that \"high\" is believed to happen as the body's systems come back into balance: Your respiration is regulated; your oxygen intake is fast, deep and plenty; and your body is operating at a slightly elevated temperature, covering you in a light sweat.\n\nWhen your second wind kicks in, which takes roughly 10 to 15 minutes to happen, give or take five minutes or so (generally, that is; some people may have to wait much longer), it's because your body has stopped focusing on expelling excess carbon dioxide and started taking in more oxygen. That's aerobic energy production (also called aerobic metabolism), and that translates into less pain, easier breathing and a renewed confidence that although you might not have wanted to exercise, maybe it wasn't such a bad idea after all.\n\n**Fueling Your Second Wind**\n\nAn energy-carrying molecule known as adenosine triphosphate (ATP) fuels every living thing \u2014 you, me, plants, animals, all of it \u2014 and, when you get right down to it, it's what fuels your second wind.\n\nAdenosine nucleotides are part of the energy production systems of your body, specifically the energy metabolism of your cells. ATP is created from the process of metabolizing the carbohydrates, fats and proteins you consume. It's formed by a high-energy bond between lower-energy phosphates, adenosine diphosphate (ADP) and inactive \n\n\nThe body makes an ongoing supply of ATP, and it starts with the breakdown of sugars from food. First, a reactive process called glycolysis traps and converts glucose, a monosaccharide, and converts it into fructose 1,6-bisphosphate. Next, that fructose 1,6-bisphosphate is split into two molecules of three-carbon pyruvic acid (CH3COCOOH); that's important, because ATP is produced when those three-carbon molecules are oxidized into pyruvate, the final product of the glycolysis energy-conversion process. In short, your body is constantly breaking down the food you eat and converting it to stored energy, which can fuel that second wind.\n\nBecause ATP is critical and stored only in limited amounts, the process of hydrolysis and resynthesis is circular and ongoing. ADP and Pi combine to synthesize and replenish the body's ATP, and through hydrolysis, ATP is broken down into ADP and Pi as needed for energy. That equation that looks like this: ATP + H2O \u2192 ADP + Pi + energy [source: Encyclopedia Britannica].\n\n\n**The Physiological Process of a Runner's High**\n\nThe human body fuels itself through three types of energy production methods, depending on how intense and how long you engage in that physical activity: phosphagen, anaerobic and aerobic energy production.\n\nWhen energy is needed in a hurry, it's the phosphagen system that gives the body immediate energy, lasting only for seconds; ATP is able to fuel some pretty intense muscle contractions, but not for very long. Because the supply of ATP stored in the muscles is limited, the body can only sustain short bursts of energy, like sprinting for no more than five to six seconds [source: Berg]. During intense, short periods of exercise, ATP is rapidly replenished by creatine phosphate, which is stored in the body's skeletal muscles.\n\nAfter that first five seconds, the rate of glycolysis \u2014 that's the process that converts glucose to pyruvate, which is needed for cellular respiration \u2014 dramatically increases by 1,000 times than while the body's at rest. The anaerobic energy system, which uses carbohydrates but no oxygen to provide for the body's energy demands, takes over [source: Stipanuk et al.]. ATP is rapidly generated during anaerobic glycolysis, to be used during intense physical activities lasting between 30 seconds and three minutes [source: Gagliardi]. If the body's demand for oxygen becomes and remains greater than what you're supplying, there is an increased risk of lactic acidosis, when pH levels decrease in the body and byproducts of the breakdown of glucose to pyruvate accumulate in the body's tissues and bloodstream.\n\nMost of the body's energy needs, though, are produced through a process called aerobic metabolism, also known as mitochondrial respiration. During aerobic endurance exercise, oxygen is required to generate energy from carbohydrates and fats \u2014 and to keep up the production of ATP, although its synthesis is low when the aerobic metabolism has kicked in. When the measure of your oxygen consumption (V02) reaches the maximum volume of oxygen your body can use (V02max), you've arrived at your second wind. You're what some refer to as \"in the zone\" \u2014 you're focused, you're not in pain, and your breathing deepens to provide maximum levels of oxygen to your working muscles and maximum ATP regeneration.\n\nAs your body gets accustomed to exercising and regulating its energy needs, the odds increase that you'll see your second wind kick in more frequently because your muscles, including your heart, will be more efficient.\n\n\"I know that I'm going to have a number of highs and lows over the course of an ultra, to the extent that I don't really think of it as a 'second wind' anymore,\" says Rob Colenso, ultra-marathoner and RRCA-certified running coach. \"It's more like, I was able to properly eat and hydrate over the last hour, and so now I feel better and have gotten a burst of energy.\"","label":0,"model":"human","source":"reddit","id":2542}
{"text":"Glenn's Cookies makes great choc chip cookies. They have a giant factory with millions of dollars worth of machinery, and hundreds of employees already. Walmart wants to make Walmart Cookies without investing millions in equipment, employees, land, buildings, trucks, everything needed to make cookies, and get them to the stores (or warehouse).\n\nSo Walmart says to Glenn's, can you make them for us? So Glenn's uses slightly cheaper ingredients, a different package, and sells them to Walmart. Walmart puts a little mark up on them and sells them in store.\n\nIt's useful for lots of reasons, the main being that most shoppers fall in to 2 categories. 1. I want the best cookies, and 2. I want the cheapest cookies. Walmart sells both, gets all the shoppers, and of course they buy lots more than just cookies when they go to Walmart.\n\n\nSometimes Glenn's cookies and Walmart cookies, are bought from a third company. Pauline's food, makes cookies for Glenn's, Walmart, and several others but they don't sell their own brand. So they have low marketing cost, pretty constant demand for cookies, no worries about discount, and a very simple transport system (factory to warehouse). They just tweak the cookie recipe for different cookies, to tase a bit different, to be made cheaper, etc.","label":0,"model":"human","source":"reddit","id":2543}
{"text":"* Humans can eat all food types raw - there is nothing \"wrong\" with our digestive system. You can eat veggies, meat, and fish raw but it carries the risk of you contracting a [foodborne illness](_URL_0_) (e.g. bacteria, parasite, or fungal contamination of food). The issue isn't the raw-ness per se, but rather the increased risk of getting a foodborne illness.\n\n* ALL foods carry the risk of contracting a foodborne illness if eaten raw. Same thing goes for untreated water, in which case you carry the risk of contracting a [waterborne illness](_URL_4_) like [giardia](_URL_1_).\n\n* Modern food distribution and water treatment systems make it harder for these foodborne\/waterborne illnesses to get to you. However, we still have foodborne illness outbreaks on raw food because our system is not 100% safe. For example, when recalls are made for *E. coli* or [salmonella outbreaks on tomatoes](_URL_2_), lettuces, etc. Always try to prepare your food before eating it, this can save your life or at the very least save you from a very unpleasant couple of days.\n\n* Preparing food (e.g. cooking, boiling, washing, peeling, freezing, smoking) all help reduce the risk of contracting a foodborne illness. Cooking specifically also has the added benefit of being easier to digest and enables us to extract more calories from cooked food. A double win. \n\n* [Wild animals](_URL_3_) and domestic animals can also and often do contract foodborne and waterborne illnessess. You shouldn't let your dog drink from an untreated stream because [they can get giardia just like you](_URL_5_). Any wildlife biologist, parasitologist, or veterinarian will tell you that wild animals and domestic animals (if left untreated or in unsanitary\/crowded conditions) are\/can be rife with parasites, foodborne, or waterborne illnesses. My point is animals are also susceptible to the same, and sometimes different, foodborne illness that we are.\n\n* The only animals that have a much stronger (but not perfect) digestive system are carrion eaters like buzzards or vultures. They have very strong digestive systems that make it hard for foodborne illnesses to take hold.","label":0,"model":"human","source":"reddit","id":2544}
{"text":"Ooh, ooh! Air Force munitions guy here. Depending on the type of \"fighter jet\" you're referring to, they might not even be the same *job* in the back seat. The Navy has a high-speed jet that's very much *like* a fighter (it was actually based on a fighter airframe) and it's primary job is \"electronic warfare\"...from the skies. In that jet, the \"passenger\" (for lack of a better term) is an electronic warfare specialist. Looking for radar and other radio signals, communications, stuff like that.\n\nOther aircraft, as said, have navigators for longer flights or flights (like in Afghanistan) where we might have to navigate sparsely populated massive mountain ranges. Others are weapons systems officers (WSO, pronounced \"wizzo\") that handle the often *very* complicated weapons systems. If you're in a straight-up \"shoot down other fighter jets\" plane, you might not have a passenger, but we're rarely up against other jets in dogfights anymore. And if you're on a \"fighter\" that's really been repurposed into a flexible response light bomber, you might have half a dozen guided bombs with you, and the guy driving the jet can't really do the supersonic aerial equivalent of checking his Facebook on his phone while he's dodging traffic doing 80 down a busy highway.\n\nTLDR: It depends on the jet and the mission, but just *flying* a fighter jet is *hard* and very demanding, and if a guy in the sky wants to do anything while he's *in* the air, like dropping bombs, finding targets, navigating, listening or trying to take pictures, it's very helpful to have somebody who *doesn't* need to keep the thing in the air to help.","label":0,"model":"human","source":"reddit","id":2545}
{"text":"RD here. It's what you're eating (or not eating). You need a variety of macronutrients (protein, CARBOHYDRATES, and fat) to give your meal \"staying power\". \n\nCarbs are found in breads and fruits and sugar. They're good for instant energy BC they're broken down fast and absorbed quickly- but they're used up fast. Protein (found in meat, dairy, beans) takes longer to digest than carbs, and fat takes even longer. An ideal meal will contain all three (plus fiber, which helps you feel full because it's bulky, but isn't actually digested or absorbed by your body).\n\nSo if you eat 200 cal of like, fruit or crackers, the energy will be used up faster and you'll feel hungrier sooner than if you ate 200 calories of meat and cheese. \n\nI can elaborate if you have questions! This is my favorite topic :)\n\nEdit: said more\n\nEdit: that escalated quickly! Thanks for all the awesome questions and respectful discussion. I'd love to do an AMA later :) Thanks for making me feel useful and thanks for the gold!","label":0,"model":"human","source":"reddit","id":2546}
{"text":"Forgive me for this is on mobile. \n\nThe short answer is we don't know. The long answer is we have some interesting evidence that points towards a reason for the phenomenon but not a cause. I'm going to explain a study done at the University of Lethbridge that might shed some light on the subject. \n\nOne of the professors there (I'm sorry I don't remember his name, I believe it was Dr. Matsumoto) did a study on this. \n\nCertain cells in the hippocampus activate when you (or a rat) enter and leave a certain place in the environment. These are called place cells. This professor basically hooked some electrodes (recording devices) up to some rats and recorded the neural activity from these kinds of cells when they were plodding around a circle. He then compared activations from when they were awake to when they are asleep. He found that that these cells activate sequentially in relation to where the rat is in the environment when they are awake. But here's the kicker, they activate at 3x the rate while the rat is asleep. \n\nSo judging from this the rat was experiencing what they just had done IRL while asleep. But three times faster. \n\nNow like I said this doesn't fully answer your question but I hope it shines some light on what is happening. \n\nTldr; Rat run in circle. Experiences time in a certain way.\n\nRat go sleep, same brain times but multiplied by three, seeming longer.\n\n\nProper credit to \/u\/syncdata","label":0,"model":"human","source":"reddit","id":2547}
{"text":"As others have said, the Rosetta Stone played a huge role in deciphering hieroglyphics since it included translations in languages we already knew. But how could it be deciphered if we didn't have that kind of cheat sheet? \n\nA fascinating example is [Linear B](_URL_2_), a pre-Ancient Greek language discovered on stone tablets on the island of Crete. It was long assumed that it would be completely indecipherable without some sort of \"Rosetta stone\", but we cracked the code in 1952, thanks to decades of study by [Alice Kober and Michael Ventris](_URL_1_).\n\nThe first breakthrough came after Kober diligently recorded the frequency and position of each symbol on the tablets (While this type of analysis is not hard to do with computers today, this took *years* of work for Kober). In doing so, she discovered many instances of the same groups of symbols, but with consistently different endings. Through this, she realized that Linear B was an inflected language with different endings based on usage, like verb endings in Latin and Spanish. \n\nShe also noted that there were about 200 unique symbols in total. Being an expert of many languages, she knew that this was too many characters to be alphabetic (each symbol representing a letter - English, for example), and too few to be logographic (each symbol representing a word, like Chinese). She surmised that each symbol in Linear B likely represented a syllable. \n\nNow we have a clear understanding of what *type* of language Linear B was, but how do we determine what any of it means? This is where Ventris stepped up. \n\nHe theorized that these tablets likely had location names, and knew that location names often stayed similar over long periods of time. So he basically did ~~brute force~~ trial-and-error using the ancient Greek names for towns in Crete: What if a particular group of symbols are syllables that mean something like, \"ko-no-so\", meaning the Cretian city of Knossos? After exploring this idea in countless ways, he eventually discovered a pattern that confirmed this: When he interpreted one particular set of symbols as \"ko-no-so\", other symbols began to make sense. Slowly but surely, that first bit of translation led to him fully deciphering the entire language. \n\nEDIT: As \/u\/QuarkMawp pointed out, brute force was not the correct term.\n\nAnd since this has gotten some traction, if anybody is more interested in this and other sorts of amazing cryptography achievements throughout history, I highly recommend [The Code Book](_URL_0_) by Simon Singh. It covers a broad history of immense achievements in cryptography including Linear B, along with things like development of new codes in Renaissance Europe, cracking the Enigma Machine code in World War II, Navajo Wind Talkers and modern Public-Key Encryption. It's very informative and engaging, and also very accessible for the layperson.","label":0,"model":"human","source":"reddit","id":2548}
{"text":"This is a complex issue but I'll do my best to ELI5.\n\nRisk factors for suicide are much higher for men than they are for women. These include severe mental illness, substance abuse, access to lethal means, and a higher rate of overtly aggressive behaiours. Furthermore, there are far fewer supports available for men than there are for women (e.g., if a woman's husband kicks her out of the home, there are likely women's shelters nearby, not so for men). The supports that are available to men are much less likely to be used due to stigma and stereotype surrounding men seeking help in relation to their distress, depression, grief, bereavement, etc.\nThe social aspect of support also comes into play here. Women are more likely than men to have a social group with whom they feel comfortable discussing their problems and experiences of low mood. We see this in the divorce literature that demonstrates post-divorce depression to be significantly higher in males than in females. \n\nIn a nutshell, men have higher rates of suicide risk factors and fewer protective factors against suicide attempt and completion.\n\nEDIT: phrasing","label":0,"model":"human","source":"reddit","id":2549}
{"text":"To understand this, you have to know why we have urine at all. Your blood has to have a very precise chemical balance for your body to be able to work. If it's off even a little bit, you will get sick and eventually die, unless it's corrected. Your kidneys have the job of doing the corrections. They work by filtering out any excesses, returning blood that is chemically \"balanced\", and they are very good at it. The stuff they filter out is basically urine. The most important things filtered out are extra water, extra salt, and urea, a chemical produced continually by the body. Another chemical filtered out by the kidneys is urobilin, which makes urine yellow. Removing certain amount of water is necessary to the process, whether your blood has excess water or not.\n\nIf you are dehydrated, meaning you have not been drinking as much water as your body needs, your kidneys will do their job by removing the bare minimum amount of water to get the other stuff out. That means the urine will be concentrated. Think of it as being like strong tea. Because everything including urobilin is concentrated, its color will be dark yellow. This is actually normal first thing in the morning because you don't drink water while you're sleeping. During the day, it's a reliable sign that your body could really use more water. But it doesn't have to be clear for you to be healthy; a well-hydrated person's urine will still be light yellow unless they are drinking much *more* water than they need.","label":0,"model":"human","source":"reddit","id":2550}
{"text":"There is a disorder called narcolepsy where people can't truly control their sleepiness. This disorder is divided into two types: type 1 and type 2. Type 1 means that you can collapse at any given moment with something known as cataplexy. Type 2 just means you're tired often, and at times have a nearly uncontrollable urge to sleep.\n\n & #x200B;\n\nWhy are things like this for those suffering from narcolepsy? Because they have low levels of hypocretin (for type 1).\n\n & #x200B;\n\nYou see, in order to fall asleep, your body needs to muster up a whole bunch of hormones (things that control how your body is behaving). Things such as melatonin take time to enter you system and start working. Even if you take a melatonin supplement, it'll still take some time to work.\n\n & #x200B;\n\nThe only real way (that I can think of) to allow people to fall alseep at a moment's notice would be to always have the hormones (such as melatonin) or lack thereof (i.e. hypocretin) in their system. But if you did this, and had the hormones being employed at all times, then the person would have narcolepsy.\n\n & #x200B;\n\nThis is my take on this. I'm not a doctor or anything, but, in short, I believe it's because these hormones take time to start working (they need to disperse throughout your body, after all) and having them always working would cause serious problems, such as narcolepsy.","label":0,"model":"human","source":"reddit","id":2551}
{"text":"*Edit* - This was supposed to be a supplemental post to others, not make it as a high top-level comment.  So here's an ELI5 explanation people would expect from it.  More mathematical explanation has been pushed below that.\n\n\nWhen you want to transfer electrical power, you have to push electricity through a wire to reach its destination.  As electricity flows through a wire, it heats it up slightly.  This is how a light bulb works - electricity flowing through a very thin wire heats up the wire until it glows.\n\nWe're going to think about it like water.  If we want to transfer energy with water, we need to put it under pressure (blowing through a straw) and we need to move it through a pipe.  The more water we move under a given pressure, the more energy we'll transfer.\n\nWater flows through pipes.  Bigger pipes let water flow easier.  The reason is that water rubs up against the sides and friction causes it to slow down.  Larger pipes have a lot more area that isn't rubbing up against the sides.  Think a soda straw versus a coffee straw - a coffee straw is a lot harder to drink with, or to blow air through.\n\nBut the friction between the water and the side of the pipe also depends on how *fast* the water is going.  If the water is moving slowly, not much heat is generated.  As the water moves faster, a lot more heat is generated and energy is wasted.\n\nSo ideally we'd like to transfer our water slowly through large pipes to deliver it over long distances.  But since it's a long distance, we can't afford to make the pipe that large.  So the pipe has to be fairly small and thus we need to make sure the water moves *very* slowly.\n\nGoing back to electricity, water pressure is like Voltage, and the speed of the water is like Current.  Current times voltage equals how much power we're transferring.  So if we increase the pressure by a factor of 10 - increase the voltage, we can drop the speed of the water - the current by a factor of 10.  If we multiply the voltage by *one million* we can drop the current to *one millionth* of the original speed and still transfer the same energy.  At those super-slow speeds, the water rubbing against the pipe isn't going to waste any energy at all.\n\nAC power lets us use a cool device called a transformer that lets us take in voltage and current on one line, and output a different voltage and current with the same total power on another line.  We can't do this efficiently with DC power.  So we use AC power because we can step up the voltage to super-high levels, and drop the current to a trickle.\n\n  \n\n\n---------------------\n\nLarge number of correct answers - but none are describing the mathematics of why, so I thought I'd add it.  The Short version is that resistive loss of power transmission is a function of resistance in the wire, multiplied by the square of the current.  So high current leads to *very* high resistive losses heating up the power line and wasting energy.  Some examples:\n\nPower (energy in a set amount of time) is a function of the voltage on the power line multiplied by the current.  So if I wanted to run a winch that lifted up a 1kg weight at 1 meter per second, I'd need a motor than ran on 10 volts of DC power with a current of 1 amp.\n\n > Electric Power Draw:  \n > 10Volt x 1Ampere = 10 Watts = 10 joules\/second.   \n\n > Potential Energy of Weight:   \n > 1m\/s x 1kg x ~10m\/s^2 = 10 Nm\/s = 10 joules\/second\n\nSo they're balanced.  However, if I had a motor than ran off of a 20 Volt power, it would need only draw 0.5 Ampere.  At 100 Volts, it'd need draw only 0.1 Ampere.\n\nThen the question goes to power distribution. Why is there a limit to how far we can deliver electricity from a power plant?  We have to run electricity through wires.  And when we run electricity through a wire, it heats up slightly and loses some energy.\n\nThat power is equal to the resistance in the wire multiplied by the *square* of the current.  RI^2 = resistive loss.\n\nSo lets change our example above a little bit.  Now we're lifting huge wrecking ball that weighs 10,000kg.\n\nTo lift it at 1meter\/sec would require 100,000 joules\/sec or 100Kilowatts.  Using our 10 Volt motor, we would need to draw 10,000 Amps.\n\nNow lets say that the we have a very thick wire that is 10 meters long, and has 0.0001 Ohms of resistance per meter.  So almost no resistance.  \n\n > Overall resistance R = 10m x 0.0001Ohm\/m = .001Ohms  \n > Overall Current = 10000 Amps  \n > Resistive Power Loss = 0.001O x 10000A x 10000A = 10^5 W or *100 Kilowatts of power as waste energy!*\n\nWe're burning as much energy just *transferring* the power to the motor as we're actually using for the motor.\n\nSo lets switch our 10 Volt motor out for a 100 Volt Motor.  The power demand remains at 100 Kilowatts.  \n > 100,000KW = 100Volts x ?Ampere  \n > current I = 1000Amps\n\n > Resistive Heat Loss = 0.001 Ohms x 1000Amps x 1000Amps = 1000 Watts = 1 Kilowatt of waste heat.\n\nStill a lot of waste, but it's actually manageable now.  we're only spending 1% of our power draw heating up the wire during transmission.\n\nIf we used a 1000 Volt motor, we'd only pull 100 Amps and our restive heat loss would be 10 Watts across the whole thing.  A tenth of the energy spent on an incandescent bulb, spread over 10 meters.  The wire wouldn't even be warm in your hand.\n\n\nSo clearly, when transferring power with electricity, we need to keep the current as low as possible because the energy lost is a function of current-squared.  We also can't make the resistance too low because thick wire becomes prohibitively expensive over millions upon millions of meters of power lines.\n\nSo if we want to transmit, say, 1 Gigawatt 10 kilometers from a power plant to a city, with 6 gauge wire (1 Ohm per kilometer), we need to figure out how bad that will be.\n\n > P = 1 Billion Watts  \n > R = 10 Ohms\n\nIf we step the voltage up to 1 Million Volts, we'd still have 1000 Amps of current, and would lose 100 Megawatts of power - 10% to heating up the power line.\n\nIf we get up to 10 Million Volts, we'd need 100 Amperes, and now our losses are 10 x 100 x 100 = 100 Kilowatts of loss.  Which is still a ton of energy, but its spread out over 10 kilometers and represents only one ten-thousandth of the energy transferred as a loss. \n\nAs others have repeatedly said, AC power lets us use transformers, which very easily and efficiently let us maintain the *power* of electricity flowing, but trade voltage for current.  So I can transfer the energy of a circuit running at 10 volts with 100 amps into a circuit running with 1000Volts at 1 Amp.  Or 1 Million Volts at 1 miliAmp.\n\nSo for power distribution in our country, we generate power at a station, we step it up to hundreds of thousands of volts and transfer it across the country on those *massive* high-voltage power lines.  Those lines go to a few dozen sub-stations located around your town, that step the voltage down to ~10,000Volts.  That energy is carried along power lines to neighborhoods, where every 1 to 5 houses share a small green box somewhere that transforms the voltage down to 120 Volts, which is then delivered to your wall outlet.","label":0,"model":"human","source":"reddit","id":2552}
{"text":"Humans have selectively bred a small number of species to help provide better for human needs at the expense of the animals natural survival. Sheep are one of those species.\n\nPrior to domestication sheep would have grown a much smaller amount of wool up to the point where it served their needs, like the hair on a bear or a wolf. It doesn't grow indefinitely.\n\nThen humans came along and found these relatively docile sheep sitting around and someone realised their wool was very useful to make things like clothing out of. Over many generations humans captured and bred these sheep to harvest their wool from. Humans also realised that by breeding the males and females with the most and best wool, the offspring would usually end up with even more wool than the parents. By repeating this process over time we ended up with the types of sheep we have today, who are completely reliant on humans to regularly shear them and remove the wool. This is domestication and selective breeding. Without us they would suffer many problems like overheating and being far too heavy.","label":0,"model":"human","source":"reddit","id":2553}
{"text":"From a previous answer by \/u\/Zaburino\n\n >  You're right in that shaking your hand out after hurting it doesn't actually relieve pain, but the vigorous motion seems to block the most intense stuff from really registering as painful. This is all because of how our nerve signals communicate sensory information to our brain. While understanding and treating pain is a massive part of modern health care and a sub-field with a lot of room to grow, what's needed to answer your question is fairly straight forward.\n\n >  When you slammed your finger in the door, you(most likely) felt the immediate pain of the smash and then a rising, duller pain that comes on after a second or two. This is because pain nerves come in two general types; a thicker neuron with a sheath that transmits intense pain signal quickly from a small radius around the receptors in your skin, and a thinner neuron that more slowly transmits pain signals from a more general region of your body. When pressure is put on the skin, the faster pain signals can be physically blocked from reaching the brain while the slower ones pass through. \n\n >  So how is this all related to shaking? By quickly shaking your fingers around, you are subjecting your fingers to g-forces as they are in a constant state of acceleration and deceleration, which could potentially provide enough pressure to dull some of that initial intense pain.","label":0,"model":"human","source":"reddit","id":2554}
{"text":"I often go to the animal shelter in my city to take the dogs out for a walk. There was this adorable american bull terrier (or something like that) that the owner had kicked out of his car in a parking lot. The animal shelter named him Spooky for some reason and the dog would NOT listen to that name, he'd drag me through the park while I called his name to no avail. At one point the animal shelter put his picture on their facebook and one lady freaked out because it was her dog who she gave to a aquintance (who then left the dog in the parking lot). She refused to pick up the dog because she didn't want to pay for his stay (130euros). She said in the comments that his real name was Neo. Next time I took him for a walk he was pulling the leash again, dragging me behind him until I yelled 'Neo, stay!'. As soon as he heard his name his ears perked up, he started to waggel his stumpy piece of tail and stayed right next to me the entire walk, amazing sweet dog who got adopted a few weeks later :)\n\n\nEdit: I've found an old picture of Neo :) _URL_0_","label":0,"model":"human","source":"reddit","id":2555}
{"text":"The spots aren't, strictly speaking \"black\", they are \"no signal\" areas.\n\nSight is basically a chemical reaction. The rods and cones of your eyes produce dyes. (See \"Rhodopsin\" and\/or \"visual purple\" et al.) These dyes are how your eyes see.\n\nPhotons come in, intersect, and change these dyes. This is what produces the initial chemical stimulus that becomes the nerve impulse response to light.\n\nThe photon(s) intersecting with the dyes \"uses up\" the dye.\n\nYour body is constantly making new dye and cleaning up the used-up dye.\n\nWhen you look at a bright light it uses up a lot of dye. Then those rods and cones don't have enough to really generate a good signal.\n\nSimilarly, if you've been in darkness for a while, you've got a lot of dye built up and you can see really well in the very dim light. Further the very dim light uses up very little dye and so you continue to see well in the dimness.\n\nSo this whole mechanism is why the room looks \"darker\" right after you turn off the lights, and then \"your eyes adjust\". It's also why the \"blue spots\" move with your eyeballs, because its the individual sensors that are exhausted.\n\n(EDIT: yes, your eyes also _physically_ adjust to darkness or brightness by opening or closing the iris, but that's a different level of responsiveness. After two or three seconds the dark room will be somewhat more visible because of the iris thing, but it can take minutes for the dye levels to get good, and after half an hour the room might be really quite visually available. etc.)\n\nSo your eyes work on eye-fuel, and the more light they process the more of that fuel is exhausted. As it gets exhausted the cells that are exhausted put out less signal for the same light and \"dark patches\" seem to appear in your vision.","label":0,"model":"human","source":"reddit","id":2556}
{"text":"This has been explained a couple of times here in ELI5. \n\nBasically, the first-past-the-post system ensures competition between two opponents. If I have 100 votes, and 3 participants, then those 100 votes will be shared between the 3. So, instead of having two participants with a close competition, the third participant just takes votes from one of the other two, meaning that one of the participants could win having a minority of the votes.\n\nLet me put you an example:\n\nYou have Bill Murray, Michael Jordan and Kanye West running for the position of \"Best Person Ever\". I like Bill Murray and Michael Jordan, but I dislike Kanye West. And most people would agree with me. But those same people also prefer Jordan over Murray (or the other way around).\nSo the time of election comes, the votes are casted and surprise surprise, Kanye West is declared Best Person Ever.\n\nHow?\n\nWell, let's say there were 100 voters. Of those voters, 65% hated Kanye West and would have prefered any of the other two. But, as I said, they prefer one over the other. So you have 65 votes divided between Bill Murray and Michael Jordan. \n\nThat's 32,5 each, but because that's not possible let's say Michael Jordan had 33 and Bill Murray had 31 (I am biased, I know). Each of them, even though they were prefered by the big majority, have less votes than Kanye West, who has 35. Thus, the least likely and less wanted candidate won.\n\nThat's why there won't be any third party rising up in the U.S. any time soon. Even though most would prefer a third party, those voters wouldn't agree on one single party to vote for, so their majority of votes would still be lost, ensuring that one of the two bigger parties still wins. Maybe even one the majority of voters doesn't agree with.","label":0,"model":"human","source":"reddit","id":2557}
{"text":"Dang. After lurking for months, you made me create an account just for answering your question.\n\nWe don't really know. It is an assumption because the ocean is vast and deep and we haven't looked at much of it yet. \n\nImagine you are tasked with mapping the United States, but with the following limitations:  \n1. It is forever completely dark.  \n2. You are in a hot air balloon floating several miles above the ground, and you can't land.  \n3. You can dangle yourself down from the balloon a few hundred feet, no more. Dangerous.  \n4. Actually, you *can* dangle deeper but that involves multi-million dollar equipment and a large crew, and even then you can  only stay down a few hours at best. Even more dangerous.  \n5. You can dangle down remote controlled probes, attached to the balloon by a cable at all times.  \n6, Probe or person, all you have are a few puny 15-Watt bulbs to illuminate your immediate surroundings. Your visibility is never more than five to eight yards, if you're lucky.  \n7. You can make a noise and listen to the echo.  \n8. Every day of your hot ballooning costs around $10,000 to $30,000.  \n9. Many of the people funding you think your endeavor is pointless. \n\nWhat you'll be doing now is float relatively aimlessly around over the land, maybe looking at your echo sounder for promising places to investigate by dangling a probe down or a person to get a camera picture. That way you'll discover trees, and maybe a very small bit of road. Or corn plants. If you're very lucky, you let down a probe over Manhattan island and you'll get a picture of the side of a single building. Then, you may find a second one. You deduce that there may be four or five of these structures, or even more, but you cannot be sure. That way you float around, pinpointing down here and there, with distances of miles between probes, and hundreds or thousands of miles between clusters of probes, and with that information you make a map of the United States. \n\nNow imagine how that map would look. I guess it would be fair to say that you'd miss out on a LOT. In fact, unless you had some other information to go after, a thing like Los Angeles might go totally undiscovered.  \nAnd unfortunately that is exactly the situation that ocean research is in. What we do have are sonar pictures that cover great swathes of ocean (but by no means all) but they have a lousy resolution; also the deeper the ocean the worse the resolution. It is okay to spot mountain-sized features and you may be able to spot house-sized features that way if you're lucky. Anything smaller and you are out of luck. Cameras deliver great pictures but although their spotlights are really powerful, they don't penetrate the sea water for more that a few yards.  \nSea water stops most, if not all, forms of radiation sooner or later (not sure about gamma rays but I doubt anyone would want to do ocean surveying with those). Very long radio waves (the kilometer range) does penetrate kinda okay but cannot be used for mapping; the images would be coarser that the sonar ones anyway. \n\nThen there's the trouble with the spotlights: We know much of ocean life is attracted by light, and other is indifferent, but what about the organisms that immediately flee when a light turns up? For all we know, there may be a veritable pogo party below there but we'll never know because it happens outside the reach of the lamps. \n\nSo that is where we are. We do incredibly coarse imaging with sonar and have ridiculously few, tiny pinpoints of imaging systems that so far allowed us to map an area of ocean floor, put together worldwide, of not more than a few football fields, if that. \n\nThat way, I'd say the idea that we have already found 1\/3 or ocean life is very optimistic. Even more when we assume that we have found most of the larger animals whilst having to acknowledge that there is very probably an incredible abundance of tiny life that we have yet to discover. \n\nAnd don't get me started on bacteria, which - safe to assume - are everywhere, but the ocean variants of which are found to be so far impossible to grow in a lab - all we can find is their DNA. \n\n2\/3? Hah!\n\n\nEdit: Okay, maybe this is the time I should start addressing the actual question. But I'll stick with oceans beacuse it's just such a great example ;D\n\nSo you are stuck with a map of the United States created by the method detailed above. However, you try to be Not Bloody Stupid so you acknowledge that your method, although it is the best available, is woefully inadequate and that there is very, very much left to discover. So how do your put a number to that in order to hopefully satisfy some of the people mentioned in No 9.?  You make an educated guess.  \nIn this particular case, to improve the quality of the \"educated\" bit you can do the following: You find the spots you've visited with a camera on your sonar pictures and look at the surrounding area. If the sonar picture looks very similar, you can assume (a dirty word in science) that these areas that look the same probably share the same life forms. Or you try to derive rules that govern this environment. For example, you have frequently seen trees, and under very few of them you spotted deer. You know next to nothing about these strange creatures but you know that they must eat something, and you can chalk up that something as unknown life forms. Also, there is usually something that eats the deer, so you chalk up that too.  \n\nAnd so on. The list could go on but it remains the fact that with various observational methods, deductions and statistics you can get a very general idea of what is out there, but you can never be sure until you've gone and looked.  \nThese numbers that are quantifying the unknown are guidelines at best; a general idea to acquire funding for more research for example.  \nBut when someone starts throwing such numbers around pretending to know they're definitive, well, I'd be very suspicious.","label":0,"model":"human","source":"reddit","id":2558}
{"text":"All the answers here are correct for a certain historical period. However, it's important to remember that for the majority of the time the Atlantic slave trade was in operation, religious conversion was not a priority. There were a number of reasons for this:\n\n1. In many colonies the average slave lived only 5-10 years, so conversion was deemed not worth the effort. This was especially true in the Caribbean. It was only when the mortality rate dropped and whites began to see established intergenerational slave communities that anyone thought it might be worth trying to make new converts.\n\n2. In colonies with a higher proportion of slaves (e.g. Barbados, where whites numbered less than 10% of the total population) there was a constant fear of slave uprisings. The authorities wanted to restrict Christianity because they feared that some of the Bible's more humane messages might give their slaves some revolutionary ideas.\n\n3. More generally, slave owners throughout the Americas were (kind of) concerned about the theological implications of making their slaves Christians. There are all kinds of warnings in the Bible and in Catholic and Anglican texts about enslaving co-religionists. Slave owners didn't think it would cause much trouble, but they were concerned that if they converted their human chattel there might be a chance that the authorities would then declare the enslavement of Christians unlawful. And that would be a very expensive mistake.\n\nNow, in the British colonies in continental North America, the people who made religious decisions and the people who mad economic decisions were one and the same. So there was no danger of the local plantation owner having his slaves preached at by the church deacon, because there was  a good chance that they were the same man. Religion at the time was about hierarchy, but, contrary to the responses here, the best way to keep a slave population at the bottom of the social hierarchy is to never initiate them into it in the first place.\n\nWhat ended up happening (again, in the 13 colonies - my knowledge of non-British slave systems is patchy) was that in the early-mid 18th century, the first in a series of religious revivals swept across the colonies. Now religion was rendered less hierarchical, and people started to think that anyone could talk to (a) God, and (b) other people about God. So now it's not only the local vicar who can convert heathens, it's any God-fearing Christian. \n\nThe situation as it subsequently developed was not therefore of the slave-owning class's making. Zealous individuals converted slaves of their own initiative and against the express wishes of the colonial elite. Once that damage was done, the slave owners just had to make the best of a bad situation by emphasising (as others here have pointed out) the hierarchical bits of Christianity. But it's wrong to say that the beneficiaries of the slave system actively converted anyone.\n\n**TLDR: Slave owners never really converted anyone because slaves were easier to handle if they weren't Christian. It was only at the tail end of the Atlantic slave era that any widespread conversions started to happen.**\n\nSOURCE: *Inhuman Bondage* by David Brion Davis.","label":0,"model":"human","source":"reddit","id":2559}
{"text":"Part of it is evolutionary; to encourage you as a male to propagate the species. First, there is a lot of stimulation which occurs before, during, and after orgasm all of which are *usually* deemed pleasurable; arousal, touch, build up, and orgasm. All of these reinforce the desirability of the act of copulation (and masturbation). When you finally orgasm, your brain is flooded with chemicals such as endorphines and oxytocin. These chemicals make your brain 'high', reduce feelings of pain or discomfort, and may even promote feelings of attachment (or love, or affection, or whatever). These chemicals are extremely powerful and it makes your brain want to do the act again (usually). [See: _URL_1_ ]\n\nUrination, however, is about normal day-to-day functions; every minute, your bladder is filling up with waste products dissolved in water. Over time, your bladder gets more full, triggering nerve endings which tell you it is about time to empty your bladder; but you can ignore it for a while depending upon circumstances. *If* your bladder is very full and *if* you release it, you *can* feel a pleasurable release which can make you dizzy or weak-kneed. This is an alleviation of discomfort, however, and not the same as 'pleasure' from orgasm. Theoretically, your brain should be desirous to avoid such discomfort in the future.\n\nThat being said, there *may* be a small portion of the male population which might feel better urinating than achieving orgasm: See _URL_0_","label":0,"model":"human","source":"reddit","id":2560}
{"text":"Air is made of little balls called molecules, which are made of smaller balls called atoms.\n\nNow, light can interact with molecules (really the atoms) in 3 primary ways: reflection (like a mirror), refraction (like how light is bent when looking from air into water) and diffusion, aka scattering.\n\nDiffusion is similar to reflection, but instead of reflecting the light in a single direction, it scatters it in every direction. It's the interaction we are most used to, nearly all materials scatter light. It's why a piece of paper is white, no matter what angle you look at it from. The light doesn't need to reflect into your eye, because the paper scatters the light in every direction.\n\nGasses can scatter light too, though! Both oxygen and nitrogen, which make up a lot of the air, are good at scattering blue light. The sun appears yellow (and not white) because some of the blue gets scattered as it passes through the air, and the yellows, reds, oranges, etc all keep travelling straight, until they hit your eye.\n\nThis blue scattering makes the rest of the sky is blue, and this causes the sky to be blue.\n\nNow to answer your question: as the sun rises and sets, there aren't as many reds\/yellows\/oranges as the sun isn't shining as intensly down, it needs to travel through a LOT of air to get to you since it's coming in from an angle. This means that a lot more blue light gets to you and the surrounding area. And when there's snow on the ground, snow reflects every color (since it's white), and since there's not as much of the reds\/yellows\/oranges, it scatters even more blue.\n\n\nEDITS: changed a couple \"diffusions\" into \"scattering\" to be consistent and understandable. Also changed a typo of \"matter\" to \"water\" (in refraction)","label":0,"model":"human","source":"reddit","id":2561}
{"text":"Actually here's the thing about Mona Lisa: \n\n1. Leonardo Da Vinci painted it. He is the foremost Renaissance artist.  Artist's credibility adds to the paintings popularity.\n2. Napoleon Bonaparte hung the painting in his master bedroom in 1800.  This - I think - was the first tipping point of making the painting one of the most popular paintings in the world.\n3. 1804, Mona Lisa is hung in the Louvre - and others can now glimpse at the painting that Napoleon slept with.\n4. But the real tipping point for the paintings popularity only hit in August of 1911 - when Mona Lisa is stolen.  Stolen from heavily secured Louvre which experts said was impossible.  No one knows who stole it or how.  Conspiracy theories abound.  The painting is talked about in every newspaper.\n5. After 2 weeks of much fan fare, Police arrest Guillaume Apollinaire on suspicion of theft. He is the only person they have arrested.  Apollinaire implicates Pablo Picasso.  The rumor of Picasso stealing the Mona Lisa adds in a lot more fuel in making Mona Lisa very very popular.\n6. Picasso is questioned and released.  Guillaume Apollinaire himself is released after 5 days.  Everyone is still clueless as to who stole the painting.  But conspiracy theories abound.\n7. Two years after the theft, the Mona Lisa is finally found when an employee working at Louvre tries to sell it to an art gallery in Florence for $100,000.\n8. When the Mona Lisa is returned to the Louvre, it draws massive crowds.  People visit the Louvre only to see this one painting. \n9. And then it hit the Paris Hilton effect.  Its popularity added to its popularity.  So much so that most people don't know why it is popular in the first place.\n\n^^[source](_URL_1_)","label":0,"model":"human","source":"reddit","id":2562}
{"text":"HISTORY LESSON KIDS!\n\nDuring WWII, the US put in a wage freeze.  Where companies couldn't increase the amount they paid their workers.  Their European allies didn't do that.\n\nIn order to attract better talent employers got creative and started the practice of offering \"benefits\" like healthcare insurance.  This was wildly popular and by the end of the generation it was cemented into the minds of americans that healthcare is provided by employers, not the government.\n\nNEXT\n\nThe US gets involved in the Cold War with the USSR.  The US government puts out a truly **massive** amount of propaganda (some true, a lot exaggerated) about the harms of socialism and communism.\n\nThe outright fear of anything approaching socialism, and the well understood philosophy that employers provide healthcare, not the government, have brought us where we are today.\n\nThis also doesn't take into account the real issue that people think it's not a good financial decision (You said elsewhere people oppose \"free healthcare\" which is a sign of ignorance of the actual costs for healthcare), that it's not the federal governments place to infringe on the states in this way, and that the government is not equipped to provide this service.","label":0,"model":"human","source":"reddit","id":2563}
{"text":"When people hack sites for passwords, they usually get a list of the hashed passwords. That means when you put in your password, the site can check if it's the same password you signed up with but it doesn't know what the password is. It's like a one-way secret message.\n\nThe way hackers figure out passwords is they know common hashing techniques and they guess common passwords using those techniques. Since \"password123\" is a common password, they'll put that in the hash, see what comes out, and match that output to the stolen list of hashed passwords they got. If they can't guess your password to input, then they won't be able to know what it is.\n\nComplex passwords make it harder to guess the hashed passwords once they're stolen. The biggest factors for making a password hard to guess are the total numbers of characters you can use and the length. So forcing you to have three special characters and two capitals and a number doesn't really help, but allowing you to use any character and requiring your password to be long does help. In other words, \"a%6L7\" looks like a more securepassword than\"!XthisismypasswordforthissiteX!\", but the latter is actually more secure since it's longer and can possibly use just as many symbols. Longer passwords are harder to guess because the possible combinations of guesses increase quickly as you add additional characters.","label":0,"model":"human","source":"reddit","id":2564}
{"text":"Because we made them that way!\n\nBananas that grow naturally, in the wild, [actually look like this](_URL_0_). They're pretty tough to eat, too, much harder and more fibrous -- stringy -- than the bananas in the supermarket. People used to cook them, to soften them up -- eating a raw banana was once like eating a raw potato!\n\nWhen farmers started building banana plantations and growing their own, they deliberately went around and picked wild bananas that were softer, sweeter, and longer than average, and only used those to start their farms. When the plants grew up, the bananas were just a little bit nicer than natural bananas. So they did it again: they threw out any plants they had that produced tough or ugly fruit, and instead, planted seeds from their best, longest, sweetest plants. \n\nWhen their new batch of plants had grown up, they were even better! Then they did it *again*. They threw out the worst plants, and planted seeds from the best plants to replace them. Every year they did this, until eventually, the only bananas they were making were very soft, tasty, sweet bananas with small seeds and a really long and thin convenient shape.\n\nA similar thing happened with carrots. Carrots are normally purple, but Dutch people started liking lighter-coloured carrots about 400 years ago, and they eventually got all the way to orange varieties, which are now the most common.","label":0,"model":"human","source":"reddit","id":2565}
{"text":"Hmm, tougher than it seems... Oh, I know a way, but you really have to think like a five-year-old.\n\nSuppose I told you, \"take two steps *forward* three times\". You are now six steps away from the starting position, in the \"forward\" direction. This is 2x3=6.\n\nNow imagine you're at the starting point again, and I'm telling you, \"take two steps *back* three times\". You are now six steps away from start in the \"back\" direction. This is -2x3=-6.\n\nNow you're at the starting point again and I'm telling you, \"*turn around* and make two steps *forward* three times\". You are now facing the opposite way, so you end up the same six steps away in the \"back\" direction. This is 2x(-3)=-6.\n\nFinally, you're at the starting point and I'm telling you: \"*turn around* and make two steps *back* three times.\" See? You're moving \"backwards\" *while* facing \"backwards\", so you end up six steps away in the *forward* direction. And this is -2x(-3)=6.\n\nThe nice thing about this explanation is that you can actually try it out.\n\n*EDIT: fixed missing minus sign in third example, thanks for noticing*","label":0,"model":"human","source":"reddit","id":2566}
{"text":"From what I read months ago about this issue, the rule has a loophole that says that it is acceptable as long as an adapter to micro-USB is included in the box. The speculation was that Apple would simply include the adapter with European iPhones.\n\nSince then, more interesting developments have occurred. The USB group is promoting a new connecter standard called \"USB C\" that, like Apple's proprietary Lightning cable, is reversible. *Further*, while Lightning cable is not thought to be able to support full USB 3 transfer speeds (5Gbps) to to a deficiency of pins, the USB C connector standard supports USB 3.1 (10Gbps) *and* a new USB-based standard for carrying power that allowed USB C to carry up to *100 watts*. To put 100 watts in perspective, Apple currently produces 3 different power supply adapters for their laptops: 45 watt for MacBook Airs, 60 watt for 13\" Macbook Pros, and 85 watt for 15\" MacBook Pros. That means that 100 watts is easily enough to power and charge Apple's most powerful laptops.\n\n**Most interestingly**, rumors abound about a new ultra thin and portable Mac laptop that is so thin that it eschews all connectors besides a single headphone port and a single USB C port. As in, you will be using that USB C plug as the primary means of charging the laptop. So if you need to charge your laptop and plug in to USB devices and external displays simultaneously, there will probably be port(s) on the new power supply to allow that.\n\nWith Apple promoting USB C as a major new connector, my bet is that they eventually replace their Lightening ports with USB C, which will be compatible with Euro regulations.\n\n**EDIT:** I dug up some articles\n\n[Overview and history of the 12\" MacBook rumor](_URL_1_)\n\n[Article about the USB C port on the rumored 12\" MacBook](_URL_0_)\n\n[Technical details of the 100 watt power supply and DisplayPort over USB C specifications](_URL_2_)\n\nSo it looks like you are able to get up to up to 10Gbps data transfer, 4K video feed, HD surround sound audio feed, and 100 watts of power simultaneously through 1 USB type C cord if I am understanding the technical details.\n\nTo extrapolate the way Apple has productized in the past, it looks like Apple will be able to make cheap passive adapters to convert the USB C port into: a standard USB connector, a gigabit ethernet connector, a thunderbolt\/DisplayPort connector, HDMI, DVI, and VGA connectors, maybe firewire if they feel like continuing to support it.\n\nThere has got to be a way to plug in displays and peripherals when you're charging the thing. Maybe the standard power supply will include at least a pass through USB C connector or 2 that can be adapted to whatever ports you need. Maybe more ports than that.","label":0,"model":"human","source":"reddit","id":2567}
{"text":"Many ancients (Greeks, Romans, Chinese) chose twelve hours from sunrise to sunset.  Nobody knows why but twelve is a more convenient number to divide than ten (12 is divisible by 2, 3, 4, and 6).  Later going to 24 equal hours per day is a fairly obvious extension.\n\nThe ancient Sumerians started a tradition of counting by 60, much as we now count by 10.  Probably this is because, again, 60 divides evenly by many numbers.  This tradition led to dividing hours in 60 minutes and minutes into 60 seconds.\n\nThe French tried to introduce decimal time with 10 hours, 100 minutes and 100 seconds in 1794 but it didn't catch on.  They abandoned it even quicker than their new calendar.  There hasn't been a serious attempt since.\n\nPeople are just too used to the existing system and the advantages of decimal time don't outweigh the cost of changing.  The metric system uses the second as its unit of time and changing from the 86400 seconds per day we have now to a decimal 100000 seconds per day would be problematic.\n\nApart from costs, the change would be dangerous.  We don't change seconds for much the same reason that the foot is still the standard unit of altitude for aircraft.  Any change to use metres would inevitably cause crashes as people mixed up the units.","label":0,"model":"human","source":"reddit","id":2568}
{"text":"So I worked in POS design for a major retailer for many years.\n\nThere are a few answers. As others have said, they build them out for peak load. Putting in a bunch of cash registers that get very little use, while not cheap, isn't that big of a number on their bottom line.\n\nAdditionally, you have excess capacity for when systems go down. Your store staff isn't going to be able to do more than swap out a few basic plug in components, and in many companies, they won't even go as far as that. You will need to wait a day or two for a tech to show up. Hell, even if your staff could do something like swap in a new scanner, you are still knocking that line out of service for the amount of time it takes someone to go find a spare, unbox it, set it all up, etc.\n\nAlso having a bunch of lanes, even though they aren't in use at the MOMENT can simplify bringing added cashiers in or out quickly. You can have someone hop onto a new register and get going without disrupting the existing line for the minute or two it takes to swap someone out.\n\nAlso, in a perfect world, stores are rotating their use among the registers. This helps reduce the wear and tear on them making failures like a swipe reader or pin pad wearing out less likely, and discovering that that register you planned on using for the biggest day of the year is dead.\n\nEdit: Quick additional item i thought at. You also have product at the store in those lanes, and want to try and maximize what sells, what the variety is between lines, and collect metrics on it. Did the pogs not sell well because you only put them on every 3rd checkout lane, and the store in question for whatever reason never uses lanes 3 6 and 9?  Lane 2 is heavy on magazines that are all at the end of their run, lets send some extra traffic there and try and move them. Hell, even \"the candy they stocked on lane 5 is almost expired, lets work that one extra hard today\".\n\nWe played around with all kinds of optimization stuff but found it was pretty much ignored at the local level for various reasons, some valid, so we gave up on it, but I'd be surprised if someone on the scale of a walmart or target wasn't using automation for at least some form of lane selection.","label":0,"model":"human","source":"reddit","id":2569}
{"text":"They're a lot like Yahoo, they produce web content for people to enjoy.  They don't really DO much of anything under their own name anymore (other than maintain a disappearing, antique dial-up system that I can't imagine is making them much money anymore) but they own a lot of websites you probably use.  Engadget, TUAW (RIP dudes), etc... \n\nJust like Yahoo owns Flickr, for example.  Or Google owns pretty much everything else. It's all the same basic business model, start with the search site, and purchase\/design supporting products to go along with it. \n\nThe reason you didn't know that is because AOL continues to do everything both badly AND much later than everyone else.  They were years late to the web content game because they...I really don't know what they thought was going to happen. I guess someone was banking that broadband internet would just be a fad.  So they kept AOL the service provider going, like some kind of horribly slow, buggy ISP version of Weekend at Bernies. Finally, years later, they snapped out of it and went \"Wait, what? Where is everyone?\" and got with the program.\n\nThey only run 1.3% of all web searches, and they've tried to make inferior versions of pretty much every other web service out there.  Well, I say they try to \"make\" crappier versions of things, whereas what they really did was WILDLY succeed at BUYING crappier versions of things everyone else had perfected years ago.  \n\nRemember the smash-hit social media website Bebo?  Of course you don't, no one outside of Ireland (where they briefly actually did pretty good) does.  That didn't stop AOL from buying them for 850 million in 2008, only to sheepishly sell it to a hedge fund operator in 2010 for a whopping 10 million dollars.  That doesn't include the cost of flying in the world's greatest sad-trombone-sound-making-guy to really call attention to how idiotic that move was.  For those of you playing at home, that is a *staggering* ROI of -98.8%.  It directly cost the CEO at the time his job, apparently.\n\nLest you think the hedge fund managers scored some kind of killer deal, Bebo then went bankrupt in 2013.  I know, I know, how the heck could that have happened, right?! Nothing makes sense for me anymore either, after I learned that.  Calculating the overall ROI various Bebo owners have combined for now breaks math as we know it, so later in 2013 someone picked up Bebo's former shell of itself for a song and dance.  Who was this?  THE FOUNDERS OF THE SITE WHO SOLD IT FOR 850 MILLION TO AOL FIVE YEARS PRIOR.  They just restarted it in 2015 and with AOL's luck it will cure cancer.\n\nSo currently, in 2015, AOL spends its days handling a tiny percentage of the world's search traffic, paying some guy named Rusty 10 bucks an hour to watch over the cobweb infested shack that houses AOL's dial up network, and watching \"You've Got Mail\" while it cries cheap tears and drinks Popov, thinking of the good old days.\n\n**edit**: re-reading this just now, I feel like I may have momentarily veered off on a slight tangent.  I'm not re-writing it because I used to work for AOL and OH MY GOD they are the just the worst, you have no idea.  Hopefully it still answers your question. \n\n**edit 2**: Whoever gilded this post, seriously, that was really really nice of you and it's really appreciated. \n\nI also wanted to make this link a little more visible than it would be if I put it in the comments:\n\n_URL_0_\n\nI see a LOT of people mentioning friends or relatives are keeping AOL service just for the e-mail address.  AOL never did a good job at advertising it (go figure!) but there is definitely an easy, straightforward way to cancel the service and keep your @aol.com e-mail.  You can even get the E-mail back if you cancelled the service previously but miss having the old address! (Click the section that says \"How do I reactivate a cancelled AOL Account? (web users)\"). Don't worry, you can reactivate it as a free account, you won't have to pay!\n\nDon't get me wrong, some people just want to stick with AOL, and more power to them.  But if you know anyone stuck paying money just to keep an e-mail address they love, set them free!","label":0,"model":"human","source":"reddit","id":2570}
{"text":"As a former DJ for a party scene, I can tell you that grouping all performers under the moniker of \"DJ\" is something of a misnomer.  I DJ weddings for money nowadays and I call myself a DJ for that.  All I do now I just move from song to song because the crowd doesn't need all the fancy mixing.\n But when I was younger and had the time, I would spend hours working on hearing a track and finding the perfect time to bring in a new track, learning its intricacies.  It wasn't easy to do if it is going to be done properly.  I always considered a quality DJ at an EDM event to be a bit of a storyteller.  They had to consider the role they played first (an opener, the guy before the headliner, headliner, overnight dweller or the sunrise guy), and then factor that into an equation that matched the crowd and the vibe of the event.  A truly masterful performance would feed off the crowd and build over the set, increasing intensity at times and rolling it back at others, never knowing when one track ended and another began.  \n\nMy favorite set I ever experienced was from a DJ named Marco Carola who was playing at a party right next to railroad tracks.  He saw a train coming and managed to work the sound of the train and its horn into the music he was playing.  It was the perfect blending of sound and environment and the crowd knew it.","label":0,"model":"human","source":"reddit","id":2571}
{"text":"A webpage is made up of a lot of different files. The more complicated the page, the more files there are. This also depends upon the way that the page is designed and coded.\n\nAs such, a page typically will not render (display the final page) until ALL files have been received. \n\nImagine loading a page from the Wall Street Journal that has an imbedded video. The text and style commands (CSS) will go through almost instantly even on a slow connection. However the video may take several seconds. Depending on how the page is coded, it may wait for the video to load (this has nothing to do with buffering and actually playing the video) before rendering the page.\n\nWhen you close it, the browser stops waiting for whatever file still needs to load and just renders what it already has.\n\nThe biggest offenders of this are typically flash animations (adverts or videos) and large image files. Adverts can be the cause for this, which is why some people will say that they experience faster connections when using ad-blocking software.","label":0,"model":"human","source":"reddit","id":2572}
{"text":"Cop here. Short answer is, it depends.\n\nIf it's a serious crime and the drugs and money were evidence, the drugs and money will be retained in evidence for 5-7 years for potential appeals, or longer, depending on the wishes of the prosecution.\n\nIf the crime isn't so serious (as in 2-3 years or less) and the money wasn't proven to be derived from, or in furtherance to, illicit activity, it will most likely be returned to the owner. (This follows the same asset forfeiture laws as vehicles, houses, boats, clothing, etc. so a separate hearing is usually initiated to test\/verify if the money was illicit.) \n\nEach jurisdiction has their own set of rules, regulations, and laws, but generally the drugs and money will either be disposed of (by at least two witnesses) or they will be kept. (Drugs can be used for training or future investigative operations. Money goes back to the parent government, NOT the seizing agency.) There is usually some forms that need to be filled out and approved by the chief of the law enforcement organization and the prosecutor's office and\/or judge. \n\nIf the drugs are kept, they are stored in the property\/evidence room and only checked out for operations\/training - they're weighed when they leave and weighed again when they come back. If there's any missing and it wasn't used in an approved training or operation, it would be your ass. The drugs could be used for K-9 training or controlled burns (how do you think that cops and confidential informants know what marijuana smells like - they, like the dogs, have to have it signed off in their personnel\/training records.) We will also use it to do field test training for those that don't get a lot of exposure to it (probation\/parole officers, investigative assistants, confidential informants etc.) Operations is what you suspect - reverse buy\/busts or buy\/walks. \n\nAs I said, money that has been seized (and approved for forfeiture during a hearing) funnels back to the parent government and they disburse it as they see fit. At the Federal level, that is the Department of Justice.","label":0,"model":"human","source":"reddit","id":2573}
{"text":"Let's analyze this more critically.\n\nFirst, which professions are critical? We can obviously ignore middle management, and the corner store clerk, but Police, fire, emt's are a given so we'd have to hope that no one decided to steal anything, get in a fight, or set anything on fire.\n\nNext are hospitals. I'm sure that most patients could be stabilized for 24 hours, but there's considerable chance something could become unstable and some people would probably die.\n\nNow lets consider the prison system. It could probably be set up so none of the prisoners would starve, but you give an entire prison 24 hours unsupervised, and I guarantee someone is going to get out of their cell and start a riot.\n\nI see a lot of people mentioning power stations. Most power plants are self sufficient, so probably nothing. They are also designed to fail safe, so you might get a couple of shutdowns, but not much more. Certainly not a catastrophic across the country blackout (again assuming no one purposefully tried to cause one when no one was looking). But what about nuclear? It's true, the older plants could go critical, but again there are many many fail safes and barring something like an earthquake they wont reach critical mass. Also newer nuclear plants, like the ones they put in subs since power plants haven't been built since the 50s, are designed with a negative coefficient of something (forgot the word at the moment), but basically if it starts getting out of control, the reaction actually starts impairing itself.\n\nTL;DR: If you assume no crime, and preparations work perfectly so nothing fails unexpectedly, not a whole lot. Some people in critical condition might die, as well as anyone that had a major accident, but people die all the time. In reality, the simple absence of a police force would probably cause rioting and it's downhill from there, as it has when police forces have gone on strike in the past.\n\nGreat question.\n\nEdit: Lots of people in critical condition would die and thanks for the gold.","label":0,"model":"human","source":"reddit","id":2574}
{"text":"In and around your joints is a very special fluid called \"synovial fluid.\" This fluid is like oil or lubricant for the joints. This fluid kind of feels like egg yolk and it is great at making sure your joints don't grind together and hurt. This fluid is made by a kind of skin or membrane inside your joints called a \"synovial membrane.\" Now this fluid is pretty cool. It not only oils up the joints but it also protects them from damage. The fluid becomes kind of tough when a force is put on it and immediately goes back to normal when the force is gone (**edit** for those of you wondering this is called a *shear thickening* fluid or a *dilatant*). Synovial fluid also contains quite a few gases like the oxygen you breath in and the carbon dioxide, which you breathe out. The fluid also contains nutrients and so the fluid helps put good stuff into the cells it touches (and cartilage) and takes the waste out, like carbon dioxide. \n\nWhen you crack your knuckles you are making a small separation between your joints. This separation increases the volume of the space that your synovial fluid likes to sit in (since you are pushing your bones up and down more than they usually go). Now synovial fluid likes to fill up the entire space it sits in, by quickly making the volume or space larger it creates what we call a \"negative pressure\" like a vacuum almost around around the fluid. When this happens there isn't enough fluid to fill the space so it gets filled by the gases in the fluid. Usually the gases stay in the fluid (they stay in solution like when you dissolve salt in water) but the gas only stays in the fluid (at body temperature) under a certain pressure, when you decrease the pressure the gas gets to escape. This forms a bunch of bubbles that quickly pop and make the cracking sound you hear. \n\nSo far as science knows cracking knuckles don't damage your fingers in any lasting way. The fluid does a really good job of protecting and lubricating your joints and as long as those synovial membranes are healthy you will keep  making more.","label":0,"model":"human","source":"reddit","id":2575}
{"text":"Short answer is that the officer in question wasn't charged with a crime.   \n\nIgnorance of the law is not a defense to criminal charges, which is what people normally mean when they say ignorance of the law is no excuse (and that is, more or less, true).  The Heien ruling wasn't about any alleged criminal behavior on the part of the officer, it was about whether a stop was constitutional or not.\n\nThe standard for a constitutional stop is not the same as the standard for criminal behavior.  An officer doesn't need to know you've broken the law to stop you, an officer needs to have a reasonable belief that you've broken the law to stop you.  In the Heien case, the vague language of the NC law made the officer's mistaken understanding reasonable.  \n\nThis is, by the way, in keeping with years of Supreme Court precedent.  For a police officer to violate your rights they need to know that they are doing so (or at least a reasonable officer would know that their actions violated your rights).  \n\nConstitutional violations are not the same as criminal acts and are judged according to a different standard.","label":0,"model":"human","source":"reddit","id":2576}
{"text":"There are some things that should be mentioned in order to get to the main cause of it.\n\nThe first is that Hitler didn't use to give direct orders. He preferred to give vague suggestions, let his underlings struggle between themselves to fulfill (what they thought were) the F\u00fchrer's desires, and in the end support those whose ideas he liked. So if Hitler wanted to, say, go out for his birthday, he wouldn't say \"Hans, go and make reservations in the bar around the corner for the next weekend, and don't forget the strippers\". He would gather his underlings, tell them that his desire was that his birthday would have alcohol and women, and then let them try different things to please him until someone did something he liked. This will be important later.\n\nNow, Jews and other minorities were harassed and prosecuted from the beginning of the Third Reich, but not on a systematized basis. Some laws against them were passed, some were mistreated, some were killed, but there was not a \"kill them all\" order from above.  In fact, sometimes the prosecution had to be stopped due to internal protests, like when German women married to Jewish men protested against their incarceration.\n\nOne of the key points in the history of the Holocaust was the [Wannsee Conference](_URL_0_) in 1942. Up until this time the killing of Jews was organized at most on a regional basis, and every commander or military governor did it as they pleased.  The Conference was a gathering of several high ranking German officers and politicians in order to find a \"final solution to the Jewish problem\". It was at this meeting that the Holocaust as we know it began to take place: the killing of Jews and minorities went from a region-to-region basis to something bigger: a systematic plan to \"clean\" occupied Europe.\n\nSo, we have the Holocaust in motion now, and it was put in motion under secrecy. When the tide of war went against Germany and defeat was imminent lots of documents were destroyed, whether intentionally or in the midst of the mayhem of war. Among those documents were, no doubt, a lot of documents related to the Holocaust.\n\nNow, remember what I said about Hitler before, how he didn't use to give direct orders? Well, it also means that there probably never was a direct order of \"Hans, mind killing all the Jews for me?\". There are testimonies from surviving high ranking members of the Reich that he gave a sort indirect request to get rid of the Jews. So, we have no single \"Kill the Jews\" order signed by Hitler. The Wannsee Conference was also kept secret, and only a single copy of the minutes survived. A lot of relevant documents were also lost and a lot of relevant actors died during the ending stages of the war.\n\nNow, how does this relate to Holocaust denial? Even though the historical record is incomplete and will likely never be 100% complete, **the evidence that survived the war still overwhelmingly supports the existence of the Holocaust**. Deniers have political and\/or ideological reasons to claim that the Holocaust never happened, so they point at these holes in the documentation and in the testimonies, and then they claim that therefore the Holocaust is either a wild exaggeration or a straight fabrication. They need the Holocaust to be fake in order to support their ideologies, so they willingly ignore the huge historical evidence against them to focus on small inconsistencies.\n\n**tl;dr: Holocaust deniers have an axe to grind, and because the Holocaust gets in the way they need to believe that metric fucktons of evidence are somehow undermined by small inconsistencies in an unavoidably complex record.**\n\nEDIT: fixed typos, grammar.","label":0,"model":"human","source":"reddit","id":2577}
{"text":"My father worked in various divisions of Tropicana for nearly 40 years, going from factory work and into corporate.  He has more knowledge about the industry than nearly anyone in the world, though he retired several years ago.\n\nHere's what he has to say:\n\nA standard box of oranges (as bought from a grower in Florida) weighs 90 lbs. That box when extracted by a processor will generate 5.5 to 6.0 gallons of orange juice. A typical box of oranges will supply 180 to 220 oranges ... depending on the maturity and the variety of orange. That means that it takes about 34.8 oranges to produce a gallon of OJ.\n\nRe cost .... the economics of \"table fruit\" that you buy to eat is different than the economics of field run processed fruit. Table run fruit is sorted for appearance, boxed, and sold at a premium. Some varieties of table fruit are also processed but mostly used as table fruit and sell at a significant premium to processed fruit. Valencia, Parson Brown, \"Pineapple\" oranges and Hamlins are the main varieties of oranges used in Florida to make OJ in processing plants. Extractor do not \"grind up the fruit\". There are 2 types of extractors .... one \"reems\" the fruit like you do at home and the objective of the reem is to get all of the juice, pulp and inside of the orange without impacting the white interior of the fruit (albedo) which is very bitter. The peels and waste material are then sent to a feed mill where they are pressed to reduce liquid content and dried to make cattle feed. The pressed liquid is run through an evaporator to turn it into molasses and added back to the cattle feed to sweeten it up.\n\nA comment in the string says \"don't let them tell you they don't add water because they do\". They don't add water to not from  concentrate Orange Juice .... it is against the law and no reputable brand would do this.\nThe cost of the oranges is so different because when you buy table fruit it is at most a bag .... processors sign contracts to buy whole groves of oranges .... sometimes buying millions of 90 lb boxes at a time. If you look in the commodity exchange ... you will see \"Orange Juice Concentrate Futures\". This is the price a processor is expecting to pay for a standard pound solid (about one gallon of single strength orange juice) in the future. That cost typically runs from $1.25 to $2.00 ..... for about 35 processing oranges. (See math at the top of this note)\n\nNuf said ...","label":0,"model":"human","source":"reddit","id":2578}
{"text":"One reason is because a nice easy platform to run on for elections is to \"get tough on crime.\" The form this comes in is more things being illegal, especially drugs, mandatory sentencing, so people don't \"get off easy\" and things like the Three Strikes Law. The concept as far as I can tell on the Three Strikes Law is that someone who commits a third felony is a \"career criminal\" aka a broken worthless member of society who only causes problems and should be locked away for the rest of their lives for the safety of others.\n\n\"Getting tough on crime\" also causes the problem of trying to pull back those laws. If a politician were wanting to make sentencing lighter, his\/her political opponent could say they were \"soft on crime\" and would put voters in danger since a lighter sentence means less deterrence. What they fail to say is that harsh sentences hardly work as a deterrence. To many people, the risk vs reward, the reward always wins out, either not considering the possible sentencing when buying weed, or thinking they're smarter than those other burglars who just weren't good enough to not get caught.\n\nFinally, as others have said, there are private prisons. If you believe that politicians are immoral enough to purposely create laws intended to imprison people to increase the paychecks of private prison owners and get a \"donation\" as a reward, making more things illegal to create more prisoners would make sense. While I believe this is the case for several politicians, I think some are just ignorant about the facts on deterrence and honestly believe having harsher sentences will make people stop breaking the law.\n\nBut that is just my take on what I learned in some Criminal Justice classes I took and my own observations, so take it as you will.","label":0,"model":"human","source":"reddit","id":2579}
{"text":"I think there is a lot of weak answers here.\n\nI know I'm 9 hours late, but in the event you read this as an actuary I can tell you that the reason this is the case is because race is not as easy to define as gender.\n\nMale and female is a constant easily observable trait. There are people who are mixed race, or not even totally aware of what their racial composition may be in the form of adoptees etc.\n\nI can assure you if genetics worked differently and everyone came out of their mother explicitly blue, yellow, orange or whatever and statistical data backed up that orange people drove the worst they would be charged more for insurance.\n\nAs an anecdotal example, consider South Americans. There are South Americans who may consider themselves Native American, Black, White, or a mix of them all. I work with an afro-Honduran who is somewhat light skinned who prefers to label himself as Black but would probably be labeled as \"latino\/hispanic\" by most of the public not familiar with afro-Honduran culture.\n\nThe ELI5 answer is that gender is black and white (disregarding trans people I suppose - but for insurance purposes 99% of the time they will be assessed based on their birth gender), where race is most certainly not.","label":0,"model":"human","source":"reddit","id":2580}
{"text":"It takes a huge amount of mechanical pressure to kill bacteria. There is a food preservation process called [Pascalization](_URL_1_) that uses pressure to kill molds and spores. It is used as an alternative to [Pasteurization](_URL_0_), which uses heat. The pressures involved a pretty huge -- 50,000 PSI. That's giant hydraulic press kind pressure. \n\nPascalization is not as effective as Pasteurization. I've only ever seen it used for very acidic products, like grapefruit juice. Strong acidity excludes many of the most dangerous food-borne pathogens, and so pascalization is more to prevent spoilage than to protect humans. It does not render food completely sterile, but it does extend shelf life.\n\nWhen you step on something as small as a bacterial cell, you are also stepping on the goop the bacteria is living in (microbiologists call it a \"matrix,\" but I think \"goop\" is more illustrative). There is pretty much always goop, or the bacteria wouldn't be able to survive. Some bacteria do swim some of the time, but most of the time, they rely on diffusion to move compounds around their immediate vicinity. If you shrank yourself down to this scale, water would be like damp concrete or hard packed earth that is being vibrated (not a perfect analogy, but close). You can push things through it, but it takes a lot of work. Swimming is very, very hard work, and you'd want to avoid it if you can. Water with other stuff in it -- i.e., goop -- would be even stiffer. The cells are buried in that. Even if the layer of goop is completely invisible, from a cell's point of view, it could still be like sitting at the bottom of a lake.\n\nWhen you step on the goop, what will happen is the goop will squirt around the hard structures to which it is attached. This probably won't bother any cells that aren't already ruptured or breached. They have to be tough to move through the goop, and the kind of violent disturbance you would cause by stepping on the goop is probably not very different from other disturbances that the bacteria rely on for long-distance travel.\n\nAs your foot comes down on the floor, some of the goop will get trapped in small cavities, and in those places the local pressure could get quite high. Probably not 50,000 PSI, though, and so I doubt many cells would be physically harmed. [Unless you are the girl in the Paul Simon song, maybe.](_URL_2_)\n\nWhat will happen, though, is that some bacteria will be separated from the nutrient environment they were growing in, and that probably will kill them.","label":0,"model":"human","source":"reddit","id":2581}
{"text":"1s and 0s are typically processed in *chunks*, not individually.\n\nSay you've got a system that works like this: 0000 represents a black pixel, and 0001 represents a white pixel. When the monitor reads 0000, it generates a black pixel, and when it reads 0001, it generates a white pixel in the pixel immediately following that.\n\nNow throw in a few extra commands. Maybe 0100 means \"go to the next line.\" Now you could have a series of chunks of data that looks like this:\n\n    0000 0000 0001 0001 0000 0000 0001 0001 0000 0000 0100\n    0000 0000 0001 0001 0000 0000 0001 0001 0000 0000 0100\n    0001 0001 0000 0000 0001 0001 0000 0000 0001 0001 0100\n    0001 0001 0000 0000 0001 0001 0000 0000 0001 0001 0100\n    0000 0000 0001 0001 0000 0000 0001 0001 0000 0000 0100\n    0000 0000 0001 0001 0000 0000 0001 0001 0000 0000 0100\n    0001 0001 0000 0000 0001 0001 0000 0000 0001 0001 0100\n    0001 0001 0000 0000 0001 0001 0000 0000 0001 0001 0100\n    0000 0000 0001 0001 0000 0000 0001 0001 0000 0000 0100\n    0000 0000 0001 0001 0000 0000 0001 0001 0000 0000 0100\n    0001 0001 0000 0000 0001 0001 0000 0000 0001 0001 0100\n    0001 0001 0000 0000 0001 0001 0000 0000 0001 0001 0100\n    0000 0000 0001 0001 0000 0000 0001 0001 0000 0000 0100\n    0000 0000 0001 0001 0000 0000 0001 0001 0000 0000 0100\n    0001 0001 0000 0000 0001 0001 0000 0000 0001 0001 0100\n    0001 0001 0000 0000 0001 0001 0000 0000 0001 0001 0100\n\nTo us, this is just a bunch of 1s and 0s, but the computer will read this and display a checkerboard pattern, like this:\n\n    xx  xx  xx  xx\n      xx  xx  xx\n    xx  xx  xx  xx\n      xx  xx  xx\n\netc.\n\nNow imagine that you add in some extra colors. Maybe 0000 is black, 0001 is white, 0010 is red, 0011 is blue, 0100 is green, 0101 is orange, 0110 is purple, 0111 is yellow, 1000 is pink, 1001 is brown, 1011 is gray, 1100 is beige, etc.\n\nSuddenly you've got the possibility to make a *lot* of different shapes and colors in that grid of 1s and 0s above.\n\nNow, modern games have a *lot* of 1s and 0s in them. Say you've got a game that uses 32-bit color.\n\nIn the example above, I was using *4-bit* color. Each \"bit\" is either a one or a zero. The maximum number of colors I could handle at 4 bits represents all of the different possible combinations of 1s and 0s that can be represented by one bit:\n\nEach bit can be one of two things, and you have four of them, so\n\n2 x 2 x 2 x 2 = 2^4, or 16 possible colors.\n\nWhat *32*-bit color means is:\n\n2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2\n\n= 2^32 = 4294967296 possible colors.\n\nThe incredibly high amount of colors allows for more subtle and realistic shading. A lot of math behind the scenes tells the program how to use these colors, and how they should change in response to everything else.\n\nSo, long story short, video games are *extremely* complex, and 1s and 0s add up!\n\n**edit:** got a little trigger happy copying and pasting twos! I think I counted right this time","label":0,"model":"human","source":"reddit","id":2582}
{"text":"Practically none. Iraq was invaded because the neocons thought they could remake the Middle East into a region of free-market democracies by toppling dictators. The easiest one to topple was Hussein, but the only clear excuse they could all agree on to do so was the WMDs. Everyone thought he had them, even if there was no proof, so they \"stovepiped\" intelligence - pushing raw, dubious intel up the chain without critical analysis in order to bolster their weak argument - and supported intelligence they probably knew was phony. They believed they would find WMDs, we would be welcomed as liberators, we would reshape the Middle East in our favor (which has too many benefits to list, control of oil being only one), and we would remind the rest of the world that our military is still number one and we can use it where and how we wish to enforce our will (the subtext there is, after the end of the Cold War, the neocons were worried the military-industrial complex would lose its relevance and, therefore, its bottomless well of money). \n\nTL;DR: the money to be made from Iraqi oil is nowhere near as important as the money to be made from the US military-industrial-security complex.\n\nEdit: forgot about Curveball.","label":0,"model":"human","source":"reddit","id":2583}
{"text":"In Japanese, you either type in the sound using roman characters i.e. A, B, C, etc. or you use a special type of keyboard where you push \u304b, for example, and it will show the other 4 characters that begin with the 'k' sound. The phone will then see what sounds you're using and guess which kanji (the characters from Chinese) you want to use. You can scroll through them.\n\nIn Chinese, you type in pinyin which is the phonetic sounds Chinese uses. The phone will then guess which characters you want and you will choose. Now, phones will let you 'cheat' by only typing the first letters of the pinyin (sounds). For example, instead of typing 'beijing' to get \u5317\u4eac, you can just type 'bj' and \u5317\u4eac, among other things, will appear. \n\nBoth systems are pretty easy to use. Chinese doesn't have phonetic characters like Japanese does, so you have to use pinyin. Japanese doesn't require using roman characters but many people (especially foreigners) do. \n\nAlso, there are not millions of characters. I think I remember reading that Japanese has somewhere around 60,000 most of which are unrecognizable by almost everyone. Chinese probably has the same or double if you count both their traditional and simplified forms. In any case, they're far short of a million. \n\nEdit: Apparently Taiwan uses a special phonetic system called Zhuyin or Bopomofo, but I've never seen it anywhere in mainland China. I'd be willing to bet that nobody I know here in China would recognize it. Sorry to have inflamed so many delicate, Taiwanese sensitivities. If we're really going to get picky, we're talking about \u666e\u901a\u8bdd, so I'm going to base my information around what \u5317\u4eac does. What Beijing does is not use Zhuyin.","label":0,"model":"human","source":"reddit","id":2584}
{"text":"OP is probably aware that printer ink is wildly inflated, but the important part of the question is about the apparently extreme inexpensiveness of printing without that inflation markup.\n\nBasically, you are paying for flexibility. The colorful commercial printing you see everywhere is usually achieved through something called [offset printing](_URL_0_). This method makes it very easy to print thousands of something, while sacrificing the flexibility of printing just one copy. It requires a relatively labor intensive process to get started printing once the design is complete, but then you get to benefit from huge economies of scale.\n\nDesktop printing, on the other hand, requires no initial labor investment after the design is complete. The machine (typically [ink jet](_URL_1_)) can reproduce any image imaginable by mixing very small quantities and very precise locations. This requires that the ink be free from impurities and come out of the nozzle at a extremely regular rate of flow. This contributes to some of the cost (mostly in upfront research and development) of desktop printer ink, but as others have pointed out, a lot of that price is \"because they can.\"\n\nThe commercial printing industry *does* use ink jets, too, in very special circumstances. Those massive vinyl banners you sometimes see in malls or sporting events are sometimes done on *massive* bubble jet printers. You can get vibrant, photograph-quality prints at obscene dimensions this way.\n\nSource: I took a tour of Fujifilm Imagetec's factory in Japan where they use industrial-scale bubble jets a few years ago.","label":0,"model":"human","source":"reddit","id":2585}
{"text":"A combination of the following (one, the other, or both, to varying degrees of limitation and\/or price):\n\n* You own a website, online service, or mobile app; but Verizon slows it so it takes a long time to load *unless* you pay them.\n* You subscribe to the internet as an end user. Facebook and Google load slowly, Netflix and YouTube run at standard def only, and LiveLeak and BitTorrent are totally blocked. Your ping time in games is abysmal UNLESS you pay extra for a \"Premium Subscriber\" package.\n\nISPs then introduce their own content services, which will of course run much faster than anyone else's over their own connections. They take all the ad revenue, and you are stuck with their service which they can do whatever they want with (including charge for it - but charge less than what they charge to use a competitor's). The original content providers slowly die off, as people don't want to pay for the \"Netflix\" package since they already pay for Netflix, or for the YouTube package since YouTube is supposed to be free.\n\nThey make all the money and control all the information flow. Little startup companies who can't afford to pay for all their user's bandwidth? Gone. With the competition eliminated, these companies have no reason to really innovate, and the internet becomes a crappy text-based version of cable-TV: super-high subscription fees, crap content, and no freedom to innovate.\n\nIt's really a shit excuse to sap as much money from people as they can, but if it happens it will cost much, much more.\n\nEDIT: One silver lining is that we may see more startup ISPs who DON'T pull this kind of crap - but that will be very difficult because the big boys provide all the \"information superhighway\" wires that connect the internet across long ranges, and they'll just charge the small ISPs to lease bandwidth from the big connection lines.","label":0,"model":"human","source":"reddit","id":2586}
{"text":"EDIT: I'm putting this here because I think people are not getting to the end of my post before replying. I do appreciate all the reading suggestions, but I got the point already :)\n\n--------------------\n\nStarting? This trend is way into its course. 1984 and Brave New World are probably the first dystopias (some lit major correct me if I'm wrong), and these are from the 40s\/50s.\n\nSci Fi (and when I say Sci Fi I mean actual Sci Fi, not Fantasy) during the 50s was all about how the future would be all good, but then some authors started writing about the dangers of the future, not only its promise. See things like the Man in the High Castle, by Philip K. Dick, from 1963.\n\nWhen the 80s rolled Cyberpunk became all the rage, and there are few things more dystopian than Neuromancer by William Gibson. It's been downhill (in feel, not in quality) since then, at least.\n\nMovies take a longer and don't go as deep, because typical American audiences go to the movies to escape problems, not to be exposed to them. But there are good examples even there, see Blade Runner (1982).\n\nThe specific end of the world theme, with its sub genres of nuclear wasteland, zombie apocalypse, natural disasters, etc, I believe comes from the general sense of fear from the 50s. For the first time, people were contemplating a real danger of the the end of civilization. Some escaped into fantasy (Lord of the Rings) or future-as-salvation (Star Trek, Star Wars) stuff, others started doing things like The Day The Earth Stood Still and I Am Legend (the 1954 book  and the early film adaptations, not the Wil Smith stuff).\n\nAnyway, OP, you may have just noticed it, but this trend has been going for at least 50 years. Probably more. \n\nEDIT: As others have pointed out, the Book of Revelation has anything I quoted beaten by 2000 years. But it goes even further back than that, every religion has some notion of the end of the world, and there are many religions older that Christianity. Although I'm sure some people will object to classifying this as fiction :)\n\nEDIT 2: Here's a thought that I had after reading some of the replies: we are probably as obsessed with the end of the world as we are with the beginning. We wonder where we came from and where we are going. It is not a big stretch to wonder the same thing about the world outside of us. We have been obsessed with these things, beginnings and ends, since we realized that we ourselves have a beginning, and will surely have an end. The first story about the end of the world was probably dreamed up by the first ape that got to wonder about her own mortality. For all I know, \"popular culture started to be about the ending of humanity\" before there actually was a humanity.","label":0,"model":"human","source":"reddit","id":2587}
{"text":"Wood houses are built using wood skeletal frames,  the \"bones\" of these houses are studs.  They provide sufficient structural support to keep the building standing and intact for decades, even centuries. You dont need to build housed out of solid wood, as it would just be unnecessarily expensive.\n\n Drywall replaced [lath and plaster](_URL_0_). lath are thin wood planks that would be nailed to the studs in ceilings and interior walls.   They don't provide structural support, they just create a flat surface between the studs, which would then be plastered.   This was a multistep process that took several days to complete a room.  Drywall, on the other hand, prefabricates the walls, ready to install.  Just nail the sheet to the wall and you're basically done.  Experienced drywallers can do i ln a day what lath and plaster-ers could do in a week, while offering a comparable result. This dramatically cut down construction time, required less labor, and thus saved money, so it was widely adopted.","label":0,"model":"human","source":"reddit","id":2588}
{"text":"Because your blood sugar drops and you get hypoglycemic which causes the nausea or the puky feeling.\n\n a simpler way to explain this- When have been hungry for a while our bodies start slowing down to preserve nutrition. This affects blood circulation to put it simply which is why some of us feel light headed, dizzy,etc. \n\nEdit 2- another reason for nausea is the acid produced in the stomach. Excessive acid on an empty stomach can cause nausea\/ vomiting as well.\n\nEdit 3- Before my inbox explodes, I would like to say I don't think Hypoglycemia is the only cause here. It's different things for different people as I explained above.  We all have different tolerances towards hunger. This is not a thesis on starvation. \n \nEdit 4- link in my post below in the thread \n\nEdit 5- RIP my inbox. \n\nNot all of these things will happen if you've skipped a meal. It's different for different folks. Some of us have blood sugar issues and some don't. A person in this thread got a stroke after starving for nearly a week. \nSo, I am not saying if you skip a meal your body will completely shut down. There are different stages of hunger. \nEdit- language","label":0,"model":"human","source":"reddit","id":2589}
{"text":"Separation of church and state is much-misunderstood concept.\n\nThe principle is designed purely to protect an individual's religious liberty. It prevents the government controlling religion - so the government can't tell the church what to teach or what to believe. It also prevents the government imposing a state religion. So it prevents the state forcing everyone to become Muslim or everyone to become Christian etc. So the state can't impose a religion on anyone, nor can it tell a religion how to operate.\n\nObviously, this doesn't mean that there aren't laws that apply to religious organisations - tax regulations, for example. It also doesn't mean that religious practices are freely permitted if they violate other laws (such as murder). Conversely, it doesn't mean that religious convictions can't be applied when voting or making the law (if that reflects the will of the people). For example, a senator can vote against abortion because that is the belief of his Christian-majority state, but couldn't vote for a law that requires everyone in America to be a Christian.\n\nSwearing on the Bible is a symbolic gesture, reflecting the large numbers of people who identify themselves as Christian in the US. It is a symbol of the commitment to tell the truth in the knowledge that God sees the truth and will know if you lie. Arguably, for an atheist, it is a powerless gesture. However, it is not a violation of church and state, because that's not something the principle was intended to cover - it is not establishing a state religion, but merely reflecting the beliefs of much of the country.\n\nTl;dr summary: if a law doesn't force you to practice a religion you don't believe in, or prevent you exercising your religious beliefs, it doesn't violate the separation of church and state. There are, of course, debates about what is covered - such as freedom of religious conscience vs equality legislation.\n\nEDIT: Thanks to a number of people who have pointed out that you are not required to swear on a Bible, and can substitute for something of equivalent personal significance. This is significant as it means that no one is forced to ascribe significance to a deity they don't believe in. If it seems like everyone does it, I think that's just because in a largely Christian country, that's what a lot of people choose to do because it's significant for them personally.\n\nPS: Thanks for all the upvotes!","label":0,"model":"human","source":"reddit","id":2590}
{"text":"I can give an example. First you need to understand that in each area there usually only is one cable company, you basically get that company for cable or you don't get cable (there are satellite options but many towns and apartment buildings have restrictions on satellite dishes) and for internet cable is usually the fastest option so usually you just take the cable.\n\nMy brother was in the state of New Jersey and he notice his cable bill went up (again). Since it had just gone up a couple months before, he looked into it and he saw a new fee (which comcast words so that at first glance you might think it's a tax... they do this a lot \"government regulatory fee\" and things like that). At the time he was in journalism school so he decided to call them up and ask what the deal was. They informed them that the state had recently started imposing a fine on the company in areas where there was no competition (DSL, etc.) and that's why their's a fee. His response...\n\n\"So let me get this straight. You're a monopoly, I don't have any other choice as to who to get cable\/internet from in my area. Correct?\"\n\n\"Yes.\"\n\n\"And you make a lot of money because of that and the state has decided to fine you for that?\"\n\n\"Yes.\"\n\n\"And so you're passing that fine on to me?\"\n\n\"Yes. Have a nice day.\"","label":0,"model":"human","source":"reddit","id":2591}
{"text":"A lot of people have been skipping many critical issues. While creating an operating system is a very ambitious project, and the investment would be incredibly high, I do not believe that is the main issue. In fact, I don't even believe it is a very big one. I'm sure there are countless companies that would be happy to pay the price to develop a new kernel, shell, and GUI. Provided you could guarantee user adoption and manufacturer support.\n\nThere are individuals who have created operating systems on their own, yes they are very limited, but it's very doable.  As an example, check out [Visopsys](_URL_0_), and the well known [Linus Torvalds](_URL_1_), who wrote the original Linux kernel (and imported the bash shell).\n\nThe real problems arise **after** you have created the operating system. In order to create an OS, you will inherently need support for at least one CPU architecture, in today's environment, the obvious choice would be [x86-64](_URL_3_). After you have your kernel created, you need a shell, and also a GUI. Lets just assume all of this is done.\n\nNow, lets say Joe Shmoe goes and buys a computer. He gets the latest processor, motherboard, video card, sound card, and SSD. Since you are a large company, with good programmers behind your project, you have kept your kernel up-to-date. It supports all of the latest CPU features, and Joe's computer is crazy fast on your OS. His SSD is also supported out-of-the-box, since this is easy to implement.\n\nHowever, Joe has no sound. The built in Ethernet on his motherboard doesn't work, and neither does the card reader that came on the front of his case. And while you have included a basic Video Driver, his Geforce GTX 9996 Extreme Edition HD x2 lags while playing even the most simple games. This is because you don't have driver support for the motherboard, card reader, sound card, or video card. \n\nNow, you could contact Nvidia and Creative Labs, and tell them that you need support, but why should they shell out $10,000 to create a driver for your operating system? Lets forget the issue that none of their developers have any experience with your operating system or development environment. You don't have any deals with Dell to sell a million copies of your operating system paired with their hardware, you don't have wide support for other devices, and there is very little demand for your product. \n\nBasically, you can't get people to use your operating system because of a lack of manufacturer support, and you can't get manufacturer support, because of a lack of people using your operating system.\n\nAdd the fact that you cannot support proprietary APIs like Microsoft DirectX (no support for new games), you don't have the industry standard office suite Microsoft office (lack of interest from professionals), a lack of audio and video manufacturer support (no video or music editing for hipsters), and all you are left with is an operating system that at best, might reach the popularity of Linux (which has less than 1.5% of the Desktop market). \n\nI doubt you could even reach that percentage, since everyone who is well established in the field, specializes in development environments and APIs that you don't support. Effectively, you would be asking developers to learn an entire new environment, with the promise of opening them up to an additional 1% of the market. It's not worth it for them, or for you. This means that companies like Adobe aren't going to bother creating a version of Flash for your operating system, so no YouTube. There are countless little programs that daily life on a computer requires, and all of these would need to be ported over to your operating system in order to perform things that are taken for granted on Windows.\n\nThe operating system itself plays an important role, but it relies heavily on the support of hundreds of manufacturers, tens of thousands of developers familiar with their environment, and established software (Microsoft office, Adobe Photoshop, Chrome), and the use of APIs that developers are already familiar with, and are supported by video card manufacturers, such as DirectX. It's not just the windows driver that is needed in this case, but hardware and software support on the part of the video card as well ([pixel shader](_URL_2_) is a good example).\n\n*Edit: I just remembered that YouTube is now HTML5 based, but the sentiment remains. While a good high quality browser ported to your system would support HTML5, there would still be many similar issues. I should mention that Netflix is moving away from Microsoft SilverLight in support of HTML5 as well. Open standards are taking the lead in online video streaming right now, which helps, but there is a long way to go. And lets face it, porn plays a major role as well, which is still dominated by Flash and SilverLight. I have even noticed a few impressive games using OpenGL, but it will most likely never reach the popularity envisioned for it in the 90s. New online office suits are also very impressive, and show a lot of potential, especially with Word, but other formats are still lacking in support (such as Powerpoint). These may seem trivial, but if you rely on any of them on a daily basis, then even one makes all the difference.*\n\n*Edit2: My first gold! Thank you very much mysterious stranger, hopefully I'll see you out in the wasteland!*","label":0,"model":"human","source":"reddit","id":2592}
{"text":"The lock on system sends out a frequency wave that is picked up by an antenna that's main job is to Interpret that signal and sound off a warning in the cockpit.\n\n\n\n\n\nBut guys, since this post blew up, and somehow my post is on top, I just feel I need to tell everyone I'm not an avionic tech. Just an airframe & powerplant guy with VERY basic knowledge of the warning systems, and specifically hands on with only the f-16. Block 40 and 42 models.\n\n\n\n\nu\/miori1230  replied with _URL_0_'s not just frequency that the antenna interprets. It also looks at how long the pulses, and how often those pulses occur. Some signals are continuous wave, so their pulses go on forever. The combination of frequency, pulse width, and pulse repetition interval help the antenna narrow down what it is that's causing the signal. Sometimes thats enough to figure it out exactly, but a lot of times it needs more information to resolve ambiguities.\nIf a missile has locked onto an aircraft, the signal from the missile will be focused directly on the plane and the plane will always be able to see the signal. If it's a navigation radar, it's probably rotating (think of those radars on the top of small boats or at the airport). That's just one more way to narrow down the range of possibilities.\nAs someone else mentioned, the F-35 and a lot of newer radars use agile frequencies so the plane can't really figure out that there's a signal there to see. A plane flying high above the earth will naturally see all kinds of stray radar pulses, most of which it doesn't care about. Those new radars are random enough and use so few pulses that it appears as though they're just part of the background noise.","label":0,"model":"human","source":"reddit","id":2593}
{"text":"From an evolutionary view it is beneficial for us to easily remember bad memories. This helps us avoid repeating mistakes we have witnessed and keep us out of dangers way e.g. My friend was eaten by a bear in that cave, I remember this in great detail and avoid that cave in future so I'm less likely to be eaten by a bear.\n\nI think it might be similar for things we find embarrassing. It's dangerous to do things that alienate you from a group since we are social creatures that rely on acceptance and group work to survive. If we remember things we've done before that threatened our social standing then we aren't likely to repeat the behaviour in future and avoid exclusion. That's just speculation though. \n\nEdit: for people asking for more info on this I highly recommend looking up evolutionary psychology.\n\n'Evolutionary theories of emotion view emotions as adaptive traits - they help the organism to adapt to the demand of the environment and thereby survive' (Izard, 1977; Plutchik, 1984).\n\n[Here is some info on evolutionary psych and emotions](_URL_0_)","label":0,"model":"human","source":"reddit","id":2594}
{"text":"I'm a lawyer and I advise my clients to write these things into their job applications.  Here's whats what: companies don't want equality assurance, they to avoid a PR nightmare.\n\nThere have been instances when well-intentioned companies opted for gender-blind, race-blind hiring and end up hiring 40 privileged white males.  It's a publicity nightmare because someone like Al Sharpton will call the company racist, or somebody will sue saying they weren't hired because they are a woman or are black.\n\nTo avoid this, companies are VERY race and gender conscious when hiring: pick the best candidates, and if the crowd isn't diverse enough, sprinkle in a few minorities and\/or women into the mix.\n\nI'm not saying that this is good or bad, but it's the way it is.  No client I've ever had (and I've had a lot of brand name clients) gave a shit about a diverse workforce . . . to them, diversity is just some inefficient hassle foisted upon them by lawyers and politicians.\n\nEdit:\n\nThe top comment says:\n\n > That information is kept separate from your application. They just want to see if they are gathering applications in an equal way. If they only advertise the position in the Wall Street Journal and every applicant is white, then they know that they might need to branch out a bit more to get a more diverse pool.\n\nThis is really cute, but it is entirely untrue.  Large companies do not care about a diverse applicant pool, they care only about having a legal or PR defense in case they're accused of racism or systematic discrimination.  They care only about token diversity; they want individuals they can point to and say, \"See, we're not bigots, we hired Judy, and she's a African American lesbian Muslim.\"","label":0,"model":"human","source":"reddit","id":2595}
{"text":"I used to be guilty of showing things to my friends and expecting a certain reaction out of them, and it took me such a long time to finally realize why it doesn't work. Basically, it just creates a certain 'all-or-nothing' pressure that feels gross both to impose on someone else and for that person to notice is being thrust on them. Even just saying \"Hey I did watch this already but I'd love to rewatch it with you!\" subtly turns people off since they're worried you're focusing more on their reaction than discovering something together. \n\nIt's an understandable mistake to make - I always get a vicarious rush when someone I know discovers for the first time a long-time favorite of mine (that actually pretty much sums up nerd-dom in general). But what I've realized is that you need to either wait for them to discover it on their own, or very casually ease them into it while accepting that despite them being on the same wavelength as you in general, they might end up hating it - and that's totally okay.","label":0,"model":"human","source":"reddit","id":2596}
{"text":"As you probably know, the speed at which motion picture film runs through the camera determines its frame rate, given in frames per second (fps). When run through a projector (which you can think of as a backwards camera) at the same speed, the movement looks natural to us. If turned more slowly or quickly, however, it plays out in fast or slow motion, respectively (the terms \"undercranking\" and \"overcranking\" are still used for these techniques, derived from the literal cranking mechanism used to run early cameras and projectors).\n\nObviously this enthralled audiences, and early camera operators took advantage of this at times, but the cliche of its ubiquity happened more by accident. In the early days of the medium, both cameras and projectors were usually operated at a lower speed than the 24fps that later became the industry standard (particularly with the advent of synchronized sound in the late 1920s). I've shown silent films while working as a projectionist, and they're often distributed with instructions to be run at 18fps so that movement shows up normally. If shown at 24fps\u2014which has often been done, either because of insufficient equipment or human error\u2014you would be seeing everything at 1.5x the speed of the actual motion, hence the cliche of old films running in fast motion.","label":0,"model":"human","source":"reddit","id":2597}
{"text":"its not water that makes skin stay moist - its oils in the skin. ^edit to be completely clear, the water is still important - the oil just acts to prevent that water from evaporating. \n\nIf you have dry skin, its because your skin's natural oil production is for whatever reason lower than it should be.\n\n\n\nEdit: I'm not a dermatologist, just a lumpy potato that resembles a [scrotum](_URL_0_), so any questions are going to get some less-than-stellar answers. try \/r\/Dermatology or \/r\/AskADoctor \n\nDouble edit: Dermatologists\/Doctors are there for a reason, if you have a serious skin condition or are worried about your skin, talk to them and see what your options are.\n\nTriple edit: There's got to be a few dermatologists out there somewhere who can help verify answers or answer questions throughout this thread! If anyone viewing knows someone, see if you can ask them and see whats up.\n\nFinal edit; Figures one of my most well-voted comments on ELI5 involves a scrotum joke. Cheers all!\n\n---------------\nUseful Links:\n\n\/u\/ieatbugs posted [here](_URL_1_) a link to \/r\/SkincareAddiction on the types off Moisturizers out there that might be of some use to people\n\n\/u\/steve-s posted [here](_URL_2_) a link to an _URL_3_ article that has some information on natural skin oils\n\nBy request from \/u\/LgNBullseye , a [selfie](_URL_4_). That is really a potato, I promise :p","label":0,"model":"human","source":"reddit","id":2598}
{"text":"A lot of animals do have the females raise the offspring, others have both help, or just the male, and others still just lay the eggs and abandon them.\n\nAs to why these strategies develop: the simple answer is that it works in regards to the environment they live in. Something to understand about animal evolution is that \"survival of the fittest\" isn't necessarily about being the biggest, baddest, toughest creature out there and not \"fit\" in a exercise-y sense, but rather that animals evolve to be the best \"fit\" to their environment. This can include size, teeth and claws, and brainpower, but it also includes things like social dynamics, integument like feathers or fur, and reproductive behaviors. Because, at the end of the day, an animal could be aggressive and dominant but it's not going to be \"fit\" to it's environment if it can't reproduce successfully.\n\nFirst of all, the ancestral strategy and most popular for animal reproduction is the hands-off approach: where the young as eggs are released en masse into the environment to fend for themselves and a few make it to adulthood. This has the obvious disadvantage of most of your young being eaten and you wasting resources, so over time some animals may have started taking care of their young, laying smaller clutches of eggs, and eventually, in some cases such as some sharks, snakes, and mammals, hatching and nurturing the eggs inside (usually) the female's body and giving live birth. Now, with the advantage of your young being more likely to survive to adulthood, you're stuck with the disadvantage of them requiring many more resources from you to raise, so then you start evolving different strategies to minimize effort and maximize offspring that make it to adulthood in your environment and according to your biology.\n\nFor example, with songbirds and pigeons both parents incubate and feed the young as they grow rapidly. The eggs they lay are often very large compared to the size of the female and take a lot of nutrients out of her. But with jacanas (a family of wading birds), the female lays several clutches of eggs with different males and the males raise them while she wanders around protecting her territory. She's thought to be able to afford this because of how rich the food and calcium gets during their breeding season. On the other extreme, megapodes (in the same clade as chickens and quails) simply lay their eggs in mounds like crocodiles do and the eggs hatch fully flighted and ready to take care of themselves, even though they do go a step above the ancestral behavior and guard the mound.\n\nSo it's mostly about what works in the environment and how much the parents have to invest into creating their young in the first place. It's possible that female mammals especially might have more interest in their young because of how demanding pregnancy is on the mother. Males join in or females opt out according to what best \"fits\" the environment and the demands of the young.","label":0,"model":"human","source":"reddit","id":2599}
{"text":"That's because you mostly know the top products from Germany. The high costs of producing make it impossible for German companies to compete on the international market in the low price\/quality segment. \n\nOr in other words if a Germany company builds a crappy and cheap car, a Indian company can produce the same crappy car cheaper. Great quality and engineering is the only way to position a car on the international market and be successful.\n\nEdit: \nI'm going to add some things because i wrote this post in a hurry and some things may not be as clear as i wanted them to be. I was talking about products made in germany. And i think most products from german companies made somewhere else don't get recognized as \"german products\", but there are german companies that produce  cheap products (and many of them do so in other countries).\n\nThe important point i wanted to make is, that it is hard to compete with low prices when you produce in a country with high wages and taxes and social security. That is why companies that want to compete international, tend to focus on other things. I only mentioned quality in my comment, but to be honest i left out that there are other things like service or design and many other things that a customer is willing to pay for. And usually people don't want to pay more for something that isn't \"better\" in some way. \n\nI hope this made my comment a bit better and thanks for all the up votes!","label":0,"model":"human","source":"reddit","id":2600}
{"text":"It can happen with newer\/poorly trained dogs and handlers. As others have said, experienced dogs and their handlers are able to read\/judge each other well.\n\nThe most commonly utilized 3 breeds in the US at the moment (German Shepherd, Belgian Malinois, Dutch Shepherd) are historically used as perimeter herders for livestock. They will go after\/herd something that is fleeing if they have a pretty strong instinct intact. That of course is ideal when picking out candidates for training, but can take a lot of work to focus the instinct.\n\nA lot of dogs are also trained to be more in tune with the emotions the suspects are expressing too, so chances of a well trained dog going after an innocent are slim.\n We used to do a training exercise called \"the happy dance\" where the handler would stand calmly with the dog at plotz(laying down) and another worker would pass by and think aggressively and make gestures. The dog would get up at the alert between the person and it's handler and go nuts at the guy. Then the guy would turn around, calm down, and be happy sounding and the dog would  change it's posture and lay back down almost instantly. \n\nSource- worked with a K9 security company, and owned a pretty high drive flunk out Dutchie (flunked because his drive was the wrong kind for the job. Unless the perpetrators were always on bicycles... or were actual sheep.)\n\nEdit- for all those saying \"so I should just turn around and say WHO'S A GOOD BOY\" or \"so I should just try to play with him\", I know you're trying to be funny but no. You should honestly stop and lay on the ground with your face down. The dog will still come towards you, but they are often trained to guard the suspect on the ground until the handler arrives to arrest. The handler will call the dog off.\nThat game we played was to test the dogs protective instinct for the handler or a threat more than anything but is an exercise for the dogs judgment no less.","label":0,"model":"human","source":"reddit","id":2601}
{"text":"First off, teachers don't work 9\/12ths of the year. They don't just show up on the first day that students arrive and they don't leave when students do. They don't stop working at 3 when school ends for students, but keep going into the night and often weekends doing grading, prep, parent calls, conferences, meetings, etc. I'm not a teacher myself because I couldn't handle it. Honestly. I know way too many teachers and know that I couldn't handle the job they do.\n\nSecond, many places require more than a Bachelor's degree. Where I live, a postgraduate degree is required. Teachers are also typically required to attend regular professional development courses throughout their careers. These are extra university courses that they have to pay for regularly.\n\nThird, it's not a lot in comparison to other jobs that have less responsibility and requirements. A median salary of $53k is in the same ballpark as restaurant managers ($53k), construction crane operators ($53k), postal carriers ($51k), bricklayers ($51k)... none of which require investing in education and none of which are responsible for our children's futures.\n\n**EDIT** So I don't have to repeat myself a million times in the comments: No, I'm *not* saying that people in blue collar jobs don't have skills or don't deserve to earn money. Don't put words in my mouth. I've worked blue collar jobs and fully appreciate how skilled tradesmen can be and how necessary those jobs are. I *am* saying that it costs more in time and money to get the degrees necessary to become a teacher while most trades are learned either through cheaper, shorter-term courses and\/or on-the-job training. I'm sure you can point out trades that cost more to learn, but I'd also bet most of those pay more. If not, then they too are probably underpaid.\n\n**EDIT 2** [Source for the numbers was the Bureau of Labor Statistics.](_URL_0_)","label":0,"model":"human","source":"reddit","id":2602}
{"text":"I'll expand on the other comments. \n\n\nYes, there are several viruses that will result in the common cold, the most common being rhinovirus. The fact that there are multiple viruses that can give you the disease is NOT the major reason that you can get the cold more than once, since you can be infected by the same virus, like rhinovirus, many times in your life. This is because the virus mutates very quickly. \n\n\nWhen human cells in your human body replicate, they have to create an exact copy of your DNA. Your cells do this very, very efficiently and effectively because there are *many* methods of fixing mistakes in your DNA. When viruses replicate their DNA\/RNA, which is the part that they inject into you so you get sick, they (usually) *do not* have any way to fix mistakes in that DNA\/RNA. \n\n\nThese mistakes in DNA\/RNA lead to proteins (little parts in cells that actually do work) that have slightly different shapes. After infection with rhinovirus for the first time, our immune system will be trained to recognize the very specific shapes of the proteins that come from this virus. However, since the virus can mutate so quickly, the second time that you're infected by rhinovirus, the proteins from the virus will probably be a different shape than last time so your immune system can no longer fight it off effectively. \n\n\n\nEdit: If anyone has specific questions, I'll literally be sitting here all day.","label":0,"model":"human","source":"reddit","id":2603}
{"text":"Kodak is a great example.  They were not some flash in the pan, and they did not just get folded into another company, they rode the horse straight into hell.\n\nSo this huge company based around film and optics.  They were a great employer, did a lot of stuff that sounds like Apple would do it, hired the best talent, thought creatively, etc. etc. etc.\n\nBut then the market changed.  At first no one really thought digital cameras would matter much, they were very expensive and had questionable quality.  So Kodak ignored them.  Then digital got cheaper and better.  Kodak ignored it.  Then consumers started to realize the crazy benefits of digital over film and started jumping ship in a big way.\n\nKodak tried to change gears.  They spent a ton of money trying to make products to print digital photos (in stores, at home, from special digital cameras).  They tried to make digital photo frames.\n\nThe problem was that they were a huge company and huge companies need huge products.  Its not that any of their ideas were necessarily bad ones, its just the market was shrinking.  People were not going to spend hundreds of dollars on film\/development any more.  Today in your life you might print one or two hundred photos at the total cost of 20 bucks, but no more will you have to drop serious money on photography.\n\nAnd so the layoffs started (and layoffs are expensive because of severance pay laws), and then lawyers got involved, and pretty soon the once cutting edge company was nothing but liabilities and tears.","label":0,"model":"human","source":"reddit","id":2604}
{"text":"Because Apple does not have a dominant position in the operating system market. MS got in trouble because they bundled IE with Windows, and made it impossible to uninstall it.\n\n\nGiving away software is fine, but if you have like 90% of the desktop OS market and then force everyone to have your web browser installed, you're essentially abusing your position in one market (operating systems) to rig the browser market in your favor.\n\n\nLet's imagine 95% of all cars in the world were Toyotas. Then Toyota decides that they want to sell tires too, so they add a system to prevent the car from starting unless Toyota brand tires are on or in the car somewhere. Even if you make the best, cheapest tire in the world, you can no longer sell your products to 95% of the population, because Toyota used their dominant position in the car business to shut you out of the tire market.\n\n\nThat's bad for consumers because tires would no longer be a competitive market. The vast majority of people would be forced to go with Toyota brand tires, no matter how crappy or overpriced they are compared to the competition. And that's why it's important to keep separate markets separate, because when a company dominates one market, they can abuse their position to muscle in on unrelated markets even if they don't have a competitive product.\n\n\nIf Apple had a 90% market share and made it impossible to uninstall their business tools, they'd probably be in trouble too.\n\n\n**edit**: Lots of people seem to be complaining about the fact that tires are not 100% the same as browsers. Yes, because this is just an analogy. The whole point is to make it easier to understand why certain anti-competitive behavior is bad for consumers, not to mirror the original situation 1:1 with all its intricacies. Arguing that computers have more hard drive space than cars have trunk space is really not that helpful.","label":0,"model":"human","source":"reddit","id":2605}
{"text":"Seeing as how any answer to this question is a spoiler I won't be trying to hide any spoilers or such. Also this is all from memory so I hope I'm actually correct.\n\nSilent Hill was a small East coast American town. The people of Silent Hill worshiped a demon and wanted to birth it into the material world. At one point, a girl with psychic powers was born to a woman and the demon cult endeavored to use this girl as a conduit to birth the demon. \nUnfortunately for the cultists, the girl was hard to break, so, in order to make her an empty subservient, they attempted to rip her personality out of her(or split her into two beings or something) so that they could have only the power and body remaining. When they did so, they created two people: the new infant who was found by a passing driver named Harry Mason(who adopted her and named her Cheryl) and a vengeful, powerful creature who had her original name, Alessa.\nYears pass by and Cheryl is a young girl(just turned 7) and she suddenly has the nagging feeling to go to Silent Hill and become complete again.  Mason is unaware of her past however and is driving by Silent Hill when an image of Alessa causes him to veer off the road, crash and fall unconscious. When he comes to, Cheryl is gone and he spends the rest of the game attempting to find her in the town that is now a purgatory prison for all of the people who were involved in the cult and, by association, Alessa's pain and torture. \nDue to Alessa's dark magics, the town is now infested with monsters and demons that are all manifestations of her own mind and psyche. These monsters seem to basically be mindless killers with behavior loosely based upon things that Alessa saw, enjoyed, or hated in her life. \nTo make a long game short, Harry Mason goes through the town, fights the cult, saves Cheryl(in an incredibly strange way I don't really want to get into the birth of the demon god at the end go to wikipedia or something.), and leaves the town to continue raising her.\nThat's the first game of the series, if this sufficed and you want to hear about the other games I can go into detail. Otherwise I don't want to type another essay. \n\ntl;dr, Bad people hurt a little girl who makes Silent Hill really scary.","label":0,"model":"human","source":"reddit","id":2606}
{"text":"Two things: specificity and deliberateness. When the Final Solution was put into action at the height of WWII, it was with a view to wiping out entire classes of people - Jews, gypsies, homosexuals, and the disabled, to name a few. The Nazis undertook this work with a level of planning that was unprecedented in human history and has never been matched since. They made an earnest attempt to kill every single Jew in occupied Europe using all the techniques and technologies of the industrial age. This is why the Holocaust was such an appalling event: firstly because we attach a special significance to genocide compared to other forms of mass murder, and secondly because of what an indictment of the industrial age it was. To expand on this second point a bit, the Holocaust gave us - probably for the first time in history - an act of genocide in which the virtues of a supposedly civilised society were put to a perverted use, as tools of extermination. The Nazi regime coordinated countless soldiers, bureaucrats, engineers, scientists, doctors, politicians, and industrialists, and deliberately concentrated all of their talent and learning to destroy European Jewry.\n\n\nFor sheer numbers, Mao (and Stalin) put Hitler in the shade, but theirs were passive acts of mass murder, and they probably weren't genocides. Mao starved millions in the 1960s, but the starvation wasn't the goal - it was a simple by-product of CCP policies. Likewise, Stalin would probably be guilty of ethnic cleansing in his treatment of numerous ethnicities in the USSR, but these acts were always in service of his paranoia, and never because he made a concerted attempt to bring the state's full industrial might to bear on a particular ethnicity. \n\nWhether or not a certain dictator might have intended to kill so many people seems like a semantic difference, and to the victims it wouldn't really matter. But, we differentiate between murder and manslaughter for a reason, and for the same reason we also put Adolf Hitler in a category of his own. Sheer numbers are not at issue here: what really matters to us when we compare these things is mankind's capacity for evil, and the Nazis' methodical genocide is a far darker reflection on our species than Mao's criminal neglect.\n\nObviously, this is just a perspective, and I'm happy to take criticism from anyone who disagrees.\n\n**Edit:** A lot of people are pointing out that it's a result of cultural bias in the West and\/or that Stalin and Mao gget off lightly because their crimes were confined to one country. I'd definitely agree with that, but I'd still maintain that if you were somehow able to revive all three and take them to the Hague for trial, Hitler would still get the most severe sentence. I guess I came at this from a quasi-legal perspective, and I'm grateful to everyone who offered cultural explanations, most of which I hadn't previously considered.","label":0,"model":"human","source":"reddit","id":2607}
{"text":"A little late to the party, but adding my two cents:\n\nApart from the chemical changes that happen on the soda because of it's package, an often overlooked aspect about taste is that it's a complex sense. It's not just your taste buds giving you what you ultimately experience as taste: its smell (it's not always the same as its taste), temperature and texture also add to the experience, as well as anything else going on on your mind at the time.\n\nGet two bottles, one plastic and one glass, and put them on a freezer. Once out, they'll be roughly the same temperature, but the glass will probably feel colder.\n\nPlastic is not that great of a termal conductor, while glass is better at it. Hence, when you place your lips on the plastic bottle, it does not feel as cold because it does not take as much heat from your lips as the glass bottle would do. And as we all know, when it comes to soda colder is always better.\n\nSmell is also affected by the material of choice for your packaging. Plastic tends to trap more the smell (I'm guessing it's more porous so trapping the small particles is easier) while glass gives you a smoother sniff.\n\nEvery single thing going on in your mind at any given moment alters your perceptions. What you ultimately perceive and cannot describe with words (your [qualia](_URL_0_)) are a result of a very complex and unique set of phenomena. Sounds, images, sensations, smells and tastes are all altered by each other all the time.\n\n**EDIT:** English is hard. Latin is too. Also, as some of you have pointed out, glass is a bad conductor. I just pointed out that glass is BETTER than most plastics at it. The point of this is that what you ultimately experience as taste is not just the chemical compounds of whatever it is you're tasting; it's how you PERCEIVE them, from how they interact with your tongue, nose, skin, etc. to how you process (mostly subconsciously) the whole situation.","label":0,"model":"human","source":"reddit","id":2608}
{"text":"When you miss an entire night of sleep or are under sleep deprivation for not getting enough sleep multiple nights in a row your body basically goes into extreme recovery mode the next time you get to sleep. This is called a REM rebound. On a normal healthy night your body goes through 4 stages of light to heavy sleep followed by a REM cycle. REM is when your mind gets a lot of its recovery time. The cycle goes from 1-2-3-4-REM then resets in different intervals throughout the night. You go through this process 4-5 times a night.\n\nWhen you are  \"behind\" on sleep your body basically skips the beginning of parts of the cycle and stays in REM sleep proportionally longer than usual. You also dream during REM sleep so this is why you can (in my experience) have insane dreams after an all night. But to answer your question, no because after sleep deprivation, your body will correct by having REM rebounds with basically extra REM sleep until you are recovered.\n\nBonus: Fun Fact about power naps. The first total sleep cycle is about a 20-25 minute cycle. This means the end of your first REM cycle will be after this point. From here the cycles get progressively longer. So by sleeping for only 20-30 minutes you are getting the most recovery (most REM) in the shortest total time. Which is why you can feel so rested after a short time.","label":0,"model":"human","source":"reddit","id":2609}
{"text":"Hiccups are an involuntary spasm caused by the diaphragm, that flat disk shaped muscle under your lungs that, when it contracts, pulls air into your lungs. \nThere are several causes for hiccups (laughing\/crying too much, anxiety, pneumonia, indigestion). There are several methods to stop hiccups and why some of the old wives tales may have some scientific backing.\n\nFor example holding your breath causes the build up of carbon dioxide (CO2) and your brain practically tells your diaphragm, \"get your shit together because we need oxygen!\" This is the same with drinking copious amounts of water in varying positions other than in an upright position, jumping jacks while holding your breath, etc.\n\nAnother solution is to stimulate the neural pathway from the brain to the diaphragm, this often involves stimulating the back of the throat because that's were major nerves travel. So tickling the back of the throat by having many stimuli (i.e. Eating a spoon full of sugar or salt) will cause over stimulation. Also stimulating the gag reflex, albeit not pleasant, can result in stopping hiccups. \n\nSource: I had hiccups for 2 weeks.","label":0,"model":"human","source":"reddit","id":2610}
{"text":"I wrote up a very detailed (perhaps too detailed) history of Watergate in this sub last year. [Here it is.](_URL_0_)\n\nIn very abbreviated form:\n\n* Under the auspices of the Committee to Re-Elect the President (CREEP), Nixon's executive staff (incl. his Chief of Staff, Attorney General, and others) used campaign donations as a slush fund to run a \"dirty tricks\" operation to harass political opponents via forging letters, planting provocateurs at political rallies, etc.\n* Although the CREEPs didn't brief Nixon on the details of their operations directly, he knew that CREEP was a dirty tricks shop, who ran it, and the kinds of actions they took, and was briefed on at least some operations after the fact.\n* CREEP authorized a burglary of Democrstic headquarters in order to secretly (and illegally) tap their phones, located in The Watergate Hotel. (It's really a hotel\/apartment\/office building.)\n* The burglars got caught, and they had the names and contact info of WH staff on them, as well as a $25,000 campaign donation cashiers check in their bank accounts.\n* After the burglary, the CREEPs briefed Nixon about it.\n* Nixon held a press conference in which he falsely denied knowing anything about it, falsely claimed he had assigned WH Counsel John Dean to investigate, and falsely claimed Dean found no connection to the WH.\n* Nixon publicly ordered the FBI to investigate, but privately ordered them not to look too thoroughly. This was obstruction of justice, a felony.\n* He also secretly ordered the CIA to interfere with the FBI investigation, which is also obstruction of justice.\n* He later ordered various staffers to lie to the grand jury investigating Watergate, which is perjury. Yet more obstruction of justice.\n* As a result of the investigation, it came out that there was a secret taping system recording conversations in the Oval Office. Nixon tried to fire the Watergate special prosecutor when he demanded the tapes, and ultimately also fired the new Attorney General and Assistant AG when they refused to fire the prosecutor. (\"The Saturday Night Massacre.\")\n* Eventually Nixon handed over the tapes, which showed all the lies and obstruction of justice mentioned above, as well as the essentially immoral and illegal purpose of CREEP. The tapes also showed that Nixon routinely suborned the FBI, CIA, and IRS to investigate and persecute his political opponents and members of the press.\n* There is an 18 and a half minute gap in the tapes, which no one has ever explained. Given that the WH did turn over multiple instances of the president suborning oerjury, siccing federal agencies on private citizens, and obstructing justice, what could possibly have been so bad that they had to destroy it?\n\nImpeachment motions had been in the House already, but after the tapes, everybody knew it was a matter of time, and Nixon resigned. Ultimately 49 people went to jail for their participation in Watergate and CREEP, including the burglars, Nixon's Chief of Staff, several other Exec staffers, the former Attorney General, John Dean, etc. Prosecutors were very seriously considering pursuing Nixon himself for obstruction of justice; if they had done so, he would have almost certainly been convicted based in the evidence of the tapes. But President Ford pardoned Nixon as one of his first official acts. And so Nixon lived out his days in California, unmolested.","label":0,"model":"human","source":"reddit","id":2611}
{"text":"Hi guys. Software Engineer here. This might be a little bit more for 13 year olds than for 5 year olds but please bear with me.  \nAbout 10 years ago I discovered hacking in Counter-Strike. I tried out a couple of different cheats and even managed to make previously detected cheats undetectable by the VAC system.  \n\nCommon with almost all first person shooters is that hacking in them is a lot easier than with strategy games. For example, it is possible to \"hack\" games like Red Alert 2 by removing the fog of war or by increasing your money, but that'll usually only work if you are the host (and if you're not, you'll be getting reduced fog of war at best which is indeed some advantage but a really good player will always win against a lousy player with fog of war hacks)...\n\nBack to the main point, hacking in first person shooters is incredibly easy. It is trivial with today's technology to hook into the game executable, read the memory (and even modify it) and leverage that information to simulate mouse movements. What you really want to do (as a cheater) in a first person shooter is either see through walls or aim automatically. Aiming automatically is really easy when you have access to the memory, because you'll know the exact pixel coordinates of the enemy that your system is about to render and you'll know that you're aiming smack in the center of the screen (so (400, 300) if you're playing on 800x600 resolution), the vector that your mouse needs to travel is therefore **incredibly** easy to calculate, and moving the mouse is also really easy since these games run on Windows which uses the Windows API.  \nNow the really interesting thing is wallhacking, your game client doesn't get the information about enemies until it becomes likely that you will have to render them soon. If you've ever tried to use a wallhacking cheat, you may have noticed that your wallhack doesn't show entities (players, grenades, barrels, etc) that are being clipped by the server (clipped in this case means not being rendered due to being obviously out of view). What some wallhacks do is try to guess where players are by trying to sound-spot them, some of you old-schoolers may recognize early stages of this kind of hacking as `step step step` as seen from across the map. There are also other details that can be considered but for the sake of explaining to a Layman I think I'll leave it at that.\n\nSo there you have it, and here's a short TL;DR:  \n1. It's easier to hack in first person shooters than most other games **since the hacking happens on the client side**.  \n2. Games like Dota and LoL (RA2 was used in my example) are difficult to affect on the client side to produce better results than a human player would.  \n3. Hacking games is pretty much a solved problem, and it's very boring. If you're going to hack a game, you should probably stop playing that game and find something more exciting.  \n\nGreat summary [here](_URL_0_).","label":0,"model":"human","source":"reddit","id":2612}
{"text":"Galadriel is a Noldor, which were a sect of the ancient elves that were first created, Noldor elves being founded by the second elf to \"awaken\"(which in Tolkien speak means created). She is of the elves that inhabited Valinor, and the only remaining alive to have lived there when the Silmarils were a top their towers. The Noldor committed an unforigveable act, which was to commit violence against other elves, and were exiled from Valinor, and doomed so that essentially anything they tried would end in their defeat and the fall of the Noldor. Galadiel was of the sect of Noldor that chose to not return to Valinor and beg for forgiveness, but to sail East, and become the first elves to inhabit Middle Earth., and soon after they began a long and bloody conflict with Morgorth. Eventually, the Elves that remained in Valinor(Valar) caught wind of Morgoth and came to Middle Earth, which led to his defeat. The stories of the Noldor pridefully challenging Morgoth despite knowing he would defeat them, especially of Fingolfin's one-on-one duel with him in front of Angband, are very fun reads.\n\nGaladriel was offered a chance to return to Valinor, but she stayed there, having lost all her family during the war against Morgoth. In Middle Earth she settled in Lothlorien and oversaw the younger elves.\n\nThe offspring of Galadriel married Elrond, and the offspring of that included Arwen. It is also during this time that Annatar, Sauron in his Maiar form, convinced the Noldor to create the Rings of Power, and was in fact one of the elves that held a Ring of Power after their creation, but she chose to not use it, as she had an immense distrust of Annatar.\n\ntl;dr Galadriel is one of those first elves to be created by the Song of the Ainur, and all of those elves had immense power and were essentially badasses.\n\nSorry, Tolkien lore nerd here.\n\nedit: Well this thread and my inbox certainly exploded","label":0,"model":"human","source":"reddit","id":2613}
{"text":"There are a few different ways of measuring temperature close to absolute zero, but here  I will just explain one of them.\n\n & #x200B;\n\nWe can start by reminding ourselves that temperature is really just a measure of how much energy something has.  So a hotter atom will have more energy and move faster than a colder one.  One common way to measure temperatures close to absolute zero is by measuring the kinetic energy of the atoms at that temperature.  Usually when performing experiments at very low temperatures (just a few milli-Kelvin), the experiment traps the atoms with some type of field (usually magnetic) and forms a condensate, or cloud.  So you can imagine that as each atom in that cloud is cooling down, the cloud becomes more and more dense because each particle does not have enough energy to escape.  Then the experimentalist can turn off the trap and the atoms\/particles will fly away  and the cloud will expand ballistically.  The cloud size increases with time, and this increase is a direct observation of the velocity of the atoms and therefore their temperature (there is some fancy math that tells us that the temperature is proportional to the square of the size of the clouds shadow if we continue this analogy).\n\n & #x200B;\n\nThe technical term for all of this is the Bose-Einstein condensate.  Here is a more indepth review of this topic using the same analogy:  [_URL_0_](_URL_0_)","label":0,"model":"human","source":"reddit","id":2614}
{"text":"Kung Fu is chinese for what literally amounts to Hard Work.  Anything can be Kung Fu, from martial arts, to being a surgeon, to otherwise.  In general, its used as an umbrella term for most of the Shaolin martial arts.\n\nKarate is a term that today means 'empty hand,' but when it was first introduced to Japan, it meant 'foreign hand,' as it came from Okinawa, and before that, it can be traced back to China, with much of it coming from the shaolin temple.  As such, many techniques in Okinawan Karatejutsu have similarities to Shaolin styles.  Karate today is taught in different styles, but it basically revolves around striking, grappling and chin na (pressure point techniques\/eye gouges, etc.)\n\nJujutsu literally means \"Gentle techniques.\"  It is called that, because jujutsu is very easy for the user to execute, but devastating for the opponent.  Samurai jujutsu is the predecessor of Judo, Aikido and Aikijujutsu. Jujutsu is the art from which most of the submission techniques originate, but is meant for unarmed defense against an armed opponent.  Judo is jujutsu but with the majority of the lethal techniques removed, and some more emphasis added on non-damaging submissions.  Only Judo, Brazilian jujutsu and other sport forms forego teaching defense against an armed assailant.  \n\nAikijujutsu originated from jujutsu, but changed dramatically as it was taught, and most techniques I've been exposed to use a person's own energy against them.  ~~Aikido is the tournament form of Aikijujutsu, and has some of the more lethal techniques removed.~~  Many comments below correct me on Aiki, so I graciously concede to them.\n\nTae Kwan Do is similar to karatejutsu in that a lot of it has been altered from the original towards what we have today which is a lot of footwork based martial arts involving almost exclusively kicking or less handwork.  Older forms of TKD are strikingly similar to Okinawan karatejutsu and Shaolin kung fu.  \n\nNinjutsu is basically a combination of Samurai jujutsu, some forms of striking techniques based off of jujutsu, and specialized stealth techniques involving distractions, using teamwork for traps, and assassination.  Ninja often used traps like hornet nests stuffed in jars to be thrown at pursuers, knives, or even had women working as geisha use their hairpins in order to dispatch targets.","label":0,"model":"human","source":"reddit","id":2615}
{"text":"Just throwing this out there, but I have been making jerky since I was a kid.  Beef, Deer, Elk etc.  (I grew up kinda country) Well...\n\nI have discovered that lean pork roast makes jerky as well as beef (IMHO) for PENNIES on the dollar.  Costco (I'm in Austin) Sells these pork roasts for around $2.50\/lb in individually packaged 4 packs.  That way you only have to make one at a time a freeze the rest.  So buy that, trim off all of the fat and cut the meat up into 1\"x1\" strips.  Place in bowl.  Add 3 dashes of liquid smoke, 2tbls soy sauce and about a 1\/3 cup of Worcestershire.  Mix up and marinate overnight.  Take it out and add a third cup of seasoned salt and your preference in cracked black pepper.  Then dehydrate.  I use a store bought dehydrator, max power for 11 hours.  This makes the jerky really dry and tough, but that is how I like is.  Dry it to taste.  \n\nI am not going to tell you that it tastes exactly like beef, but after making jerky my entire life, I haven't made anything besides pork roast jerky in 2 years.","label":0,"model":"human","source":"reddit","id":2616}
{"text":"Good [previous answer](_URL_0_) via \/u\/AriGold317:\n\n >  \"There is an accurate perception that boys develop the fine motor skills necessary to hold a pen or pencil as much as six years later than girls. And then for boys to make correctly shaped symbols in specific horizontal alignment is even more difficult. It seems that boys develop the larger muscle mass for upper body strength before their brains can precisely control the movements of the smaller muscles in the wrists and fingers. There is also scientific analysis demonstrating that a boy\u2019s brain develops many of the abilities for handwriting much later than a girl\u2019s brain. A group that promotes separate schools for boys and girls, National Assoc. for Single Sex Public Education cites research by Harriet Hanlon, Robert Thatcher and Marvin Cline that details the differences in boy and girl brain development. Clearly, then, there are some measurable differences in muscle growth and brain development that result in the broad, general perception that a large percentage of boys are not capable of even average handwriting skills until a few years later than the early grades at school.","label":0,"model":"human","source":"reddit","id":2617}
{"text":"Bernie Sanders is an Independent Socialist US Senator from Vermont. He recently announced he is running for president on the Democratic ticket against Hillary Clinton. Many people here (including myself) like him because A) he isn't Hillary Clinton, B) he thinks the biggest issues facing us are climate change and income inequality, which many people here agree with, and C) he has been very consistent about caring about and bringing attention to these issues. If you look at a list of bills he's tried to pass, it looks like the wet dream of a modern American progressive. He is also taking no money from super pacs or billionaires.\n\nThe TL:DR: He's not Hillary Clinton, he cares about the issues we care about, and he seemingly has no reason to run other than the fact that he wants to help people, otherwise, why get into a race that everyone says he has no chance of winning?*\n\n*don't listen to those people though, he's already raised a ton of money just off of small donations, and has 175,000 volunteers already signed up. At the least, he will change the conversation in a valuable way, and I actually think he has a chance.","label":0,"model":"human","source":"reddit","id":2618}
{"text":"Kimberle Crenshaw coined the term, though the concept had been thrown around a lot before her by people like Audre Lorde or by the combahee river collective. The idea is that bigotry and oppression manifest in different ways depending on our identity.\n\nThings like racism and sexism exist, but popular narratives frame them usually in only certain ways. Crenshaw noted that while women weren\u2019t allowed suffrage until 1920, there were other laws preventing citizenship for women of other races from voting. Not only that, the suffrage movement discounted the voices of black women and their inclusion for the sake of the success of their movement. In that sense, sexism manifested differently between white women and other women.\n\nAnother example Crenshaw uses is domestic abuse. We like to think shelters from abuse are easily accessible, but factors like immigration status can curtail that access. Immigrant women might not leave abusers due to fear of being deported. And language barriers might not even prevent immigrants from getting information on where they can find a shelter, but shelters sometimes turn women away due to not having bilingual resources.\n\nUltimately, intersectionality is simply recognizing that oppression and bigotry doesn\u2019t always manifest in a singular manner, and we need to account for that. Black women don\u2019t experience sexism in the same way that white women do, and they don\u2019t experience racism in the same way that black men do. Acting intersectionally involves taking into account a spectrum identities on an issue and listening to people we hear from less to move beyond the simpler, more popular narratives.","label":0,"model":"human","source":"reddit","id":2619}
{"text":"From *Leviathan* by Thomas Hobbes:\n\n > The imaginations of them that sleep are those we call \u2018dreams.\u2019 And these also, as also all other imaginations, have been before, either totally or by parcels, in the sense. And, because in sense, the brain and nerves, which are the necessary organs of sense, are so benumbed in sleep as not easily to be moved by the action of external objects, there can happen in sleep no imagination, and therefore no dream, but what proceeds from the agitation of the inward parts of man\u2019s body; which inward parts, for the connection they have with the brain and other organs, when they be distempered, do keep the same in motion; whereby the imaginations there formerly made, appear as if a man were waking; **saving that the organs of sense being now benumbed, so as there is no new object which can master and obscure them with a more vigorous impression, a dream must needs be more clear in this silence of sense than our waking thoughts.** And hence it cometh to pass that it is a hard matter, and by many thought impossible, to distinguish exactly between sense and dreaming. For my part, when I consider that in dreams I do not often nor constantly think of the same persons, places, objects, and actions, that I do waking, nor remember so long a train of coherent thoughts, dreaming, as at other times, and because waking I often observe the absurdity of dreams, but never dream of the absurdities of my waking thoughts, I am well satisfied, that,  > being awake, I know I dream not, though when I dream I think myself awake.\n\n**Real Eli-5**: When there is an overwhelmingly bright light in your field of vision, this makes other lights appear dimmer. E.g. the sun preventing vision of the stars. Likewise, during the day, your sense impressions of the world (vision, hearing, touch etc.) are quite vigorous and incredibly constant, \"dimming\" the vividness of thought. At night, when the more bodily senses are sleeping, and the only thing you are doing is dreaming, your dreams appear much more vivid, just as the stars appear much brighter.\n\nAnother way of pointing this out is that there is an inverse relationship between the vividness of a \"day-dream\" and how much attention you're currently investing in the environment around you.","label":0,"model":"human","source":"reddit","id":2620}
{"text":"_URL_1_\n\ntl;dr CO2 dissolves into your water and through the magic of chemistry creates a mild acid responsible for the stale taste. that's assuming this poster had the correct answer\n\n**edit** as \/u\/macfearsome clarified below: \n\n > It's not about co2 dissolving into the water (like in sodas), it reacts with water to make carbonic acid. Reactions and dissolving are totally different processes. Reacting is a chemical change, so H2O + CO2 - >  H2CO3 as opposed to liquid water surrounding gaseous carb dioxide\n\n**edit2** as \/u\/e90brad pointed out:\n\n > This reaction will be most prominent in the meniscus, at the surface of the water. Which is the first thing you taste.\n\n > Leave a glass of water overnight and pour a fresh one in the morning for a side by side comparison. The meniscus on the glass left overnight will be multiple times thicker than the fresh glass.\n\nfor those not in the know, the [meniscus](_URL_0_) is the curve along the upper surface exposed to air on the inside of a class of water, where it looks like the water is \"sticking\" to the glass and creating a slight depression (more prominent in containers with smaller radii, as in the pictures of test tubes on the wiki link)\n\n**edit3**: I've been getting quite a few comments just saying something to the effect of, \"This is wrong, the other guy is right.\" *Please include links to sources and simple explanations of your arguments and I'll add them to this post*","label":0,"model":"human","source":"reddit","id":2621}
{"text":"From last time:\n\n\"Black Americans have been in something like a permanent state of identity-crisis, that will probably not abate until either:\n\n* Terms like \"African-American\" are accepted as fully and as un-ironically as \"Polish-American\" or \"Irish-American\", or;\n\n* Race itself becomes such a nebulous, blended, and indistinct thing that skin-color is regarded as no different from eye or hair color.\n\n\nIn the meantime, a particular challenge for black Americans is disconnection from historic familial roots. An Irish-American family might name their kid Sean or Daniel or Molly or Colleen or Mary, with some connection to those who came before (even if those names might bear little or no resemblance to ancient Irish names and culture).\n\n\nMost black Americans bear family names from the slave-owners of their forbears, or arbitrary names given to freedmen. A white American man named, say, Robert DiGiacomo might go by \"Bobby\", and might consider himself mostly German\/Scots, but he knows where his name comes from, and he knows that his father was descended from an Italian. If he wanted to, Bobby G could probably trace his ancestry back to specific people and families from any number of countries.\n\n\nA black man named Robert Smith might have little more than a vague idea that one of his ancestors was once owned by a man named \"Smith\". It is unlikely that he could reliably trace most of his family tree back further than slavery, since good records were not kept, about the lineage and ancestry of slaves. And any \"deep past\" records of his roots might actually refer to white parentage that abandoned or rejected their multi-racial offspring. He might not be able to able to find the specific African language, name-tradition, or region his ancestors came from, even if he tried.\n\n\nAs a result, many Black Americans have chosen to embrace an entirely new notion of heritage and identity, based on the global infusion of African culture into a worldwide diaspora. This could include elements of Caribbean, Creole, French-colonial, and Anglo-American influences, as well as pan-African culture (and Africa is a very big place, with wildly-divergent cultures, easily as different as Irish is from Greek, or Japanese is from Indian).\n\n\nOne example of this embrace of Black pan-culturalism is choosing or creating names that might sound exotic in any language. People who know the names of their ancestors might choose names that come from the same tradition. But when you don't know the names of your ancestors, or when you know their legal names to be \"fake\" names given to them by the people who bought and sold them like chattel, it's not so easy.\n\n\nIf you know something vague of where you came from, and that you are part of a diaspora that has influences the world over, you might choose to give your child a name that reflects that uncertain melding of cultures.\n\n\nIndian parents might name their kids \"Vijay\", Swedish-Americans might name their kids \"Gustav\", Japanese might name their kids \"Haruto\", Italian-Americans might go with \"Antonio\", etc...\nBut Black Americans descended from the nebulous heritage of slavery have no obvious tradition of forefathers to turn to, when it comes to naming their children, except maybe slave-names.\nSo many choose to invent or adopt new names, as the ancients did in other cultures. Just as names like \"Antonio\" or \"Robert\" or \"Seamus\" were once invented and applied to children, so names like Leshawn or Taniqua are invented or adopted by people who are not without a culture, not without a heritage, just without a fixed vocabulary, due to its newness.\n\n\nThe African diaspora has had a massive global influence on culture, but it happened in very different ways than other historically-recent diasporas. We were not around 1,000 or 10,000 years ago, when the Europeans or Africans were first inventing names.\n\n\nIn the great re-combinator that is global cultural evolution, Black America has emerged as a new distinct cultural tradition, much as Celts and Gauls diverged and became things like Scotch, Irish and German, hundreds of years ago.\n\n\nThe culture of \"Black America\", and of the African diaspora more generally, is still in its infancy. We're still in an era where people who lived under Jim Crow are alive and kicking, and the last slaves are only a few decades dead.\n\n\nAs people with names like Kanye, Obama, and Deshawn become more prominent and influential participants in the global economy of ideas, their names will begin to sound less strange. We are seeing the emergence of a new global cultural tradition, with ethnic and historical influences that are distinct from the existing ones.\nBlack American culture has a very troubled and difficult past, and much of it still has a troubled and difficult present, but its present is no worse than that of, say, the Irish from 150 years ago. (\"How the Irish Became White\" is an interesting read on the topic of historical race-identity).\n\n\nBlack America, and the African Diaspora more generally, is still in the process of inventing itself, as a cultural identity. And that includes names. It has contributed a tremendous amount of good to the world in its early days, and there is no reason to think it won't get better.\"","label":0,"model":"human","source":"reddit","id":2622}
{"text":"It was actually fought for. The amount of time spent at work was one of the major themes of labour\/union movement struggles in europe and NA, starting in the early 1800's. The industrial revolution brought with it 12 and 16 hour days in factories, and once people started organizing, this was challenged. \n\nOne of the most successful movements was the \"Eight Hour\" and \"Nine Hour\" Movements in the mid 1800's to early 1900's, and the results of their successes helped grow a new standard of 8 hours. Strikes and riots were originally the tools used to make change (for example, the Haymarket massacre in Chicago and general strike on Philadelphia), but laws enforcing hours were either ineffective or non-existent. The normalizing of the 8-hour day (at least in NA) increased over the late 1800's to early 1900's in the form of various legislation, political campaign promises and some industries taking the lead (most notably Ford).  Arguably the standard was near universal by the time of the US depression in the 1930's, when the \"New Deal\" was implemented to provide more regulation of labour and a safety net for workers.\n\nAlthough the 8 hour standard continued ever since, there are still problems to overcome, and efforts to improve to even lower hours. A significant problem is that, since the late 1970's, there has been a rise of precarious and part time work. Often workers in these jobs must work more than one job to earn a living, and as a result can often find themselves working more than 8 hours a day.\n\nThere have also been efforts to reduce the workweek, most notably in Europe, to 35 or 30 hours per week. Although these experiments have surely been a goal of labour movements, there has also been interest form a policy perspective and some employers have experimented with the idea (Sweden in particular).","label":0,"model":"human","source":"reddit","id":2623}
{"text":"There's quite a lot of misinformation in this thread so I'll jump in with an explanation.\n\n'Most' DRM schemes used to protect games work by scrambling (encrypting) the actual game program.  The program that you run therefore isn't the game itself merely a stub that performs the following:\n\n1. Check that this is a genuine game and the user is allowed to run it\n2. Decrypt the actual game program\n3. Run the actual game\n\nThere are many methods crackers use to break the protection but one is similar to the following:\n\n1. Install a genuine, licensed copy of the game\n2. Run the game allowing it to decrypt itself in memory\n3. Use a software tool to 'save' the unencrypted program code from memory to a file\n4. Make the program executable and remove all the software 'tendrils' that the DRM leaves behind\n\nNo. 4 tends to be the hardest part and can often be a cause of controversy within [The Scene](_URL_2_).  Sometimes cracks will be [nuked](_URL_1_) because they fail to meet the required standard by cracking groups.\n\nNote: There are a few DRM schemes that don't fall under this umbrella (such as Codemaster's [FADE](_URL_0_)).\n\nEDIT:  So I guess this \"blew up\" as they say.  Thank you for the gold mysterious stranger.\n\nEDIT2: Thanks for the comments but ELI5 is **not for literal five year olds**.   Neither is it for Comp Sci majors with too much time on their hands.  LI5 means friendly, simplified and layman-accessible explanations which means I may have taken a few liberties with some of my terminology but judging by the response I believe the correct meaning was conveyed.","label":0,"model":"human","source":"reddit","id":2624}
{"text":"So, here's what's fun. Whenever we experience something emotionally significant, it gets \"bathed\" in hormones that strengthen our memory of that event. \n\nThis is fantastic for fun memories of playing in the mud, baking cookies with Grandma, first kisses, weddings, babies. etc. But our brains don't differentiate. \n\nIf something really insignificant happens- but we have a strong emotional response to it- our brains still act accordingly. The hormonal \"bath\" strengthens the memory, making it easier to recall. This is part of the reason why we're so good at remembering embarrassing events, awesome moments, harsh break-ups, etc. They're all emotional, and they've all been reinforced by our brains' recall mechanisms. \n\nIt's not always terribly helpful for modern times, but you can imagine how effective it would be for our ancestors. \"OH SHIT, I'm being chased by a tiger again! What did I do last time this happened?!?\" -This is what our brains are trying to accomplish, but the end result is you recalling that time you farted in 3rd grade, next to a cute girl. As you're trying to fall asleep before a final exam. Brains are great.","label":0,"model":"human","source":"reddit","id":2625}
{"text":"Bookies do two things when arranging bets to make money. First, they try to make even odds so that there are equal numbers of people on both sides of the bet. Second, they make it so the odds don't quite even out and there will be money leftover for the bookie.\n\nFor example, say a bookie is taking bets on a football game between the Texans and Cowboys. If too many people are betting for the Cowboys, the bookie will adjust the point spread to encourage more people to vote for the Texans. They keep doing this until people are betting on both sides evenly. They also make it so that you have to bet something like $110 to win $100. Now let's say 10 people bet on the Cowboys and 10 on the Texans and they all bet $110. That's $2,200 total. The Texans win with the point spread. The bookie pays each person who bet on the Texans $210 - the original $110 back plus the $100 they won. That's $2,100 he has to pay out total. The bookie gets to keep the leftover $100. Put another way, each Cowboys bettor put up $110. $100 of that goes to each person who bet on the Texans for winning and $10 is leftover of the bookie.\n\nSince there are an equal number of bets on both sides, the math works out the same no matter who wins.","label":0,"model":"human","source":"reddit","id":2626}
{"text":"A common way is to use a nuclear reactor. The chain reaction produces a lot of neutrons. By construction a neutron transparent \"window\" into the reactor, some of the neutrons will escape through the window, in a crude beam. You can then place the experiment in the beam.\n\nSome experiments, can simply be done in the reactor - and a small test tube containing the experiment can just be installed into the reactor core.\n\nWhile fission reactors are low tech and can produce very large numbers of neutrons, they are limited in the energy of neutrons that they can produce. Many designs of nuclear reactor use a \"moderator\" to reduce the neutron energy for the chain reaction, and this will reduce the energy of the neutrons available for experiments. Even \"fast\" reactors without a moderator are limited to the maximum energy of fission neutrons.\n\nWhere faster neutrons are required, a fusion reactor can be used. However, while fusion reactors can be constructed extremely cheaply and compactly, this type of design has been limited to very low power and therefore very low neutron production rate. A new generation of advanced \"compact neutron source\" fusion reactors optimised for very high neutron production rates (_URL_0_).\n\nSome of the latest technology versions are so powerful and efficient, that they are often easier, cheaper and safer to use than fission reactors. Due to the risk of fission reactors being subverted for production of nuclear weapons, many governments are pushing for research institutions to replace their fission reactors with fusion sources. You can always add a moderator to reduce the neutron energy if the fusion neutrons are too fast for your experiment.\n\nFor experiments requiring neutrons which are faster than the maximum fusion neutron energy, then you need to use a particle accelerator and a \"spallation target\". A proton accelerator is produce a beam of high energy protons - which are directed at a target made from a heavy metal. The protons hit the nuclei in the target and smash off neutrons. These can have energies as high as the proton beam energy.","label":0,"model":"human","source":"reddit","id":2627}
{"text":"Socialism can mean several different things depending on context.\n\nFormally, socialism is an economic system where the production of stuff (eg food, housing, and \"commodities\") is controlled by the people doing the producing. This is opposed to the current system where a small class of wealthy individuals owns controlling interest in production, and a majority of people exchange work for money paid to them by the owners.\n\nIn practice \"socialism\" encompasses everything from state ownership of all land and productive material on one end of the spectrum, to some state intervention into the market economy in order to alleviate the pain of capitalism on the other end of the spectrum. The \"some state intervention\" is the common practice in developed economies, and fundamentally all mainstream political debate takes place in that range of action.\n\nMost people worldwide don't hate either the concept of socialism or socialism as currently practiced. There is a hard dislike of the term socialism in the United States, possibly as a legacy of the cold war, possibly because no mainstream politicians defend it. The \"socialist\" policies in place in the United States are very popular, however. Think Social Security, the VA, graduated income taxes, etc.","label":0,"model":"human","source":"reddit","id":2628}
{"text":"There is no new pigment, but just how the blood looks *through* your skins pigment. Bruises can look different depending on where they are, how deep beneath the skin, and how severe the bruise is.\n\nBruises happen when you bleed without breaking the skin. Sometimes with more forceful trauma, like twisting your ankle, you bleed in the deeper tissues and the blood gradually seeps into your skin over a longer period of time (hours to days). \n\n**Red Bruises**\nWhen you first get a bruise \u2014 especially one near the surface of your skin \u2014 it usually appears red. The color comes from fresh blood leaking into your tissues. Fresh blood is bright red because it contains both iron and oxygen.\n\n**Blue Bruises**\nWithin a few hours, blood that has leaked from your injured blood vessels loses the oxygen it was carrying. As this occurs, the blood becomes darker and your bruise begins to look more bluish or purple.\nNote that if you have a deep bruise, the red stage may have already passed by the time you are first able to see the bruise. So the first color you see may be a bluish purple color.\n\n**Purple Bruises**\nTypically, over one to three days (depending in the severity of your injury), a bruise becomes more intensely purple and may even appear black. This occurs as red blood cells break down and iron is released into the injured area.\n\nWhen red blood cells break down, they release an iron-containing protein called hemoglobin. As your bruise begins to heal, your body converts the hemoglobin into other colored chemicals. The presence of these chemicals causes your bruise to change color as it heals.\n\n**Green Bruises**\nYou\u2019ll know your bruise is beginning to go away when you notice it turning green. You\u2019re likely to first notice the transition from purple to green at the edges or center of a bruise. The green color is due to the presence of a hemoglobin breakdown product called biliverdin. The last part of the word, \u201cverdin,\u201d comes from the Latin word for green \u2014 making it easy to remember (and impress your friends).\n\n**Yellow Bruises**\nAt long last, your green bruise will eventually turn yellow as it enters the final stage of healing. The yellow color is from the final breakdown product of hemoglobin in your skin, a chemical called bilirubin. The yellow fades as your body clears away the last of the debris from the bleed, leaving you with bruise-free skin that is none the worse for the wear.\n \n\n[This](_URL_0_) is where I got the information.","label":0,"model":"human","source":"reddit","id":2629}
{"text":"Other people have already pointed out the cost and radiation exposure.  Another factor to consider is incidental findings.  If take 100 people and give them all a full body scan, you'll probably find some weird-looking thing in at least 20 of them.  This guy has a thing on his kidney which could be a benign cyst or it could be a cancer.  This lady has some stuff caked onto the ovaries, could be ectopic endometrial tissue but could be cancer.  This guy has a lump in his abdomen, could be a meckel's diverticulum but it could be a cancer.  Maybe two of the people with a weird finding actually have cancer, and one of them is probably too far along for it to be helpful to have found it now anyway.  Meanwhile, the other 18 people who turned out not to have cancer just ended up having unnecessary surgery.  With those 18 unnecessary surgeries, two had severe post-operative complications, one of whom died, two are now pooping into a colostomy bag, one is impotent, one is infertile, and two now have chronic pain.  So overall we've maybe saved one life, but also lost one life and substantially messed up several others.  Overall, it appears that this group of 100 people is worse off for all having gotten a scan.","label":0,"model":"human","source":"reddit","id":2630}
{"text":"The estimate you get from google is based on someone driving the speed limit over the suggested route.  It can also take into account things like current traffic and weather conditions.  \n  \nThe fact of the matter is that unless you are driving a relatively large distance, speeding doesn't actually get you there all that much faster. So the estimate is still fairly accurate.  \n  \nLet's say for example that you are driving 20 miles to work.  Let's also say that 16 of those miles are on the freeway. We will also say that you drive 25% faster than someone who is following the speed limit. \n  \nThe equation we use is Rate * Time = Distance.  \nBut we want time so we will be using time= distance\/rate. This will get us fractions of an hour. Then we multiply the result by 3600 to get the time in seconds. \n\n**Driver A drives the speed limit:**  \n(2\/35) * 3600 = 205 seconds to get to the freeway  \n(16\/65) * 3600 = 886 Seconds on the freeway  \n(2\/35) * 3600 = 205 seconds to get to work after exiting the freeway  \nSo we have a grand total of 1,296 seconds.  Or 21.6 minutes.  \n\n**Now for driver B driving 25% faster**  \n(2\/44) * 3600= 163 seconds to get to the freeway  \n(16\/81) * 3600 = 711 Seconds on the freeway  \n(2\/44) * 3600 = 163 seconds to get to work after exiting the freeway  \nSo we have a grand total of 1,037 seconds.  Or 17.2 minutes.    \n  \nSo if everything went perfectly (you hit every green, no slow drivers, etc) you got there about 4.5 minutes faster than estimated. Still well within an acceptable error margin.  Also, it is pretty unlikely you would hit the best case scenario.  You would also need to be driving 25% faster the WHOLE route.  No slowing for lights, offramps, slow traffic, traffic jams, police, etc.\n  \nThese estimates also get updated by google as they receive data about how long a specific route ACTUALLY took.  Then they use mathematical algorithms to adjust the estimates based on historically how long it actually takes people to travel on those routes.\n\nEdit: Updated to the correct equation.  Thanks to Kstingrays","label":0,"model":"human","source":"reddit","id":2631}
{"text":"_URL_1_\n\nwatch all 3 episodes; it's literally about answering the question you posed.  These two paragraphs from the wikipedia article sum up the biggest factors:\n\n_URL_0_\n\n > The first step towards civilization is the move from nomadic hunter-gatherer to rooted agrarian society. Several conditions are necessary for this transition to occur: 1) access to high protein vegetation that endures storage; 2) a climate dry enough to allow storage; 3) access to animals docile enough for domestication and versatile enough to survive captivity. Control of crops and livestock leads to food surpluses. Surplus frees people up to specialize in activities other than sustenance and supports population growth. The combination of specialization and population growth leads to the accumulation of social and technologic innovations which build on each other. Large societies develop ruling classes and supporting bureaucracies, which in turn lead to the organization of nation-states and empires.[2]\n\n > Although agriculture arose in several parts of the world, Eurasia gained an early advantage due to the greater availability of suitable plant and animal species for domestication. In particular, Eurasia has barley, two varieties of wheat and three protein-rich pulses for food; flax for textiles; goats, sheep and cattle. Eurasian grains were richer in protein, easier to sow and easier to store than American maize or tropical bananas.","label":0,"model":"human","source":"reddit","id":2632}
{"text":"The difference between the colors observed during a sunrise and the colors observed during a sunset is most likely caused by differences in the ~~density and temperature~~ composition of the atmosphere. If atmospheric conditions are held constant, the colors during a sunrise should in theory look identical to the colors during a sunset.\n\nLight appears redder during a sunrise\/sunset because the light from the sun must travel a greater distance through Earth's atmosphere to reach us ([graphic](_URL_3_)). Earth's atmosphere preferentially scatters short-wavelength light (e.g., blue light); the more atmosphere the light must travel through, the less blue light will reach us ([graphic](_URL_0_)).\n\nBut then why aren't sunrises just as red as sunsets? ~~This is because the density of the air tends to be lower in the morning than the density of the air in the evening. As the day progresses, the heat from the sun increases the humidity of the air (i.e., increases the number of blue-light-scattering water molecules in the air). Pollution also increases during the day as the number of active power plants, factories, cars, etc. increases. As a result, the density of the air is often much higher in the evening; and, consequently, more blue light from the sun is scattered, leaving a much higher proportion of red light.~~ Edit 2: Evening air often contains more particles than morning air. This is potentially due to changes in atmospheric conditions caused by thermal radiation from the sun and\/or due to increased human activity. The increased number of particles in the air scatters even more blue light, making sunsets appear redder.\n\nEdit 3.1: Many users have mentioned that there could be a subjective aspect as well. And I agree. It's certainly plausible that certain biological and\/or psychological mechanisms could influence or exaggerate our perceptions of visual phenomena like sunrises and sunsets. u\/Gonzo_Rick most accurately [summarized](_URL_1_) this part of the answer:\n\n >  When light hits the retina, it sends neural impulses directly to sleep regulating parts of your brain ([more specifically, the suprachiasmatic nucleus]) that wake you up. Blue light in particular elicits the strongest reaction. Coupled with the fact that [the concentration of active rhodopsin (the night-vision protein that drastically increases your sensitivity to light) in the rod cells of your retina is probably much higher], that blue light might seem more piercing and prominent than other wavelengths in the morning.\n\nEdit: u\/positive_root gave a more accurate [explanation](_URL_4_) regarding the struck-out portion of the third paragraph, which I have added below. Thanks for clearing that up!\n\n >  Holding pressure constant, atmospheric density is a function of temperature. So in the morning the atmosphere is cold, has multiple stable layers, typically much higher density than in the evening when convection is still settling down. Also high humidity typically leads to haze particles, large enough to be [Mie scattering](_URL_2_). You say \"blue-light-scattering water molecules\" but all relatively un-clumped molecules with a dipole [scatter blue light](_URL_5_) preferentially, most notably nitrogen, the main constituent of the atmosphere.\n\nEdit 3.2: ~~~~~~~~~~~~~~~ TL;DR ~~~~~~~~~~~~~~~  \nSunrises and sunsets are both caused by the scattering of short-wavelength (e.g., blue) light by particles in the air. In the evening, there are usually more particles in the air than in the morning. The increased number of particles in the evening causes even more blue light to be scattered making sunsets appear redder. Sunrises may also appear bluer because our eyes are usually more sensitive in the morning and our brains are most sensitive to blue light in general.","label":0,"model":"human","source":"reddit","id":2633}
{"text":"Most it is due to the history of data compression on the PC.\n\nThe first data compression programs appeared in the late 1980s, and through the 1990s there a number of competing standards, zip, rar, lha, and arj among them.  Zip emerged the winner and became the de facto standard for a while. \n\nAbout that same time, the inventor of the zip format, Phil Katz started to have mental health and substance abuse issues, and the program was no longer being improved.  PC's transitioned from command line DOS to GUI Windows, and rar beat zip at making the jump by making WinRAR, the first widely used compression program for Windows.  This allowed RAR to get back into the race, and both WinZip and WinRAR remained popular.  \n\nHowever, the rar algorithm was patented, while Katz had opened the zip format to everyone.  As file compression started to become embedded into other programs, zip regained the crown once again.\n\nThings didn't change much until 2008, when Igor Pavlov released 7zip.  It was a lightweight, but powerful program that handled all common compression formats, and offered a new format, 7z, with improved compression ration.\n\n >  Is there a meaningful difference between them from the end users perspective? \n\nNot really.  WinZip and 7zip are free, WinRAR is commercial, but you can get fully functional \"trial\" version for free...no one ever pays for it except corporations.  The zip format is more widely used, rar might be what you have at work, and most people think 7zip is currently the superior program.","label":0,"model":"human","source":"reddit","id":2634}
{"text":"It has to do with what the purpose of a Constitution is and what questions it was created to answer.  In many countries, especially in Europe, the question was \"We have a country, how do we govern it?\"  That is, the existence of the country is taken as a given and the constitution simply describes how the government should be organized.  In countries like these, when there's enough unrest that they decide to adopt a new constitution, those in power get to stay in power (mostly) and the government can hobble along until it can regroup.\n\nThe United States is different.  When the 13 (former) colonies wrote the US constitution, the question was \"We need to create a country, what form should it take?\"  The United States of America DID NOT EXIST as a nation until the constitution was ratified and crated it.  In a sense, you could say that the constitution IS the United States of America.  The US constitution can be changed via the amendment process, but if the USA ever decided to throw it out whole cloth, the federal government would immediately lose all power.  The President, Congress, the Supreme Court, and all of the supporting institutions would lose all legal power they had and all government responsibilities would revert to the States until (and if) a new constitution could be ratified.\n\nThe reason that the US constitution is treated as sacred text is because in a sense it is.  The government is not allowed to pass any laws that contradict it (and it's the Supreme Court's job to enforce this), and if they ever tried to throw it out they would all immediately be out of a job.  It's the legal backbone that holds up the entire federal government.","label":0,"model":"human","source":"reddit","id":2635}
{"text":"You may be working off an incorrect assumption about what a floodplain is and how probability works. \n\nELI5: it isn't called a 100 year floodplain because it is a guaranteed flood every 100 years. The 100 year floodplain is an area near a source of water like a river or creek that has a 1% chance to flood each year. So it is commonly thought that over 100 years it will flood at least once. However because it hasn't flooded in a plain for the last 99 years does not mean it will flood next year, a flood is just as likely to happen in the 100th year as it is year 1. \n\nWhy do people build there? The same reasons people build anywhere - because the land fits some criteria of theirs like location, value\/cost, sentiment, idiocy. It is a calculated risk as well, there may always be that chance of a flood, but there's always the chance of a tornado, hurricane, blizzard, tsunami, nuclear reactor meltdown, etc... You pick your fights and take the risk the same as when you get out of bed in the morning or pick which ninja turtle is your favorite.\n\nYou should check out some YouTube videos on probability if you want  someone to explain it better than I can here. The most common example is flipping a coin and the probability that it will be heads or tails and how just because it has been heads 99 times in row means absolutely nothing when you flip it the 100th time. \n_URL_1_\n\n_URL_0_\n\nEdit: Thank you for the reddit gold, kind stranger! To think I almost SAPd out and didn't post this. I'm just a guy who has worked with HUD and other government property\/home ownership programs and knew a thing or two.","label":0,"model":"human","source":"reddit","id":2636}
{"text":"The propaganda is mostly for domestic consumption: that is, the people of North Korea themselves should be convinced (\"brainwashed\" would be a better description of what this kind of propaganda does) that all is absolutely wonderful.\n\nBut don't think for one moment the people of North Korea are taken in by it. But North Korea has an immensely powerful army and a very oppressive regime, so people are unlikely to speak out. The execution of Kim Jong-un's own uncle served to show that even the Dear Leader's closest relatives aren't safe.\n\n**EDIT:** Okay, okay, I should have said North Korea has a *large* army. Do people just hit \"reply\" before reading any of the replies? And I'm not trying to suggest that the North Korean military could win a war; I'm trying to suggest that the North Korean military can successfully intimidate the population. The power that the military has -- at least as I see it -- is actually political more than military, a point I didn't make clear. The way I understand the facts (such as they are), I think North Korea looks like a military dictatorship with a puppet leader. It may not be a very good military, but it's running the country. **EDIT ENDS**\n\nStill, the people of North Korea actually have access to South Korean media (not officially, and probably illegally as far as I know, but access all the same) and are clearly aware of what's going on. We saw the same sort of thing in eastern Europe during the Cold War.\n\nBut there is a huge element of self-delusion in play as well. I think it's pretty much accepted that Kim Jong-un is not actually in charge: he's the puppet king. The real power behind the throne is the military, which is immensely powerful and has all the privilege. Kim himself is very likely unaware of this: he probably genuinely believes that the country is rich and all the people love him.\n\nThe last communist ruler of Romania, Nicolae Ceau\u0219escu, was in a similar position. Almost exactly 25 years ago -- on 21st December 1989 -- he gave a speech to what he thought were the adoring crowds in front of the palace. His face when he realised the crowd had turned against him is one of the defining moments of the collapse of communism in Europe: on Christmas Day of that year, he and his family were court martialled and immediately executed.\n\nRegarding North Korea, it's worth pointing out that a lot of the stories of mass starvation may be exaggerated. There are clearly shortages, but as secretive as the country is, the most dramatic stories come from people with the opposite political agenda: North Korean dissidents and South Korean agents who want to discredit the North Korean regime. For example, it's very likely true that Kim's uncle was executed: the story that he was executed by being torn apart by dogs almost certainly isn't (it would have been by firing squad).\n\nSame with stories of mass food shortages. Certainly there isn't much in the way of luxuries, and certainly it is likely to be difficult to get hold of supplies; the evidence, as scant as it is, is that people manage to feed themselves surprisingly well, even if they do have to resort to the black market.\n\n**VERY IMPORTANT EDIT** For the love all that's holy, I know Kim is said to have studied in Switzerland; please stop telling me this. *Said* to have studied in Switzerland: there is no really good evidence for that, although it seems very likely. But guess what? Haven spent time in Switzerland doesn't mean he knows what life is like for the ordinary North Korean. He may know what westerners *say* life is like for the ordinary North Korean, but he'll have been shielded from ordinary North Koreans all his life and all he likely knows is that what people in the west say about life for ordinary North Koreans doesn't square with what he's been shown, and certainly doesn't square with what he experienced himself. It's not like the people who really run the country are going to give him guided tours of roadsides littered with corpses -- even if they do exist.\n\nLESS IMPORTANT EDIT: But thanks for the gold, whoever you are. Sorry, the notification got buried under all the \"Kim went to Switzerland\" replies.","label":0,"model":"human","source":"reddit","id":2637}
{"text":"It really depends, because web sites can actually have layers\u2014like pie!\n\nA simple web site can be made of stuff that doesn't really change. Like [my dumb little homepage](_URL_7_)\u2014it's the pie equivalent of a tin pie plate full of whipped cream: just some html files and some images. No databases, no programs, no fancy forums or really anything to do but click some links. All things being equal, sites like that are pretty resistant to being hugged to death, because there's not much to them. The web server just has to send the page and the page's image and maybe a tiny bit of CSS (and maybe a font or two). Modern web servers are very, very good at serving simple pages like that made up of things that don't change\u2014they can cache them and send them out to lots of clients at once. They can generally do that for as much bandwidth as you've got available.\n\nMore complicated sites have more layers. Like Reddit! If my dumb little homepage is a whipped cream pie, Reddit is a 12-layer key lime+cherry+apple+turducken pie. Putting aside how many actual physical servers reddit uses (it's a lot), Reddit is a collection of lots of different applications working together. A web server application serves up the static stuff\u2014the unchanging boring HTML and CSS, some of the images\u2014and a couple of different databases work together to summon up the kind of stuff that changes a lot\u2014like the comments. Other applications handle things like thumbnail scraping, making sure votes get recorded, making sure everyone reading threads can see votes that get recorded, and tons of other tasks that make reddit look and act like reddit.\n\nOf course, reddit can't hug reddit to death, but reddit is actually a relatively simple site\u2014though it's enormous. But reddit's popularity means that when a link gets voted enough, it can send hundreds, thousands, and sometimes hundreds of thousands of visitors to an unsuspecting, unprepared site.\n\nIf the site is simple like my dumb homepage, what happens is that the web server spits out pages as fast as it can, which is often fast enough to keep up with the demand\u2014until things either stabilize or the site \"runs out\" of bandwidth. That's when so many people are clicking on it that the network connection going to that server is full. When that happens, you'll generally see errors generated by your own web browser, because it can't reach the site.\n\nMore complicated sites\u2014like an image gallery that uses some php to generate the moving bits, or a neat visual web application that some person wrote where you can click on the screen and draw pictures, or whatever\u2014fail in different ways as they get busier. Usually, the underlying web server is fine, but the appliction part gets overwhelmed and runs out of memory, or the server's disks can't keep up. Sometimes, if the application uses a database like MySQL, the database isn't configured to handle the load (maybe the admin never expected the site to get so popular!) and it spits out weird \"too many connections\" errors rather than serving up its queries to the application.\n\nGenerally, in that case, you'll see \"gateway errors\" or \"internal errors,\" which is the web server telling you that something's wrong with the application side and it's very sorry but it can't do what you're asking it to do within the period of time it's supposed to wait (the timeout period). Since the timeout period has elapsed, it's giving up and apologizing.\n\nThese kinds of problems occur when there might be plenty of bandwidth available still to the server\u2014but some component of the web app is overloaded.\n\nSometimes\u2014particularly when you're dealing with a web site that's not a typical application like a forum but instead something weird and cool (like, say, a neat graduate school CS project that does fancy visualizations)\u2014stuff does indeed actually crash, rather than just time out and give up. The kinds of errors you see when that happens can be all over the map, from regular 404 \"not found\" errors all the way up to awesome cool debug pages you're not supposed to see unless things go really, really wrong.\n\nGenerally, when reddit hugs a site to death, you're seeing something related to a dynamic part of that site\u2014a database or an application\u2014breaking under load, and the errors you get back are generally from the static web server beneath the application telling you that the dynamic part of the site is broke (though generally not in a way that makes any sense to anyone who's not a sysadmin).\n\nSometimes, it takes manual intervention to fix the problem, which is why sometimes reddit hugs a site TO DEATH and it's gone for minutes or hours\u2014the person who made the page has to restart an application or reboot the server. Other times, particularly if the problem is just bandwidth-related, it'll fix itself as things stabilize.\n\n**tl;dr** - web sites have layers (like pie!) and usually the more complex the site, the more layers. When reddit hugs a site to death, it's because one of the layers (maybe a database, maybe an application, maybe even something on the server itself) ran out of resources and stopped responding in a reasonable amount of time. Sometimes something crashes, but not all the time. Internet\u2014it's complicated!\n\nedited to add - grammar brain fail. too much pie.\n\nEdited^2 - in case anyone is wondering what effect this is having on my dumb little web site, here's some stats as of about 11:40am CDT on July 4. [Overview](_URL_12_), [per page](_URL_0_), [traffic flow](_URL_8_), [referrers](_URL_1_). (This is from my [Piwik](_URL_6_) installation, which is a free self-hosted alternative to Google Analytics.)\n\nIf you want to see something neat, rather than just my dumb homepage, [check out my noir-styled Elite Dangerous fan-comic \"Fangs,\"](_URL_4_) hosted on the same server.\n\nEdited^3 - I'm being asked what the web server is behind my dumb web site. [Here's an explanation](_URL_2_). It's a home-built i7 quad-core with an SSD and 16GB of RAM, running Ubuntu 14.04 server. The web stack is [HAProxy for SSL termination](_URL_11_) - >  Varnish for caching - >  Nginx.\n\nEdited^4 - Here's some updated stats from about 13:45 CDT. So far, this post has thrown about 8400 unique visitors to my dumb homepage. [Page view stats](_URL_10_). [Total stats](_URL_3_). [Visitor breakdown by hardware](_URL_9_). [Visitor breakdown by OS  &  browser](_URL_5_).","label":0,"model":"human","source":"reddit","id":2638}
{"text":"The best *short* answer to this question is \"maybe, but probably not.\" The more appropriate answer is \"there are people who have dedicated their lives to studying the impact of wages on inflation and they still have a ways to go.\"\n\nDepending on the basic set of assumptions you would like to use to start, wages, prices, and inflation can all be expressed as linear functions of eachother. However, the actually impact of adjusting wages like this would be a litte *weird.* \nAs an economist, it's almost impossible to tell what would happen if we suddenly changed minimum wage to $15. we aren't good at predict that. What we *are* good at doing is predicting what will happen given a unit-change in wages. This distinction is vital. We can (approximately) tell you how much prices and inflation will adjust if you increase wages by one monetary unit, but after that adjustment the world is a different place so trying to estimate the effect of this kind of wage hike is really *really* tough.\n\nWhat is for sure: the labor market would respond, price levels would change, inflation would change, and there would be large-scale adjustments to individuals' preference structures with respect to certain types of goods as well as their incentives to work.\n\nPersonally, the last point is the most interesting to me as it plays on the inherent short-sightedness in people. Let's take a closer look at what I mean. When an individual sets his or her choice of optimal labor supply, price levels, wages, and expectations for future discount of savings\/investment\/Yada Yada come into play. The effect of an increase in wages like this is almost certainly nonlinear as the rational person would realize that inflation would likely catch up to them in the future vis-a-vis prices and interest rates. But, right now they are ahead of the curve so they adjust consumption and savings in the current period to maximize utility based on a belief about the distribution of effects from the wage hike. In every period after that, using Bayesian updating to adjust preferences and expectations based on the outcome of the last period, rational individuals would continue to adjust their labor supply, consumption and savings decisions all the way until the n^th period. It's quite possible that, when we expand this our to infinity, we actually see no change in the individual's utility stemming from this wage hike, because the shock was isolated and the effects drowned out in time.\n\nI'm just rambling on my phone at this point. Suffice it to say that the answer to your question is impossibly difficult to answer. But the odds of the increase resulting in things returning to equilibrium is *very small*. After all, things have never returned to their original stead states after any major economic event. The economy simply finds a new direction, pattern, and speed with which to baffle economists and torment graduate students.\n\nSource: piled higher and deeper in economics,  \n\nEdit 2: Truly humbled for gold. Thanks anon!\n\nEdit 3: Double gold. I love you guys. Thank you so much. \n\nEdit 3.5: I'm going to rewrite the post to fit ELI5's amazing format. Thank you to everyone for the critique","label":0,"model":"human","source":"reddit","id":2639}
{"text":"There are several possible reasons:\n\n- Timing. It may be a medicine you have to take throughout the day (like every 8 hours for example) rather than all at once.\n\n- Unusual use. Some medicines can be used to treat problems they weren't originally developed for. This may require an uncommon dosage.\n\n- Insurance. Insurance is *really weird* and may only pay for a certain size of pill. They may agree to pay for 10mg pills but not 20mg pills, so if you need 20mg a day, the fastest and easiest way to get around insurance shenanigans is instructing you to take 2 10mg pills instead of 1 20mg pill. Otherwise, your doctor will have to write your insurance a letter explaining that yes you need this and they should cover it or you'll have to pay out of pocket for it. Usually the pharmacy will contact the doctor and ask for permission to substitute.\n\n- The pharmacy is out of the larger size. This happens from time to time. Again, they'll clear this with the doctor first.\n\n- It just doesn't exist in a bigger size. Drug companies make their medicines in commonly used dosage types. Smaller is generally better. You can usually just take more pills if you need a bigger dosage, with some exceptions, but it's harder when you need a smaller dosage and only bigger pills exist.\n\n- Bigger pill sizes may be available only with a prescription, but the smaller ones can be bought over the counter. Ibuprofen (Advil) is an example of this. You can buy 200mg pills over the counter, but 600mg and up require prescriptions. In this case, the pharmacist can't give you the bigger pill unless you have a prescription from your doctor saying you need it. But giving you smaller ones is a-ok.\n\n- Bigger pills can be a pain to swallow. Going back to the ibuprofen example, if you've seen the larger sizes, they're huge. Swallowing them can be difficult, especially if the patient is a child or elderly, so going with smaller pill sizes or the medicine in a liquid form would be better for them.\n\nThere's probably more that I didn't think of, but these are really the most common.\n\nSource: Former pharmacy technician.","label":0,"model":"human","source":"reddit","id":2640}
{"text":"The baby boomers by sheer number create the largest economic impact on the American society. They will for more then twenty years define our political and cultural landscape. \n\nThey are the demographic to be milked out of existence by the carpetbaggers of big business. The sharks that brought you reverse mortgages and are busily trying to convince the world that you do not owe your children anything and a 50k automobile makes a perfect Christmas present.\n\nOf any American generation the boomer has been the recipient of the very best in educational and social programs available in this nations history, they received the full benefit of the post depression safety net, bought and paid for by the blood of their fathers in places like Normandy and Okinawa. \n\nNow that they have capitalized on that fortune they no longer feel the need to pass those programs on to future generations. The baby boomers hit political  majority in in the early eighties and began bleeding the system by refusing to increase budgets to mirror inflation, effectively cutting those programs to the bone.\n\nIn effect those actions directly made today\u2019s society of  undereducated, easily manipulated and increasingly disenfranchised voters of today.  We now balk at raising the minimum wage in favor of cheap hamburgers, refuse to acknowledge that while welfare abuses happen the majority of people helped need the help. As a country we refuse to honor our grandfathers by providing a full spectrum and sound education to all children.  And that lack of education leads us to wonder why people seem to want to kill the babyboomers.\n\nNo one wants to kill them. They are an entry in an actuary table. Their existence, like most others is going to cost far more then it took in.","label":0,"model":"human","source":"reddit","id":2641}
{"text":"Governments typically take on debt by issuing **bonds**. Bonds last a specific amount of time and pay out a particular amount. For example, you might buy a 1-year government bond at a price of $100, for an interest rate of 3%. The government is essentially promising that after 1 year, they'll pay you $103. Bonds often last a lot longer than a year, there are 5-, 10-, even 50-year bonds. The longer the timescale, the higher rate of interest.\n\nSometimes a government directly takes on a loan from a major bank or some other financial institution. And that's a loan like any other, the lender gives you cash up front, you're expected to gradually pay it back, with interest.\n\nIn America, the vast majority of government debt is owned by American citizens. Usually individuals but also corporations and banks. There is a small amount that is owned by foreigners, either foreign individuals\/corporations, or by foreign governments.\n\nThe consequences of government indebtedness can range from trivial to catastrophic, depending on the type of debt, the creditworthiness of the government, and the size of the debt. A lot of economists say government debt that is lower than 110% of annual GDP is healthy and manageable. Anything higher than that can become a problem.\n\nOne major consequence of course is that the government has to pay the debts back! And that can eat up a lot of a government's budget, which means less to spend on other things like the military, services, education, or the social safety net. The US government currently spends about 6% of its annual budget on paying back the debt. Some governments spend a lot more than that. One could argue that even at the relatively low level that the US is paying, it's still a bad thing because the government is paying all this interest to investors, which is essentially money going straight from the taxpayer to rich people and big banks. But many would say that the benefits of borrowing money to finance government spending are worth it.\n\nNow, in more severe cases, government debt can be a much bigger problem. For example Greece. Greece is so heavily indebted, and its creditworthiness is so low, that the lenders willing to loan them money to bail them out demand stiff cuts to spending as well as the selling off of government assets to pay for debt service. This is very bad for Greece, and has caused a large-scale economic contraction there that has lowered living standards considerably.","label":0,"model":"human","source":"reddit","id":2642}
{"text":"An itch is caused when a very small object (such as dust, a small bug, a fiber of clothing or hair) comes into contact with your skin. The small object, which need only be a few microns in length for you to feel it, stimulates a small number of pain receptors on your skin. When only a very small number of pain receptors are stimulated, it causes you to have an itching sensation rather than a pain sensation. If a large number of pain receptors are stimulated, it causes you to feel pain. Itching and pain use the same neural pathway but elicit opposite responses: itching causes you to scratch (to remove the stimulus from your skin) and pain causes you to retract from the stimulus (to escape something dangerous). \n\nNature has crafted an elegant solution if you think about it. Using the same pathway is efficient, and the best way to get rid of very small objects is to remove them from the skin but if a large number of pain receptors are activated, it is likely that the stimulus is too large to remove quickly, so jerking away from the stimulus is a better response.","label":0,"model":"human","source":"reddit","id":2643}
{"text":"Because that's how the universe works.\n\nTo really understand this, you have to understand that when you \"sit still\" you're still moving. You're moving through time. How do you know? Because if you sit still for a minute you reach one minute into the future of when you started sitting there. If you weren't moving through time you would just stay at that moment forever. That doesn't happen, so you must be moving through time.\n\nNow, let's say you and I are sitting still together and you decide to stop sitting still. You start moving forward. You are now moving a little bit in space, but you're still moving in time as well. Here's where it gets weird, and if you don't want to get into some mildly complicated math you have to take my word for it: you're always moving the same total speed. That speed is the speed of light. When you were sitting still you were moving at the speed of light through time. Once you started moving, some of your speed went into moving forward, which left a little less for moving through time. This means that while I'm still going one minute into the future every minute, you're not\u2014if I look at your watch when my watch says its been one minute, then your watch will say it hasn't been quite a minute. Now, the speed of light is *really* fast, and you probably aren't moving forward very quickly, so you only needed a little of your speed to move forward and most of it is still going through time, so our watches are probably still pretty close. As you start going forward faster, though, more of your speed is going into that so you have less to move through time and our watches start to be very different. So, what happens as you get close to moving forward at the speed of light? You get close to not moving *at all* through time. My watch says a minute, an hour, a day, a year have gone by while yours says it's been less than a second. If you ever actually got to the speed of light (you can't), then you would not be moving through time at all and I would see your watch just stopped as you flew off at the speed of light.\n\nNow, you're moving forward at the speed of light and you want to go forward faster. That's too bad; you always move at the speed of light, and you don't have anything left to borrow from your movement in time.","label":0,"model":"human","source":"reddit","id":2644}
{"text":"Here's something I know a bit about!  I too am appalled at the state of the Netflix UI - only I extend that to the PC as well.  It's one of the worst user experiences ever created.  It actually devolved from something that was more or less tolerable many moons ago.\n\nNetflix became very popular in the Machine Learning crowd because of their almost magical recommender engines.  They are well respected because of the algorithms that say \"You would like Terminator 2 because you watched The Matrix\" and so on.  These algorithms segregate movies into niche categories (automagically).  The company also gained a lot of cred based on the way they manage their HUGE networks of computers.  Long story short, they became an engineer centric company.\n\nA while back they discovered a programming paradigm called \"Functional Reactive Programming\".  This is a simple and old way of programming that people are suddenly rediscovering and repackaging.  It's a way of dealing with streams of data.  In this case the data are requests (\"What should I WATCH???\", \"What dark-romantic-vampire-comedies would I enjoy?\", etc) and the responses to those questions.\n\nWell Netflix had such great \"success\" (the engineers loved the way it made their code simple and readable) that they rewrote everything around this paradigm.  Since they are an engineering centric company, this meant that the UI was rewritten around these changes.  As a backend engineer I can tell you that if I influenced my company's user experience directly it would be terrible, because I care about code and not at all about the user.  Users are dummies.\n\nThat \"Users are dummies\" mentality is no joke.  The popularity that Netflix has due to automating away their entire business only feeds their philosophy.  If computers are so much better at deciding what people want to watch than the users themselves... why bother giving them tools to search and browse the library?\n\n1. TL;DR - Engineers are calling the shots.\n\nAlso:\n\n2. If you have full access to browse the selection (like you used to) you would realize their selection isn't very good, very large or very impressive.  Their current system masks that fact very well.\n\n3. To their credit, the genres are now created mostly by their algorithms.  They don't have a hard-coded, unchanging grid of \"movie categories\" like most businesses would have.  It's a massive, many-dimensional, many-clustered, always-changing, relationship centric sort of dataset.  (Sorry not ELI5)\n\nEDIT: grammar 'n stuff\n\n**Addendum**\n\nI had no idea this would create such a stir.  No, I do not *truly* believe Netflix dismisses users or that the designers are not trying.  My wording was lazy when I said the UI was rewritten entirely based on an old programming paradigm - but not nearly so drastically as might be expected.   [Here, the Cross Team tech-lead for the Netflix UIs describes](_URL_2_) a variation on a \"dated\" pattern from GoF, and then excitedly shows how the front-end code has changed significantly as a natural repercussion.  They weren't merely adapting existing elements, REST calls and event loops to match a new endpoint.  They took advantage of the fact that they were less constrained to provide what feels like an infinite swipe (Ring buffer).  This design decision came from functional prototypes from the API team, not the other way around.  [Here](_URL_1_) and [here](_URL_0_) are two similar videos from Netflix.\n\nI love the company and have attended their talks in person.  They cannot please everyone.  My only grievance is that there is no technical reason that the Netflix curated experience cannot exist in parallel with fine-grained controls for filtering and freely exploring relations.  My opinions aside, the combination of clustered recommendations and an asynchronous stream-like API makes it dead simple to implement the uninspired carousel style priority list of genres with redundant entries, specifically on peripheral hardware.","label":0,"model":"human","source":"reddit","id":2645}
{"text":"He did get input. But his word was final. The military never actually likes Hitler as much as the general populace, if at all. They frequently voiced their disagreement, but were shot down. Paulus wanted to try a breakout at Stalingrad. Hitler said to hold ground and wait for a break-in that never came. On the morning of d-day, before the sun was up, Runstedt was awake and trying to take charge of the situation. He knew the airborne invasion was just a prelude to beach landings and wanted to mobilize armor to prevent them from establishing a beachhead, but high command refused to allow it without Hitler's approval, which he first denied then finally came at 14:30 the next day, by which time the allies were fully established on the beaches. Even Rommel disagreed with Hitler's never retreat mantra, it runs completely counter to modern maneuver warfare, and they made their discontent known. Hell, it was the seasoned, high ranking generals who attempted to assassinate Hitler.\n\nSo, he got input, but often disregarded it, because Hitler was quite convinced that he knew best.","label":0,"model":"human","source":"reddit","id":2646}
{"text":"We can freeze cells now but they aren't organized tissues, just cells lines (immortal) and I'm not too sure if primary cells and be frozen or if there really is a point if they aren't immortal. When you freeze cells you have to give them some kind of broth that is rich in all the things needed to survive. You also need a good drying agent to pull all the water out of the cells so you don't get destroy the cells from the expansion of water when it freezes. Typically Dimethyl sulfoxide (DMSO) is used. The problem is DMSO is very toxic so you got to make sure you don't have too much DMSO in solution. I've mostly have used animal cell lines so when preparing a sample for cryopreservation I would use 90% Fetal Bovine Serum (FBS) and 10% DMSO. In human cell lines I'm not too sure if you can use FBS for culturing, at least you can't when used for biologics. The issue with freezing cells is it's very stressful and and not all will survive, enough will survive to propagate them again. Freezing an actual human with all the different cell and tissue types might be a problem since they may have different thresholds for how much of a beating they can take and since not all the cells will survive the freezing and thawing process the person might die. Even if 10% of all your cells die that could be a big problem. It's also really difficult to instantly provide all of the essential nutrients to all your cells cell lines are in direct contact with the culture medium. The DMSO that was used to dry your cells also needs to be removed from the frozen cells since it'll kill them and doing that in tissues will be more difficult. \n\n**TL;DR** It's probably not possible to cryopreserve an organism of any type due to the shock and methods we currently have.\n\nHope I didn't fuck anything up with my explanation.","label":0,"model":"human","source":"reddit","id":2647}
{"text":"Games actually send very little data in the grand scheme of things because rendering is handled locally so you can send small amounts of data and just handle the interpretation client side. It's like the difference between being told to draw a triangle over the phone rather than having to actually send a picture of a triangle through the mail to the other person. Also online games use interpolation and other techniques to hide the effects of delay or lag to the end user.\n\nEdit: Okay so for those wondering interpolation is when you have a set of data and you generate new data points that fit in with the existing data.  You use this in online games to make things look smooth because in reality the internet isn't. there's always delay(lag) when dealing with online among other issues like packet loss and data arriving out of order (packet 2 arrives before 1 does). If you tried to code a 1 to 1 experience then everyone would be teleporting around the screen all the time because by the time one movement confirmation is received it's already out of date you're really dealing with past information. You also need it because games don't update instantly the game logic is only updated in certain certain time steps (frame rate) and data is only sent out over the internet in certain time steps (tickrate).\n\nFor example, say in the previous update we know that a player was at the orgin (0, 0 , 0) but in the next hes reported as being at (0, 10, 0). We dont want to just move that player to (0, 10, 0) because it's gonna look like he teleported so we use interpolation to move him smoothly to this new location. The end user sees something more like this:\n(0, 0, 0) to - >  (0, 2, 0) - >  (0, 4, 0) .......... to it finally hits (0, 10, 0) and now it actually looks like the player moved more naturally to that position. \n\nAs for the other tricks I mentioned there's a ton. Well first there's just straight lag compensation. Interpolation makes things look smooth but they also need to feel smooth so you have to code the game so that it takes into account the lag that is happening and still provides as 1 to 1 of an experience as possible. Some games use movement prediction to try and hide lag. Have you ever played an online game when a player is lagging or disconnects and their player keeps walking into the wall? that's generally why that happens. Others use roll back techniques as well that work well in small cases. If push to far though you get snap back or rubber banding issues. even simple things like having hit registration in a shooter (the hit noise in halo or the hit markers in COD) make the game feel less laggy to the player.","label":0,"model":"human","source":"reddit","id":2648}
{"text":"Stoicism is mainly about learning to cope. Originally it included a whole theory of the universe about fate, determinism and free will, theology... it's been called fatalistic and pessimistic. But we can adapt the main philosophy to the modern day.\n\nStoics believe your reaction to events in the outside world is what controls your state of mind. So it's not that you're frustrated because your boss yelled at you. You're frustrated because you think she has no right to yell at you, or because you think she was unfair, etc. The point is your own thoughts created the frustration.\n\nStoicism helps you to embrace the idea that things are not always going to be as you wish they were. It doesn't mean you don't care about anything. It means you develop patience to remain calm and emotionally healthy. Then, you are able to work towards making things better productively. A stoic doesn't hate, doesn't get frustrated, holds no grudges, and can never be found at the computer at 3:04AM because \"someone is **wrong** on the internet\".","label":0,"model":"human","source":"reddit","id":2649}
{"text":"The above explanations are very good, and so to add an interesting example of where pi appears in interesting places, one of the earliest forms of the Monte Carlo method, which is a VERY popular method of numerical approximation using computers, was an experiment ran by Pierre Simon Laplace.\n\nHe used a version of an earlier experiment by George Louis Leclerc and by dropping needles at ruled paper he found that the total needles thrown divided by the number of needles intersecting the ruled lines was a multiple of pi ( he predicted this would happen based on the equation, but it was still an interesting discovery).\n\nI believe there was a semi-famous soldier who conducted this experiment while he was injured during the war, but I don't remember the exact details, this is based on an undergrad presentation I did a while ago and I already forget the details.\n\nTry it yourself: Use ruled paper and a needle where the length of the needle is half (or as close to half as possible) of the distance between the ruled lines on the paper, and then start dropping the needle on the paper and keeping track of the total number of drops and the number of times it intersected. After a while, grab a calculator and divide the total drops by the number of intersections. The longer you do this the closer to pi you'll get.\n\n**Edit**\n\nAlot of people asked for the GitHub repository, here it is:\n\n_URL_0_\n\n- the actual methods are in the PresentationExamples file, the first being the standard buffon's needle simulation, the second being the same with the exception of not using cos or pi, and the third is an example of Monte Carlo integration.\n\n- main basically gives you the option of which to run, and then automatically prints out the time taken to run the simulations, the result, and the standard deviation.","label":0,"model":"human","source":"reddit","id":2650}
{"text":"The truth is that in the majority of cases we don't know! It is group of muscle fibers independently contracting, but why this happens is usually unknown. Some people will have electrolyte disturbances (hypomagnesemia, hypocalcemia), some have metabolic problems (hypothyroidism, low B12), some will be hypovolemic (dehydrated or overuse of diuretics), some have structural problems (flat feet!) but again the majority of people with recurrent leg cramps DONT have any identifiable cause of their cramping.\n\nLooking a little deeper, it seems as though before the cramping there is a period of increased motor neuron hyperactivity, this depletes muscular ATP, which in turn causes the cytosol to become overloaded with calcium. When a muscle cell has too much calcium the actin myosin bridges can't unlock (I think - it has been a LONG time since I learned about muscular physiology) and this causes the cramp!\n\nSource: primary care doctor, have sent labs on many patients with leg cramps and never found an identifiable cause...","label":0,"model":"human","source":"reddit","id":2651}
{"text":"Great question. For starters numbers are an abstraction. In reality, things just are. There is no number, because there are no category of things that can be repeated. No apple is truly the same as another and therefore a person cannot have more than one of anything. The real world is infinite in its complexity. \n\nHowever, the human mind is not. The human mind is simple and must make assumptions and estimations to get along. The human mind considers an apple and another apple and doesn't see their infinitly distinct reality. The mind sees an abstract simplified token - just an apple and another apple. Two apples. \n\nThis is a kind of magic. Representing several things as though it was a modified version of one thing, frees up the mind to do so much. It allows us to store large amounts of information *outside* of our bodies. \n\nThe simple human mind can only really conceive of about 3-6 things at once. If a person without counting is asked which group is larger and is shown two groups, one with 33 apples, and another with 31, is extremely difficult to tell. But with numbers a person can count. They can set aside the reality of the apples and use several kinds of abstract representation to tell how many there are. They can arrange the apples into groups of three - which can be easily identified - and use their fingers outstretched to represent their place in counting each group. This is storing information outside of oneself. \n\nThis is a **profound** transformation. It can be shown that numbers are a kind of representative logic. Adding the ability to store information outside the human body transforms humans from just an animal into *Turing complete*. Turing machines can \nSolve any problem that is computable given enough time. \n\nTo the extent that we are right that one thing is like another thing, abstraction and counting save us a lot of brainpower. It's a kind of compression. When we use numbers to represent things, we discover that there are certain logical properties that can rearrange these groups (numbers) in ways that are more understandable without affecting their accuracy or changing the number at all. For instance, three groups of 10 apples is the same as 30 apples. Multiplying doesn't do anything to the groups but it does make a simpler token to represent it in our memory (30 as opposed to 3 sets of 10). \n\nThese conceptual simplifications let us represent other relationships we discover. Like the fact that planets (from the Greek for *wanderer*) seem to look like stars that moves throughout the sky. By putting numbers on how much they move we can compare this that are hard to directly observe - just like the large groups of apples. And we can store that information outside of our minds so we can compare it over long periods of time. \n\nComparing these numbers lets us discover patterns that describe how the planets behave like Newton's equations of motion and gravitation. What's more, they let us predict how they will behave. \n\nThere is a lot more to say and I think the most interesting and readable author on the subject is Bertrand Russell in his book [introduction to mathematical philosophy](_URL_0_) \n\nListen to the first chapter for a fantastic answer to your question.","label":0,"model":"human","source":"reddit","id":2652}
{"text":"I do this a lot too. Its called 'Operant Conditioning.' Essentially, you actually end up training yourself into waking up before the alarm goes off because you know the noise that it makes will rudely wake you up. This rude awakening can be considered as **negative** reinforcement, a punishment. Initially, you will wake up to the alarm. As time goes by, however, you will just start to wake up before the alarm because subconsciously you will not want to hear the loud noises. Eventually, it will not even matter what time you set the alarm. You will end up waking a minute or two before it goes off.\n\n & nbsp;\n\nEDIT: I understand that punishment and negative reinforcement ARE different. However, in the scenario described, the punishment (the alarm noise) *negatively reinforces* your sleeping habits (oversleeping).  \n\n & nbsp;\nPunishment - A penalty inflicted for an offense  \nNegative Reinforcement - When a certain stimulus (usually an aversive stimulus) is removed after a particular behavior is exhibited","label":0,"model":"human","source":"reddit","id":2653}
{"text":"PETA, in a nutshell, believes animals should live naturally without humans interfering with their lives. They are not about saving animal's lives, they are about gaining animal's rights.\n\nThey claim that they put these animals down because their lives have already been ruined by humans and cannot be rehabilitated to live as PETA thinks they should.\n\nThey are a cult. They are very systematic in the information they release to their followers and very often have duped people into believing that they are there to \"save\" animals. Very few animals they \"rescue\" actually get adopted elsewhere... the only ones that do are the ones you see them tout around on social media.\n\nedit for spelling\n\nanother edit because holy fuck RIP in peace my inbox\n\nguys i'm really sorry i didn't source any of this, aside from the Bullshit videos (which is obviously biased, as it is literally a show looking for the shady shit, but it's still worth watching). in all honesty i didn't think this would get any attention, and i wrote it kind of quickly. if you want to check back here over the next few days i'll be looking for sources so i can respond to you properly :)","label":0,"model":"human","source":"reddit","id":2654}
{"text":"If this measurement of 1 billion years turns out to be true and is the same for all galaxies, then it is significant because if we know the speed of the rotation, then we can measure a lot of other things about the galaxies- their masses especially. The equations for how this works would be out of the ELI5 boards, but they are connected.\n\nAlso- if one billion years -per-rotation is the speed, it leads us to more questions. !!And this is how science works- solving problems and finding new questions!! \n-Why is it 1B years?\n-Are they the same direction?\n-Are bigger galaxies necessarily older because of what this new data states?\n-Does this change when two disc galaxies merge? And does this shine any light on non- disc galaxies?\n-And the black holes at the centers of many galaxies spin- does this mean the spin gets faster, slower or doesn\u2019t change when the stars are swallowed up into the black hole since the mass of the stars just become added to the mass of the black hole?\n\nAlso by knowing the speed and mass we can begin to measure the amount of energy making the galaxy spin- which could lead to identifying more information about the black hole. \n\nAnd once know about the mass and energy we can study the effects of the galaxy on it\u2019s neighboring galaxies or giant dust clouds. And so on. \nIt\u2019s science! And it\u2019s always so exciting!","label":0,"model":"human","source":"reddit","id":2655}
{"text":"Kay, the low down on the difference between *taste* and *flavour*. \n\nTaste is governed by taste buds. Taste is the major factor about whether something will taste nice to us. These receptors pick up: \n\n**Sodium** (salt) \n\n**Sweet** (sugars) \n\n**Sour** (acids) \n\n**Savoury\/Umami** (L-glutamines)\n\n**Bitter** (flavanoids and a few other things that bind hard to taste receptors and is associated with poison and vegetables and alkoloids) \n\n*And maybe fat, but only some people seem to notice the difference between full-fat and low-fat indicating either some people don't have it or maybe it's become numb or... this is still a complicated one and currently research is still being done*\n\n**You cannot smell tastes, we just have smells that we ASSOCIATE with sweet, savoury, umami, etc. Eat a teaspoon of vanilla to prove it**\n\nWe then have **flavour**, which is everything else *and* taste. Flavour is governed by aromatics, fat bound flavours, etc. Flavour is governed mostly be smells. We have two types of smell, orthonasal (front of nose smell) and retronasal (the smell of a food when it travels up the BACK of our sinuses). Just because something smells like something one way doesn't mean that it smells the same the other way, as such things sometimes don't taste precisely like they smell (think coffee as some of it's chemicals literally only work in one direction).\n\nNow taste is the main thing we pick up. So the candle wax would be bitter, a taste we've been taught to avoid. When we've eaten the candle it may also have different retronasal notes, so the flavour may be different. Some of the smells are also locked into the wax so you won't even taste them. And thus it's gross.\n\nSource; A guy who's licked soap and studies food","label":0,"model":"human","source":"reddit","id":2656}
{"text":"I actually wrote a thesis on this. Bad words are related to culture (including time period). Whatever is considered socially unacceptable in that culture is usually the root of the bad word. \n\nI am going to use American society as an example because I think it has a pretty good timeline of cursing (and it is what I used in my paper). Many Americans come from Christian backgrounds, especially back in the 1800s. So you will find that older people consider words like 'goddamnit' and 'hell' to be bad words. A curse word takes what is sacred and violates it, so anything with the word 'god' used in vain, or saying, 'Jesus Christ' is considered naughty.\n\nThen, moving up some years, sex became a much more unsure thing, so curse words that became more and more popular revolved around sex-- 'fuck' became the naughtiest of the naughty. Words like, 'dick', 'cunt', etc. also became bad. Cunt, I think is the most interesting word because to this day, people who are ok with saying fuck still won't say cunt. There's something very uncomfortable and sacred about vaginas for people in America. \n\nIn this day and age, you might think that every cuss word boundary has been crossed-- even PG13 movies are allowed to have one 'fuck'. But current American society still has curse words-- racial and homophobic slurs. Even people who are comfortable using all the above words in public still consider themselves above using words like 'nigger' or 'faggot'. In this day and age, we consider the personal identity to be sacred, and insulting anything that has to do with that in a big cultural no-no.\n\nThis seems to be true across cultures although the subject of what is sacred may change. In Quebec, for example, many of the curse words revolve around the sacred vessels of the Catholic church, such as tabernacle or baptism or chalice. Catholicism has a big presence there and that is why their sacred words are used to insult. In Arabic, the biggest insult is to curse the vagina that a person came from-- which is almost akin to a 'your mom' joke.\n\nBasically, whatever is sacred to a culture, it is incorrect to say that thing in a moment of pain or anger, violating whatever that word is.","label":0,"model":"human","source":"reddit","id":2657}
{"text":"The lump-like thing we feel when overcome by emotion has to do with how the nervous system deals with stress. The part of the nervous system that handles stress is called the autonomic nervous system. It controls bodily functions that we do not consciously control, such as digesting food and pumping blood through the heart. However, the autonomic nervous system is also important in dealing with emotional states.\n\nWhen an animal encounters a stressful situation, its autonomic nervous system kicks in to allow it to either fight or run away. It does this by increasing the flow of blood and oxygen to the necessary muscles. The same applies to humans. Even when we experience emotions such as grief or sorrow, the autonomic nervous system responds as it would to anger or fear by increasing the flow of oxygen through the body.\n\nTo increase oxygen intake, the autonomic nervous system makes us breath faster, and expands the glottis, the opening in the throat that allows air to flow from the larynx to the lungs. The expansion of the glottis in and of itself does not create a lumpy feeling, until we try to swallow. Since swallowing involves closing the glottis, this works against the muscles that open the glottis in response to crying. We experience the resulting muscle tension as a lump in the throat.","label":0,"model":"human","source":"reddit","id":2658}
{"text":"To add to the discussion: Once upon a time, college students were expected to be relatively literate in Latin, and usually Greek. This was so they could study the classics -- the cornerstone of a university education -- in the original language.  Plus it helped to distinguish the educated from the uneducated.\n\nAs a result, when fraternities were assembled, they created (secret) mottoes in one of these languages, presumably because it sounded rather cool.  The initials of these organizations are the first letters of their mottoes.  For example,  [Phi Beta Kappa (\u03a6\u0392\u039a) stands for \u03a6\u03b9\u03bb\u03bf\u03c3\u03bf\u03c6\u03af\u03b1 \u0392\u03af\u03bf\u03c5 \u039a\u03c5\u03b2\u03b5\u03c1\u03bd\u03ae\u03c4\u03b7\u03c2 or in Latin letters Philosophia Biou Cybern\u0113t\u0113s, which means The Love of Wisdom, Learning, or Knowledge (Is) the Helmsman or Guide of Life.](_URL_0_).  This same tradition is true of all other Greek organizations, though I imagine the mottoes are no longer kept secret.\n\nAnyway that's why they still use Greek letters, long after the Greek language was dropped as a required study.\n\nEdit:  Lots of replies to the effect that fraternities are nothing more than hotbeds of sodomy.  Given how many seem so obsessed with the topic, I can't tell if the comments express *disapproval* or *envy*.","label":0,"model":"human","source":"reddit","id":2659}
{"text":"Ophthalmologist here.I don't see anyone touch on the right answer so I'll chime in. \n\nOur neutral eye position is actually not looking straight ahead, but actually directed outward towards your ears (this is known as exotropia). In fact, if you look at a picture of our bony orbits, they actually point away from each other. Our eyes overcome this by constantly converging, or turning both of your eyes toward each other.  We are very good at this, as demonstrated by how easy it is to cross our eyes, but nearly impossible to move our eyes in separate directions in any other fashion.  Whenever the fusion of our two eyes is broken, most people tend to have their eyes drift towards their ears (this is called exophoria).  When we get tired, this exophoria happens more as the drive to fuse our two eyes diminishes. This causes a slight double vision and describes what you are referring to.\n\nFun fact, as you can imagine, our eyes drift towards our ears when we fall asleep for the reason I described above.    Babies are generally born with their eyes drifted outwards. And our eyes drift outwards when we pass away.","label":0,"model":"human","source":"reddit","id":2660}
{"text":"Some of it is what's called \"Point of Purchase\" merchandise in the business. When you see that cooler next to the check out lane, those small soda bottles are marked up more because they are chilled, yes, but also because the store knows you'll buy something like that on an impulse because you want a cold soda immediately no matter the price. Same thing with small bags of chips or bubble gum which can be marked up slightly higher than normal in this case. This is slightly different than just buying things in bulk to save money.\n\nAs for water being expensive, it costs that much because people are willing to pay for it. It's just water after all, if everyone said, \"Hey, what the hell are we playing 2 bucks for something I can get out of a tap for 5cents a gallon?\" then it wouldn't exist. Bottled water can taste better than tap water depending on your water source, but a water filter at home would do about the same thing for hundreds of dollars less. It's a human quality to assume paying a lot for a product means it's better, even though a lot of it is just a placebo effect of increased pleasure.","label":0,"model":"human","source":"reddit","id":2661}
{"text":"For most businesses it is more or less just like a personal account. When you start getting into larger corporations they have dedicated employees that manage their money in investments, then many smaller accounts tied to different departments within the company. However it is usually easier to just have company credit cards where all the bills just go to one place. The biggest difference with most business accounts is there may need to be more than one person's approval to initiate any large transactions.\n\n And yes they are mostly the same banks dealing with businesses, in fact most of your everyday banks make more of their money off of businesses than personal accounts. They actually use the money from what are essentially long term savings accounts collectively to loan out to businesses (along with personal car\/home loans) at a higher interest rate than they are paying you to keep your money at their bank, which is a large part of how they make their money.\n\nWhat most consumers think of when they think of a bank is just a place to hold their cash safely isn't primarily how the bank makes money. By offering free or low-fee checking and savings accounts the bank really just wants you to be a loyal customer when it comes time to take out a mortgage for your first house.","label":0,"model":"human","source":"reddit","id":2662}
{"text":"There is more than one type of slavery.\n\nIn ancient Athens, conquered people became slaves, but their children were born free. They retired as free people, and it was really more like having your country invaded then being given a job you have to do.\n\nOthers like the Romans would keep foreigners as slaves, and you could be born a slave. These slaves were invisible though to society - they belonged to a household and had to do the jobs they were given, but they had time off, sometimes got paid, they fell in love and got married and had families. You couldn't tell if someone walking up the street was a slave or not. These slaves are the ancestors of the European peasant, the common folks. It was only in about the 17th century or so that we got the idea that people should be allowed to say 'no' if their Lord told them to do something.\n\nThe American South did not view Africans as human beings, but as animals. They broke up families, they engaged in forced breeding programs where men were beaten until they raped the woman they were told to breed with, they recklessly endangered slaves lives, they did not allow any sort of dignity. 'Chattel' slavery is the most powerfully dehumanizing form of slavery, and it only ever existed in the Caribbean and American South.","label":0,"model":"human","source":"reddit","id":2663}
{"text":"There's something called 'Theory of Mind' that says humans understand that there is a brain in their heads with individual thoughts, other brains out there with different thoughts, and the fact that these brains are not the same consciousness. The fact that you can sit there and think about thinking (\"Why can I hear my own thoughts? Why can't I hear someone else's thoughts? Why do other thoughts not happen the way mine do?\") is something that has developed in human beings for millions of years, and has only solidified fairly recently in the evolutionary timeline. \n\nWhat's interesting is that young children often show that the development of 'understanding your own thoughts and the thoughts of other people' is not completely after birth. If you show a 3 year old a box labelled crayons, they will say that there are crayons this box. But when you show them the box, it is actually filled with candy. The child can accept this, but if you ask them, \"What will your friend say is in this box?\" They will answer, \"Candies,\" and be unable to understand the concept that another brain would hold a different perception of something, even though they just made that assumption. Their theory of mind is only partially developed because they can hear their own thoughts, and only their own thoughts.\n\nThe \"how\" of your question is hard to answer because there isn't a specific part of your brain that says \"YOU WILL HEAR YOUR OWN THOUGHTS\" and if you knock it out, it'll stop. The fact that you can hear your own thoughts seems to be a result of a developed self-consciousness, awareness of other brains, and language centres that let you think about thinking, about yourself, and do so with words. \n\nI'm not actually sure if this is what you're looking for, but I hope it helps!","label":0,"model":"human","source":"reddit","id":2664}
{"text":"Mostly by selling un-aged product as well.\n\nSo the first year I sell 90% of what I make right away to recover my operating costs and lay up the remainder to age.\n\nThen the next year I might sell 80% of the new stock, and 10 percent of the first year production.\n\nThen the next year I might sell 70% or 60% of the new, and various percentages of the older stock.\n\nBasically, this is why most real distillers (as opposed to people who just buy and relabel) have an entire product line.\n\nAs the business grows the numbers are adjusted.\n\nIn the ideal your best aged stuff is a little rare and so hard to get... and so commands a high price and so top earnings.\n\nThis is particularly correct for a winery where the different wines have different aging profiles including many kinds of wine that do not age at all. (Crisp white wine turns to bitter ass very quickly if aged, for example.)\n\nLook up \"young versus aged wine\" in your favorite search engine.\n\nBasically you start with a young product and as you bank reputation you start to bank aged product.","label":0,"model":"human","source":"reddit","id":2665}
{"text":"A coma is a state of unconsciousness which someone can not be roused - this is what differentiates it from something like sleep for example. During sleep, we are unconscious, but with the right stimulus (shouting, poking etc) we will wake. \n\nComa is different and caused by loads of reasons:\nSome are intentional - my job as an anaesthetist means I intentionally induce a coma using drugs. This means people are unconscious, cannot feel pain and crucially, cannot be woken until I decide they can  This is very useful for having an operation (!) but also what is typically referred to in the news as a \u2018medically induced coma\u2019 - we are anaesthetising a patient to allow recovery, usually in an intensive care environment. \n\nPeople can be in a coma because of abnormal levels of toxins in the blood - byproducts such as urea (a product of normal physiology which increases if the kidneys don\u2019t work), ammonia (again, normal in very low amounts but increased if the liver doesn\u2019t work) or medications (intentional overdoses of medicine such as benzodiazepines or opioids). These people in comas may improve once the body clears the problem as they are reversible. The problem lies with the increased amounts of the drug or toxin affecting normal function of the brain. Most commonly this is alcohol. \n\nMore concerning are comas due to issues with the brain itself. This might be due to a bleed, such as seen in trauma, a stroke where the brain doesn\u2019t get enough oxygen and dies and an increase in pressure inside the skull which reduces blood supply and means the brain doesn\u2019t get enough oxygen. In these cases, coma may be irreversible unless the brain is delivered oxygen in minutes. This can be seen in people who have had a cardiac arrest - their brain has been starved of oxygen so it cannot function normally even if a heart beat returns, but the most basic functions of the brain still persist. The person is alive in the sense of having a heartbeat and breathing, but in a coma because the areas responsible for consciousness have been irreversibly damaged, and no stimulus will promote wakefulness. \n\nHopefully that helps! Often it\u2019s not the body itself that decides when to \u2018wake up\u2019, more the resolution if possible of whatever is causing it in the first place, and hoping no long lasting damage occurred in the mean time\n\nEdit - if people find this useful, I am happy to expand, but that probably takes it slightly out of the ELI5 zone!","label":0,"model":"human","source":"reddit","id":2666}
{"text":"Many regular soaps contain substances such as chlorine or alcohol to kill bacteria.  Soaps that are labeled \"antibacterial\" often contain an additional agent such as triclosan or triclocarban.\n\nThose last two work by messing with the bacteria's ability to do its normal jobs for living.  One of these jobs is to build and maintain their walls.  Their walls are important because it helps them keep good stuff in, send bad stuff out, and keep bad stuff out!\n\nThey're like little factories inside, and all day they're bringing in resources to help them build and maintain their walls.  Well one day the UPS truck shows up just as it always does at 9:00 AM.  The 9:00 AM delivery always contains the pieces necessary to work on the walls, so the security guards at the cell wall sign for the delivery and thank the truck driver.\n\nThe resources delivered plug in perfectly where they're supposed to for making bricks for the walls, so the factory workers chug along happily plugging them in.  After plugging them in all in the workers begin to realize that something isn't right:  the new bricks for the wall aren't forming correctly with this new delivery.  It's too late to do anything about it, though, because all of the brick-making spots already have this new resource plugged into them.  So even if we got an emergency delivery of the CORRECT resource right now, we wouldn't even be able to use it.\n\nThe factory workers look our their windows in horror as they see their precious wall outside begin to crumble around them, helpless to do anything to repair it.  Through the new ruined wall they're left unable to properly accept new deliveries of any resources, or send out their spent waste resources.\n\nThey spend the rest of their short, miserable lives simultaneously starving to death and drowning in their own shit.","label":0,"model":"human","source":"reddit","id":2667}
{"text":"Sunburn is a type of radiation burn caused by exposure to the sun. To put it simply, the radiation from the sun has bombarded the atoms that make up your DNA, causing irreparable damage. These cells with damaged DNA can no longer self-replicate, meaning they must be shed and replaced.\n\nLike any burn, your body responds with inflammation of the affected area. This is the redness we typically associate with sunburn. What is happening is your body's immune system has recognized that there is a region of localized damage to your skin, and has begun to repair that damage by replacing the skin in that area with new cells. \n\nTo do this, it increases bloodflow to the damaged area in order to supply more oxygen to help with the 'repairs'. Depending on how *deep* the damage is, the more extreme the immune response will be (i.e. first, second, or third-degree burns).\n\nRedness can appear within an hour of exposure, but typically you'll experience the most pain between 6-48 hours. The pain and redness you feel is actually your body's response to the sunburn, and will take several hours to fully peak.","label":0,"model":"human","source":"reddit","id":2668}
{"text":"Police are general law-enforcement officers hired by any number of different agencies (city, county, university, hospital, etc). They are hired by any agency with local government authority.\n\nSheriffs are elected officials usual governing over a specific county in a state, separate in legal authority from the police. They have legal jurisdiction over any areas not already incorporated by a police department (such as areas in a county outside of city limits), and they also run legal processes and usually jails within the county. The sheriff can appoint deputies who generally have similar legal authority to police officers.\n\nRangers in the legal sense are only found in Texas, and have the highest policing authority in the state. Park Rangers are appointed in federal natural parks and usually have policing authority within their boundaries, but they also act as conservationists and naturalists.\n\nMarshals are deputies of the Federal US Courts and can subpoena or arrest those charged with a federal crime. They don't have the full authority of police or sheriffs but are generally seen as an interstate federal authority similar to police.\n\nEDIT:\n\nTroopers are usually part of a general state police force. These are most often only used in areas of state government jurisdiction, most commonly on state highways.\n\n[Info taken from this site](_URL_0_)","label":0,"model":"human","source":"reddit","id":2669}
{"text":"Something important people seem to be overlooking or missing:\n\n**The Korean government was ahead of the game.**\n\n^^^.\n\nFollowing an economic crisis in the late 90s, they realized very early how important the internet and broadband would eventually become for business and economic recovery, when the rest of the world hadn't spied the internet's full potential.\n\nWhile the rest of the world was still getting excited over their AOL dialup CDs, the Korean government was making massive investments in infrastructure, funding, and policies way back in the 90s. And they *continue* to invest. \n\n^^^.\n\nThis Wikipedia article gives a decent summary:  [Telecommunications in South Korea - Internet](_URL_1_)\n\nThis case study (from way back in 2003) is worth skimming too: [Broadband Korea: Internet Case Study](_URL_0_)\n\n... especially p.12, where it notes some of the other factors that helped make Korea the \"perfect storm\" for broadband development:\n\n* 80% (insane!) of the Korean population living in densely-populated urban areas\n* industry competition\n* the size and importance of the electronics industry in the Korean economy, which aided hardware rollout\n\n^^^.\n\nPeople seem to be focusing on how the US system has failed rather than how the Korean system has succeeded. Korea is where it is because they *planned ahead and made it happen*. It didn't happen by chance and blind luck. (Which is basically the system the US has relied upon!)","label":0,"model":"human","source":"reddit","id":2670}
{"text":"It's an evolutionary advantage to itch.\n\nThe itch originally is a response to insect bites, the body itches so you scratch and get rid of the bug.\nEsonophils cells secrete cytokines when an insect bites you causing the nerves in your skin to itch, when you scratch you're telling the same nerves that you got rid of the bug and it can relaxes now.\n\nThe same response can happen from other causes like autoimmune disease such as eczema where the same cells (Esonophils) secrete cytokines and other substances. The body can't differentiate the reasons these cells were activated for and for all your body knows is that you need to scratch.\n\nNeedle injury causes you to itch also because it's basically the same as a bug.\n\nChickenpox and similar diseases causes lesions in your skin, these lesions are full of inflammatory cells that secrete substances irritating the nerves in your skin, same thing ...\n\nEdit: I want to add that scratching is satisfying and enjoyed feeling because anything protective or evolutionary useful to your body\/genes has a reward system. Sex is the biggest example, food is tasty, scratching is relaxing.","label":0,"model":"human","source":"reddit","id":2671}
{"text":"Any time you have a large number of objects near each other, gravity is going to tend to pull them all to a point.  UNLESS they are revolving around some point in space.  And if they are moving at all, there is almost certainly some amount of revolution to the combined motion.  \n  \nWhat you really look at is the overall average motion of all the objects combined.  So even if they are revolving in different planes, or even different directions, the average is going to be some revolution in a single direction around a particular axis.  And as they all settle down, they will converge into a disc that lies in the plane orthogonal to the axis of the overall average motion. \n  \n--  \n  \nEDIT:  OK, I understand this is not ELI5 enough.  I am penitent.  If you don't mind, I'll try again:  \nSay you have a large cloud of little objects in space, pebbles or whatever.  Everything has gravity, so all these things will try to pull together until it is a single ball.  Taking all of their gravities together, you could say they are all being attracted to the center of the cloud.  When it all comes together, it is nearly certain that the ball will be spinning because every little piece will add its motion to the ball- some will push it one way and some the other, but when you add it all up, there's going to be some total that is more than zero.  It will be moving through space, and also rotating.  The rotating part is the part that will help us answer the original question.  \n  \nSo let's look at our cloud after it has been around a while but before it becomes a ball.  The pebbles are all pulled towards that center point, but they aren't just sinking straight towards it - they will sort of spiral in.  At the beginning, some will be spiraling one way and some the other.  The ones that are going against the majority are going to get pushed like a guy on a crowded sidewalk until they are going roughly the same way.  \n  \nNow look at any one pebble, revolving around the point at the center of the cloud.  It's moving in an oval-ish path, which is naturally 2d.  Then look at just two pebbles- imagine these two near-circles around the same point, but at an angle to each other.  The two pebbles each have a tiny bit of gravity and they are going to try to pull together.  So over time, the angle between the circles will become less and less until they are in the same plane.  This will happen to all the pebbles, all tilting their paths closer and closer to each other.  And since the motion of each individual is naturally a 2d path, when they all eventually pull each other together, the overall shape is 2d.","label":0,"model":"human","source":"reddit","id":2672}
{"text":"Anesthesiologist here.\n\nThere are two broad categories of general anesthetics, which are divided based on how they are administered: intravenous and inhalational.\n\nLet's start with intravenous. The most common IV general anesthetic is Propofol by far. Propofol is the milky white drug you sometimes see on medical shows. You may know of it as the \"Michael Jackson Drug\". Us anesthesiologists sometimes refer to it as \"Milk of Amnesia\" (or maybe that's just me). The induction dose (the slug we give you at the start to put you to sleep) is calculated based on body weight. Typically around 2 milligrams per kilogram of body weight. For the super obese patients we use an adjusted value that's somewhere between their actual body weight and their \"ideal body weight\". Younger patients, alcoholics, and red heads will need more milligrams per kilogram. Older patients, and those with certain neurological or medical issues, or those under the influence of certain drugs (medical or recreational) will need less.\n\nAfter I determine you are unconscious, I will stick a breathing tube down your throat, hook it up to my anesthesia machine, and dial in the inhalational agent. In terms of inhalational agents, the three most common ones are Sevoflurane, Desflurane, and Nitrous Oxide. These agents are not dosed by milligrams, but by concentration in percents. We measure the percent concentration of the agent in the exhaled breath. Sevoflurane is dosed to about 2%, Desflurane 6-8%. The remainder percent is typically a mixture of pure oxygen and medical air. Nitrous Oxide by itself would not be sufficient to keep you under general anesthesia (it would require  > 100%!), so if we use it, we usually use it to \"cut\" either the Sevo or the Desflurane (i.e. 50\/50 O2\/Nitrous plus 1% Sevo). Each agent has its unique advantages and disadvantages, and I use many factors to determine which agent is best for you.\n\nHow do they work? Big picture wise, they decrease the activity of excitatory circuits in the central nervous system and increase the activity of inhibitory circuits in the same (particularly that of a chemical called GABA). They suppress your sympathetic nervous system as well, which may reduce your blood pressure and\/or heart rate.","label":0,"model":"human","source":"reddit","id":2673}
{"text":"***Edit: Here's a [crude drawing](_URL_0_) to help visualize it.*\n\nBob and Joe are friends. Joe lives just around the corner from Bob, so Bob decides to walk to Joe's house. He walks down his street, turns right at the corner, and walks down Joe's street. He then walks down the path from the sidewalk to to Joe's front door.\n\nSuzy and Jill are friends. Suzy lives around the corner from Bob in the opposite direction of Joe (left at the corner instead of right). Jill lives next door to Joe. Suzy decides to walk to Jill's house, so she walks down her street, passes Bob's street, and continues onto Jill's street until she turns to walk from the sidewalk down the path to Jill's front door.\n\nEven though Bob and Suzy can each get to their friends' houses, their friends share a street, so they both have to walk down the same section of road to get to their friends' houses. There isn't a single road that goes straight from Bob to Joe, and there isn't a single road that goes straight from Suzy to Jill. They have to share *part* of the path.\n\nOne day there is road construction, and Joe\/Jill's section of the street is blocked off at the corner (shown in orange in the picture). Now neither Bob nor Suzy can reach their friend. Bob and Suzy could theoretically walk to each other's houses, because the intersection itself isn't totally blocked. Only the section that goes to Jill and Joe.\n\nNow imagine that the road is a wire that you send a message through. In order to actually make a connection directly to someone else's computer, there would have to be a single wire going directly from your computer to their computer. Really there are hubs where a bunch of wires connect, like the intersection of Bob and Suzy's streets. That hub is then connected to other hubs where the wires split off again to go to the individual houses, like how Bob went down the path to Joe's door, and Suzy went down the path to Jill's door.\n\nTo shut down the connection to a large area like Syria, one would shut down the hubs that allow connections within that area.","label":0,"model":"human","source":"reddit","id":2674}
{"text":"video game animator here! :)\n\nCharacters in games are moved programatically not through the animation.  The animation is just there to let the player know what the character is doing.  99% of video game engines will move the character in a straight, linear motion.  So when a character walks upstairs, it's just moving diagonally up at a constant speed.  It's incredibly time consuming for an animator to match the animation to the linear motion for several reasons:\n\n1. Unless there's only one single instance of using the stairs, going up and down the stairs will usually be a variable.  You don't know how many steps each stairs has unless the animator makes an animation for each character matched to each set of stairs.  It can be done, but it takes a long time.  Most animators just create 1 looping animation which can then be used for every time the player goes up the stairs.\n\n2.  It's also difficult for an animator to create a looping animation without being able to first get the angle and speed the program is using for moving the character, and then sort of \"counter animating\" to adjust for the program's movement code.  Again, it can be done, but then that animation will only look nice for that specific stairs.  There's little reason to go through so much effort to create stair-walking-up animations for each set of stairs when you can just create one loop and apply it generically to all of them.\n\nIn very large companies such as Ubisoft, Activision, EA, etc... where they have an army of animators, they do have the time and the budget to create animations for very specific purposes, such as Batman's cape when he punches using Punch1, then Punch variation 2, then Punch variation 3, etc...  Smaller companies simply do not have time to dedicate their animator to do these very minute detailed animations that only fit one specific purpose.  Larger companies can.","label":0,"model":"human","source":"reddit","id":2675}
{"text":"I have only watched Musk's presentation, so there might be details elsewhere that I have missed.\n\nThe main purpose of the shingles is aesthetics. These solar shingles are designed to look like regular house shingles. This starts serving the community who had the money and desire for solar power but did not want the big ugly panels.\n\nUnless there is an efficiency edge (I don't think so) or a decreased cost edge (He kept saying they were a similar price of a regular roof, but I have no numbers to back up this claim) the only thing these new shingles do is aesthetics.\n\nPowerwall is a newer technology that is supposed to solve the problem of uneven use and generation. Solar panels only make energy during the day, but people still use energy at night. \n\nPowerwall is just a giant battery that will store your solar power made in the day, and let you use it at night. Again, batteries are not new, but the affordability of giant batteries is a new thing. \n\nAlso note, he specifically says that he does not intend for this kind of technology to replace utilities. He says if we get off gas heating and gas cars, we will triple the amount of electricity we need. That means we need to increase production by three times of what we currently do.","label":0,"model":"human","source":"reddit","id":2676}
{"text":"I saw some answers that are good but didn't see any I liked or that cover some of the other aspects.\n\nThere are a few different types of money laundering, mostly depending on what you're doing with the money.\n\nThe first is disguising the source of the money. This is used when you sell something illegal, drugs are a classic example. The money is converted into cash somewhere, the cash is then spread out to avoid triggering investigations, and then all that money is deposited in a centralized receiving account. Simple examples of this are things like someone else posted about the construction contractor that bills for work not done. Mid-sized examples use night clubs and bars, places where mark-ups can vary widely and cash is king, this allows the club to mark as sold thousands of drinks, entries, or sometimes even entire full night events that never actually happened. I expect that right now there is a rise of using cryptocurrencies to do this because the volatility can hide a lot of bad things. For large accounts the money generally goes international using a large number of international transfers to hide the money source, the money then goes through a combination of the large and small areas to reach the goal. \n\nFor even larger amounts you build something. Say a large building or complex of buildings in a really tacky gold color. Everything is built super cheap, but for some reason buyers pay over market rate, and your investors somehow make massive returns. You then brand yourself as a real estate genius thinking you're amazing at making deals, when really you're just the patsy.\n\nThe second reason is to hide the destination of the money, this is actually how some of my clients paid me, even though everything I did was legal. For this the business will often generate a fake theft. \"Someone\" skimmed the money coming in, embezzling it, the money finds it's way into a duffle bag, and that duffle bag of cash is used to pay people. This is the same basic method that is used to pay people under the table. For larger amounts a charity is setup, the company makes donations and the charity sends the money along. In my case I eventually worked through a family trust account, my clients hired the trust, the trust paid me, this is so much easier than trying to find a way to deposit a duffle bag full of cash without raising suspicion. Since my work was legal I didn't bother laundering, but my clients thought I was laundering through the trust.\n\nThe third category is simply to disguise what you're actually doing, and this can often be legal. Maybe you need to pay a pornstar to not tell everyone you like to be spanked with a magazine. For this you generate a false business. An intermediary consultant is hired, the consultant is paid an exorbitant rate, usually many times the normal going rate for their work, the extra is paid out. This leaves clean hands for the person paying and the recipient knows exactly where the money came from. Like I said this can sometimes be legal, sometimes it isn't.","label":0,"model":"human","source":"reddit","id":2677}
{"text":"Good sports AI needs to fool you.  The goal is to convince the human player that what the AI opponent did is what would plausibly happen in the real world.  You want the human player to say \"geez, that was good\" and even \"I hate this game (because it's legitimately hard)\"... but not \"that was stupid\" or \"bullshit, I hate this game because that would never happen.\"  \n\nThousands of \"IF - THEN\" statements *could* be written, one for every possible scenario, that's not really feasible.  It's time consuming, expensive, error-prone, requires too much domain expertise, and is hard to modify.  So an AI programmer will try to categorize and recognize broad and specific game\/team\/situation scenarios and then apply statistical lookup tables containing decisions made in those scenarios, historically.\n\nTable lookups are cheap from a computational perspective and can be very convincing, if they're based on the appropriate slices of past observations.  Less code can equal fewer bugs, but more reliance on data.  This historical data can be purchased or licensed or compiled and then sliced and diced as needed.\n\ne.g. \n\nScenario:  Team X is in post-season play, versus Team Y.  Away game.  Leading by N runs.  Q is at bat.  R is next up.  Count is J and K.  M is pitching and has thrown G pitches with H hits on him this game, so far.  Runners in scoring position.\n\nDecision for the AI to make:  What pitch to throw now?\n\nOne AI approach:  Find a pre-compiled lookup table that matches some or all of that scenario.  Look at team and player behaviour that scenario.  Historically, what pitch was thrown next?  Don't worry about the outcome for now -- **AI is successful if it's realistic, not if it wins all the time**.\nSo look at the types of decisions that were historically made in *this* scenario, and assign a frequency or % to the various decisions.  Then roll the dice so the AI opponent takes an action that's statistically \"correct\", yet not totally predictable or repetitive.\n\nThis is oversimplified.  The tables are obviously multidimensional.  Maybe with dozens of dimensions.  But computers are great at that, and with a tabular approach, relatively little code needs to be written.  \n\nHistorical stats can't answer all the \"what to do now\" questions for AI, but they can get the AI into the right ballpark.  So to speak.  This allows the hand-tuned AI code (read: the expensive, trade-secret stuff) to focus on fine details or to catch necessary exceptions.\n\nIt's funny because you want the human player to \"hate\" your AI's choices, but hate them for the right reason.  i.e. \"Dammit, yeah, that was a good pitch to throw.\"  That's good AI.  But everyone's a critic.  And when you've paid hard-earned money for a game, everyone should be a critic.\n\nSource: Was a sports AI programmer.   EDIT:   < company withheld to protect the innocent >","label":0,"model":"human","source":"reddit","id":2678}
{"text":"I think the 3 parts deserve separate answers. \n\nYou don't have to eat hygienic food. Keeping good hygiene lowers your chances of catching a disease, but you'll be perfectly healthy for quite some time even if you eat unhygienic food until that unlucky day you die of dysentery. Hygiene is one of the major part of how we managed to increase the average expected life span from 40 to 80 years. \n\nNow the healthy part. Your body evolved for millenia in a world where food is scarce. Your body craves things that give you the most amount of energy (sugar, fat, and meat) and salt. (salt is surprisingly rare in nature) Your body will not stop once it had its fill since you never know if you'll starve for the next few weeks. In modern society, none of these problems exist, so moderation is needed. \n\nCooking actually goes way back, much before hygienic or healthy eating developed. This means no one's really sure how it started, but here's one of the more popular theories.\n\nA large brain requires a lot of energy. Unless you really exert yourself, a significant portion of your daily energy use is keeping your brain alive and running. So it stands to reason that cooking led to intelligence since cooked food is easier to digest, which means you get more energy from the same amount of food stuffs. Cooking also has the added benefit of killing parasites and other pathogen, as well as neutralizing some toxins. These are all things that you use energy to overcome. Lastly, cooked food tend to be softer, meaning you have to spend less energy to chew. If you're not sure what this means, try eating a raw chestnut some time and compare it to a cooked one.\n\nAs for why you need to continue eating cooked food when you're not a starving caveman, the long tradition of eating cooked food means you evolved to eat cooked food. Your teeth and jaw are not as robust as your long ago ancestors', and your digestive system is shorter. (cooked food is easier to digest) Wisdom teeth, for example, was just another set of teeth that fit in perfectly in your ancestors' bigger, stronger jaws. That said, I suppose you can eat uncooked food anyway since there's so much food today.","label":0,"model":"human","source":"reddit","id":2679}
{"text":"Mexican here. Getting a US visa to work is more difficult that you would think for Mexicans. First of all, people who enter the US illegally are mostly poor, for reference, I'll let you know that the minimum wage is around 6 dollars a day here. These people don't speak English, most of them didn't go to\/finish high school, and have a lot of kids. They don't have a single chance of getting a visa. They cross the border because the whole system is already adapted for them to cross it. When an illegal immigrant gets to the US, they will find a job that's designed for illegal immigrants: the salary is low enough to be illegal but high enough to interest an immigrant.   \nNow, the visa being hard to get is partly our fault because we keep going there illegally and making it more difficult. I say \"partly\" because other central and southamericans also enter the US illegally through our border, and of course, we get blamed for it.  Nevertheless, with our shit economy, I understand why some people just go \"Meh, either I die here, or I die trying to get a better job.\"","label":0,"model":"human","source":"reddit","id":2680}
{"text":"Ok, sidebar here, from a nerdy ((and pedantic) massage therapist:\n\nThere are lots of responses here, all giving very reasonable and more or less science-y sounding answers expressed with absolute confidence. They're all missing one important thing.\n\nHere's the somewhat awkward truth: we don't really know. In fact, the answer to most questions about massage is \"We don't know for sure.\" There are a lot of things that we know definitely work, and we have some theories as to why, but we aren't really sure. \n\nThere are a couple of really good reasons for this. \n\nFirst, massage is really hard to study in a scientifically rigorous way. You can't even do a proper blind study - every subject will know whether or not they got a massage! \n\nThe second reason is that to really understand how massage works, you'd have to be able to see what's going on in a living body on a sub-microscopic level in real-time, and we don't have any way to do that. Through decades of studying chemical structures of proteins and other components in muscle tissues and nerve fibers, scientists have developed a really good sense of how these things work, and *some* of the ways things can go wrong. But there are still lots of failure modes that aren't well understood yet.\n\nSo, if we don't know exactly why your neck gets tense when you're stressed, or why massage fixes it, what *do* we know? Well, let's start with a basic understanding of the feedback loop that regulates the tension in any given muscle (or really, any cluster of muscle fibers within a muscle):\n\n1. Sensory receptors in and around every muscle in the body, every second of every day, send information up to the brain. This includes pressure sensors in the muscle that detect contraction, stretch sensors in the tendons, chemical receptors throughout the muscle tissue that detect inflammation, and more.\n2. The brain receives all this info, and compares it to past experience to estimate the current position, motion, and state of every body part, including the current tension of each muscle. (When you close your eyes and wave your arms around and you can feel where they are, this is what makes that possible.)\n3. The brain determines the difference between the position and motion of each body part vs where it wants those body parts to be, as well as the current contraction state of each muscle, and contracts or relaxes all muscles accordingly. (This is why you're capable of walking, or even standing, without falling over.)\n4. Return to step #1.\n\nSo, if a muscle is more tense (that is, trying to contract more) than it should be - or, if just a few fibers in a muscle are tense, as in the case of a \"knot\" - the problem could be at any step of that process. Perhaps the tissues are sending incorrect or contradictory information to the brain, or the brain is misinterpreting the information, or the brain is sending the wrong signals to the muscle, or the signal from the brain isn't reaching the muscle properly. And of course, there are several possible causes for each of these.\n\nMassage could help any of those situations. It can increase lymphatic flow, flushing out metabolic byproducts that might interfere with nerve signals or with the muscle's ability to respond. It could also \"recalibrate\" the brain's sensorimotor system, by sending a wide range of unusual signals - for example, I can stretch the tendon at one end of the muscle without simultaneously stretching the other end - something that rarely happens naturally, forcing the brain to \"recalibrate\" its idea of what a given sensory input means.\n\nWithout being able to trace individual nerve signals, let alone look at the chemical changes inside a muscle cell from one millisecond to the next, it's surprisingly hard to tell for sure exactly which of these things is happening, and why, and how. Which is why the most honest answer to why massage works is still almost always \"I don't know.\"\n\n & #x200B;\n\nSo, with that huge disclaimer, let's finally come back to your original question: my own pet theory, based on what I've read and my personal experience, is something like this: \n\nUnder normal circumstances, it's very rare that you'll keep the same muscle active for an extended period. You're always making small movements and adjustments, giving any particular group of muscle fibers a chance to relax every now and then. When you're under stress, however, as others have pointed out, your brain switches to \"fight or flight\" mode, releasing cortisol, adrenaline, and other hormones into your body. This can cause you to tense up all your muscles in preparation for sudden action. It can also help you to hold very still, perhaps as you focus intently one one thing. And then, of course, the mental distraction of the stress may cause you not to consciously notice that you've held one muscle tight long enough that it's getting uncomfortable.\n\nAt some point, when you perform this unusual action of keeping a muscle tight for a very long time, something goes wrong in the feedback loop above. Perhaps the tight muscles inhibit lymphatic flow, causing a build up of metabolic byproducts, causing inflammation. Or perhaps fatigue of the overworked stretch sensors causes the brain to misinterpret their signals, or start to ignore them completely. For whatever reason, that muscle (or part of a muscle) keeps trying to contract more than is appropriate. Then you get a massage, and (one way or another) everything is cleared out and recalibrated, and you feel better.\n\nWhy does this happen in the neck, specifically? Well, it can happen anywhere, and different people \"carry their tension\" in different places. The thing about the neck is, those muscles are pretty busy. If you're not lying down, you're probably using those muscles to support your head. And especially if you spend your days at a desk, looking at a screen or at paperwork or anything involving focused attention, not looking around much, while feeling stressed and anxious... it's just asking for trouble!","label":0,"model":"human","source":"reddit","id":2681}
{"text":"I think there is no better way to learn about this than to read [the unreasonable effectiveness of mathematics in natural science by wigner](_URL_0_)\n\n > how is math universal ?\n\nthe laws of logic by which math operate are derived from our perception of the natural world. i.e essentially counting, symmetry, geometry, classes, sets. however in a universe where counting or symmetry or geometry or sets does not appear to apply our system of math will fail. as for this universe, they apply throughout the observable universe, so it's perceivably universal (form our point of view atleast).\n\n > would aliens have the same math as us ?\n\n\nassuming they are intelligent and have the same degree of perception and ideas of logic, depends on how advanced they are and how much they have explored the field. but they will have come to the same conclusions. may be their number system might be different but the relationship between the objects in the system would be the same and the operations thereof. i.e something like a+b=c would still hold and be discovered by them.\n\n > Isn't it just an arbitrary system of calculations? \n\nthe laws that make calculations work, are not arbitrary, they are based on a system of logic that governs all math. The set of operators, that do the calculations, may be defined arbitrarily but as per these laws. eg: incrementing a number by a unit number will give us the next number regardless of what system we are using. we ourselves have many systems of counting - binary, decimal, octal, hexadecimal, ~~roman~~ but the concept and the \"law\" of addition, subtraction, multiplication and division hold across all those system. incrementing a number always brings us the next number, regardless of the number system it's applied to.\n\nit could be argued and it should be noted that the system of logic, is sort-of arbitrary, but it stems from our objective perceptive of the universe and what we perceive to be true. but unfortunately there is no way of proving that our system of logic on which all math is built is true from within the system itself. but we have always perceived it to be true, by constantly applying it to the physical world and getting satisfactory results.\n\n > Would we be able to communicate with aliens through mathematics?\n\nif we want to communicate math, logic and facts about the natural world (based on our perception), then probably yes, as long as their system of logic is also the same, that is to say, their perception of the universe is more or less the same. if we want to communicate the humor, poetry, literature, current affairs or small talk then probably not - memes are far more efficient for those :P\n\nShameless plug for good ol' math\n\n\n\n >     Mathematics, rightly viewed, possesses not only truth, but supreme beauty, a beauty cold and austere, like that of sculpture, without appeal to any part of our weaker nature, without the gorgeous trappings of painting or music, yet sublimely pure, and capable of a stern perfection such as only the greatest art can show. The true spirit of delight, the exaltation, the sense of being more than Man, which is the touchstone of the highest excellence, is to be found in mathematics as surely as in poetry.    --BERTRAND RUSSELL, Study of Mathematics\n\nEDIT: making the answer as accurate as possible based on feedback.","label":0,"model":"human","source":"reddit","id":2682}
{"text":"This is for actual 5 year olds:\n\nImagine you're hungry. You're trying to decide what to eat. Almost every day, you eat at your favorite restaurant, Microsoft Windows. The food there is good enough, nearly everyone eats there, some people REALLY like it but most people think it's just whatever. Some people who claim they have a \"finer palate\" prefer the restaurant Mac OS X, which has food that's supposed to be both gourmet and homemade. A lot of people like to eat there just because it's hip. However, you decide to go to your local farmer's market called GNU\/Linux, and you buy all the ingredients for your meal and come home, then you cook your meal yourself. Now, your meal is exactly how you want it! It was a little extra work, and it might not look as fancy, but it was delicious and very cheap!\n\n(GNU\/Linux is actually free, so the farmers would give you the ingredients free of charge)\n\n\n*for the people who want to go on about distros, maybe you can think of it like a bunch of different frozen meals in a grocery store freezer? It's not a whole restaurant experience by any means, but it's nearly ready to go. And you do have to do a LITTLE work yourself.*\n\n\nAlso, a bit of semantics: the actual term for Linux-based operating systems is GNU\/Linux. This is because most of the code is part of the GNU project; the Linux Foundation only deals with the kernel. To use another analogy, imagine you have a car. Almost all of the car (tires, seats, framework, axles, windows, doors) are made by one organization (GNU Project) and the engine is made by another (Linux Foundation).","label":0,"model":"human","source":"reddit","id":2683}
{"text":"Throwaway because sort of embarrassing personal hygiene regimen, rather not have my GF know how much attention I pay to my body hair. ^^^Don't ^^^you ^^^judge ^^^me, ^^^you ^^^don't ^^^know ^^^what ^^^this ^^^is ^^^liiiiiiikeeee.)\n\nIn highschool biology class, we each had to look at our own hairs under a microscope one day in class. We discovered that each of my hairs was about twice the diameter of anyone else in class. My heritage is largely eastern european, and I have extremely thick, coarse hair everywhere. My father is the same, he literally has hair all over his body except for two slats on his back where his shirts rub it away. I haven't developed the underarm-to-thigh level of hair yet (thankfully), but he has. \n\nI have to use hair clippers with a 1\" attachment everywhere on my body, otherwise the hair does not stop growing. I've pulled 3\" hairs off my legs, and ~5\" hairs off my scrotum. I trim everything. I need to trim my eyebrows to about 1\/4\" every few days, and pluck the random outliers in order to not have a weapons grade unibrow. I only shave my face once a week, because it isn't worth it. I literally produce stubble within three or four hours, and my beard is so coarse it obliterates razor blades. I NEED to shave my ass-crack down to the skin at least once a week, otherwise pooping is like forcing a meatloaf through a screen door. \n\nI believe the reason the hair is able to grow so long is that due to it's thickness, it does not break or pull out easily at all. Combined with my hair growth rate of about 1\" a month, things get...ahem...quite hairy. \n\n**EAT IT:** It has been brought to my attention I have not answered the question. In short, my loins are honest-to-gosh, presented in technicolor proof that hair located in areas aside from the head can grow much longer than 2 centimeters. If I were to forego trimming completely, I believe over time my body hair would continue to grow to emotionally uncomfortable lengths. Maximum body hair length is probably due more to a variety of genetic factors and lifestyle choices, and not applicable generally.","label":0,"model":"human","source":"reddit","id":2684}
{"text":"It's important to keep in mind that a conductor is nearly always the leader of the orchestra, too.  So for weeks and months before you see him\/her waving a baton around, they were in charge of auditioning band members, selecting the music, organizing rehearsals, making artistic changes or interpretations to the piece as it goes, and directing each section and musician.  So there was a hell of a lot of behind-the-scenes work that we don't always think of.\n\nAs far as actual performance, a good conductor is giving a dozen instructions at once.  Speed, volume, and style are all communicated by single movements.  Are his beats large, waving motions, full of drama?  I'll bet there'll be horns blaring long, sonorous notes.  Are they short, tight, precise movements, close to his chest?  You're almost definitely going to be hearing soft, short staccato notes.  Is he facing the trombones, pushing one hand down while conducting with his other hand?  He's asking them to play softer - maybe the acoustics in this room aren't what they're used to, and they're overpowering the rest of the band more than expected.  There are a million little adjustments that will go on in any given performance, and a good conductor can make them on the fly in very clear ways.\n\nAnd of course, all of that body language goes to the audience too.  When you see that person swinging their arms in big, wide arcs, you'll get excited, even as the music begins to swell.  And when you see them sweep their arms in, you'll lean in, straining to hear the soft parts of the music.  The conductor is a visual cue to you to tell you what your ears can expect.","label":0,"model":"human","source":"reddit","id":2685}
{"text":"The voice you hear inside of your head is simply called \"inner voice\".  This voice may be:  \n\n* Your own, or at least what you interpret your voice sounds like to yourself.\n* A made up voice - some voice which you may have heard at some point in your life, or an entirely new one consisting of multiple voices smashed together much like voice actors do, to give you a representation of the character or overall feeling of the book or subject.\n  \nIf you want to try out the made up voice, just use your inner voice to read something in a different dialect, or for the bilinguals and multilinguals out there, translate the sentences as you're going into another language with your inner voice...... now distort that voice with a new dialect in that *new* language.  The options are limitless*.\n\n* A distinct voice you have *undoubtedly* heard... Such as the ability to read the following as Morgan Freeman:  \n  \nIt is a strange thought knowing we can manipulate each other's brains to do such wild things.  Peanut butter brittle is delicious.  No no, child... Keep reading.  This is still Morgan Freeman.  Now this is Arnold Schwarzenegger.  GET OUT OF HERE.  WHAT ARE YOU DOING?  \n  \nFor Linguistics, this is usually covered in Psycholinguistics and Neuro Linguistics, as they are the most concerned with inner voice speech.  \n  \n*^^^^^Containedwithintheparametersoflanguagesanddialects,notactuallylimitless.\n\n**EDIT:**  For those of you who are talking about your inner voices, here's something that happened to me:\nI was doing a lot of T-Pain impressions to some friends to annoy the shit out of them...  Well, it backfired...  I had my inner voice switch with T-Pain's for a good two weeks.  It was an absolute nightmare.","label":0,"model":"human","source":"reddit","id":2686}
{"text":"Federalist #39 (the series of letters written by the authors of the Constitution wherein they told us what it means) is very clear about why the Electoral college exists, and it isn't about \"communications difficulties\" or \"the people are too stupid to directly elect a President\" \n\nThey did it this way because they did a thorough study of the history of civilizations and discovered the both democracy and federalist governments have inherent flaws that tend to cause the society to collapse into despotism. The looked a Greek, Roman, and Lycean democracies, Germany Federal Republic, and others. The first 3 Federalist Papers are little more than a history lesson, and fascinating. \n\nOur system of government is crafted to avoid those inherent flaws by combining some parts of democracy (the House of Representatives, every citizen has an equal say therefore populous states have greater sway) with some parts of a Republic (the Senate, every state has an equal say regardless of population). The electoral college is the combination of those two ideas, neither wholly democratic nor wholly federal, but a combination of the two that overcomes the weaknesses of each.","label":0,"model":"human","source":"reddit","id":2687}
{"text":"Key things are transfer payments.  You could create iamkimi Brands that owns all your product's names and trademarks and charges iamkimi Europe and iamkimi America to use those names and trademarks.  Doing this smartly lets you recognize the profits wherever iamkimi Brands is located rather than where your sales are. \n\nThis gets your tax rate down to the level charged in a low corporate tax country (Ireland is the common example) at about 12.5%.  \n\nHowever, you can go much further.  Doing this uses some unintended consequences of EU and EU member nation laws, with the goal of creating income that no one calls earned in their nation.  \n\nBy setting up a Bermuda company that has an office in Ireland, Ireland doesn't consider it an Irish company, and Bermuda doesn't tax the income the company earned in Ireland.  You can't just do that with the first firm, but by taking a series of steps (the original was to have the iamkimi Brands pay a royalty to a Dutch subsidiary (iamkimi Netherlands) and then pay another royalty to a second Bermuda\/Irish company (iamkimi Brands2) you create income that neither country recognizes as corporate income.  \n\nAnd the US allows foreign subsidiaries to earn income indefinitely without tax so long as it's reinvested overseas, and you report to your owners the consolidated financial statements which include the income and assets from all your subsidiaries (the US firm iamkimi Holdings owns all the above firms so reports their income as it's own).","label":0,"model":"human","source":"reddit","id":2688}
{"text":"To add onto jeezfrk's comment:    \n\nPrice is controlled by what the market is willing to pay.   If someone is paying cash, then they usually can't afford to pay that much.    \n\n1. In steps the Government, with the best of intentions offering guaranteed student loans and \"free money\".    \n\n2. Suddenly a lot of people can afford the product, demand goes up for a limited amount of product (seats in class)\n\n3. Colleges raise prices to lower demand and increase ~~profits~~ spending ability (stadiums, coaches, etc)\n\n4. G'ment sees that the loans are no longer adequate and increases the amounts they'll loan.\n\n5. Go to Step 2\n\nOf course, we're reaching a point where people are questioning if the product (college education) is worth the cost, and quite a few people are saying \"nope!\"    Depending on the field, ROI is pretty shitty, really.\n\nSame basic thing happened with health costs, Insurance made \"free money\", so people didn't have to think about what it would cost them and the prices lost their downward pressure.\n\nEdit:  Fuuuuuck.   So this is what it's like being a top comment...","label":0,"model":"human","source":"reddit","id":2689}
{"text":"Because they don't know who they can trust - anybody they talk to about this could go and talk to the secret police about it, and then they and their family would have a very hard time of it.\n\nBecause they don't know they deserve better than they're getting - it's like if you only ever got sprouts to eat and never ice cream and chocolate, and didn't even know ice cream and chocolate existed. How would you know that there was better-tasting food you could ask for? They've only ever known hardship, they don't know how good the South has it.\n\nBecause they don't have the weapons. It's very difficult to go up against people with guns and tanks when all you have is everyday objects like hammers and forks and fists. They could do \"Human Wave\", which is basically \"everyone runs at the guns until they run out of bullets and then everyone left takes down the gunmen\", but that needs a lot of people to be very sure that they want to do this, and it's a scary and bloody thing.\n\nBecause even if one town did manage to throw out the government, the rest of the country would never know, because the government controls the news. Nobody would dare ask where the tanks and soldiers were going, because otherwise they're clearly Western spies and deserve to be shot.\n\nBecause a lot of them are quite scared of us. Their government tells them that the rest of the world wants to destroy their country and raze it to the ground and hurt the women and children, because this makes them look like a better option than us. The people there aren't sure whether or not to believe them a lot of the time, but they're certain that they're at least alive on what they have at the moment, and the chance of bringing hordes of Western monsters to eat them and their family is not one they're keen on.\n\nSo to sum up: They don't know they should, they don't know who would help them, they don't have the weapons, they wouldn't know if anyone else was doing it, and they certainly don't know what would happen next.","label":0,"model":"human","source":"reddit","id":2690}
{"text":"3rd year md here - cough is a reflex, i.e. it doesnt need cognitive control for the cough to occur, i.e. any particular irritant along your airway will trigger a cough. Some people can have a more sensitive airway and thus this reflex is triggered mote often.\n\nWhilst it does not need a cognitive input - the reflex itself passes through the brain - and it can be very nuch possible that cognition may influence a cough - whilst I can have a tickly throat i still can control whether to cough or not esp if your in a quiet room. There is an area in the brain called the cough center (we divide the brain in small modules responsible for specific functions called nuclei or centers).\n\n Certain medications - opiates and opiate derivatives example codeine and morphine, inhibit this cough center, hence why they stop coughs and hence why codeine linctus is a cough suppressant.\n\nSleep may have a play with the cough center as well, and the degree of irritation of the irritant also has to be considered, example if its a lifethreatining situation (i.e. no oxygen is making its way to the lungs hence to the brain - a cough  is exteemely likely to occur)","label":0,"model":"human","source":"reddit","id":2691}
{"text":"Imagine your parents gave you $50 for the latest video game. There are other kids selling sub-par lemonade, old video games, comics and orange juice on the block for various prices from $0.05-$0.25 a cup. A days labor earns them anywhere from nothing to a few bucks. And you see these kids with their mud-pies and urinade, and you think to yourself, \"I could totally do better.\"\n\nYou decide to put your $50 into a lemonade stand instead of a video game. Hell, you even innovate and add sugar to your lemonade (crazzzzzzzzzzy). Now, you can competitively price your lemonade or advertise Johnny mud-pie for a small portion of his daily $0.50 profits, but that'll just detract people from what you're offering. Plus, they might find it mildly annoying.\n\nNo, you have bigger plans than that. You're not running a mom-and-pop lemonade stand, you're going platinum. But nobody knows you, everybody knows Johnny mud-pie and Orange Juice Simpson. How do you bring people to the table? *light bulb* You spent $5 getting this lemonade stand started, why not use the remaining $45 for a year of operational costs and give your lemonade out for free.\n\nAt a price of nothing, your lemonade quickly becomes the rave. All the other kids have already blown their profits and can't sustain their business models, not when the competition is offering a product for free! Everybody's coming to you. Who would pay $0.20 for pee, besides Pedophile Peter, when they can get your awesome lemonade for nothing?\n\nOver the course of a year, you've built up a popular customer base. You've innovated a bit more, your lemonade now has caffeine in it, and all the local grown-ups stop by your stand instead of going to one of the 30 local Starbucks. Not to mention, everybody that passes through your neighborhood pulls over. Some people even offer a donation, but you lower your shades, give them a half smile, and in the deepest voice you can muster, tell em, \"It's on the house bud.\" It's time.\n\nJohnny mud-pies has moved a few blocks away. His mom caught his dad with somebody in their room. IN THEIR ROOM. They're going through a divorce and Johnny spends half his time on the other side of town. He's richer now though, Johnny. He's got money coming in from two separated neglectful parents instead of one neglectful couple. His allowance has tripled as his parents try to buy their way to his teeny little heart. He's thinking of getting back into the game. He's found a new source for mud. But he needs your help. He's seen your business acumen, the way you drove him out of business with a product that wasn't even in the same category.\n\nJohnny offers to buy some space on your lemonade stand to market his product. A year ago, you would've said hell no. Your customers would've said hell no. Hell, you wouldn't grow so quick as new customers would've been put-off. But now you got a steady stream of lemonade junkies. They ain't goin nowhere. Word gets around that Johnny mud-pies is raking in made cash. All of a sudden, several new and old business are popping up around the cul-de-sac and around the block. You started with 50 bucks, you grinded on that chump change for a minute, and now you can't even count the money you're making.\n\ntl;dr It's like cocaine, first hit's free.\n\nEdit: tnx 4 the gold n all the love guys","label":0,"model":"human","source":"reddit","id":2692}
{"text":"This is the [Commission on Presidential Debates](_URL_0_). They've host the debates since 1988. It is a non-profit funded by the two major parties. In 2000, in order to prevent a similar Perot effect or Nader probably in that year, they made a rule that any candidate to be included must be polling at 15% nationally. This is probably because the two parties do not want to give up the spotlight and therefore power. \n\nBefore that, the League of Women Voters, which is actually mostly a good governance\/good campaign group than a women's advocacy group, hosted the debates. George H.W. Bush and Michael Dukakis's campaigns in 1988 conspired to rig the format of the debate and the LWV disowned the presidential debates. They still are probably the organization hosting your local debates.\n\nI recommend looking over the \"criticism\" section of the [Wikipedia article on the CPD](_URL_1_). There are some starting points for learning about the alternatives and the lawsuits against the CPD. I know in the past, C-SPAN has hosted a third-party debate. \n\nedited to fix wikipedia link","label":0,"model":"human","source":"reddit","id":2693}
{"text":"There are various theories in Christian theology which attempts to explain this. The particular theory that I personally believe is true as Christian is this:\n\nGod has already in a sense defeated Satan and made him 'powerless'. This happened through Jesus dying on the cross. When Jesus died he took the sin of the world upon him, so that anyone who believes in him wont die but have eternal life (John 1:29, John 3:16). Satan seeks for people to die and prevent people from being in relationship with Jesus. Jesus 'defeated' him by providing a way in which imperfect people could be in relationship with a perfect God. A couple of supporting passages are 1 John 3:8 and Hebrews 2:14-15.\n\nSo Satan has been stripped of his power over death, because Jesus defeated death on the cross. However, we believe he is still active in this world and still seeks to tempt people away from life and towards death (tempt people away from Jesus and towards sin [disobedience to God]). However he judgement is already assured and is only a matter of time. On the last day Satan will be defeated completely and be judged by God (Romans 16:20 alongside 1 Timothy 3:6).\n\nELI5s of complex theological topics are hard...\n\nTL;DR: Satan has already been defeated, but not destroyed by Jesus death on the cross. He will be destroyed on the 'day of judgement'\n\nDisclaimer: Christian theology on the matter does vary, but I find this explanation the most biblically consistent of any I know.","label":0,"model":"human","source":"reddit","id":2694}
{"text":"Video compression works by throwing things out.  The trick is to only throw out the bits that you won't miss.  There are lots and lots of settings that determine what gets thrown out and when.  If you use transcoding software like handbrake with the default settings, the converted video will look good, but it won't be all that small.  You can start playing around with these settings, but unless you know what you're doing, the video may come out even bigger or it might look bad.  It takes practice and effort to know what to change for what types of movies.\n\nTo get *really* small (under 1.5GB) 720p rips that actually look good, you need to adjust these settings not just for the whole film, but from scene to scene.  This requires a lot of work, and getting it right is almost an art.  But no matter how good someone is at it, the h.264 codec is still not perfect and you're still throwing things out to make the video small.  Some people are more sensitive to the distortions caused by this extra compression.  You may not notice the difference (or at least not enough to ruin the experience), but others might.  So between the effort involved and some people still not liking these super-compressed rips, they're not very common.\n\nBut there is hope.  The Motion Picture Experts Group (MPEG) has [recently finalized the specifications](_URL_0_) for a new, better compression scheme called h.265.  It is said to be twice as efficient as the current popular codec, so in the future, visually-identical 720p videos can really be half the size they are now without all the effort that is currently required.","label":0,"model":"human","source":"reddit","id":2695}
{"text":"Nowadays? It can mean many different things depending on context and who's speaking.\n\nIt can mean:\n\n- Regulation of economy (left) vs. Freedom of economy (right)\n- Liberalism (left) vs. Conservatism (right)\n- Libertarianism (left) vs. Authoritarianism (right)\n- Communism (left) vs. Capitalism (right)\n- Globalism (left) vs. Nationalism (right)\n- Radicalism (left) vs. Traditionalism (right)\n\nAre these all accurate? Not really, but they're just examples of ways the terms are used. Of these, only the last is actually an accurate representation of the *original* use of the term - in the times of the French Revolution, anti-monarchists in parliament would sit to the left of the president.\n\nThe \"Left-Right\" scale is only really useful as a descriptor where there exist very few parties; generally, there will be at least one liberal (left-wing) party and one conservative (right-wing) party. When it comes to describing individual peoples' politics, it just doesn't allow for enough nuance to be very accurate.\n\nEdit: I've had a million comments telling me \"That's not what *x-wing* is **really** about! It's the other way around!\"\n\nYou're missing the point I'm making here. People's perceptions of left and right wing are all over the place - they're so incongruous and inaccurate that the terms themselves are becoming increasingly unfit for purpose. People's individual politics are just too nuanced for any sort of dichotomy to be helpful.\n\nThere's so many misconceptions and so much confusion about these two terms that I'd be pretty glad to see the end of their use.\n\nEdit2: plugging \/r\/badpolitics as a late afterthought","label":0,"model":"human","source":"reddit","id":2696}
{"text":"Scars are not made up of cells, rather they are collagen plaques that replace missing cells (acting like cement). So when cells are replaced the scars don't just disappear. There are some processes that cause the scar to shrink by pulling the edges of the scar closer and closer (cicatrisation). There are also little machines called enzymes that can chew down the scar slowly but not always completely.\n\n**EDIT**: Often in young children, scars heal faster and even disappear because their repair systems are more efficient than in older people. There are also other factors involved as well, for example, skin fold lines called Langer's lines (_URL_0_) show the natural direction and folds of skin. What surgeons found was that if you cut skin in the same direction as skin naturally occurs, scarring is minimal; whereas if you cut skin against the grain, the scarring is much larger. This is very important in modern surgery because patients don't like huge scars when they have surgery.\n\n-----------\nTattoo ink is taken up by white blood cells that ingest them. These cells are called macrophages, they essentially engulf and 'eat' the ink, but are unable to break down the ink. Because they cannot digest the ink, these cells remain coloured by the ink that they consumed. As with all cells, these macrophages do eventually die, but when they do, the ink is ejected and taken up by new macrophages which continue the cycle over and over again (because the ink is not fully broken down).\n\n**EDIT**: Some users have also pointed out that sunlight is also a factor. This is true, as sunlight can break down pigmented molecules which makes the ink appear less saturated or vibrant. This is because when pigments break down, the molecule cannot absorb the same colours of light anymore. Also, when the pigments break down, the macrophages can digest the leftover products more easily. This is also the basic concept of laser tattoo removal, except lasers are hella cooler.","label":0,"model":"human","source":"reddit","id":2697}
{"text":"Many answers here mention that remotes use IR, which is one-way, while controllers use bluetooth, which is two-way. This is close to being the answer, but I just wanted to add a bit extra. \n\nThe important thing to note is that bluetooth is connection-oriented. Devices establish explicit connections with each other, and then either device can talk to the other over the established connection. \n\nNow, because devices want to connect as soon as they can, they have to constantly be trying to connect, meaning at regular intervals they're sending out a signal just asking \"anyone else nearby speak bluetooth?\" This uses power. \n\nOf course, devices can get around this by either giving up and going to sleep after a while, which means the user will have to \"wake up\" the device before it tries connecting again. \n\nAnother reason controllers might draw more power is that electronics just always draw a little bit of power, even when they're completely off, and more complicated electronic with more components are going to draw more idle power. And since a controller is much more complex than a remote, it draws more idle power. On top of that, since the best way to prevent this idle power loss is to physically disconnect the battery, IR remotes can just use every single button as a physical disconnection, so the battery is only even connected to anything when a button is pressed. In a controller, a bunch of components need to be on continuously, even when no buttons are being pressed (like the bluetooth radio). So, to counteract the idle power draw, controllers come with on\/off switches, which can be used to physically disconnect the battery from the electronics and saving power.","label":0,"model":"human","source":"reddit","id":2698}
{"text":"i'm 30 and started smoking when i was 12.  by the time i was 15 i was a full fledged pack a day smoker.  over the years i have witnessed many people fall prey to the habit.\n\nyou start by smoking here and there, maybe you don't inhale at first, maybe you just like to blow it out your nose or smoke it like a cigar.  it's only on certain occasions.  \n\nif you are fiddling with cigarettes, you likely have friends that are smoking, also.  so you are hanging out, maybe having a few beers, and you see them smoking and decide to be social with them, and also have a cig, but alas, you have none.  so you bum a few off a friend for the night.  you kind of like it.  it just feels good to have one with some drinks or just when you're talking and laughing.  soon you do this every time you get together for a drink\/movie\/whatever.  it's not a big deal, it's just once in a blue moon.\n\nafter a while, your friends start complaining that you are always asking for cigarettes but you never seem to have your own, you cheap son of a bitch.  those shits are expensive.  maybe next time you could come prepared and let *other* people bum them.  so you buy a pack prior to the next gathering.  it feels weird to order them at the counter, but whatever.  it's only for the night and you feel like an asshole smoking everyone's cigarettes all night.  you see how they act when they run low.  you don't want to be that guy.\n\nso now you have your own pack of cigarettes.  it's just for the socialness of it.  but at the end of the night you have leftovers.  you leave them for a friend because what the hell do you need them for?  the night is over.  it's not like you're going to smoke them tomorrow.\n\neventually there comes a time where you decide you may as well keep the cigarettes you bought.  they *are* expensive and you're sick of having to buy them every time you go out, so you'll just save them for next time.\n\nnow, a weird thing happens when you have your own pack of cigarettes.  maybe you'll get the idea to just smoke one after a shitty day, maybe you won't.  maybe you'll have your own 6 pack while you watch the game.  if you have a few beers, you will notice a strange urge to have one of those cigarettes.  for the past few months, every time you've had drinks with friends you've had a few cigarettes.  it's like they go together.  it's just what you do.  but there's no one else there, it's just you.\n\nso you have a little debate with yourself.  you don't *need* the cig, but you sure would like to have it.  it's not going to ruin your night either way.\n\nif you decide to have that cigarette, you have fucking failed.  you are doomed.  *doomed*.  what you have done is solidified an association with having drinks and smoking cigarettes.  it is no longer a social thing.  you love the little buzz you get.  you love playing with it as you sip your drink.  you love trying to make smoke rings or whatever other cutesy shit you do to amuse yourself while you smoke it.  you don't realize it, but you now have a habit.  an itsy bitsy manageable habit, but a habit nonetheless.  \n\nyou may now find yourself looking forward to outings with friends because you can't wait to have an occasion to smoke a cig or two.  you might notice a feeling of \"nakedness\" if you have beer or two with dinner but no cigarette.  you might hang out with your smoker friends on non drinking occasions and feel that same sense of something missing.  then you see someone light a cig and it hits you.  you *want* that fucking thing.  shit.  you have another internal debate with yourself about whether or not to have a cigarette without the drinks.  you don't even have cigs on you.  if you decide to bum one now, you are officially screwed.  you gave in.  now you've solidified an association between social occasions and smoking.  you will come to expect this at gatherings.  going out to dinner?  let me join you for a cigarette.  cookout?  you don't mind if i have one of those, do you?  i didn't bring any because i'm not drinking.\n\nso now you smoke at social drinking occasions, you might smoke when drinking at home, and you also smoke when just hanging out.  your friends again chastise you for bumming their cigarettes.  buy your fucking own if you want one that bad, they say.  you promise to bring your own next time...\n\nnow you smoke often enough to expect a cigarette after certain occasions.  after dinner? smoke.  movie's over?  smoke.  drinks at bob's?  smoke.  you dun goofed, and it's all downhill from here.  you've accidentally built cigarettes into your life.\n\n**tl:dr** a chain of events will lead to your expecting cigarettes on certain occasions, and if you buy them to fulfill this expectation, you're screwed.","label":0,"model":"human","source":"reddit","id":2699}
{"text":"TL;DR - It's a little difficult to know actually.\n\nFirst it is a draft text. Negotiations like these go through dozens if not hundreds of draft texts. Each one can change things drastically - or just be updated punctuation. This could be one which has been tossed, or one which is about to be released as the official version. No way to know really from what I've seen.\n\nSecond, These treaties often have a huge amount of lee-way. This allows Pro-Copyright parties to claim victory and Anti-Copyright Parties to claim the sky is falling. An example of this might be text which states \"And the Government shall take all reasonable actions to enforce the Copyright Provisions laid out in the above.\" \n\nA reasonable action would vary from state-to-state. In Canada, for example, jail time for copyright infringement is unlikely to be found constitutional (IMO). More likely the punishment wouldn't vary much from the current laws in Western Countries - these sections are mostly aimed at Africa\/Third world places where infringement is rampant and no controls are enforced. It also \"sets the bar\" for countries looking to join the TPP by providing some guidelines to work by. \n\nThe biggest implication is that Copyright laws may be extended so that works gain even more copyright protection. Reddit is (unlike most of the other assertions) broadly correct that copyright at this point is a harmful mechanic in society. Without getting into a rant, TPP or similar treaties all generally see an alignment \"upwards\" of standards within member nations. A good example of this is Canada, when it signed a Free Trade Agreement with the EU, added two years of Patent protection to medicine so that it and the EU were the same. You could expect similar provisions within the TPP to avoid any state undermining others. \n\nThis is all very broad, but that is because I wouldn't get into the sky-is-falling basket until you have a real text in hand. Understand that Governments negotiate in private to avoid this sensation - for example, one provision might look very deadly alone, but your Government may only have agreed to it because you were gaining several other concessions for it which seem mild and garnish no attention. \n\nI know you don't want links, but I would recommend reading the top entry on this blog: _URL_0_\n\nMichael Geist is a Law Professor at the University of Ottawa and holds a view Reddit would generally agree with. I highly recommend giving it a read as it is relatively brief.\n\nedit; Thank you kindly for the gold!","label":0,"model":"human","source":"reddit","id":2700}
{"text":"So! It appears my food background may come in handy again.\n\nBasically, this depends on the type of meat. It would be unreasonable to believe that giant ham or turkey is one solid muscle when sliced at the deli.\n\nEssentially there are several ways:\n\nWhole cuts a solid muscle that is removed from the animal and no additional meats are added. BUT! what I believe you are asking about is processed meats. \n\nProcessed meats are usually never one whole muscle. For example ham: Hams are normally taken, emaccerated (basically imagine a large piece of meat run through a giant lot of knives. Those knives make large cuts and gashes in the meat). This is then added into vacuum tumbler with other meat proteins (other parts of the animal that aren't as large or desirable to look at. This is then tumbled with the addition of phosphates to help \"glue the meat together\", and then some flavorings normally called a brine (sugar, salt, a cure and other flavors) are added. The vacuum tumbling action mixed with the meat and phosphate causes the meat to bind together in a sort of sticky way. This ham is then shoved into a ham casing (a VERY tight fit!). This is then smoked or cooked, causing the meat to blend together and look like a whole muscle. This is typically the ham or turkey you buy at a deli.\n\nTL;DR: Meats like these are usually several parts of the animal that are \"formed\" together. Add flavor, shove in casing, cook, slice, eat!","label":0,"model":"human","source":"reddit","id":2701}
{"text":"I used to work at a fine dining restaurant that served a fancy brunch buffet every Sunday. \n\nOne day a very thin lady started to show up every Sunday right when we opened.  She always came in alone and made a bee line for the bowl of peel and eat shrimp.  She would load her plate up with nothing else then go back to her seat and eat all of them. Then she would wait until we refilled it and then she would go back again.   She would do this until we were out.   \n\nThe waitstaff wondered how she was able to eat 6+ pounds of shrimp in a few hours.  I thought she was putting it into her purse.    On her third visit, one of the female staff discreetly monitored her every move.   She quickly found out that our guest would make quick trips to the bathroom where she would vomit up everything she ate and then go back and load up on more shrimp. \n\nShe got the incorrect nickname \"Anna\" short for Anorexic (It should have been Bulimia but we weren't clever enough to come up with an innocent female name we could refer to her as in front of other guests).   \"Anna\" would also rarely speak to us.  She would smile when appropriate, and either nod or shake her head when asked a question. \n\nOnce our boss (who never came in on Sundays) found out he did some quick calculations and figured she was losing us too much money.    Rather than ban her, he simply had us change our peel and eat strategy.  We broke up the peel and eat shrimp from one big bowl into several smaller bowls scattered throughout the buffet line.   We were told NOT to refill them until she left.  \n\nThe next Sunday, \"Anna\" showed up and at first frowned at the change in her routine. She had to go through the entire buffet line to get all of the shrimp.  She also had to empty each of the bowls as she came across them.  Her usual attempts at discretion were no longer working and the guests in line with her started to make comments like \"Wow- are you on a shrimp only diet?\"  and \"Hey, leave some for the rest of us!\"   \"Anna\" hurried back to her seat, resumed her usual ritual of binging and purging and then sat quietly waiting for us to refill the bowls.  I only refilled the one at the end of the buffet so other guests could get some before she got to it.    I left the rest of them empty.  If any other guest asked for them, I would tell them we were out then quietly take a small bowl out to their table (out of sight of \"Anna\" of course).    \n\nAfter about an hour of sitting with a plate of shrimp peelings, \"Anna\" was forced to actually interact with us and ask about the shrimp.  When she was told we were out she became visibly upset and stormed out.     We never saw her again after that.","label":0,"model":"human","source":"reddit","id":2702}
{"text":"Most of your brain can't really process faces as a whole. Instead, it looks at all the individual pieces of a person's face (eyes, nose, mouth, marks, spacing, etc.) We have a special part of the brain that is basically just dedicated to keeping track of all of these different parts and the relationship between them. This part of the brain is what \"assembles\" all of these parts into our perception of that person's face.\n\nWhen you start taking away parts of the face, it gets harder and harder to assemble that final image. The eyes, being very unique between people even at a glance, tend to contribute very strongly to our ability to piece together that final image of a person that we ultimately recognize.\n\nIncidentally, some people have [facial blindness](_URL_0_). The part of the brain that pieces together the parts of a face into a single image doesn't work in these people. They can still see all the individual parts of a face, but since they can't combine them, they are unable to recognize a person based on their face.\n\nedit: missed a word.","label":0,"model":"human","source":"reddit","id":2703}
{"text":"1. No known species of reindeer can fly. But there are 300,000 species of living organisms yet to be classified, and while most of these are insects and germs, this does not completely rule out flying reindeer, which only Santa has seen.\n\n\n\n2. There are 2 billion children (under 18) in the world. But since Santa doesn't appear to handle Muslim, Hindu, Buddhist, and Jewish children, that reduces the workload to 15% of the total - 378 million or so. At an average rate of 3.5 children per household, that's 91.8 million homes. One presumes there's at least one good child in each. \n\n\n\n3. Santa has 31 hours of Christmas to work with thanks to time zones and the rotation of the earth, assuming he travels east to west. This works out to 822.6 visits per second. This is to say that for each Christian household with good children, Santa has 1\/1000th of a second to park, hop out of the sleigh, jump down the chimney, fill the stockings, distribute the remaining gifts under the tree, eat the snacks, get back up the chimney, get back in the sleigh, and move on to the next house. Assuming that each of these 91.8 million homes are distributed evenly (which we know to be false but for the sake of these calculations we will accept) we are now talking about .78 miles per household, a total trip of 75 1\/2 million miles, not counting bathroom stops. This means that Santa's sleigh is traveling at 650 miles per second, 3000 times the speed of sound. For comparison, the fastest man made vehicle, the Ulysses space probe moves at a poky 27.4 MPS; the average reindeer runs at 15 MPH.\n\n\n\n4. The sleigh's payload adds another interesting element. Assuming that each child gets nothing more than a medium sized LEGO set (about 2 pounds), the sleigh is carrying 321,300 tons not counting Santa, who is inexorably described as overweight. On land, conventional reindeer can pull no more than 300 pounds. Even granting that 'flying reindeer' (see point one) could pull TEN TIMES the usual amount, we cannot do the job with 8 or even 9, we need 214,000 reindeer. This increases the weight, not even counting the sleigh, to 353,430 tons. Again for comparison this is 4 times the weight of the Queen Elizabeth 2.\n\n\n\n5. 353,000 tons traveling at 650 miles per second creates enormous air resistance. This will heat the reindeer in the same manner as a spacecraft re-entering the earth's atmosphere. The lead pair of reindeer will absorb 14.2 QUINTILLION joules of energy. Per second. Each. In short, they will burst into flame almost instantaneously, exposing the next pair of reindeer, and creating deafening sonic booms in their wake. The entire team will be vaporized within 4.26 thousands of a second. Santa, meanwhile, will be subjected to centrifugal forces 17,500.06 times the force of gravity. A 300 pound Santa would be pinned to the back of his sleigh by 4,315,015 pounds of force.\n\n\n\n6. Conclusion: If there was a Santa, he's dead now\n\n >  lost original source, anyhow have it? \n\n**edit:** thanks to manvsfriction for the link to snopes [_URL_0_](_URL_0_)","label":0,"model":"human","source":"reddit","id":2704}
{"text":"I can only speak for the U.S. But there are multiple reasons. For domestic, non-foster care adoption, it is mostly legal fees and court costs. And if the mother does not have health insurance you may be responsible for the medical fees. You are not allowed to pay the mother for the adoption, but you can do things like buy her a car if you feel she needs one. \n\nFor adoption from China a lot of the money goes to the insane amount of government forms needed from both the U.S. And China. You also have to pay around $3000 to the orphanage. A large amount if the expense is travel. You have to stay in China for at least 3 weeks and that adds up.\n\nIn the grand scheme of things, it is not that much money. When people act shocked by it, I ask them how much they paid for their last car. And which is more important to you? A child or that shiny new car?\n\nAlso, in the states there is a large tax credit for adoption .(I think it is up to 12,000) now. \n\nSource: adopted from China about 9 years ago, my brother has adopted 3 kids domestically.","label":0,"model":"human","source":"reddit","id":2705}
{"text":"Yes, there's an amount of pain a person can feel before they pass out and it is measured on the pain threshold (literally the scale that determines the minimum amount of stimuli to elicit a pain response to the polar opposite), but it is *entirely* subjective. Furthermore, burning to death isn't a super awful way to die (not the best either). What happens is that it's an extremely painful, but shortly lived pain because what is happening is your most superficial nerves are being destroyed, after that - not so much pain. Most people who are set on fire actually die from asphyxiation from inhaling the toxic gases and smoke their body is producing, so it's not entirely known when exactly the pain stops and the dying begins. I think, perhaps, acid would be a more painful way to die because it would eat away at your skin and organs while cauterizing your blood vessels until you eventually died and there is no chance of reversing the effects as there would be if someone is set on fire.\n\nHowever, there is a pain index called [The McGILL Pain Index] (_URL_1_) and you may be able to see how they came up with that scale to better answer the question.\n\nEdit: More information about [how] (_URL_0_) they created the McGILL pain index.","label":0,"model":"human","source":"reddit","id":2706}
{"text":"There are 3 ways to achieve backwards compatibility:\n\n1) Make the new console essentially a more powerful version of the old one. This is why the Wii could play GameCube games, and the Wii U could play Wii games. They used components that were compatible with software made for the previous console. This is kind of like how a new PC with the latest CPU can play games that were made for PCs 10 years ago. The hardware is fundamentally backwards compatible.\n\nThe problem with this approach is it limits the options for designing a new console. You can't necessarily use the latest and greatest CPU if it's not compatible with the CPU in the old console.\n\n2) Include the hardware of the old console in the new one. This is how the PS2 had backwards compatibility with PS1 games, and how early PS3s had PS2 compatibility. While the new console was significantly different from the old one, they still incorporated the hardware necessary to run the old games.\n\nThe problem with this approach is that it makes the console more expensive because you essentially need two consoles in one. That's why they dropped PS2 compatibility from later PS3s.\n\n\n3) Software emulation. This is how the PS3 could play PS1 games, and how the Xbox One can run Xbox 360 games. The new console doesn't have compatible hardware, but instead it has a program that pretends to be that hardware so it can execute programs written for the old console.\n\nThe problem with this approach is that your new console needs to be significantly more powerful than the old one. Emulators take a lot of processing power to be able to run games at a playable rate.\n\nThe PS3 used a very unusual CPU called the Cell. And Sony's plans for it didn't really work out. Developers found it hard to use, and although it theoretically had a lot of power, games typically performed better on the Xbox 360 which had a more normal architecture.\n\nSo Sony didn't want to use the Cell again. They wanted to use a more typical PC-like architecture that developers know how to use effectively. So option 1 was out.\n\nHaving the PS3 hardware alongside the PS4 hardware would have been too expensive, so option 2 was out.\n\nOption 3 isn't viable because the PS4 just isn't fast enough. Thanks to the weird architecture of the PS3, emulating it would be very complicated. So there's very little chance of them making a PS3 emulator that performs well enough.\n\nMicrosoft have had success with option 3, but that's because the Xbox 360 had a more typical hardware architecture. So in a sense it's easier to translate software instructions for the Xbox 360 into ones that the Xbox One can understand.","label":0,"model":"human","source":"reddit","id":2707}
{"text":"Not sure why no one is mentioning that the city was going broke due to poor management and people moving to the suburbs. The result was almost 10,000 police officers off the streets while corruption for the rest rising. They were poorly paid and supervised not to mention it was a dangerous job that attracted the wrong types of people. Throw in cuts in welfare, loads of crack\/cocaine coming from Florida via Columbia and Mexico, rise of violent gangs, closure of psychiatric hospitals and treatment centers, falling property prices, loads of abandoned buildings especially downtown. Not as many people on the streets especially at night made it easier for criminals. The mob took full advantage and contributed to a lot of the violence even though they kept some neighborhoods mostly in Brooklyn safer.     \nEdit. I would like to add that people left the city in droves back then to the suburbs in Long Island and NJ on the newly made highways commuting back during the day or moved to places like California and Florida. That resulted in less tax dollars. New York had been loosing industry and businesses for years that either relocated or became outdated. TV and film moved to LA. The rise of the financial industry had yet to happen, all those banks and the jobs they created started the 90's.","label":0,"model":"human","source":"reddit","id":2708}
{"text":"Visceral fat is not necessarily dangerous in and of itself but is associated with insulin resistance leading to type 2 diabetes and metabolic syndrome.\n\nSecondly, visceral fat doesn't exactly pile up in specific spots. If your organs are rocks, visceral fat is kind of like the dirt packed around them. It's all over the place and fills a lot of space. This is compared to subcutaneous fat that can be removed  in liposuction, because it's found just under the skin and is kind of in a different layer from the vital organs.\n\nAnother issue is that fat is vascularized, meaning that it bleeds. Surgery is performed with a specific goal in mind and is planned to achieve that goal in the least invasive and most efficient way possible. We've stopped cutting people wide open to remove an appendix and instead do these surgeries laparoscopically (could be it's own ELI5) because the least damage possible is done. Surgery is very much something we do when we can't accomplish something in a different way.\n\nRemoving visceral fat would be a very long, very bloody surgery that would basically be like digging up a lot of dirt in a lot of places. Think tearing up your backyard. If you did it, it would be a big operation with lots of bleeding and a horrendously long recovery. It's just not practical. Surgery is very traumatic to the body, and this would be a massive one.\n\nThere are far more practical options - namely lifestyle modification. It's free, no medicines, no surgery, and has the best outcomes. Bariatric surgery to promote catabolism would also decrease visceral fat if lifestyle modification failed.\n\nTl;Dr - Cutting things out is kind of a last resort. Cutting out visceral fat is not practical and would be very dangerous because of where it is in the body (everywhere).","label":0,"model":"human","source":"reddit","id":2709}
{"text":"I can't speak for the others, but most Chinese keyboards work using the pinyin system (_URL_0_), which is a transliteration of the sounds of the language (so that it can be written in the letters of the English alphabet). Basically, you download a font package that puts a little icon on your task bar to switch between writing in English and writing in Chinese. When you have the Chinese font selected, you just type in a pinyin word using English characters, like \"ma.\" \n\n\nThe thing is, Chinese is a tonal language. That means you can say \"ma\" in many different ways and it means different things. There are five tones in Mandarin (high flat, rising, dipping down then up, sharp drop, and neutral). Just like with English, sometimes even the same sound means a different thing (e.g., \"plane,\" \"plain\"). So for \"ma,\" there are something like 80 possible words for the sound. You can type in just the English letters (\"ma\") or maybe the English letters plus the tone (\"ma4,\" sharp drop in tone). When you type that in, a bunch of characters will appear just above your cursor and you can use the arrow keys or your mouse to select the right one. Usually, the characters appear in order of how common they are, so it doesn't take too long to find the right one.\n\n\nThere's also keyboards that have character strokes, different types of lines that are used to compose a Chinese character. Because these strokes are used in a very specific order when writing any Chinese character, you could also use that keyboard by typing the strokes in the right order. I've never used that sort of keyboard, though.","label":0,"model":"human","source":"reddit","id":2710}
{"text":"Technically speaking Blackwater was not specifically hired (on paper) to fight or to wage battles. They were hired on a contractor basis to run protection details for specific individuals, both in the private and public sector. The US Govt hired them to do certain tasks and cover dignitaries. And Haliburton\/KBR (Dick Cheneys company) hired them to play machine gun bodyguards to cover the oil men. So the difference is they are specifically hired for protection, and not hired in a basis to actually instigate or start battles. Where as a true mercenary would be specifically hired to fight battles. \n\nThat is what the difference is \"supposed\" to be, and what Blackwater was \"supposed\" to do. (And a good majority of them did do).\n\nHowever, the reality of it though is a good portion of blackwater jumped through 1 legal fucking loophole after another... and they sure as shit were mercenaries. 4 \"Contractors\" are serving serious time while their \"boss\" hung them out to dry, and sold blackwater to a group of investors only to make millions.","label":0,"model":"human","source":"reddit","id":2711}
{"text":"Browsers such as Firefox and Chrome implemented standards proposed by the W3C (a body that sets standards for the web) which make a page to look the same across browsers. Microsoft saw these standards as a threat and did not want the web to be interoperable (i.e. one browser can be replaced with another); so it proposed a different set of web standards that favored them and implemented them in Internet Explorer.\n\nFast forward ten years: Microsoft's clients, especially large organizations, which use Internet Explorer have still retained their old websites but the outside world has moved much faster. Microsoft cannot do both i.e. adapt to the outside world and support its existing clients, so it tries to do a bit of each leading to Internet Explorer being suboptimal.\n\nAs for the hatred towards Internet Explorer, its initial lack of support for W3C standards resulted in website designers having to work hard to display a website in Internet Explorer and all other browsers. Although it is not as true today, the hatred continues.\n\nEdit: Added the last paragraph","label":0,"model":"human","source":"reddit","id":2712}
{"text":"I don't know about Java but I can explain Adobe. \n\nAdobe Flash and Adobe Reader are 'catch all' tools designed to read a huge library of file types and any special characteristics of those files.  This is especially true for Adobe Reader.  The Adobe Acrobat suite can do far more than simply convert Word documents to PDF.  As an example, you can embed full 3D environments in a PDF file!  Spiffy! The problem is that Reader and Flash require extra programming to support these features even though many people don't use them.  This increases the amount of coding required which means the chances of vulnerabilities also increases due to all the additional code needed to ensure these features work.  When a vulnerability is found, it's usually related to one of these advanced features that few people use.  Since the application doesn't let you pick which components to install, nor does it have a separate installer for each of these advanced features, the entire application has to be re-installed to fix this one problem.\n\nThere's also a second part to this.  The downside to Adobe having their software installed on almost every computer on the planet is that hackers are going to focus on them.  This is also true for Java.  Their software is everywhere.  This makes them a prime target as the number of computers that can be exploited whenever they find a new vulnerability is huge.\n\nAdobe could easily fix this if they released a 'Lite' version of their Reader and Flash players that contained only the essential tools to view simple PDF files and videos but they choose not to because it goes against their 'one app that does everything' model.  While there's no replacement for Flash and we're kind of stuck with it, there are plenty of PDF viewers that are far more secure than Adobe Reader.  They don't contain all the advanced features of Reader but that's okay - no one uses them anyway.\n\nSource - I used to build, test and deploy Adobe patches for a large company that used their products heavily.\n\n*Edit - clarity in the first paragraph*","label":0,"model":"human","source":"reddit","id":2713}
{"text":"Most of the answers here are plain wrong.\n\nActually, you try to have the load as close the the helicopter as possible (within reasonable limits, of course). There are several problems with dangling it far below:\n\n* Vibrations can build up in the wire, even to the point where the hook releases (I've seen that happen).\n\n* The load becomes a giant pendulum, which takes skill to keep in check, as it tends to have a will of it's own. All manouvres have to be planned further in advance, and done with more precision.\n\n* When the load has been dropped, the long wire is a potential hazard if it's not weighted down properly. You don't want it to snap up into the tail rotor.\n\n* Maximum speed is lower, due to above problems.\n\n* The pilot has a harder time being really accurate when hooking\/unhooking, as the load will be further away.\n\nUsually, the load is just hanging a meter or two below the helicopter. You want enough clearance so that the cargo won't hit the helicopter, should the weight shift.\n\nThere are exceptions, of course:\n\n* When the circumstances don't allow a short wire. For example, if there are trees or you are building a power line.\n\n* When making a movie. It looks more impressive with a long wire.\n\n* When the load is large, so that the downwash will push down on the load. For example, some large antennas or building materials.\n\nSource: My father was a helicopter pilot, and I often worked with him.\n\nNote: I know they usually don't use a wire, but I don't know the proper English word for the nylon loops used. \"Stropp\" in Swedish.","label":0,"model":"human","source":"reddit","id":2714}
{"text":"Source: I'm a Windows 10 insider member and have been using it as my daily driver on my Surface and my desktop since the start of the program.\n\nThe easiest way to look at it is this:\n\nW7 was a very nicely polished *desktop* operating system that was very well optimized.\n\nW8\/8.1 was Microsoft's attempt to duct tape together two completely different computing environments: Desktop and Mobile. Of course, execution was awful, and the idea of jumping back and forth from tablet to desktop just didn't work well. (Plus an empty app store didn't help things either)\n\nW10 is Microsoft removing the duct tape, giving Windows 7 a new paint job, and attaching some mobile features to the desktop environment with what could be considered a welding gun, for a much more polished finish.\n\nInstead of forcing Windows users to jump recklessly back and forth between desktop and Mobile environments, regardless of their computer type, Microsoft has essentially put the choice in the user's hands. Are you using a desktop? If so, here's desktop mode. Users can use a more familiar desktop environment, with the return of a repainted start menu, plus, all of your favorite desktop apps are still there, including a new web browser that's a vast improvement over IE. Of course, its a Windows app instead of a regular program, but they covered that too. In desktop mode, a familiar action bar is present at the top of Windows apps, making the transition from program to app less severe.\n\nNow there's also a new feature called tablet mode. It's specially designed to be used when on a tablet, and switches the start menu to something similar to what we saw it Windows 8. Apps also become full screen, but the taskbar is still present at the bottom, to retain familiarity. And fret not! You can still use desktop apps in tablet mode, its just that tablet mode is, well, designed better for tablet stuff.\n\nA new feature W10 offers is Cortana, Microsoft's fun personal assistant in direct competition with Siri and the Google Now lady. She's really excellent, calls you by name, and really works as an effective PA, like, better than Siri or GNlady imo. She also searches with Bing, so there's that too.\n\nW10 also offers everyone's favorite, Virtual Desktops! Basically, this allows you to minimize clutter on your desktop, by separating different programs you're running into different Desktops. This has been something on Unix systems for years that people have been begging Microsoft to adopt, and they finally have, so yay!\n\nMicrosoft also used some magical wizardry to shrink down Windows noticeably. With W10, if you ever need to factory reset your PC, the reset function actually rebuilds Windows from scratch, rather than copying files over from a different partition, which saves a huge amount of space. Plus, if your computer comes prepackaged with bloatware, you can simply do a factory reset and have a fresh new Windows install!\n\nWe might be seeing some more apps coming to W10 soon as well. With the whole universal app platform, apps can be easily designed to run and change appearance whether you're on desktop, tablet, phone, or HoloLens (maybe even xbox, who knows), which simplifies things for developers. Why develop 4 or 5 apps, when you can develop one? (Plus, easy Android and ios app porting is coming, making more app creation even easier), so that's pretty great too if you're into that kind of thing.\n\nFinally, if you have Windows 7, you can upgrade to W10 free within the first year of release, so it won't even cost you anything to upgrade.\n\nOf course, there's a lot of under the hood performance improvements and changes as well that are a little to complex for an ELI5, but those are the basics that you would see as an end user.\n\nTl;Dr: W10 is a very well polished operating system with almost seamless desktop\/mobile switching, that will still be familiar to users of Win7, but with better performance, less bulk, and a heap of new features, plus, its free, so why not.\n\nEdit: \/u\/ncshooter426 reminded me: Windows 10 will also have DX12, which will be the new version of Direct X, which will allow for some pretty sweet next-gen gaming in the future.\n\nPlus, there's a lot of enterprise improvements as well, including Microsoft Exchange  &  enterprise security features, so for the IT guy inside all of us, we can rest peacefully knowing that our data is safe.\n\nSome of the under the hood improvements courtesy of \/u\/DerJawsh:\n\n* Built in virus protection using MSE, not essentially the best, but still, native protection.\n\n* Hybrid boot to make boot up extraordinarily more quick.\n\n* Overall performance improvements, making Windows faster than ever before.\n\n* New, Revamped, and highly functional task manager.\n\n* Native ISO mounting support so you can run virtual disks without having to use a third party app.\n\n* File transfer options, you can now pause, or cancel a file transfer and see much more detailed statistics about it. \n\n* Greatly enhanced driver tech which means that performing a clean install will be MUCH easier. Windows will be able to detect your GPU and download a driver automatically for it. \n\nObligatory \"Thanks for the Gold!\" Edit.\n\nOne more edit for those who are curious (from my comment below):\n\nHere's a picture of what my desktop looks like (with start menu open) in the latest build:\n\n_URL_0_\n\nAnd heres a picture of everything in tablet mode, if you're interested.\n\n_URL_1_\n\nAnother edit cause a lot of people are asking: live tiles on the start menu are removable. Right click on them and press \"Unpin from Start\". If you remove them, its more similar to Win7's start menu than it already is.","label":0,"model":"human","source":"reddit","id":2715}
{"text":"ER Physician here.\n\nThere seems to be a bit of confusion. I think it's because some people are talking about concussion, others are talking about an undifferentiated head injury, and others are referencing intra-cranial bleeding. These are separate issues.\n\nIf you are diagnosed with a concussion - go ahead and sleep. You need physical and mental rest to start the recovery process. You have suffered an injury of brain function. The vast majority of people are in no danger of clinical deterioration or death unless you suffer another blow within a short time window and are unlucky enough to get 'second-impact syndrome'. There is no bleeding, no skull fracture, etc., with a concussion. Yes, a small percentage of people with head injury can have delayed bleeds. There needs to be some degree of vigilance in the elderly, alcoholics, and people on blood thinners...but overall this is a very small subset of people. \n\nHowever, if you have an undifferentiated head injury, then drowsiness becomes problematic. Are you becoming altered because your intracranial pressure is rising from your epidural or subdural hematoma? Or are you just sleepy? Some people have mentioned the 'talk and die' phenomenon. Quite right. Immediately after the injury you may be completely lucid, only to become more and more somnolent as you are herniating your brain down your foramen magnum and squishing your brainstem. This is very different from a concussion.\n\nTL; DR: Undifferentiated head injury needs evaluation to rule out serious problems. Concussion gets you a free 'sleep all you want' ticket. Key is in the diagnosis.","label":0,"model":"human","source":"reddit","id":2716}
{"text":"Hotel IT Guy (of sorts) here:\nIt can't be ignored that this is a rather profitable revenue stream for hotels. A large hotel, of around 400 rooms, may spend upwards of $50,000 per year on bandwidth costs and upkeep to run a resort wide WiFi network. Which means they need to sell about 10 WiFi connections at $15 per day to break even for the year. Everything on top of that is gravy.\nNow for the justification from the hotel side:\nIf you're clientele consists of business travelers or upscale vacation goers they expect a certain level of service for anything they get; be it food, leisure activities or internet access. That means if Mr. VIP CEO has a web conference at 1am because he is video chatting with people on the other side of the world the WiFi in his room better work or there will be hell to pay. In order for the hotel to attract that type of clientele, and more importantly, maintain that guest as a return visitor they need to have a ROBUST WiFi infrastructure.\nThe internet that is given away for free at cheaper hotels and motels is usually offered on a 'best effort' method. Meaning if it doesn't work the hotel is not required to do much to fix it because you didn't pay for it. These networks are usually far less robust and not well managed, meaning that one guest can bogart all the bandwidth at the expense of others.\nAt the hotel I work at the compromise for this is that they offer 'free wifi' in the lobby and other public spaces so if someone complains saying they really need to check their email or something they can just hang out in the (very well appointed) common spaces.","label":0,"model":"human","source":"reddit","id":2717}
{"text":"Someone says you've done something bad, so you are arrested and you go before a judge that will decide whether or not to take them seriously -- not to decide if you did something bad, just to decide if it's possible.\n\nThe judge says \"OK, maybe you did something bad, but it's going to take a lot of time to get everyone together to figure this out.\" Then, \"You're innocent until someone proves otherwise, so it wouldn't be fair to punish you before you've gone to court; is there anyone here that has a problem with me letting this person go home until their trial comes?\"\n\nAt that point, the prosecutor says something like: \"well, this guy has already been in prison for murder before\" or \"let me show you the video of him eating the victim\" and the judge will respond \"wow, that's super serious - you can't go home because I think you might be dangerous; bail refused\".\n\nOr, the prosecutor will say something like: \"Well, this is a very serious thing and what if they run away?\" and the judge might say, \"I see that, but this person's never had problems before; I'll tell you what, how about we agree that they set aside some money in an account; if they don't come back to court when they're supposed to they lose the money and you can use it to pay someone to go get them - in exchange, we let them carry on with their lives until the trial\" at which point the judge will consult a table and lookup some customary figure for bail.\n\nLastly the judge can just say, \"well, this is just stupid, this person's a well known member of the community, the crime isn't particularly violent and the situation doesn't seem to indicate that they'd do something like that again, even if they did do it, so the person can go without bail as long as they promise to come back later\".","label":0,"model":"human","source":"reddit","id":2718}
{"text":"Reagan is a venerated figure among Republicans because he represents a return to relevance for conservatives in America. They had largely been out of the White House ever since the fall and failure of Herbert Hoover's presidency. \n\nThe only two elected Republican presidents were Eisenhower (who had the whole \"War Hero\" thing going for him) and Nixon (who benefited from a split Democratic vote in '68 and a terrible democratic campaign in '72). \n\nReagan is also credited as the president who brought down Communism. The fall of the USSR and the related collapses of Soviet supported states is the largest US \"victory\" that we've had since WWII. Whether or not Reagan's policies actually brought about this change more quickly than anyone else's is a debatable matter, but serving presidents get more credit than they deserve when things go well, and more blame than they deserve when things go wrong.\n\nHis successful presidency also redeemed the disgrace of the Nixon administration in the minds of many conservatives.","label":0,"model":"human","source":"reddit","id":2719}
{"text":"It's useful to remember that there are very, very few countries that have borders with a country that drives on the opposite side of the road.  Most of the countries that drive on the left are islands or are geographically isolated eg. Britain, Japan, Australia.\n\nHowever, there are some that must deal with the problem.\n\nChina has to deal with Macau and Hong Kong which both drive on the left.  At the border, there are fairly complicated over\/under passes which divert traffic from one zone into the other while also moving them to the opposite side of the road.  The end result is that traffic keeps flowing without stopping (Source: I've been to Macau  &  Shenzhen).\n\nFrom some Google research, countries like Thailand have border crossings with a system of traffic lights which serve a similar purpose to the Chinese road passes.  As your lane lights up, you are diverted to the opposite side of the road as you cross the border.\n\nFor the Chunnel between England  &  France, cars are loaded onto trains and delivered to the neighbouring country heading on the correct side of the road.  No lights or road passes needed.  Source: been there too.\n\nHope that helps.","label":0,"model":"human","source":"reddit","id":2720}
{"text":"Doing parody of things that actually exist can be a legal minefield, and one has to think about what you say to not be defamatory even if you're just joking.  No show can say that Mcdonalds uses rat meat in its burgers to save cost without immediately getting sued to death by Mcdonalds, but The Simpsons can say that Krusty Burger uses rat meat in its burgers.\n\nParody law is also unique in that being known as a parody artist makes you more immune from it, as reasonable people will not take what you're saying seriously.  South park is just so constantly over the top that it's hard to claim anything on the show is defamatory because who takes south parks claims seriously?  They literally had to write \"This is what the church of scientology actually believes\" under one bit because otherwise people would have assumed they were making things up.\n\nOn top of that South Park makes enough money that they can afford the legal resources to avoid saying something defamatory and defend themselves.  Most of the fear with parodying somebody isn't actually losing in court - it's simply getting hit with frivolous lawsuits by pissed off parties that you can't afford.  Fair use laws cover a LOT of parody.","label":0,"model":"human","source":"reddit","id":2721}
{"text":"Oh man 1 i can actually answer for once!!\n\nSo basically back in the 30s and 40s the Film Industry had a full monopoly and control over the whole visual entertainment, Since Television was not out till 1949. It was pretty normal for people to go out and see 5 to 7 movies a week at the time, so the Industry being a business first they found out that people liked certain genres more than others, this being Western, Musical, and Noir. Which such high demand for these films, the Industry would pump out hundreds of films a year for each genre.\n\nNow this did have draw backs at the time, since the Film Industry wanted to maximize profits, they didn't allow for director and writers to branch out and try new things in the business. Their ideals were make \" safe non offensive movies that everyone can recognize and want to see\", and since they controlled all aspects of the industry, the actors, writers, studios, directors, and even the cinema's and promotion were all employed by the industry majors. \n\nThis business model basically killed off film as an art form and a form of expression for a few years, until Television hit the scene in the early 50s, suddenly cinemas lost a lot of profits fast, and with the nature of competition in a business, The film Industry had to now adapt since now their average 5 - 7 views per persons a week is no longer a thing. They let go of their tight grip on director, actors, and writers, anything to get people back on those seats. More R rated films came out and other genres like Sci-fy blew up. At the same time they started trying out gimmick like 3D glasses. Basically you can thank Television for saving film as an art form. \n\n\n\nEDIT: The information I got for this post was given to me by my online college Library. [This video has all the information I stated above in full detail and more](_URL_1_). Not sure if it is Log in gated to non students. [But here is the direct link to their site seems to be a pay wall](_URL_0_)","label":0,"model":"human","source":"reddit","id":2722}
{"text":"There are a number of reasons.  \n\nFirst, the framers of the constitution believed that wisdom comes from experience and they must have believed that 35 was a sufficient age for a person to gain enough experience.\n\nSecond, they didn't want people to be elected merely because, say, their father was a good politician.  They were trying to get away from a monarchy and if a son was elected merely because his father was a good politician, that seemed a little too close to monarchy.\n\nThird, and possibly the most important, in [Federalist #64](_URL_0_) John Jay writes:\n\n >  By excluding men under thirty-five from the first office, and those under thirty from the second, it confines the electors to men of whom the people have had time to form a judgment, and with respect to whom they will not be liable to be deceived by those brilliant appearances of genius and patriotism, which, like transient meteors, sometimes mislead as well as dazzle. If the observation be well founded, that wise kings will always be served by able ministers, it is fair to argue, that as an assembly of select electors possess, in a greater degree than kings, the means of extensive and accurate information relative to men and characters, so will their appointments bear at least equal marks of discretion and discernment. The inference which naturally results from these considerations is this, that the President and senators so chosen will always be of the number of those who best understand our national interests, whether considered in relation to the several States or to foreign nations, who are best able to promote those interests, and whose reputation for integrity inspires and merits confidence. With such men the power of making treaties may be safely lodged.\n\nIn other words, by making people wait until they are 35, it gives the voters a chance to make a judgement as to how the candidate performs.  It allows the voters to pick a wise candidate based on what the candidate has done in the past.  If some flashy 21 year old ran for president and he or she just happened to be more eloquent than other candidates, voters would only have appearances, not substance, to base their vote on.","label":0,"model":"human","source":"reddit","id":2723}
{"text":"Insects don't really have brains the same way your or I do and they don't learn, ~~per say~~ per se, thanks [\/u\/BaaruRaimu](_URL_0_). They have ganglia that just cause them to have reactions based on stimuli which are very fast, that's why if you swat at a fly it flys away really quickly but seconds later it will return. It doesn't learn that you're a danger to it. **SEE MY EDIT THIS IS NOT ACTUALLY CORRECT** \n\nNow onto this, basically if there's some sort of positive stimuli for the fly (like a scent that is released by a female fly for mating) it will attract all flies in the same way towards that area and they don't really think about what could be there they are just attracted to the area. \n\nI might be misinformed on specifics for this since I'm not a fly expert, so if someone spots something I'm wrong about reply to me. \n\nEDIT: So after several replies it seems like I am in fact, very incorrect about the fact that flies do not learn, and that even with ganglia they are able to. Did some research on it myself and yes it appears that flies do have some basic learning abilities, so the first part of my answer is not actually correct. I apologies, I was told this in my first year psychology class, since we examined some differences in gray vs white matter and how gray matter has high learning potential, while white matter is more involved in fast responses (myelin sheath). I was then used the analogy I gave where if you swat a fly it keeps coming back since it does not learn. Thanks for all those that corrected me, will leave my answer up for reference.","label":0,"model":"human","source":"reddit","id":2724}
{"text":"I have not worked in a bank, so I don't know it in detail. But as an accountant\/finance manager I have worked in a treasury function for a company having the main contact with our banks.\n\nThe first I would point to is control. All the banks I spoke with said that even if processes are very automated and there is no person sitting there typing in the payment all over again, there are still people signing off on payments before it gets executed. I believe it's around ensuring documentation is correct and look after the controls etc. They don't say much about exactly what they do, they just tell me there are manual checks before it is executed. I think only some ACH payments were fully automated at the bank, but I believe that had other centralized controls.\n\nAnd you want people\/employees, to be responsible. These two trusted employees jointly said this batch is OK to execute. And not just that responsibility disappearing into a \"but the system...\"\n\nYou can have error handling. Missing mandatory information, especially for cross border payments or foreign exchange. There were situations where they needed a correction\/additional information from us on the same day or else they would for example have to reverse currency trades since they can't hold that currency.\n\nBanks also needs to ensure their own liquidity and any requirements they have around that. And they have reporting requirements to the central bank \/ regulatory authority. So end of the day, they need to reports positions etc. \n\nEven if it looks very automated on the front end for us. We use online banking and you think you do things directly into the account, then there is still quite a big machinery behind it with checks, reconciliations, FX trades, transactions with the central bank, reports etc.","label":0,"model":"human","source":"reddit","id":2725}
{"text":"Simply being in contact isn't enough to cause a pressure sore. The force of the pressure on the skin is a much larger factor in determining whether or not a sore develops.\n\nPressure sores typically form over bony prominences in your body (think of your knuckles or tailbone) and are caused by prolonged periods of pressure on the skin. For example, the act of sitting places you at risk of developing a pressure sore on your tush due to the weight of your upper body pressing down on the skin and reducing the blood flow to the skin and tissue, leading to tissue damage or death (depending on the weight and the length of time). To increase the likelihood of developing a sore, you could increase the weight (force) pressing down, increase the time, or decrease the surface area supporting the weight.\n\nIf you were to press your toes together tightly for a great length of time, then they might be a small risk of developing a pressure sore, but even then the amount of force (in this case, weight) that would be pressing on the skin is probably unlikely to do any damage.","label":0,"model":"human","source":"reddit","id":2726}
{"text":"TL;DR - Salt doesn't make ice colder.  What you're going for in both cases is a state change - you want liquid water, not solid water (ice) - without having to compromise the temperature.\n\nUnless you have some continuous source of coolant (like my buddies at work that used to make iced cream with Liquid Nitrogen, lol) the ice you have is all the coolant you'll get, so you'll want to freeze the iced cream quickly before the elements warm the coolant (ice\/water) beyond usefulness.  \n\nIn order to best transfer the heat away from the ingredients into the ice, you want the *highest possible surface area* of the spinning aluminum container to contact the coolant.  Ice cubes aren't that great, crushed ice or party ice is better (smaller chunks), but ultimately liquid is best.  You could use glycol or something, but the most available and safest liquid is water.  If you want to get water cold enough to freeze the iced cream without freezing into a solid block itself, you have to add salt to lower the freezing point.  Fun fact, ice rinks use a salt water (brine) which run through pipes in the concrete beneath the ice surface in order to maintain the temperature at around 19 degrees Fahrenheit.\n\nSalt does the same think to ice on sidewalks, except the motivation there is that you just don't want solid water (ice) around.  Liquid water will flow away and reduce the hazard\/obstacle.","label":0,"model":"human","source":"reddit","id":2727}
{"text":"Supersheesh is right, but I don't think it was very eli5.\n\nLet's take a dark room. Most bathrooms work very well for this.\n\nNow computers always send things as 1s and 0s. So lets assign things. Light off is 0. Light on is 1.\n\nNow we can send messages. But it is so slow that humans see the light dark. We don't want this, we want the light on.\n\nSo we use something computers do very well, talk very fast. So now instead of slowly flicking the light on and off, instead we turn the light on and off billions of times a second. Now the humans can't tell the difference. All the humans see is that the light is basically half as strong.\n\nSo lets fix that half strength.\n\nHumans only see particular colors. In particular we don't see infrared or ultraviolet. There are other colors of light that we don't see, or see very weakly. We take the normal lights in the room, but we block that normal light from sending infrared. To the humans this did not change the light at all. To the computer though, it is easy enough to make a computer that only sees infrared. Now we can turn that infrared light on and off very quickly for the message.\n\nTo the human the light doesn't change, but now the computers can talk to each other with light very quickly and easily.\n\nEven better there are many different infrared colors. This means that the computers can use different colors of infrared to talk and speed up their talking with each other.\n\nHowever infrared is old technology. It doesn't bounce very well in most places. This can be good or bad. It also has a lot of interference because heat shows up in infrared.\n\nSo we go back to the color choices. Since we can use any color we want, we can choose any color that humans don't see very well, but we want ones that bounce off walls very well. Right now there are many different ideas of the best color to use, and it is a very complex decision.\n\nNow you might have noticed that we have something of a problem. How do we tell the difference between a long series of 1111111...111111 and simply the light being on? Actually the fix to this is to add security. Encrypting the data means that it is impossible to have a long stream of 11111...1111 (or as close to impossible as we can make it). Instead each bit will appear to be randomly 1 or 0, exactly the result we need. So added security makes it possible to send any message and have it received.\n\nSo that's the basics of lifi. Flicking a light on and off really really quickly. Everything after that is to make it nicer for humans or to deal with seemingly weird failures.","label":0,"model":"human","source":"reddit","id":2728}
{"text":"Pilot here (PPL).\n\nAs has been pointed out, clouds are often massive and wing vortices generally affect clouds to the rear and below your fuselage. However you can see the effect as a pilot depending on the aircraft you fly. A commercial airliner leaves you with no chance of seeing it because of the speed, dimensions and limited field of view (Even for the pilot).\n\nHowever I in my little Piper can brush some cloud and look back to see the effect to a good degree and anyone in a bubble canopy has an even better view of it. I would never do that as a deliberate act though, merely if I was transitioning through, even though I am IFR rated. Cloud is never a thing you want to be near or in as a pilot if you can avoid it and indeed many private pilots have to specifically avoid it as they fly under a restriction called visual flight rules or VFR.\n\nAirliners often do have an effect on clouds that are quite far away from them. The turbulence from big jet engines can spawl around for quite some distance and affect clouds that are reasonably far away.","label":0,"model":"human","source":"reddit","id":2729}
{"text":"Think of blood type as a sort of evolutionary side-effect; there's no clear advantage to having different types, it's just that separate populations sort of evolved them separately. \n\nEverything I'm about to say is an oversimplification, but this is ELI5 so I think that's the point.\n\nFundamentally blood type is used to classify what kinds of \"immune markers\" your blood has. Someone who has type A blood has \"A\" immune markers in their blood; their body knows not to attack anything with the \"A\" marker in it, so you can similarly receive blood donations as long as the only markers they have are type \"A\". \n\nType B blood is just like A, except that it's a *different* marker; if a person with type B blood gets transfused with type A blood, their body will see the A immune markers and think the blood is foreign, triggering an immune response (which is pretty severe and can kill you). As such, someone with type B blood can only recieve blood if it doesn't have type A markers in it. \n\nType O blood effectively has no markers. This is great for donation, because it means that anyone can get type O blood without their body rejecting it. The problem is that type O people have to get blood with *no* immune markers, as both A and B will trigger an immune response. Type O blood is referred to as the \"universal donor\", as anyone can recieve it. \n\nType AB blood is blood with both A and B markers; as such, no marker type triggers an immune response and they can effectively receive blood from anyone (hence the title \"Universal Recipient\"). This also means, however, that only someone who is type AB can receive AB blood, as it will trigger immune responses in everyone else. \n\nHope this helps!","label":0,"model":"human","source":"reddit","id":2730}
{"text":"This is an explanation not just for this specific case but for large companies in general. These companies create jobs, and will also be paying taxes on a myriad of things. Sometimes the government will therefore sweeten the pot by reducing certain costs, knowing full well that overall they and the community will benefit from the implementation. This can end up being tax breaks, public utility rate break, favorable loans, etc. \n\n**There are often a lot of benefits of having a large company implemented in the region, and as such local or state governments will often try to create incentives for that to happen.**\n\n**Edit:** -Just thought I should add a couple of things based on some of the replies I've been getting. I replied to the OP in very general terms about why businesses might be given incentives because I assumed that was what was happening in this case, without looking up the specific example of Nestle in BC. I've now taken five minutes to look things up. This cost is associated with the Water Sustainabilty Act coming into effect in 2016. The 2.25$ is the current maximum cost in the province, so Nestle are not getting any special favors.\n\nI also want to mention that a lot of people have been talking about corruption etc. Honestly that's completely off topic. Yes in general corruption in the world does exist, but assuming that everyone and everything is corrupt just makes you sound like a conspiracy theorist in an RV with a tin foil hat (that sounds like it's straight out of a game of clue).","label":0,"model":"human","source":"reddit","id":2731}
{"text":"It's a private equity company, and private equity groups look for public companies that they feel are not being run as well as they could be. When they find such a company, they buy up all the stock and take it off the stock market, so they don't have to explain themselves to the SEC, to stock analysts, other shareholders (this is the \"private\" part of private equity). Then they make whatever changes they think will make the business run better, and sell the stock back into the stock market. If it works, the company emerges stronger, the stock is worth more, and they get rich when they cash out.\n\nPrivate equity is controversial for two main reasons: first, one of the primary ways in which private equity groups try to make a company more profitable is by laying off any and all redundant employees. Often times these are smallish companies with a \"family\" environment, and the managers are too nice to fire people, even when the company could save a few bucks by doing so. So when a heartless P.E. group comes in, not only do people lose their jobs, but their sense of corporate community or family is shattered, making it all the more controversial.\n\nThe second big criticism of private equity is that they usually borrow most of the money to buy the company. Then, as soon as they are in control of the company they borrow money in the company's name to pay themselves back. This isn't a problem if their plan to turn around the company works, because the company will make plenty of money to pay back the loans, and everything is peachy. But, if their plan doesn't work out, it means that the company is almost certainly headed for bankruptcy and liquidation.\n\nThere are examples of private equity deals that have worked out well, and a stronger, more profitable company emerged when the P.E. group left. But, there are also plenty of examples where the P.E. group's plan didn't work and it left behind a company that was still not very profitable, but now had a huge amount of debt that it couldn't pay, forcing it into bankruptcy.","label":0,"model":"human","source":"reddit","id":2732}
{"text":"There's an amazing interview with Rihard Feyman (famous for his work on the Manhattan Project and also explaining to NASA why the Space Shuttle challenger blew up after they were warned it was going to to blow up  when he dipped an O-ring into a glass of ice-water on tv and showed them how it doesn't work when it's cold)\n\nwhere he responds about how this an impossible question to answer, it starts with a question about magnets  and ends up talking about why ice is slippery and by the end he's  explaining quantum phyisics and the entire universe in laymens terms to the interviewer to make a point about how it's a loaded question  lol \n\n(no offense. that was just his way of answering, because that's just how his mind worked. and he was fucking brilliant)\n\n\nThis man was  a fucking legend. (also hilarious). I highly recommend watching it for the best and most interesting answer this this question and a wide range of other stuff that you'd most likely find interesting if you are wondering this question)\n_URL_0_\n\nFull interview [Richard Feynman: Fun to Imagine, Using physics to explain how the world works \\(1983\\)](_URL_1_)","label":0,"model":"human","source":"reddit","id":2733}
{"text":"Well look at it this way: sugar, and foods that turn into sugar like carbs, all break down to the same thing, glucose, or food for your cells.\n\nThe problem isn't what they become, it's how long it takes them to get there. Take a fruit for example. You're not eating just straight sugar, you're eating a lot of other components with it, like the fibrous material an orange is made out of. Your body digests it all at once, but the sugar in the fruit is tempered by the rest of the material in the fruit, causing the distribution of sugar in your blood to rise gradually instead of all at once with, say, soda or candy. The gradual increase allows the body to use the sugar as fuel without the blood sugar level rising too high too quickly, something that would trigger the release of insulin into the blood to lower the sugar level of the blood. Insulin is the body turning sugars into fats, storing them for use later instead of right now. So, if you make a habit of eating food filled with sugars, especially processed sugars which are much more potent than natural sugars (hard candy vs apple, which is sweeter?), your body releases insulin much more often, turning more sugars in your blood to fats to compensate and keep your body in balance. Foods higher in sugar are more easily and more quickly converted into fat when they don't have something else (like fiber) to balance out how quickly they're absorbed.","label":0,"model":"human","source":"reddit","id":2734}
{"text":"Okay, ex-soundie here with my $0.02:\n\nI reckon that you're listening on a bad mix. Or more specifically, the 5.1 mix downmixed to stereo, during which you are *supposed* to bump up the center channel by 6dB, but even that is sometimes not enough. **\n\nIf you listen to just the center channel of a surround mix, you will hear pretty much *only dialogue*. If you listen to the left and right channels of a surround mix, you will hear *no dialogue*. The reason for this is complex, but suffice it to say that it was to prevent dialogue from \"drifting\" left or right of the screen. It always comes out of the middle. This has the added advantage of not being mixed in with the rest of the sound, so post-processing won't accidentally quiet it.\n\nUnless you get rid of the center channel and mix it back in with the left and right channels. Then post-processing (also called mastering) can affect it and will sometimes grind down the voice levels if it's not done properly.\n\n**EDIT: People bitching that this is not sufficiently ELI5y:\n\nA *channel* is the part of a soundtrack that comes out of one speaker. For example, in stereo there are two channels: left and right. In 5.1 surround there are six: left, center, right, back-left, back-right, and low-frequency effect (for rumbles and thunder and such). When you reduce the number of channels by combining some of them together, that's called *downmixing*. Typically, a surround mix is downmixed to stereo by adding together the left, back-left and center into the left channel, and the right, back-right and center into the right channel (the LFE channel is usually discarded).","label":0,"model":"human","source":"reddit","id":2735}
{"text":"Chef here. Yes there is a big difference in different types of salt. The first, and biggest difference, is the sodium to salinity ratio. Iodized salt (table salt) has the highest sodium to salinity ratio, meaning it takes more of the actual salt to give it a salty taste. Kosher salt has a medium ratio, and sea salt usually has the highest ratio. This means that you will get more flavor from less sea salt than you will with iodized salt. This is very important for people who have low-sodium dietary needs, as you can use less sodium and get the same taste. This is why you will not get the same flavor using iodized salt for a recipe that calls for kosher salt.  \n\n\nThe second difference in salt is the size and shape of the grains. Notice how iodized salt is all the same size and very small, whereas kosher salt is usually very coarse grain. This affects not only the solvent time of the salt, but also your ability to consistently measure the same amount of salt with your hand. The grains of kosher salt make it the easiest to measure out by hand.  \n\nThese properties of salt give different salts different uses. Iodized salt is more commonly use to finish salting fried foods, as it sticks to the surface of the food better. Kosher salt is used in most commercial kitchens because of the ease of measuring, and also because its shape and salinity allow for bleeding meat. Sea salt is most commonly used in desserts in order to balance the intense sweetness of some items.\n\nEdit: For those wondering about the S\/S metric of salts, it is caused by the different crystalline structure of salts. As salt forms, it doesn't always form in the same crystal structure. This means that in some salts more ~~sodium~~ NaCL molecules form in a more densely packed fashion. This causes a higher sodium content, and less taste because it breaks down slower in your saliva. The less densely packed molecules have less sodium per volume, and dissolve more rapidly in your saliva.","label":0,"model":"human","source":"reddit","id":2736}
{"text":"Disk drives have on-board memory or \"cache\" with which to smooth out data transfers. Data is often at various places on the disk, and there needs to be time to swing the reading arm and head around to find the right place to read it from the disk. There is also the matter of when the data is read the disk is spinning by at great speed; the computer isn't always ready to accept the data from the disk, so it needs a buffer area (the cache) to put it in the meantime.\n\nThis means that drives exhibit \"burst\" speeds when reading and writing. If you wanted to save a file your computer could say \"save these memory addresses!\" and dump them directly from your computer's random-access memory into the drive's cache very quickly. Slightly later the drive actually gets around to positioning the heads and writing what is in the cache to the platters of the disk itself. However, if you have a lot of data to save it will eventually overwhelm the drive's cache and it will be forced to stop just storing it in the cache until it can work through the backlog.\n\nWhat this means is that when Windows looks at how quickly a file is being written by the drive it gets confusing results. The first few bits of the file are being accepted extremely quickly so it thinks it will finish the transfer very quickly, but after a little while the drive seems to slow down significantly. Now something it looked like would take 10 seconds is taking a minute. If you tell that drive to do other things too that minute could be come two, or even three!\n\nUltimately, the timer included in the box by Windows isn't going to be very smart. It only compares the clock to the rate of data transfer, which often doesn't consider the whole picture at all.","label":0,"model":"human","source":"reddit","id":2737}
{"text":"We don't know. Perception is a tricky thing to study, and while neuroscience has made a lot of progress, particularly in the visual system, there's a ton we don't know about why we perceive things the way we do. Pain is a hot field of research.\n\nUnfortunately, almost every other answer is taking real scientific ideas and musing about them past the point of the point of science\n\n >  There have been recent studies that link the \"pulsing\" of pain to brain wave activity, more specifically, to alpha waves. Alpha waves are broadcast at between 8 and 13Hz, or between 8 and 13 cycles per second\n\nApart from the point already mentioned that alpha waves are much too fast for the slow pulsing of pain (a  < 1 Hz rhythm), the meaning of brain waves are poorly understood. Sure, alpha waves correlate with pain, but all brain waves are large scale measurements which may or may not have anything to actually do with information processing and may just be a side effect of the underlying dynamics. There's no reason to believe we should \"feel\" any sort of oscillatory activity in the brain.\n\n >  I have modeled neurons in the past. Neurons are coupled to their neighbors, so they tend to slip into synchrony, like two pendula (see below video). When many of these are coupled, the system doesn't totally synchronize (there are always a few pendula that are oddballs, out of sync), it slips into and out of synchrony in waves. I am guessing the throbs of pain are due to your nervous system slipping in and out of high states of synchrony.\n\nThis could be a mechanism by which a low frequency oscillation similar to the frequency of throbbing pain could be generated, but I don't know of biological evidence that this occurs in pain.\n\n >  Pain is caused by your nervous system. Your nervous system works on electrical energy, all energy moves in waves, you experiences these waves as pulses.\n\nNerves do fire pulses, but they're extremely short. An action potential - the pulse you're referring to - is akin to switching a light on for a milisecond and then turning it back off for a lot longer. These are impulses, not the slowly ebbing and flowing waves of pain.\n\n >  Your heart beating.... You'll notice the throbbing in your head or a wound coincides with the beating of your heart\n\nOld theory which seems mostly discredited. (edit: [source](_URL_0_))\n\nSource: I study neural oscillations. It's good to always be skeptical of your own field!\n\nEDIT: Just realized this was in ELI5, not askscience, so my frustration at the speculations in all the other answers has subsided a bit. But it's still worth recognizing when we simply don't have a concrete answer, and not to over-interpret science!\n\nANOTHER EDIT: Thanks for the gold, stranger! And now that this is the top post, I should probably actually, y'know, explain like one is 5.\n\n*shurgs* It's probably not your heartbeat. No one knows if it's brain waves. Perception is a very hard thing to study and pain is a surprisingly complicated sensation. But there's probably a neat answer that will give us insight into how pain perception works, which will probably help us create better treatments for chronic pain, one of the biggest causes of suffering in developed countries. So become a neuroscientist and do some awesome research!","label":0,"model":"human","source":"reddit","id":2738}
{"text":"Surprised that nobody really tried to answer this, from what I can see.\n\nUltimately, most internet traffic is exchanged between networks in a process called *Peering* at places called *Internet Exchange Points(IXPs)*. ISPs and other networks sign *Peering Agreements* with each other to establish terms and conditions for sharing traffic between networks. \n\nThere are two types of peering: *public*, and *private*:\n\n* Public peering is where a large number of carriers can go to all connect to each other in a 'public forum' manner.  Public peering is most often slower for a number of reasons. It can also be used as a backup when the preferred network is down or malfunctioning.  \n* Private peering is where two carriers make a peering agreement to establish a connection directly between themselves.  Naturally, this allows the parties to agree on acceptable latency and bandwidth. The vast majority of internet traffic is exchanged via private peering. Private peering agreements are normally secretive, and a lot of shady business goes down when negotiating them.  For example, providers very frequently and intentionally neglect to upgrade their equipment at an IXP in order to gain the upper hand in peering agreements. They can now claim: \"Look, we'd love to agree to this, but we clearly need more bandwidth at this location, which will cost us money! 0:)\"\n\nSide note: A really cool site to check out if you're so inclined is _URL_0_.  You can browse through public and private IXPs, and check out some peering info on a good deal of networks. (you can log in with guest\/guest, you only need an account if you want to update the database)\n\nIn order to be able to transfer content quickly and efficiently to everyone everywhere, you're going to need to either peer directly with all the major networks, or partner with third party networks that can exchange traffic on your behalf.  The latter is what most companies do-- Netflix, for example, uses Akamai, Limelight, and Level 3 (three very fast \"Content Delivery Networks\") to push its video to customers.  Each of those CDNs have extensive peering agreements all over the globe.\n\nNetworks are constantly shifting, with internet routes being modified all the time, and problems arise frequently.  Most peering agreements include the stipulation that if problems are found on a network, there will be techs around to investigate and fix it.  A tech's ability to fix a problem, however, varies, depending on how well he knows the systems involved. Here's an example of Hulu trying to alleviate some of the pressure from their CDNs-- _URL_1_\n\nWith all that in mind, CDNs and other content providers that are categorized as \"Mostly Outbound\" tend to incur the largest fees for data transfer. In order to vastly reduce the amount of data needed to travel between networks, larger CDNs\/content providers will strike agreements with major ISPs to host content caching hardware either inside or directly adjacent to the ISP's own network(both physically and logically). \n\nGoogle's version of this is called \"Google Global Cache\"-- _URL_2_ (very bottom of page)\n\nNetflix's version of this is called \"Open Connect\"-- _URL_3_\n\nYou can see these systems indirectly by watching a YouTube video and checking the address of the streaming server.  Sometimes the address of the server may indicate your ISP, eg \"comcast-blah.blah\".  If not, if you run a traceroute on the address, you'll find that it barely makes it out of your ISP's network(most of the time, check [here](_URL_4_) for a dry article on why not always).\n\nUp until a few days ago, Comcast refused, for whatever reason, to sign on to Netflix's Open Connect.  They recently struck an agreement, though.  I imagine that this is in part due to the recent ~~Supreme~~ Court ruling allowing ISPs to throttle any traffic they deem unworthy-- Netflix I'm sure had to pony up a few more coins to strike the agreement now.  Another motivating factor was that Netflix, which was opposed to the ruling, would also be opposed to the Comcast acquisition of Time Warner Cable.  Now that they have an \"in\" with Comcast, however, they are likely to be far less vocal about it.\n\nEdit: Appeals court, not supreme court. Here's the info for anyone interested or incredibly bored: _URL_5_\n\nEdit: Oh, gold! Thanks random person! Alchemy lives after all.","label":0,"model":"human","source":"reddit","id":2739}
{"text":"This is an open question that has been puzzling researchers for 40-50 years. In very tightly controlled studies, in which measured portions of food are consumed, diet drinks are in fact associated with lower weight than sugar sweetened drinks. But only in the lab. As soon as you open the doors and let the subjects out the effect vanishes. Most studies find little to no difference between drinkers of sweetened and noncaloric sweetened beverages, and a few studies suggest the diet drinkers may be a bit worse off. It's not settled. Though note that the water drinkers always beat both groups. \n\nThere are many hypotheses, most of which assume that the diet drinkers are unconsciously compensating in some way, but it's been surprisingly hard to pin down. Alternatively there is evidence that some sweeteners can trigger the insulin system, which presumably could throw off the fine tuning of metabolism. And one theory gaining popularity is that the noncaloric sweeteners are pissing off the gut microbiome that is doing so much of your digestion for you. But the bottom line is that we don't know. \n\nDrink water.","label":0,"model":"human","source":"reddit","id":2740}
{"text":"The replies so far are on point. But let me tell you a little story of my neighborhood.\n\nI live in an European city with around 2 Mio people. Rent costs kinda exploded in the past 15 years and apartments are hard to find. People are quite upset about that. Furthermore in my neighborhood there was this old, ugly building. It was built in the 70s for a discount furniture store that closed in the 90s. There was a gym in until like 2002 and since then it's empty. Next to it is a small 60s house, the ugliest thing you've ever seen, also empty. So the city decided to buy the land and build affordable housing there. Good thing, right?\n\nHouses in the neighborhood are around 8 stories, the proposed house is about 10 stories, same as the building that is to be demolished. Additionally there's a slim tower on top of that at the corner, that's anoiter 5 stories. This should create a bunch of affordable apartments, the architects chose a very subtle approach that's neither overly ugly nor overly showy or noticeable. That architect didn't try to compensate their personal issues nor were they trying to set themselves a landmark.\n\nSo there was a neighborhood initative to prevent this building from being built because it's ugly (compared to an abandoned discount store building that has the charm of a rusting shipping container), because it takes away all the sun or just because it's new. Local newspapers picked up on this and discovered that the \"announced specs\" were off, the building is 10cm higher than the old one and their calculation from the door to the subway station was off about 2m. If you went to the article on the homepage of the newspaper or their Facebook page you could literally find dozens of people who were condemning any building with more than 3 stories and idealizing the suburbs and one-family-home as the only acceptable style of building. \n\nSo often enough the reason not to expand upwards (as they do in Asia quite often) is because of morons who complain because of boredom and change itself. I've seen several buildings and plans not making it to construction because of citizen protests. It's ridiculous and stupid and my sole goal in life is to never become one of those people.","label":0,"model":"human","source":"reddit","id":2741}
{"text":"While the Sun's color spectrum differs from artificial lights, this is not the reason this is difficult. Any light made for film making combined with the correct white balance takes care of this 100%. Incandescent bulbs cover the entire visible spectrum, only in different ratios than the sun. With the correct gel or white balance, these ratios can be brought back to natural levels.\n\nThe issue here is **distance**. Light, like many other things, follows the **[inverse square law](_URL_0_)**, which means that the further a light source is, the less difference it makes if you move closer or further from it.\n\nIn other words, sunlight hitting your house is the same intensity than sunlight hitting your neighbor's house. Or sunlight hitting your face will be the same intensity as light hitting your hand. With an artificial light, the two won't receive the same amount of light, as the closeness of the light source means the light decays much faster.\n\nWe have come to associate one with natural, and the other with artificial light. Even without thinking, we can \"tell\" that light that decays over a short distance is artificial, while light that doesn't decay is natural.\n\nAn artificial light source can never be as far away as the Sun for obvious reasons, so another issue is that its shadows will never appear parallel.\n\nThe solution is to put your light source very far away, but then you need a light source that's very very strong to compensate. This makes this type of lighting expensive, heavy and time consuming to set up.\n\nAnother issue is diffusion. The Sun is surrounded by the sky, from our point of view. The sky is a giant surface very very far away. This means the Sun is accompanied by a fill light of a huge surface that is equally very far away, which is even more difficult to match with artificial light. There is no way we can build a surface anywhere near as large as the sky, and as far away.\n\nTo light a single face you may need two giant white sheets suspended with tripods and weights, and four 5 kilowatt lights to light it. Then you need to put all this as far as you can, and hope that enough light still hits your subject. If done right, you won't be able to tell it apart from natural light. Beyond a certain point, our minds won't know the difference.","label":0,"model":"human","source":"reddit","id":2742}
{"text":"Many organisms thrive in acidic environments, or at the very least are able to survive in them. We frequently ingest organisms that could cause illness, but in small enough populations that, even if they survive, they can't get enough of a foothold in our bodies to actually cause symptoms of illness. When a large enough population is introduced all at once, there's not a lot your stomach acids will do for you, and some organisms will actually grow faster in a highly acidic environment.\n\nOn top of that, not all food-born illnesses are caused by living organisms. Listeria is a common cause of food born illness because the organism itself doesn't make us sick, rather it produces waste products that are poisonous to us. If a large enough population of Listeria is allowed to breed in food that is then cooked, the organisms themselves will die during cooking, but the toxic waste byproducts they created don't actually break down when cooked.\n\nMicroscopic organisms are extremely resilient, because they have to be.","label":0,"model":"human","source":"reddit","id":2743}
{"text":"Before any concept of subatomic particles, people knew about atoms and elements via a wide array of experiments (which could be detailed, but isn't really relevant to this story).  When you have a purified sample of an element, you can determine how much an atom of it weighs, and so we start with knowing a bunch of elements and their atomic weights.\n\nEnter Dmitri Mendeleev.  He notices that if you arrange the elements by increasing weight, you end up with repeating groups of elements that are chemically similar.  This was nice, except people knew something wasn't quite right with it, because you had to do a couple tricks to make similarly-acting elements end up in the right groups (such as swapping the by-weight ordering of tellurium and iodine or cobalt and nickel).  Once this was done, the elements gradually got numbered using their place in the table: 1 is Hydrogen, 2 is Helium, etc.  The numbers didn't mean anything to anyone, they were just useful identifiers, and they were called atomic numbers.\n\nLater, Joseph J. Thomson is working on figuring out what is going on in electric discharge tubes.  He discovers that tiny particles are being discharged from the cathode in those tubes, and he managed to measure the ratio between their charge and mass.  He calls those particles electrons.\n\nNext we go to Ernest Rutherford.  He had an experiment where he fired a bunch of positively-charged particles at a thin sheet of gold leaf, and he noticed that the vast majority of them made it through without interacting with anything, but a few of them got deflected.  This led him to hypothesize that atoms had a dense central core of positive charge (the nucleus) and then a diffuse cloud of electrons around it.  Since atoms overall usually have neutral electric charge, the number of electrons and the charge of the nucleus had to be equal.\n\nFollowing on from this was Henry Moseley.  He measured the wavelengths of X-rays sent off by various elements in a particular kind of experimental setup, and he determined that the wavelength was proportional to the square of the atomic number.  That indicated that the atomic number actually indicated a physical quantity, which was then hypothesized as the amount of charge in the nucleus.  One of the neat things this did was conclusively show where there were gaps in the periodic table: elements that should exist, but hadn't been discovered yet.  In the course of time, all those gaps were filled in, once people knew what to look for.\n\nBack to Rutherford again, a few years later.  He succeeds in experimentally stripping out everything but the nucleus of a hydrogen atom, which has a charge of 1 and a weight of 1.  He calls what he ends up with a proton.\n\nFinally, we need to discover the neutron.  We already know that the nucleus has a positive charge of the atomic number, and thus there are that many electrons as well, but the problem is that most atoms have a much higher weight than their atomic number, so something more than protons had to be in the atomic nucleus.  A leading theory was that there were a bunch of extra proton-electron pairs just hanging around in the nucleus, but that was shown to be impossible.  Enter James Chadwick, who shot a weird kind of radiation others had discovered at paraffin and other substances and measured what happened, and concluded that the radiation was in fact made up of neutrally-charged particles that had about the same mass as a proton.  He's discovered the neutron.\n\nSo, now that we know everything, we can gather up our data on elements.  The number of protons in an atom is the atomic number of that element.  That's also the number of electrons, since atoms are neutrally-charged.  Finally, the difference between the atom's weight and the atomic number is the number of neutrons.","label":0,"model":"human","source":"reddit","id":2744}
{"text":"You like honey, don't you?  Who doesn't like honey?  Honey comes from bees.  Certain types of honey bees are better at making the kind of honey that you like (they make more of it, and it's tastier, yay!), so people keep breeding more of these honey bees and less of other types of honey bees.  This is all well and good, because we're getting lots and lots of good honey, but it's a really bad thing when a disease (a disease is what makes you sick) comes along that is particularly dangerous to this type of honey bee (the disease could be anything from a mite or parasite, to an actual virus or bacterial, or some kind of latent genetic trait that makes the bees unable to reproduce).  It's bad because since so many of these bees are the same, they all get sick at the same time and they all die.  If there were more types of bees, not all of them would get sick at the same time, and the disease wouldn't spread as quickly!\n\n[Obligatory non-ELI5 disclosure:  selective breeding is only one of many proposed mechanisms for Colony Collapse Disorder.  Mites, pesticides, climate change, homogenization of pollen sources through industrial farming, and aggressive Africanized honey bees are all either considered to be confounding or alternative mechanisms for CCD.] \n\nEdit:  Special thanks to firefoxx336 for sharing [a beekeeper's perspective on CCD and alternate theory](_URL_0_) and to dragpent for providing an [ELI5 explanation for the other reasons mentioned in this post](_URL_1_).","label":0,"model":"human","source":"reddit","id":2745}
{"text":"Land is not level. It's full of crinkles and mashed together piles, and flat areas too. That creates very low areas (ocean), low areas (lakes) and high areas (land), with rain refilling the second type with fresh water. Rainwater runs downhill and has a tendency to accumulate in any sort of depression, so a lake is often \"fed\" from a very big area where uphill rain or snow has fallen, and that's called a watershed.\n\nThose lakes quite often aren't physically closed off, they usually have either permanent outlets that flow to other lakes in the form of streams or rivers, or they can create temporary passageways when flooding conditions occur such as if a beaver builds a dam. Fish then swim up these access points, and some like eels that migrate through wet woods are really good at it. And if the lake they find is healthy and that stream closes off, once they're there, they're there for good.\n\nNor are those lakes permanently stuck in one place in many cases. Some of them have had fish for millions of years back before fault lines caused a lake to split in two, or stopped an access stream or cut off access to salt water because it was redirected by a landslide or a growing peat bog or beaver dam.\n\nEven remote lakes with no apparent historical access to other bodies of water at all can have fish in them though. They can possibly get there by being transported accidentally by other creatures (e.g. fish eggs stuck to a waterbird's feathers, or in water-weed nesting material that a flying bird drops).","label":0,"model":"human","source":"reddit","id":2746}
{"text":"Towards the end of a development cycle, caution is the name of the game.\n\nVideo games are really complex (and often somewhat haphazardly put together), so *any* change risks breaking something. If you're two weeks away from launch, the last thing you want to do is break stuff, so you play it safe, you don't make *any* change unless you are absolutely certain that it is (1) safe, and (2) necessary. So assets that turned out to be unused? Eh, leave them in, they do no harm that way.\n\nThis is also why games sometimes ship with entire levels removed, or with very obvious bugs left in. You're near the launch date, and yes, you know this is a bug, and yes, you're 90% sure fixing it is safe and will break nothing else. But... what if it does? You don't have time to re-test everything else, so you have to ask whether leaving in a known bug is better than fixing it and risking breaking something much more important *that you won't even have time to discover before release*. Or you found a single tricky bug in this map and you just don't have time to fix it so.... just disable that map entirely.","label":0,"model":"human","source":"reddit","id":2747}
{"text":"I guess everyone is too lazy to reply, OP. There are some good explanations for this, but my vague, non-scientific understanding is, it is indeed a physical condition, one that we inherited from our ancestors. I'm sure you can find a more technical\/accurate\/correct explanation of this, but here's the gist of it.\n\nOur brains have a vulnerability that makes it extremely easy to get addicted to things. Watching TV can stimulate you (you are rewarded with dopamine). The more TV you watch, the more you want to watch it. The brain is rewiring itself to crave TV, because it was a source of dopamine release. Why does the brain do this? Because it worked to our ancestors favour. Their brain would be wired to be 'addicted' to gathering food, because it was necessary for survival and even the act of simply gathering the food would be rewarding to them, giving them a higher chance of surviving the next drought. This routine of gathering now becomes ingrained.\n\nSo after a while, any moment you are not watching TV, your brain will be agitated, because your primitive brain isn't doing something it thinks is useful (because you're not getting dopamine) so you will crave TV. So basically, you are addicted to a low energy, highly stimulated state, it's as simple as that. Ever notice that you browse reddit for hours, even when you've seen everything, and there's nothing even remotely enjoyable about it? It's your brain telling you \"keep looking, you'll find it! (dopamine)\". As your brain continues to rewire itself, it also starts to cull the circuits in the brain that it deems 'un-useful', such as the ability to learn. Soon, TV will be the only thing that gives you a dopamine fix, which means everything else in the world will seem boring, and this is the root of laziness.\n\nOur brains are still plastic, however. Abstain from TV for a long enough time and you will no longer be addicted to it. Don't game for a few years, you will never be compelled to game again. \n\nHere's a study that examines the physical changes in the brain when addicted to internet use, and the similarities to drug addiction: _URL_0_","label":0,"model":"human","source":"reddit","id":2748}
{"text":"The air temperature is just one of many factors in how comfortable you feel.  Others are:\n\n* radiant heat, e.g., sunshine or heat lamps\n\n* humidity.  High humidity usually makes you less comfortable because our natural cooling system (sweating) relies on evaporation.  Sweat won't evaporate if the air is already at 100% humidity.\n\n* wind speed.  Moving air will carry heat away from you more effectively than still air.  That's why weather people talk about \"wind chill\" to tell you how much colder you'll feel taking the wind into account.\n\n* body heat.  Obviously the harder you're working the hotter you'll feel.  People generate about 100W at rest but it can be over 1000W working hard.  You're probably generating less heat when you're indoors.\n\n* clothing.  Mostly clothes are insulators and the thicker they are the warmer you'll feel since they help you retain your body heat.  The outer colour of clothes can make a difference if there's radiant heat:  dark clothes will warm up more than light clothes.","label":0,"model":"human","source":"reddit","id":2749}
{"text":"As others pointed out, it's probably not much faster, it's just that your brain doesn't understand where to break up the sounds into meaningful words, so you just hear a blur of alien syllables in rapid succession.\n\nHowever, there's a little more to it. [There's been research done into this, and some languages legitimately are faster than others; sometimes much faster.](_URL_0_) BUT, that increased speed comes with a decrease in information density, so that the average amount of actual semantic content communicated per time is more or less the same across all languages (less than 5% difference across all languages). That suggests that the human brain has a fundamental speed limit on how quickly we can process spoken language, which is really cool. If that's too many science words for ELI5, the gist is that \"slow\" languages like Chinese might take 5 seconds to say 10 syllables, and \"fast\" languages like Spanish might take 5 seconds to say 20 syllables, but that 10 seconds of speech might express exactly the same information in both languages. They all sound fast if you don't speak that language, though.","label":0,"model":"human","source":"reddit","id":2750}
{"text":"All of the chemicals in your brain (neurotransmitters) that normally make you feel happy are tied to your body's natural internal clock (circadian rhythm). Aside from your body producing less of those when you are tired\/usually sleeping, this is also the time when your body repairs itself. \n\nIf you force yourself to stay awake for a long period, all of the \"unnecessary\" functions usually managed by a healthy, well-maintained brain not required for immediate survival are partitioned away or outright shut off. This sort of process occurs for almost every system of the body - digestive, energy production, brain chemistry, etc.\n\nYou becomes dull, your reaction time slows down, critical thinking goes out the window, emotional sensitivity disappears, and your body gradually starts to wear down faster. This is why people can - eventually - die from a lack of sleep. It impacts everything down to the folding of protean strands in your DNA and cellular water retention. \n\nI can ELI10 for more detailed stuff on stages of sleep and situation awareness if asked, but that's only tertiarily related to the main question.","label":0,"model":"human","source":"reddit","id":2751}
{"text":"We really don't know, and some people can not. Check out [Aphantasia](_URL_0_), which generally concerns the inability to \"visualize\" things, but extends, in some cases, to having no \"inner monologue\".\n\nASIDE: I didn't realize I couldn't do the mind picture thing until the one time, as an adult when I suddenly could for a few seconds. For whatever reason I suddenly could see a camshaft in my mind... and I'm not even a gear head. Now, of late I've discovered that CBD oil in a fairly large dose will let me turn on this feature of my brain somewhat.\n\nThere is also the inextricable question of \"what is consciousness\", which strongly seems to include this inner voice, but according to that whole aphantasia thing doesn't require the voice completely...\n\nWhen I read the literature as a lay person (and something of a librarian, and an accomplished computer scientist) it seems to me that the whole mechanism of consciousness and inner monologue is something of an error correction protocol.\n\nWe seem to have a vested interest in being able to monitor our own thoughts. The primary role feels like a \"bullshit detector\", where we can try ideas to see if they fit, or if they are worth admitting despite their ill-fit, with our \"story of the universe\".\n\nBasically we need to be able to \"try on\" what we just heard or thought, and so what we are about to say.\n\nIn terms of pure mechanism, most of the brain is an electrical\/neurological dead end. All the signaling enters and leaves through the brain stem or other \"primitive\" structures, but all of our \"higher thinking\" happens in the newer structures that don't directly touch the outside world.\n\nSo it makes sense that \"the outputs are tied back to the inputs\" in a feedback loop.\n\nWe hear sounds. Those sounds go through a decoder to convert them into information. The information goes pretty much everywhere in the \"neocortex\" systems of your brain. Those cortex operations formulate a response. As that response heads back it gets looped through the decoder. We get a chance to decide if that output is worth it. (Should I _really_ say that or is it just going to lead to violence or embarrassment?) If it gets a pass then that's what we say (or do) in response.\n\nSo people who _couldn't_ \"Try on their words before speaking\" might be a lot more likely to say something that would get them in trouble. It also lets us choose between phrasing; \"I is happy\" and \"I am happy\"; to decide which one makes us sound less stupid or whatever.\n\nOnce you understand that you experience yourself at sort of the mid-level of abstraction, that you \"hear words\" _despite_ the input just being a bunch of individual sound waves, then the model makes more sense.\n\nTry repeating a word, any word - I recommend \"door\" - aloud, again and again. You'll soon find yourself asking yourself if its a real word. The sound will literally begin to lose its meaning. We think this happens because when its repeated for no reason the parts of the brain that are so conceptual that we don't feel them running concludes that the repetition of the word is \"just a sound\" instead of being a meaningful act of speech.\n\nSo a lot of inclusive and frankly subjective thought tell us that the part of us we think of as being us is just a slab of middleware. It's the system that monitors the flow from the physical to the conceptual and back. And the system works because it can block the noise and the \"bad ideas\" from making it all the way through by saying \"nah, that can't be right\".\n\nBut since it's the same singular system it all \"sounds the same\" whether the signal is coming from the outside or the inside. But not exactly the same. Most people assign their own voice to their inner monologue. They can tell someone else's words from their own.\n\nAnd, of course, when that identity operation starts to fall apart, people start \"hearing other voices in their head\" instead of just their own.\n\nThis means the system is important but a little bit fragile.\n\nAnd none of this is \"fixed and solid scientific doctrine\" as there is no identified and absolutely distinct part of the brain where this all happens. There's no one spot we could laser out to \"pull the plug\" to turn off this feature. So it is extremely difficult to quantify.","label":0,"model":"human","source":"reddit","id":2752}
{"text":"It's an interesting question! This site suggests some answers: _URL_0_.\n\n\"After seeing the distribution of the suffixes of nationality on a world map, and studying the origins of these suffixes, I think we should be reasonably convinced that the choice of suffix is not entirely a matter of chance or taste. Instead, there are historical and linguistic factors which determine why one suffix is used for a certain nationality but another suffix for a second one.\n\nEnglish is a Germanic language,\u00a0its native suffix for nationality\u00a0is -ish, which accounts for the names of nearby nationalities. But before English had gone global and applied its suffix to other nationalities, it was influenced by Latin and French. The default suffix of nationality used in the language was replaced by the Latinate -ian\/-ean\/-an, so more recently coined nationalities made use of them instead. Later, the contact between Italy and the Far East, together\u00a0with the European colonization of Africa and South America,\u00a0brought in some nationalities ending in -ese. Then, Islamic countries near the Middle East retained their Arabic -i when their names entered English. Lastly, a few places that end in -land or Island make use of the suffixes -er\/-ic.\"","label":0,"model":"human","source":"reddit","id":2753}
{"text":"TL;DR The Wallstreet Journal published an article indicating that the Internet was highly unprofitable and no one should invest in it.  Everyone follows the advice and companies go bankrupt.\n\nIn 1997 the \"speculative market\" speculated that the Internet was a bottomless pit of cash.  Internet start ups required very little specialty, were cheap start ups and promised high rewards.  So if you throw $100,000 at a company that wants to call itself Google speculation could place them at $1,000,000 in yearly revenues and you're going... that's easy money.\n\nWhich it was.  Very few of these start ups ever did get listed on the NASDAQ, but the ones that did were just insanely high purely on speculation.\n\nOf course the business was never as good as what people were lead to believe.  The primary form of income off of the Internet should have been sales.  People should have been selling things online like eBay and Amazon.  But that's just not the kind of profit model people were going with.  People wanted to make money by offering free services.\n\nSo you might have web hosting businesses like Geocities, Angelfire, and Tripod.  People build their websites here and spread their websites to everyone they can.  Maybe each of these websites gets 100 hits a month.  But if you have 100,000 of them, well now you have 10,000,000 hits a month.  Investors like the sound of hits.  Hits sound like views which in turn mean advertising views.  Imagine a billboard in which every single month ten million people will see it... and it costs you a fraction of the cost.\n\nThis sounds great?  But it's just not reality.  In reality the same person might pass a billboard a million times.  And that's just what it was on the Internet.  The same people might visit a website 1000 times and would never look at the banner advertisements.\n\nBut advertisers were still paying for views.  Worse yet was the ridiculous amounts of money being lost to \"fake views.\"  People would be guaranteed certain traffic by advertising firms who would in turn have pop-up ads show up on websites.  Of course, no one looks at a pop-up ad, everyone is just completely trained to ignore them.\n\nSo you have these giant websites investing in advertising for their personal websites and their businesses.  Except the advertising industry is not actually providing you with any extra sales.  You're getting your view totals up, but that metric just doesn't make sense.\n\nIn the end what people wanted was unique impressions on their websites, not the same guy refreshing your page 1000000 times.\n\nSo people stopped spending money on advertising.  Advertising firms bellied up and websites with their own advertising packages began to shrink... or just crumble.\n\nWith advertising basically gone it meant that a lot of websites that relied entirely on advertising for sales simply had to go.  Google was able to \"sell search engine optimization\" by having people pay to be at the top of their search engine, it was highly successful.  Other search engines were not so successful.\n\nWhen it all came down to it people learned how the Internet worked.  A lot of the websites that survived have thrived (like Google) and some that survived are barely surviving (like IGN).\n\nAfter the crash happened the way in which we do business on the Internet fundamentally changed.  Pizza Hut for example invested in the technology to create a pizza, send in an order to the most local Pizza Hut and then schedule delivery.  People learned that the Internet wasn't a place where you could make money off of the Internet itself, but where it had to have real world implications.\n\nWe've moved on to a third wave of Internet in which we have digital currencies, mass free content, and full on digital only purchases.  A lot of people are questioning whether a lot of these really bad businesses practices can lead to another crash.","label":0,"model":"human","source":"reddit","id":2754}
{"text":"At the beginning of the 20th century, it was usual to work 60 hours a week. Then, unions, labour strikes and movements changed that (thankfully) and went down to a 40 hour week. That's what we've been working for 100 years already. Now, with the automation it would be fair to think we're up for a new reduction of the week work.\n\nSome countries such as France have gone down to 35 hours, without seeing much of a change (neither an improvement not a worsening of the work). In some countries they are trying the 6 hour day such as [Sweden](_URL_0_), it's going well for many of them, but nobody does these experiments long enough to actually see results.\n\n3 days a week, 8 hours = 24 hours a week. That's almost the 30 hours a week that the Swedes are trying. \nThe [new economics foundation](_URL_1_) actually suggests a 21 hour week.\n\nIn general, arguments in favour of reducing the time we work per week is more time for our families, more time for other activities, more time for consuming goods, better work life balance, happy employees, less unemployment. Not everything is producing things (aka working long hours), people need to have the money, the need, the willingness to buy them and the time to use them. That all makes economy boost.","label":0,"model":"human","source":"reddit","id":2755}
{"text":"Two important concepts: **debt-to-GDP ratio** and the **cost of debt**.\n\nGDP is a measure of total economic output and the principal measure of a country's total wealth. It's a bad sign when GDP doesn't grow. It's even worse when it is negative.\n\nGreece borrowed money by issuing debt (bonds) with the expectation that their wealth in the future would grow enough to cover debts incurred in the past. Interest rates (cost of debt) were high, as were GDP growth rates. Then the Eurozone crisis happened and now Greece's mortgage is underwater. At the current moment, Greece's GDP is growing at an average rate of about -4% per year (i.e., it's shrinking). The Greek government owes more than it can sustain to pay back in the payback period set by most of its bonds because the Greek economy is worth less than before.\n\nThe US is much different. Sure, the debt-to-GDP ratio much lower (80% vs Greece's 160%) and GDP is growing at about 2% per year. But the US dollar is also a world reserve currency.\n\nOur economy is so large that buying US public debt is considered \"risk-free\" for investment purposes. That's because our government both issues US debt to itself *and* can change the money supply to meet its obligations. In other words, the US government can always meet the face value of its debt obligations, even if this means that each dollar is worth less.\n\nOn top of this, the Federal Reserve also sets the Federal Funds rate, which affects the interest rate for the US dollar. This is currently at historic lows (used to be close to 0%). Since the US government borrows from itself, it can do so at incredibly cheap rates and can more or less afford to borrow at no cost.\n\nGreece doesn't have this system. Not only is it not using its own sovereign currency (it uses the euro instead), but its economy isn't large enough to dwarf the economies of other countries.\n\n**Edit**: Ok, y'all are blowing up my inbox. Just a few minor notes here for the sticklers:\n\n* The Federal Reserve is a quasi-public corporation created by Congress to further the economic and fiscal goals of  said Congress. However, it is otherwise independent from the rest of the government and funded by its own open market operations, fees rendered to commercial banks, and international currency deposits. Its Board of Governors consists of public servants whose terms do not coincide with Presidential elections, although its 12 regional banks are chartered as private institutions. Employees and staff of these regional banks are not civil servants. Of note here is that since the Fed is a not-for-profit company, any money it generates in excess of its expenses goes directly to the Treasury.\n\n* In the technical sense, the US government doesn't actually issue debt to itself. The Treasury Department does a primary auction of securities through the Bureau of the Public Debt and through the Federal Reserve Bank of New York. The Federal Reserve through Open Market Operations can buy securities from the Treasury like everybody else and buy and sell them on the open market. If the Fed sells them to the open market, the money supply decreases (it holds on to the money). If the Fed buys US bonds themselves, the money supply increases (the Fed adds new money into circulation). The US Bureau of Engraving and Printing does the actual printing of dollar bills, and the US Mint does the actual minting of coins.\n\n* Open market operations (sale and purchase of US Treasuries) are one way to control the money supply. Changing the federal funds rate and the reserve requirement are two others. The federal funds rate is usually set as a target, with the Federal Open Market Committee buying and selling Treasury instruments in order to keep the rate around that target.","label":0,"model":"human","source":"reddit","id":2756}
{"text":"When looking at the failure of dams we tend to describe the failure according to what we call the 'root cause'. For example; if you were driving a car on an icy road around a sharp corner and slid off the road  and hit a tree, a 'root cause' investigation would ask and answer the following questions; What was the damage? Car hit tree. How did car hit tree? Car drove off road. How did car drive off road? Sharp turn. Other preceding factors? Icy Road. Was he speeding? No.\n\nWe see from this that there are a number of 'causes' for why the car hit the tree. The root cause however is that the road was icy and the car lost traction whilst not speeding. We might look at redesigning the road to cater for icy conditions and stop future accidents.\n\nThere are a number of fancy named root causes of dam failures including [Slumping](_URL_0_) and [Piping](_URL_1_), amongst others. \n\nFor your specific question regarding the sudden collapse after '[overtopping](_URL_2_)' of an earthen dam (overtopping would be seen to be the 'root cause'), the answer is quite simply [EROSION](_URL_4_). Water erosion is a very destructive force for any earthen structure. Water is seen as the the great leveler of engineers and engineering designs. Flowing water will make short work of any earth structure. If the water does not erode the top of the dam it erodes the sides, abutments, downstream face, downstream toe...anything not protected and [makes short work of it](_URL_3_).\n\nEDIT: Just to further clear up my respsone; \n\nThe failure of dams can seem spectacular when the final stages of failure are happening. The amount of water\/material pouring through the breach could be seen as an exponential-type increase. I don't have data to prove this and in reality there would be far too many variables to generalise it, but the crux of the matter is erosion = more room for water to flow. More flowing water = more erosion = more space = more erosion = spectacular failure.\n\nThe failure event often starts days - or weeks - before this final failure stage. People just don't notice it unless they are actively inspecting the dams for increasing seepage, potential piping, slumping or wet spots etc every day.","label":0,"model":"human","source":"reddit","id":2757}
{"text":"Med student here. The topvoted answers don't tell the whole truth. There are two reasons as to why pain is delayed\/inhibited. \n\nELI5:\n\nThe first is that the inflammatory process takes time. It takes time for pain-inducing chemicals to be produced\/released and for these to affect nerves, by activating them.\n\nThe second reason is that your body has a store of molecules that can be released in the spinalcord in stressful situations, which actually have a pain-dampening effect. They block pain-signals travelling to the brain, making you unaware of the pain. When the stressful situation is over and the effect of the inhibitory molecules passes, you will be made aware of the pain\n\n\nNOT ELI5:\n\nReason 1: Inflammatory mediators need to be released and\/or created by inflammatory cells, and need to make their way to nerve endings. Histamine for example, also causes the blood vessels to increase in permeability, effectively making exudate from the blood flow into the tissue. The increased swelling in the tissue also presses on nerves (nociceptors - sensory nerves that carry pain signals), activating them which also leads to pain, once the signal reaches the brain.\n\nAs a fight or flight response the following happens:\n\nReason 2: Basically what happens when you dont feel pain at the actual time of injury is your body releasing inhibitory neurotransmittors (endorphins (endogenous morphines), GABA, noradrenaline etc) in the spinal cord which inhibits pain signals travelling up to the brain, and hence less, if any, pain signals reach the brain, making you unaware of the pain. \n\nEdit: Added another reason for pain inhibition. These two reasons combined explain why we don't feel pain at the exact time of injury.\n\nEdit: Added a basic version (ELI5)","label":0,"model":"human","source":"reddit","id":2758}
{"text":"*The Protocols of the Elders of Zion*, a forgery out of 19th century Russia, purports to be an account of the workings and dealings of a cabal of Jews who secretly control the world's economics and finances and business and \u2026 etcetera.\n\nIt meshed with the anti-Jewish hatred that was built by Martin Luther, the man who effectively was the father of the Protestant movement.\n\nAnti-Jewish hatred built for centuries before that, fed by Christianity's Biblical accounts of Jewish leaders (!) demanding that Jesus Christ be executed for claiming to be the Messiah, and because many highly-observant Jews have traditionally been insular, private, and \"rude\" to non-Jews, to the point of having separate economies, not speaking the local language, not marrying into the community, etcetera.\n\nFor many centuries, Christianity held that charging interest and handling money was sinful, so Christian financiers employed Jews to count and collect money, even extending to governments appointing Jews as tax collectors and heads of banks \u2014 contributing to the view that Jews controlled governments.\n\nThen the Nazis happened. And then, Israel happened.\n\nSo today, when you see conspiracy theories that include \"the Jews did it\", you're seeing sympathy towards the Nazis or toward Arabic Islamic politics. Even the KKK (and, by extension the Southern Baptists) blamed Jews for the Civil War, Reconstruction, and the Civil Rights movements.\n\nFinally, they often include \"the Jews\" in their conspiracy theories because it's ridiculously easy to blame other people and hate other people. People who carry conspiracy theories are emotionally ill \u2014 they have a need to play out a role in a psychosocial drama, one of either the Victim, the Persecutor, or the Rescuer. \n\nBlaming \"the Jews\" for the ills of the world, allows them to don the mantle of the Victim *and* the Rescuer simultaneously *and* absolve them of any guilt for being the Persecutor (of the Jews, or of anyone else). It emotionally justifies any action they choose to take, as long as it can be cast as being in opposition to \"the Jews\".\n\n---------------\n\nEdit: I wrote \"people who *carry* conspiracy theories are emotionally ill.\" \n\nEmotionally healthy people will entertain \"conspiracy theories\" as *hypotheses* \u2014 something that may be *plausible* but which lacks evidence. They will withhold judgement until more evidence for or against the hypothesis is presented, or will work themselves to build a case for \u2014 or against \u2014 the hypothesis, and will only *carry* the \"conspiracy theory\" if they themselves can find sufficient evidence to *reasonably believe* that it emerges *from the realm of conspiracy theory* \u2014 to bring it from the realm of hypothesis to the realm of working theory, something worth society considering *without the person being involved*. Or, they will simply carry on with their lives.\n\n\"Conspiracy theorists\" who derive personal emotional needs and attention from involvement in \"conpiracy theory\" movements, are emotionally ill \u2014 which *is* mentally ill. The underlying mental illnesses that motivate them are diverse and *beyond* the scope of this discussion. If you want to know more about the psychosocial dynamic at play there, feel free to research the [Karpman Drama Triangle](_URL_0_), a framework for understanding how these dysfunctional emotional and social relationships interoperate.\n\nNot everyone who puts forward a hypothesis is mentally ill. Not everyone who puts forward a hypothesis that gets labelled (by *someone*) as a \"conspiracy theory\" is mentally ill. Those who *need* the \"conspiracy theory\" to give their life meaning \u2014 who \"carry\" it \u2014 in the sense of carrying a disease \u2014 they are the ones who are ill.\n\nWhistleblowers and concerned citizens put forward important information about *actual* misdeeds, abuses of power, and conspiracies *all the time*, and part of the rhetoric of discrediting them is to label them a \"conspiracy theorist\".","label":0,"model":"human","source":"reddit","id":2759}
{"text":"With great difficulty over the better part of a century.\n\nThe Great Trigonometrical Survey of India started from the ocean in 1802, and 100 feet at a time, took measurements and did a bunch of math, and worked there way across the sub-continent to the Himalayas, completing in 1871.\n\nIt was a great scientific achievement, lead during some of its more important years by George Everest, who received a knighthood for his efforts.\n\nThe basic technique is fairly simple. You start with two sticks at sea level, a decent distance apart, and measure their exact longitude and latitude. Then you put a third stick some distance away and inland, making a triangle. Based on the distance between the first two sticks and the angles they form with the third stick, you can compute the third stick's exact position and elevation.\nOnce all that is done, you repeat, planting a stick further inland and drawing a new triangle. \n\nThe Great Survey did this with better instruments, better technique, and on a greater scale than had ever been done before.","label":0,"model":"human","source":"reddit","id":2760}
{"text":"Multiple reasons, some have already been mentioned in to comments, others haven't. Keep in mind that this is based on my experience working with this in Sweden. Mileage in other countries may vary. \n\n1. Horses are less noisy and more approachable. People can come up and talk to the officer in a different way. \n\n2. The horse still offers mobility and speed. \n\n3. Tradition and culture. In a lot of places, like Sweden (where I live) horses are fairly integrated in culture. People enjoy seeing horses out and about. Not everyone want to approach one, but that's fine. \n\n4. Crowd control. You absolutely do not fuck with one of those horses. They're huge, and on their back you have a trained police officer. It's a force multiplier. I've seen sports events riot like situations where three mounted officers replaced 20-25 officers in riot gear. The fights stopped immediately and people scattered. \n\n5. Accessibility. A mounted officer can move in areas where a car can technically move, but is advised not to because it could cause blockage or congestion. Such as walking streets or shop streets. \n\n6. Visibility. A police forces primary task is to maintain law and order. This is preferably done by preemptive measure, with things as simple as visible presence being at the top of efficiency. A mounted officer can both see and be seen easier. This has a calming effect on the public as well as encourages approaching the officer. We were often approached about very minor things, which didn't warrant any kind of report or anything, but that keeps the public happy and calm to be able to let the authorities know about.","label":0,"model":"human","source":"reddit","id":2761}
{"text":"Say you're a merchant living in 17th century Amsterdam, and there's a bunch of super-rich ladies who really, really loves cumin (as in, the spice). Unfortunately, cumin doesn't grow anywhere near Amsterdam, you have to go to the middle east or India to get some of that fine stuff so it's *super-expensive*. Over there, however, it grows like a weed and costs very little money. If only you could get some of that cheap cumin over to Amsterdam, you could make a lot of money!\n\nUnfortunately, the only way to do that is to send out a big-ass ship for like nine months. Not only is that HUGELY expensive (you have to get a ship, and a captain and crew who all wants nine months worth of pay), but it is also really risky. The boat could sink, after all, or all the cumin could spoil. \n\nSo how do you accomplish this? Assuming you're not stupendously rich yourself (and very few people or groups, aside from maybe whole nations, are that rich), how do you finance this? You could go to a bank or money-lender and take out a loan, a loan which would have a very steep interest rate that could almost eat up all of your profits. And what if the boat sank? You'd be completely bankrupt, and be thrown into prison, if the money-lender doesn't waste your broke ass first. \n\nSo, you come up with an idea. You can't finance this trip alone. But you could rather easily finance, say, one tenth of the cost of the trip. And look at that, you have nine friends, all of whom could also afford one tenth of the cost of the trip. So you all come to an agreement: you decide to each finance part of the cost for the trip (i.e. you \"buy stock\" in the enterprise), each getting shares worth one tenth of the total enterprise. With this money, you pay for the boat and crew, send them off, and then nine months later, it comes back with a cargo-hold full of delicious, super-expensive cumin. The profits far and away compensate for the initial cost, and each of you and your friends gets one tenth of the enormous profits.\n\nBut what happens if your boat sinks? Well, it's a real bummer obviously (especially for the crew!), but you only payed for one tenth of the cost, and this was a cost you could afford to eat, you're not gonna get killed over it. Since the risk was spread around to 10 people, no one landed in debtors prison. Everyone took a small hit, instead of one person taking an enormous hit. \n\nThink about this example, and all the people involved in it. The fancy ladies got their cumin (and for cheaper than they would have otherwise). The shipwrights got work building a new ship. The captain and crew all got work and a salary. The trader in India got a new customer for his cumin. And obviously, you and your friends got really rich in the process. In short, *every single person is better off*, and it wouldn't have happened if this notion of \"buying and selling stocks\" didn't exist. \n\nThis is the advantage of a stock market. It moves capital around from where it just lies around doing absolutely nothing to where it can be put to use, and in the meantime, everybody gets richer. It also distributes risk between lots of people, so you don't have to be afraid of going broke all the time. \n\nNow, obviously, there are problems with how the modern day American version of the stock market functions. But the critique is that it stopped doing what doing what it is supposed to be doing (i.e. allocating capital so that it can be put into productive use) and stopped being a means to an end, and it became an end unto itself. The basic concept of a stock market is a fantastic idea, and you'd have to be a real hard-core communist to oppose it. It is why we have railroads, cars, computers, telephones, cheap and plentiful food, medicine, and all the other modern things we value so highly.","label":0,"model":"human","source":"reddit","id":2762}
{"text":"EDITED: So, my original response got zapped by the mod (and I've no interest in debating the merits of that, let's move on), so let's try again.\n\nTo emulate means to equal or approach equality with. What makes console emulation hard is that your PC has to do it well enough to be acceptable (the game must look the same, say) and fast enough to play the same.\n\nImagine your friend drew a picture with some colored pencils. Looks great right? Now your job is to take another sheet of paper, and, as quickly as you can, draw exactly the same picture. Same lines, same shades, same colors. Your pencils are mostly the same, so that at least gives you a leg up, but boy...it's gonna be hard isn't it? Now try doing that and being told you have to draw it as fast as your friend did the first time.\n\nLet's take a detour into the \"mostly the same\" clause. If the PC has key components that are identical to the console, that's like you have the same colored pencils. You say \"I need a thick dark orange \" and lo and behold, you have the perfect pencil to hand. This is like your PC running the same operating system on the same processor chip as the console. Life is looking sweet. But if the components differ, that's like you needing a thick dark orange but only having a thin pencil. You can get there, but it's gonna take more time. In PC terms this is like having a CPU that has different chip registers from your console. Or maybe you need orange but you only have red and a yellow. Your PC is missing some hardware feature of the console and it has to cobble together a solution in software to make it right.\n\nIn short, when a PC emulates a console, you are asking it to do everything the console does, just as fast, but using its systems. It has to work harder to get it right.","label":0,"model":"human","source":"reddit","id":2763}
{"text":"First of all keep in mind that the advice given varies from country to country. A lot of it is disputed - mainly due to trying to find a one size fits all recommendation when people are all so different. A 65 year old man won't need the same things as a 23 year old woman for example.\n\nOk so how do they work it out? Well it's different for each item, they're not all worked out in the same way. Some are actually just educated guesses. Others they get from looking at studies into certain diseases that come about when you have a deficiency in something. They may look at the diet of someone with scurvy for example, and see how much vitamin C they ate compared to someone who is healthy. They can then determine the healthy person was eating enough and set it as a minimum requirement. (They would not just look at a single person though, they would look at many.)\n\nFor some some vitamins they do get a bit more \"technical\" and can take blood from someone and use a machine to look at the cells at a DNA level. If they see damage from a lack of a certain vitamin they can determine that the patient is deficient and make a note of how much they take in and increase it.\n\nSo, rather than making people deficient on purpose to find out the minimum values they wait until someone is already deficient and then figure it out from there.","label":0,"model":"human","source":"reddit","id":2764}
{"text":"There's a guy at school who talks to a lot of people. Some people tell him interesting things, and he has a very good memory, so if you're ever bored, you can go talk to him and he'll tell you one of their stories.\n\nAs time progresses, this guy gets more and more popular, and starts to have trouble keeping up with demand for stories, so he goes and finds a bunch of people to help. His helpers' memory capabilities aren't as good as his, but they can talk more quickly. Now, instead of going to the guy for stories, people go find one of his helpers. The helpers eventually relay stories they're told to the guy who still makes a point of remembering everything they're told, and the helpers can remember a few really popular stories by themselves. If someone asks a helper for a story that they can't remember, they'll go off and ask the guy, who will tell the helper, and then the helper will tell the person who asked for it. (Additionally, the helper will remember that story for a little while in case anyone else asks for it.)\n\nExcept, sometimes someone will ask a helper for a story that the helper can't remember, and when the helper goes to look for the guy, he's too busy telling stories to other helpers to tell them the one they're looking for, so the helper will have to tell whoever's asking for the story that they just can't right now.\n\nThe guy is the reddit app servers, and his memory is the reddit database servers. You don't talk to him directly; his helpers \u2013 the reddit cache servers \u2013 do. If the app servers are too busy, the cache servers return a 503 error, indicating that the service isn't necessarily gone (that'd be a 400-series error or, in the reddit infrastructure, 502).\n\nIn the case of websites that don't have a caching layer, you wouldn't see an error generated by the webserver \u2013 the request would just time out, so you'd get an error message from your browser. (You could think of that as asking a friend to go ask the guy for a story, but the friend got tired of waiting for the guy so he came back and told you that he didn't get a story.)\n\n**Edit:** You can also expand this to explain why sometimes you'll click on a link to a comments thread and then... wait... forever. What's happening is that the helper who stepped forward to tell you a story (web server responsible for your request) is trying to help too many at once (is trying to serve too many requests at once) and is too distracted to serve you properly (is too distracted to serve you properly).","label":0,"model":"human","source":"reddit","id":2765}
{"text":"To add onto what has been said so far.\n\nHistorically sailors were given rum as part of their fashions. Essentially it was part of their agreed wage.\n\nHuman nature not having changed much in the last few million years and alcohol having always been relatively expensive, captains had a tendency to water the rum down beyond what was considered an acceptable level and sailors given perfectly good rum would say it had been watered down.\n\nTo resolve this a test was developed where a small amount of gun powder was soaked in some of the rum. If it would still ignite this was considered proof that the rum was good enough.\n\nNow you may think, it's awfully convenient that this number is 50% ethanol, and you'd be right. The actual number is 57.5%. If you buy booze in the UK, 100 proof is actually that number. The US shifted the scale because it made the math easier. Yes the British navy was staffed by motherfuckers who consumed a reasonably large amount of 115 proof booze daily.\n\nIt's used for the same reason that in countries countries where the metric system is used almost exclusively you'll still buy beer in pints, which is that tradition matters an awful lot when it comes to booze.\n\nTL;DR booze with 57.5% ethanol wouldn't stop gunpowder from igniting and proved booze was an acceptable strength. The Americans rounded down to 50 for easy math and people don't like change when drinking.","label":0,"model":"human","source":"reddit","id":2766}
{"text":"A few reasons. Mostly:\n\n-The brain uses about 30% of the energy stored in the human body, and the head produces about an equivalent amount of heat despite being of comparatively low mass\/volume (typically the head is only 12.5% of the mass of a body)\n\n-The head, neck, and face have a very high density of blood vessels close to the surface. Cuts to the head and face tend to bleed a LOT even when they're just superficial \n\n-The face has such a high surface area with the nose and ears and lips, these allow for heat to dissipate faster\n\n-Blowing cold air at the face ensures that cold air can be taken in by the lungs to cool the core\n\nThese four things combine to make the face a beautiful heat sink, just like the one in your PC.\n\nAdditionally: applying cool fluids (typically water but air could work too) to the face will engage your body into preparing to go under water, reducing your heart rate and allowing you to take slower, deeper breathing. Dipping your face and whole head if possible into cool (not cold) water is one of the fastest ways to mitigate the onset\/symptoms of heat stroke.","label":0,"model":"human","source":"reddit","id":2767}
{"text":"When you make something creative (a book, a website, a painting), you get something called *copyright*. This means that you have a right to determine how people copy it; if they want to make a copy, they have to ask you for permission. The C-in-circle symbol means that the thing it's printed in\/on\/whatever is copyrighted.\n\nWhen you sell stuff, you have certain words and phrases that are connected to your advertising for it. This includes the name of whatever you're selling, but it can also cover things like taglines and memorable characters from your ads or something. This is called *trademark*, and the TM means that the phrase right before it is trademarked.\n\nNow, you can *register* trademarks with the federal government; prove that you've been using it for a while, and that people associate the trademark with you, and it goes on the official list. When you do this, other people get less leeway in using your trademark, and it's a lot harder for you to lose the right to it. So when a trademark is *registered*, people use the R-in-circle instead.","label":0,"model":"human","source":"reddit","id":2768}
{"text":"Say you have a heart attack. You heart muscle cells all die because they aren't getting enough oxygen. Heart cells can only get bigger or smaller, they can't replicate themselves. So a bunch of cells called fibroblasts show up and secrete a bunch of collagen. Collagen is a structural protein. It doesn't contract like heart muscle cells, but it adds strength to the walls and keeps the heart from bursting.\n\nIf you have a very shallow and small cut on your skin, it will recover no problem. If you have a big one, you cut out all the cells that make new skin cells. So fibroblasts lay down collagen instead. The cells still replicate and change, but now there are healthy fibroblasts instead of skin cells. You'll forever have scar tissue there.\n\nImagine the guards at Buckingham palace. Say they all line up shoulder to shoulder. Then a bunch in the middle die. Some new guy might show up, but instead of just standing there, instead he brings a gate to stick between the two remaining guards. The gate is strong, but it isn't alive like the guards were.","label":0,"model":"human","source":"reddit","id":2769}
{"text":"\"Suddenly\" isn't how I would put it.  Forgetting the path from legality long ago to illegality, the path back to legality starts in the 70s, at least in  terms of notably legal changes and\/or ballot measures and bills:\n\n1. In 1970, the federal government removed mandatory penalties for weed possession, leaving them in place for \"harder\" drugs.  This established a precedent of treating marijuana as \"not like the other drugs\".\n\n2. Oregon decriminalized weed in '73.  4 other states followed suit the next year, and then many others by the end of the '70s.  Decriminalization has made it's way through many other states, adding states as recently as a couple of years ago.\n\n3. In 2004, Oakland CA passed laws that made tried to make it it legal, period.  In the end, the actual text of the law was written to avoid being struck down and only made it a low priority, but it did setup the framework for taxation of weed - the first time that hit the books anywhere.  It acknowledged that it required state-law support to actually move forward on legality.\n\nIf you were to overlay the changes that relate to the _medical_ use of marijuana you'd see a similar trend.\n\nI'd suggest that the \"suddenly\" is a false premise in your question, but...hopefully this info is useful!\n\nedit: correct details of oakland portion, thanks to \/u\/Sluisifer","label":0,"model":"human","source":"reddit","id":2770}
{"text":"First you must understand that AI in gaming is used in the loosest sense possible.  The AI is typically a term just applied to specific NPC actors within the game.  It's simply a series of commands an actor follows in accordance with their surroundings.\n\nFor every interval of time (cycles, ticks, etc), the actor will check what is around it in the game world.  They might have a hearing range that is 2 meters, so instead of asking the game engine where the player is, they'll check if they hear a player within 2 meters of their position.  They might have an invisible sight cone(really just a visualization of a geometric function that the actor runs) that projects 10 meters in front of them at 45 degrees, so they'll check if the player falls within that cone.\n\n\nBasically, the data is there and the actors could immediately access the player's location if that was the game designer's intent, but it's not as sporting or immersive as the actor seeking more conventional means to find you, like a clear line of sight or close proximity.\n\n\nOh, or like a DM and a group of Dungeons  &  Dragons players.  The game engine is like the DM; It knows all the stats, all the player's positions, and determines all the rules governing them, but the players still have to run Search checks and such by the DM.  Sometimes a player will just give a DM numbers and the DM will translate that into \"You see a Dwarf\" or \"Hit\" or \"You step in a pile of horse dung.\"  I guess you could scratch it up to compartmentalization of information.","label":0,"model":"human","source":"reddit","id":2771}
{"text":"The nutritional information on packaging is *roughly* accurate to the amount that the average person absorbs.\n\nThe process is silly, and involves explosions and poop. Food is basically just fuel, and burning fuel can tell you how much energy was in that fuel. Scientists freeze-dry food, crush it into powder, and ~~explode~~ burn\u2020 it inside a machine called a bomb calorimeter. Reading the energy released by burning the freeze-dried food gives a general idea of how much energy was in that food. \n\nAt the same time, other scientists feed that same food to people, collect their poop, and perform the same process. Freeze-dry. Crush. Burn. That gives an idea of how much energy a person does not \"absorb\" in the digestion process.\n\nSubtract one from the other. Total energy minus unused energy equals energy absorbed.\n\nOf course, this is a rough estimate, conducted for only certain foods. More complex items are just sums of the already-taken measurements for their component parts.\n\n[[Edit: Of course these are \"standard\" values and the amount absorbed by any individual may vary slightly based on their own unique digestive tract  &  the microbes living there.]]\n\n\u2020: [[Edit for clarity: You are *setting up* an explosion by burning the material inside a constant-volume steel container\u2014the bomb. Rather than allowing it to actually explode, the expansive force is contained and measured. My ELI5 language was a bit too simplified\u2014it's my first time commenting here\u2014so at the request of one of the replies I have edited.]]\n\n.\n.\n\nSource : _URL_0_","label":0,"model":"human","source":"reddit","id":2772}
{"text":"I have some VERY strong opinions about this topic...\n\nMy kids have always eaten whatever we give them. It was never an option for them to be fussy or picky. We have never forced them to eat anything, ever, and if they didn't want to eat something we served, that was and is okay. However, they didn't get something else instead. \n\nWhen parents give kids a new food and say 'Here, *try* this', the child gets the message that they might not like it or want to eat it. When parents say 'Just take a bite', the child gets the message that it is an option for them to reject the food. Even the expression on mom or dad's face, looking anxious or even hopeful, tells the kid that this food might be weird. \n\nWhen our kids (now 12 and 13) were babies, we fed them a wide variety of foods and they only outright rejected a couple of things. We would not make any issue over it at all, but just put that food aside for a week or a month, and would then try again. Some things still got rejected, so we did the same thing - don't offer it again for a while, and try it again later, always trying to make mealtime as pleasant and calm as possible. When they were old enough to feed themselves, we filled their and our plates with the same food (in age-appropriate amounts), sat down, began eating, and they would as well. Eventually they ate whatever they were given, and still do for the most part. \n\nEveryone has preferences and dislikes, and that's okay. But the thing I noticed is that there are cultures that eat things that other cultures might find weird or even gross. Chinese breakfasts often consist of a bit of fish, some vegetables, and other savory things which American kids would find unpalatable, in favor of sugary cereal or pancakes. Latin and Asian cuisines can be quite spicy and kids in those cultures seem to relish those flavors, where perhaps Scandinavian or British folks, for example, might be amazed at that level of heat. So it seemed to me like it's a matter of what is presented as 'normal' by the parents.\n\nI also think the kind of snacks kids eat between meals will greatly affect what they are interested in eating at mealtime. Calorie dense, high fat or high sugar things, and mini-meals have not been available between meals for our kids. I love to cook and work hard on balanced, nutritious meals, so I want to make sure everyone is hungry when it's time to eat. So snacks at our house have always been things like fresh fruit, veggies, yogurt, a few whole-grain crackers, a handful of nuts, and stuff like that. My feeling is that a peanut butter and jelly sandwich is the main part of a meal, not a snack. They can have all they want of the snacky things, (because no kid should be made to go truly hungry), but they can't mindlessly eat chips at 3 o'clock in the afternoon, because then of course they aren't going to want much at dinner. \n\nOf course this is all just my humble opinion. I don't judge the parents of the kids who bring a bag of Doritos and a Coke to school for breakfast because parenting is ridiculously hard. If your kid is fed and dressed and still alive every day, God bless you because you're doing alright in my book. But I've struggled with weight and eating all my life, and I *really* want to give my kids every chance to possibly not have that struggle.","label":0,"model":"human","source":"reddit","id":2773}
{"text":"Games like first person shooters that have maps on them don't separate the world into solids, liquids and gasses like real physics does out here in non-cyberland.  Out here, when we press on a solid, it kind of presses back. Even if we break through the top of that solid and press on what's underneath it, such as if we sand off the top inch of a thick table, it's still solid underneath and so we won't go through.\n\nGames don't do that. They don't have a solid object so much as a \"layer\" that you can't penetrate through, and they do their best to write the game's logic and test the map so that you can't possibly break through. But if you DID somehow manage to pass through that \"layer\" there's no solid stuff underneath like in a real table-top to stop you, so you keep falling because the game doesn't have logic for \"if you're in a solid object\". (In fact, a lot of earlier shooter games had really hilarious stuff happen if part of your body was on one side of this layer and the other part was on the other side. Characters would often go completely spastic when that happened.)\n\nThe more complex and numerous your world's shapes and characters are and the more ways they can possibly move, the harder it is to properly detect every single tiny little spot where you might break through that infinitely-thin layer and are suddenly in BizarroWorld. It's harder to keep all that math straight at corners, on steep slopes, or when dealing with physics events that cause your game character's body to move in unexpected ways, such as multiple simultaneous explosions or getting hit with something REALLY heavy like a jet airliner. \n\nSo once you're on the other side of that layer, the game calls its \"falling code\", and your character's downward velocity starts building up. The game would normally stop you when you contact the layer that's below you... except since you broke through already, there IS no layer below you, so in some games you'll fall forever.","label":0,"model":"human","source":"reddit","id":2774}
{"text":"I'm a cop. Its hard to argue that police isn't becoming more miltarized but I think its vastly over rated.  Two agencies in my county have a bearcat (like a tank with no big gun) but it it rarely used and all the agencies borrow it when needed.  Aside from a taser, I have received no new weapons since I started 15 years ago.  \n\nEdit:  Some good comments, I'll just edit this post instead of responding to each one.\n\nI don't have access to anymore equipment than I did 15 years ago.  Well I suppose we have .40 caliber pistols now instead of 9mm but besides that and the taser, everything else is the same.  \n\nGenerally the bearcat is used for barricaded and armed subjects.  I work in the SF bay area of California which is extremely liberal and has very tight gun control.  I pulled an AK-47 and an illegally modified shotgun off a suspect two weeks ago.  Criminals are now heavily armed and the police need to be able to match them in firepower.  The bearcat would have been eventually been used if that suspect had barricaded himself in a house.  We can use it to provide our SWAT teams with mobile cover to either take the house or launch non-lethal ordinance at the house (gas, stun grenades, etc).\n\nI have read about SWAT teams being used unnecessarily.  My contention is that sometimes the news outlets don't have all the facts. Don't forget that a **judge** must authorize no knock warrants.  However, there are probably a good number of search warrants being served by SWAT teams than is truly necessary.  I just don't have enough facts to talk about specific cases.  I will note that any search or arrest warrant involving the sale or manufacture of illegal narcotics is pretty much an automatic SWAT call out.  Major dealers and producers can make $25,000 a week and they usually have guns.\n\nSWAT teams and the rise of police firearms training is to counter the evolution of the way the criminal element does business.  40 years ago, officers didn't have shotguns or vests because they weren't really necessary.  Today, pulling a handgun off a felon is not rare.  Being attacked is not rare.  Like I said earlier, I pulled an AK-47 off a suspect recently.  A shotgun and pistol cannot counter an AK-47.  If I have to respond to an active shooter situation, I want the weapon that can deliver the most rounds, with the most accuracy, with the least reload time and that is my M-4.  I have an M-4 in my patrol car.  I actually remove it for calls maybe once a year.  I have never had to fire it outside of the training range.  But if that active shooter crisis happens at your children's high school, you'll be glad that I'm trained and armed to deal with the threat.  They waited for the SWAT team at Columbine.  That shit can never happen again.\n\nEdit2:  I'll also just note that I have been a soldier and a cop my entire adult life except for 4 years of college and a few years tooling around the white collar corporate ladder.  I have never been issued a fully automatic M16 or M4 at any point.\n\nEdit 3:  Some people have argued that we don't need patrol rifles (M4).  As I've said earlier, I pull it out maybe once a year.  I have pointed it at one person in 15 years and have never pulled the trigger outside of the range.  If it is taken away from me it will have pretty much zero effect on my day to day activity.  However, if someone starts shooting at the high school I will not be going in.  I'll secure the perimeter and wait for people who have the proper weapons and training to arrive.  This can take anywhere from 45 minutes to 3 hours.\n\nEdit4: I'm going to bed. Thanks for the great discussion everyone.  A few last thoughts.   There is only three things I really need to do my job: my gun, my vest, and my radio.  If you take away the rest you just reduce my options and flexibility.  Sure some cops think they're navy seals when they go through rifle training but the rest of us know that the weapons we carry are a responsibility to protect and serve and not a booster for our egos. I don't think its a good reason to hamstring all of us.\n\nI'll say in closing that we're human just like you.  Most of us are married (some formerly) with kids.  If crime ended tomorrow and we got paychecks for helping old ladies cross the street then we would be happy.  \n\nGot up to pee.  Holy shit GOLD!!! Thanks!!!!!","label":0,"model":"human","source":"reddit","id":2775}
{"text":"I don't think you understand how air conditioners work, based off the nature of your question. Coolant doesn't supply coolness like gasoline supplies energy.\n\nAir conditioning works by compressing the coolant gas until it becomes a liquid. This compression creates heat which is radiated away into the outside air by the fan blowing outside air over the coil. The compressed liquid is then pumped \"inside\" the house into an evaporator coil. This allows the liquid refrigerant to convert back to a gas. As this phase change occurs, heat is absorbed from the surrounding air, which is usually inside air circulated over the evaporating coil. This cools down the air being passed over the coil. That cool air is then blown into the inside of the house as air conditioning. The refrigerant is then pumped into the outside portion of the air conditioner to be compressed.\n\nIn essence, heat is created by compression and dissipated with outside air. When the refrigerant evaporates, it absorbs a similar amount of heat, which cools the inside air that is circulated through the house.  The only thing that gets consumed is energy that drives the compressor and the fans that circulate air. Since the coolant travels in a closed loop, there is no need to refill unless there is a leak.","label":0,"model":"human","source":"reddit","id":2776}
{"text":"I'm not sure that science knows exactly why, however we do know a few things...The Hippocampus in our Limbic system might be the answer as it is a part of a series of processes which take in short term memory, transfer those memories to long term, and is connected to the amygadala - who is known for processing emotions and levels of fear. The temporal lobe is known for keeping information which can help with retrieval. Keep in mind this is VERY simplified because of ELI5. I left some people out, but this is the basic idea. So, to the ELI5...\n\nSo, you have several small people working inside your head while you're awake or sleeping. These people all have different jobs; some work to make sure we're safe, some work to make sure things make sense, some have personalities, while others are aspects of our emotions. \nTwo of these people have an important jobs. One is a logistician - a person who takes information from what other people tell him, and he decides if it is important enough to keep (his name is hip). The other is a man who is completely emotional, and takes note of how we feel at a given moment (His name is amy). Amy and Hip go way back, and their friendship shares a deep connection. \nSo, when Amy senses something about the world, he sends a message to Hip. Hip has received a few other messages, but what is important is that Hip wants to file the good and bad memories so that we're safe and we have identifiers to those situations. Amy helps Hip store files that they both consider important into his shared folder, and Hips other friends (such as the guys who see, feel, etc) who have been sending him messages get tagged in the file as well. That file is now in your history, with all their relevant friends tagged, and all the important aspects covered. \nWhen you experience a moment that is similar to one of those events on file, Amy and Hip call one of their friends who has the memory (we'll call him temporal, but some other friends do a similar job as him). They all open it, and they get to have a small recollection of what made that event so special. \n\n\nMemories and Emotions are tied together very strongly, and for some people, it is not a good thing. When Amy gets hurt, Hip and Amys other friends aren't happy. Amy begins to bring up dramatic experiences, such as war, famine, disease, death and begins to over-react when things cause Hip and his friends to bring up any 'triggers'. This is PTSD -  a dramatic and potent case of memory retrieval. Similarly, people who are depressed have many of their mental people tired or over-worked. It is commonly not that persons fault, and can happen because of miscommunications that happen when they talk. \n\n\nMemories and Emotions can be good though, like with just the right amount of nostalgia. If the brainiacs in your head are working together, then they sometimes bring up memories of fun times. Times when you were relaxed. You were listening to music, playing a game, talking with one of your friends... almost anything can be nostalgic if the context is right. Once you're comfortable, the team works together to store that information so that you know what it is like, and have a reference point in the future. \n\n\n*After receiving a lot of replies, maybe i should watch Inside Out. It turns out that i made something similar to their story, so that is pretty cool. \nOther people are talking about genes and why this personality quirk may have developed. Most neuroscientists and behaviorists say we develop these out of an adaptation for something - and adaptations most dominantly solve a 'problem' in their environment. It is up to speculation, but usually this introspection or reaction is for keeping us alive socially. \nAs far as the rest goes, the brain builds-itself-up (both in neurogenesis and in its evolution), so it is easy to see why the limbic system would tie together 'primal' emotions and 'complex' trains of thought and storage... they're all adjacent to each other and they all want to keep you alive and reproducing. \nWhy some people have \"good\" and \"bad\" nostalgia is usually broken down into how an individual reacts or their expectations, and the feelings that are memorized with that actual event. It turns out to be pretty personal and hard to determine. So if you dislike nostalgia, it is in your best interest to do things that aren't going to bring it up. Some people love it, some people hate it. \nIt will never feel the same, because it is two different points of view in two different time frames, with completely different perspectives. If you ask me, we just have to appreciate the fact that our brains are able to make these connections... though i do understand it is not easy at all times. I'm happy people are getting a kick out of it at least, because neuroscience and psychology are pretty cool.*","label":0,"model":"human","source":"reddit","id":2777}
{"text":"Ah something i can answer. A few years ago i was stabbed in the lungs a few times and heres what happens. Here im making the assumption that only the lungs are punctured and no other organs.\n\nWhen you breath in and out, the space between your chest wall and your lungs creates a small vacuum almost which expands the lungs to suck air in. If anything gets into the same chest cavity, and due to there being a hole in the lung, the lung deflates. This is called a collapsed lung or a in medical terms a pnuemothorax. (spelling?)\n\nThis dosent make you cough like a few people have said. The second thing that happens is the same chest cavity fills up with blood, this is called a haemothorax. With the lung deflated and the chest cavity filling with blood, its pretty hard to be able to cough up blood. \n\nAt the same time however, you aren't going to stop breathing, your chest will keep on trying to expand and contract. due to this small vacuum, you get whats called a sucking wound, which is taking a good percentage of your breath away each time you try and breath in. \n\nSo in short, im quite sure its unlikely,  be it by gravity or coughing.\nAny where else it could happen though, someone else might know.","label":0,"model":"human","source":"reddit","id":2778}
{"text":"Bankruptcy lawyer here. I'll try to keep this on a five year old level...\n\nThe first thing that happens is something called the \"automatic stay\" goes into effect. This means that nobody to whom you owe money may try to collect that money. Many of my clients state that the relief this provides from the stress of collection calls is the greatest benefit of bankruptcy.\n\nAs ANewMachine615 pointed out, there are two common types of personal bankruptcy, although some individuals file under other Chapters in certain circumstances. By the way, they are called \"Chapters\" because the rules governing are found in Chapter 7 of the Bankruptcy Code (Laws), Chapter 13, etc.\n\nIn a Chapter 7, all of your property then becomes part of something called the bankruptcy estate. A person known as the trustee is responsible for seeing that your creditors (people to whom you owe money) receive as much as they are allowed under the law. He does this by auctioning off the property of the bankruptcy estate, which, remember, used to be yours.\n\nIf you have pledged certain property as security for a debt, such as a car for a car loan, that debt is called secured debt. Usually, a person's secured debt is just cars and houses, but other things sometimes fall in that category as well.\nAll other debt is unsecured. This usually includes things such as credit cards and medical bills.\n\nAt the auction, let's say the trustee auctions off a home for $100,000, but you only owed $75,000. In that case, the $75,000 goes to whomever you owed it to, and the rest is distributed among unsecured creditors in proportion to their amount of your total unsecured debt. However, if you owed more than property securing a debt is worth, the creditor gets it.\n\nUnder the Bankruptcy Code, there are things called \"exemptions.\" These are things you are allowed to keep out of the bankruptcy estate so that you aren't left without anything. The Code allows each state to opt out of those exemptions and use their own. I believe most states have done just that. \n\nYou may also redeem or reaffirm a debt, but that is beyond the scope of a five year old explanation.\n\nA Chapter 13 is basically a court-run debt consolidation plan. You will make one payment a month to a bankruptcy trustee, and he will distribute that money to the creditors according to a plan that you have proposed and the court has confirmed.\n\nA special feature of a Chapter 13 is a \"cram down.\" If you owe more on a piece of personal property (anything but real estate) than it is worth, you may choose to pay back only what you owe on it. Again, there are some exceptions, but don't worry about those right now. \n\nAlso, in a Chapter 13, there is a \"presumptive interest rate.\" You see, when someone loans you money, they charge interest on that, which is extra money you pay them for the benefit of using their money. The court has established an interest rate that is a maximum during the bankruptcy. If you are being charged more than the presumptive rate, you can reduce the interest rate to that amount.\n\nIn a Chapter 13, all secured creditors must be paid in full (except for long-term debt such as mortgages and some car loans) as well as priority creditors (which you don't need to worry about right now). The unsecured creditors will receive some portion--or perhaps none--of the money you owe them based on  the plan.\n\nOne more thing, bankruptcy, even though it is federal law, varies to some extent from location to location. This is because judges in each court district have interpreted the laws. Some of these interpretations differ from others, so what you may be allowed to do in, say, California, you can't do in, say, Wyoming.","label":0,"model":"human","source":"reddit","id":2779}
{"text":"Because the value of the minimum wage hasn't actually risen in nearly 50 years, and there is nowhere in the country where a person can support themselves adequately on the wage. The argument that raising the minimum wage will cause companies to raise prices is somewhat true, but not nearly to the extent that the benefit to people is erased. When people, especially poor people, have more money, they tend to spend it on essentials very quickly, putting that money back into circulation and improving the economy. The effect on small businesses is also greatly exaggerated, and ignores the increased buying that will occur due to higher wages. Frankly, a company that cannot afford to pay its fulltime employees enough to support themselves shouldn't have those employees. We are still, as a society, paying for their living wage, we are just doing it through welfare and food stamp programs with large bureaucracies attached to them. Look at Walmart. On average, every Walmart store costs the government nearly a million dollars a year in payments to their full-time employees because they don't make enough while working to live. \n\nA society which expects its poorest people to work for an inadequate wage is a society that is built on debt. Not just monetary debt, but a human debt, paid in tears and misery, and it cannot continue in this way forever.","label":0,"model":"human","source":"reddit","id":2780}
{"text":"Well the lowercase letters developped out of the capital letters, also called Majuscules. In ancient times, all letters were written in capital letters. People began to write quickly and if you do that, you tend to make rounder shapes with the pen\/writing utensil. The lower case letters developped out of \"sloppy\" handwriting and people just accepted that. So for a long time it simply didnt matter if you wrote with a lower case or upper case letters. Keep in mind that not everyone could read or write and most writing of books happened in monasteries where monks had to copy books page for page, letter for letter. The really rich people would buy a book as a prestigious investment. it'd be comparable to you buying a $15.000 art piece nowadays. \nanyway: in some languages people decided to put focus on certain words, like names and nouns, since those seemed to be more important than the rest. ever since old middle-english that's only true for names, but in Anglo-Saxon(the language out of which old middle-english developed) they pretty much made the same distinction that Germans do nowadays. Nouns and names are capitalized.\n\nfun fact: the english \"I\" is still a relic of the past. There's no real \"need\" to capitalize it. My old English teacher always told us: \"the British people are so self-entitled that they have to write the I in big letters!\" \n\n\n\n\nEdit for more info:\nour modern alphabet developed from the greek letters. But one may wonder how those guys came up with writing an A to represent the sound \/a\/ . The best example here are the hieroglyphs. They're supposed to be read as any little glyph representing a single sound(consonant or vowel doesn't really matter).I'm taking an english example with egyptian writing style to explain better: Let's say they wanted to write the word \"Bridge\". They'd take the initial sound of a conventionalized wordlist. For instance they would draw a little bread bun to represent the sound \/br\/, since \/br\/ is also the initial sound of bridge. The convention is now set. whenever you want to express the sound \/br\/ all you have to do is draw a little bread bun and anyone would know that the first sound would be \/br\/. You do that with all the sounds that follow until you spelled out the entire word in little pictures. so instead of  < Bridge > , it would say  < bread, igloo, dog, jeans > . (i know it's not a great example, but i hope you understand)\n\ngreeks did something similar, but abstracted much more from the original. the old greek word for cattle began with an \/a\/ sound. the letter A turned upside down can be seen as something similar to the head of a cow or goat or whatever. you get the point. They did that for every sound that they'd have to represent and came up with the alphabet. over time it got more and more abstract and we ended up with the writing we have nowadays.","label":0,"model":"human","source":"reddit","id":2781}
{"text":"There are some reasonable answers here, but none of them capture the essential reason.  They all talk about the difference between AM and FM, but not why FM sounds better.  Here's an ELI5:\n\nCommercial FM radio sounds better because it gets more of the radio spectrum.  The reason it does is an accident of history.\n\nWhen a transmitter transmits radio waves, they go, well, everywhere.  It works by shaking electricity up and down in a rod (an antenna), and that sends out waves at a particular pitch.  If you want to think about it like sound, think of a tuning fork in a room, singing out at a particular pitch by moving air up and down.   Receivers contain tunable tuning forks that shake up and down in response to the waves that hit them.  \n\nRadio receivers then have to *decode* the shaking to create a signal.  AM radios measure how \"loud\" (strong) the incoming waves are, and use that strength to figure out where to place the speaker cone.  The transmitter makes its wave signal stronger and weaker very quickly, and that moves your speaker in and out very quickly, making sound.  FM radios measure the pitch of the incoming waves.  The transmitter adjusts its main tone slightly higher pitched, then lower again, and like that.  That makes the receiver move its speaker cone back and forth, and that makes sound. \n\nSo far so good, right?  Well, if there is more than one radio station around (hint: there are hundreds), they all work at slightly different pitches.  You adjust your receiver so it only resonates to one particular pitch, and then only one station gets received.  There's a whole branch of the government that sorts out whose radio transmitters are allowed to use which pitch (frequency).  Each station gets a range of radio spectrum it's allowed to use. \n\nSo here's the deal.  The lower the pitch, the less information there is in the signal!  Huh?  Well, if you have more than one radio station, you can't cram them too close together in pitch.   If the main pitches (the big kids call them  \"carrier frequencies\") of two different radio stations are too close to each other, then receivers can't tell the two stations apart, and the sound that comes out of each receiver will be a mix of the two stations.  So you have to separate the radio stations' carrier frequencies.  The amount you have to separate them in pitch is about four times the number of times per second your speaker cone has to move.  So if you want to transmit high pitched sound at, say, 10,000 vibrations per second, you have to keep your stations more than about 40,000 vibrations per second apart in pitch.  AM radios transmit between 600,000 and 1,600,000 wave fronts per second, which limits the number of stations you can fit in the radio band.  The difference between those two numbers is about a million vibrations per second, so you can fit about 25 stations into the band if everyone wants to broadcast moderately high fidelity sound.\n\nFM radios transmit at about 100,000,000 (100 million) wave fronts per second.  The band goes from 88 million to 108 million wave fronts per second.  The difference between those numbers is 20 million vibrations per second, so the FM band carries 20 times as much information in total as the AM band does. (thanks, \/u\/zed857!)\n\nInstead of adding more stations (you could fit nearly 400 AM radio stations into the FM band!) we cut up the band into larger chunks.  Since each station can transmit over a band 15 times wider than an AM station, it can send 15 times more information into your receiver.  The receivers use that in two ways:  (1) with FM coding, which reduces noise; and (2) to transmit stereo (which uses twice as much of the radio spectrum as mono) and higher frequency sound (which doublesthe spectrum usage again:  AM is actually limited at something like 7.5 kHz, and FM goes to 15 kHz).\n\nAM radio stations generally sound sort of washed-out and flat, because the highest pitch they can transmit is limited by the rules for radio transmission!  It's only about 7,500 cycles per second, which is not quite 4 octaves above middle C on the piano.  AM stations literally aren't allowed to transmit higher pitches than that. \n\nFM radio stations have clearer sound because they can transmit higher pitches (like the difference between a telephone and a CD), and also because they can use even more of the radio spectrum to reduce the amount of background noise in your receiver.\n\nAM came first because it's easier to make an electrical circuit that works a million times a second, than one that works at 100 million times a second.  Down in what we now call the AM band, there's just not a lot of room and technology like FM doesn't make much sense.  The whole AM band could only support about 5 FM radio stations operating at the same time.\n\nWhen radios that could operate at 100 million times a second became common, everyone settled on the FM standard instead of using the same AM technique that was useful earlier -- because there's plenty of \"room\" up there at 100 million cycles per second.\n\nIf an AM station had the same bandwidth as an FM station, it could transmit sound just as clear as FM(!)  But AM is more susceptible to some kinds of interference (like lightning or electric motors) -- so FM was the natural choice.","label":0,"model":"human","source":"reddit","id":2782}
{"text":"I can give you a very basic overview. \n\nFirst off, you have the Dungeon Master. He's the person who creates the world and makes all the big decisions. Everybody else creates a character. \n\nThe Dungeon Master begins weaving the story, explaining to the characters where they are and what their goal is- a good DM will do this with lots of enthusiasm and flair!\n\nThe characters then make their own choices based on their religion, their background, and their skills. For example- a Human who hates Orcs would never say, \"I love Orcs! Let's be friends with this Orc!\" Instead, they'd be more likely to say, \"Kill the Orc!! I hate that guy!\"\n\nApart from the storytelling and role playing, there is combat. Whenever you come across a bad guy on your journey, the DM has created a character for him and will control him. If you decide to attack the bad guy, you must roll the dice a few times. \n\nThe first time is to figure out who gets to attack first- this can be a big factor in who wins a battle. \n\nThe second dice roll is to determine whether you hit him or not- if you roll below a certain number (which the DM knows but you do not), then you miss. If you roll a 1, that is a critical failure and commonly ends with you breaking your weapon or hurting yourself. Conversely, if you roll a 20, that's a critical hit and you get to do extra damage! \n\nDamage is the third roll. That's how you figure out how much health he loses when you attack him. \n\nNow, battles aren't the only place you get to show off your talents and skills! Often, throughout the story, you will have to do a skill check. These are used to figure out if you can complete a certain task and how well you do it. \n\nFor example- you need to lie to a bartender so he doesn't know you're looking for the treasure! If you roll a high speech skill check, you might say- \"We're not searching for treasure- we're just in town to sell our wares and buy new armor.\" If you roll low, you might say- \"We... uh... treasure? I never met the guy! Ahaha... we're definitely not treasure hunting. Nope. Not us. I don't even know what treasure is!\" \n\nThere are other skill checks, too- such as climbing, riding horses, swimming, and lots more!\n\nReally, the best way to learn is get a few friends and try it yourself! I highly recommend it!\n\nEDIT:\nIf you want to learn more, get help from current players, or join a group- check out r\/dungeonsanddragons and r\/lfg! \n\nANOTHER EDIT:\nAlso r\/rpg! (Thank you, alienman911!) Also also r\/dnd! (Thanks, DevilChicken!)","label":0,"model":"human","source":"reddit","id":2783}
{"text":"Long time math tutor here. Problem is the way math is taught.\n\nMany people absolutely loved math until the moment they get ONE bad teacher that causes them to get behind on math. An entire year being behind on math makes it harder to understand the next year, which makes it even harder to catch up, let alone keep up. Thus, many kids are turned off from math from a SINGLE poor math teacher.\n\nThe other reason is also with the way math is taught. Math is creative problem solving. It's establishing rules and seeing how you can play with those rules to find neat little things. It's playing around with *ideas*. Imagine a \"literature\" class where all you ever study is grammar and spelling. You're taught how literature is supposed to be structured but you never actually *read* a book. That'd make pretty much anyone hate what they are taught as being literature without really having seen a single example of literature. This is what's happening with math. People hate math because they don't ever really *do* math at school. Following instructions isn't doing math. Until high level maths, people will only ever learn that you should memorize solutions and recognize when to apply those solutions in problems that show up. Paint-by-numbers, really. And yet, people are so surprised when I tell them that when I'm given a problem sometimes I have **never** seen a problem like it, so I try and fail **multiple** times to solve. Basically, they're used to solving it on the first try, not trying different approaches until you get a stroke of genius and solve via a very unconventional approach. That eureka moment you get is *so* satisfying but is lost on most people.\n\nI'm kinda upset at how math is taught if you haven't caught on yet :P","label":0,"model":"human","source":"reddit","id":2784}
{"text":"I'll give this my best shot - my daughter has a Port Wine Stain (PWS) birthmark that runs from her eyes up over the top of her head following what I believe are called the trigeminal nerves, and ending on her lower neck behind her ears. This is a result of Sturge-Weber Syndrome, a rare disease. Basically, there was a mutation in utero in a gene called GNAQ - causing a protein that, instead of switching off, remained switched on, leading to excessive growth of blood vessels under her skin and on the surface of her brain.\n\nI'm sure I screwed that up somehow.\n\nNot everyone who has a PWS birthmark gets Sturge-Weber, and even fewer have the birthmark bilaterally (both sides of her head\/brain. So she is a rare case than most.\n\nBecause of the extra blood vessels, areas of her brain that should receive a normal amount of blood flow are receiving more, and other areas not enough. Parts of her brain have already atrophied (died) and she will most likely experience more as she grows up. This leads to a high likelihood that she will experience seizures and stroke-like episodes, although she has been fortunate to have had no seizures we are aware of this far. We also take her for laser treatments on the birthmark - it was a deep purple color at birth and kind of looked like she was wearing a superhero mask. After two years of treatments you can hardly notice the difference between areas where the birthmark was and the surrounding skin. She just has cute pink eyebrows :-)\n\nHaving known nothing of birthmarks 2.5 years ago, I feel like I've come a pretty long way in a short time.","label":0,"model":"human","source":"reddit","id":2785}
{"text":"PREA is taken very seriously. If an inmate comes up to you claiming that he was raped the first thing you do is protect the inmate and take him away from the scene. While officers are escorting the inmate to safety the crime scene is preserved\/quarantined. An outside agency that has no obligations to the facility comes in and gathers any evidence. The facilities medical staff are not the people who perform all the nessesary physical exams (again, outside source to preserve the rights of the inmate). DNA is gathered. Clothes become evidence. PREA is taken very seriously or at least in the state of Florida. \n\n\n\nThat inmate will not be housed with any inmates considered a predator  (or even a possible predator). That inmate will be transferred to another facility where they can return back to open population.\n\n\n\nEvery year during annual in service it is reiterated and a small test must be passed to prove you understand how PREA is completed. Also, as a field training officer myself I have always made every officer I green light prove to me that they understand the proper guidelines and procedures. \n\n\nSource - Sgt. In DOC and a field training officer","label":0,"model":"human","source":"reddit","id":2786}
{"text":"The hepatitis viruses (including D and E also) were identified in a time when biologists and medical professionals knew very little about viruses, or where able to classify them appropriately. The word hepatitis means \"inflammation of the liver,\" so naturally this is what all the hepatitis viruses have in common. They all cause some form of inflammation in the liver which leads to symptoms that are similar. \n\nHepatitis A is in the picornavirus family of viruses, same family as polio. It's an illness that people get by drinking or eating contaminated foods (this is refered to as fecal-oral transmission) often in third world countries where sanitation is poor. It looks and feels way worse than it is, lasting only one month and almost never killing anyone who gets it. \n\nHep B is a member of the hepadnovirus family. It's transmitted through sex and needle sharing, making it a blood borne pathogen. It looks much like hep A at the onset of illness, resolves, but unlike hep A which has no carrier state, hep B has a carrier state which often times develops into chronic hepatitis (chronic liver inflammation) over the course of many years, and often presents with kidney failure and even liver cancer. whether or not this occurs depends on the persons immune system. Unlike hep A, when we find hep B infections we treat them aggressively with antiviral medications. \n\nHep C belongs to the flavivirus family, same as West Nile virus. It is transmitted only through blood, which makes it most common in people who share needles. Unlike hep B, hep c infections become chronic most of the time, leading to liver cirrhosis (hardening) and also liver cancer, same as hep B, but much more commonly. Infact, hep c is the most common cause of hepatocellular carninoma. We treat hep c as we do hep b, but cannot vaccinate against it (as with hep b) because the virus itself is too variable in the way it coats its outer shell. \n\nIn summary, the hepatitis viruses are not a family of viruses. They are a group of viruses from different famililies that all happen to affect the liver in some way.","label":0,"model":"human","source":"reddit","id":2787}
{"text":"I'm sorry, I'm on mobile at the moment, so can't really do this too thoroughly, but here goes.\n\nThe airplane is banked by ailerons, and yaws with the rudder. In any \"standard\" (not just standard rate or 3 degrees per minute) turn you'll experience in a commercial flight, the bank and yaw are input to perform a coordinated turn. The long and short of this is objects on the plane will feel as if they are being pulled \"down\" through the vertical axis of the plane rather than down to the ground.\n\nSo imagine you're in a car. If you have a cup of water and initiate a turn, the water would spill to the outside of the turn. If this happened in a plane (too much yaw, not enough bank) this would be called a skidding turn. If you were to apply only bank with no yaw, the airplane won't really turn (it won't turn very well, but it will turn) this would be the same as just tipping your cup. This is called a slipping turn. \n\nBack to the coordinated turn. Imagine the first scenario where you're driving a car. You initiate a turn to the left. The water wants to spill out to the right. If however the road at this moment banked to the left such that the \"tipped\" angle now counteracted the centrifugal force of water spilling right, no water would spill out. \n\nThat's how it's done. You can look up coordinated turns in airplanes. There's tons of info on this and it's one of the first things you'll learn in flight school. Also, one of your main instruments is the turn coordinator that shows if you're skidding, slipping, or coordinated. \n\nSorry I can't spend too much more time on this. As a former CFI, I'd usually do this with drawings and charts and pictures, and then we'd do a demonstration in the plane.","label":0,"model":"human","source":"reddit","id":2788}
{"text":"As an avid RC airplane\/helicopter\/\"drone\" hobbiest my opinion has two aspects:\n\n1) FUD (Fear Uncertainty Doubt).   This is the biggest part of the equation.  People are overly fearful of various possible things that \"drones\" can do.  Mostly related to taking pictures of you.  Americans believe they  have a \"right to privacy\" no matter where they are and what they are doing.   This is simply NOT the case.  First of all, the right to privacy is only concerning the government snooping on you.  Private people can snoop on you all they want as long as they don't harass or trespass.   Americans seem to believe you even have the right to privacy (from everyone) when your in a public space (such as a beach or park).   So the american overly exaggerated sense of privacy is fanning a lot of hate-flames towards drones. \n\n2) Idiots with money and a lower barrier of entry.   5 years ago the best to learn how to fly RC was to find an instructor and have them slowly and methodically teach you how to control the aircraft because they were relatively expensive and if you crashed you had weeks of work to re-build them.   With modern technologies it is now possible to build a flight brain that basically flys the aircraft\/helicopter\/drone for you.  So no instructor, no club, and readily available parts to fix or replace.    \n  What this has resulted in is the idiot wall being much much lower.   People who have zero training, zero knowledge of the laws regarding flying and zero respect for other people are buying ready-to-fly drones and causing all kinds of news because they are idiots.   Flying inside the airspace of an airport is already illegal and every RC flyer who follows the rules knows this,   Flying low over private property is already trespassing so it's already illegal.  Again those who follow the rules know this.   It's the idiots with no respect for others that are going to ruin the hobby for the vast majority.","label":0,"model":"human","source":"reddit","id":2789}
{"text":"Palpatine, like many other powerful Sith Lords, not only was able to reduce his connection to the Force's \"footprint\" but he, uniquely I believe, was able to dampen the connection to the Force the Jedi had near him. In the prequels, he uses that ability to weaken the Jedi's connection to the Force while on Coruscant which raises all sorts of internal havoc such as the Jedi not being able to divine the troubles brewing in the galaxy such as other force users being killed. He finally allows the Jedi, specifically Mace Windu, to sense some of his dark side connection as the final piece in his complex trap. The Jedi, like pawns, move against the chancellor, underestimating his power as the most dominate dark side user of the galaxy. He handily defeats the Jedi Masters sent to kill him, and use the theatrics of the attack to fully turn Anakin to the dark side. He later uses the revelation of his physical dark side corruption to garner sympathy in the senate and have him elected as Emperor.\n\nThe dark side is all about deception and manipulation.","label":0,"model":"human","source":"reddit","id":2790}
{"text":"Well there is the concept of \"Cultural Bias\" in tests. See what happens is the questions make some basic assumptions about who you are and what you expect. \n\nHere is a great example, in Philadelphia they were giving out a standardized test to children across the area. One of the questions went something like \"what is an animal that you dont see in your neighborhood? A)Dog B) Cat C) Giraffe\". Well a bunch of kids got the question wrong, because they lived in the same neighborhood as the zoo (here in Philly the zoo is in the middle of the city, surrounded by houses and businesses and such). The question assumed that all children would not have an African herbivore in their urban neighborhood, but there was a certain group of children who did. And they would have been marked wrong because they were technically giving a correct answer, but an unexpected one. \n\nNow thats a minor case, but sometimes questions can be more White, suburban, questions which might not make sense to an African American or an urbanite.","label":0,"model":"human","source":"reddit","id":2791}
{"text":"I'm a psychology BA and a current social work MA student who plans on being a therapist. This isn't my area of expertise, however, I'll do my best to explain what I know. Gender dysphoria (formerly Gender identity disorder) is a mental illness in which one feels that the sex they are born into does not match the gender roles that they fulfill,this can lead to feelings of distress and unhappiness in the person experiencing the incongruence. That's kind of the heart of the issue. This person is born male or female, but does not self identify with the gender roles that our society assigns to those sexes.\n\nNow, why is it a mental illness? We classify certain patterns of thinking, feeling, and behaving as \"pathological\" only when they meet what many researchers refer to as \"the four D's of abnormality\". \n\n1. Is the behavior causing distress? - For people with gender dysphoria, the answer is not always yes. They sometimes feel uncomfortable, but there are varying degrees of this discomfort. I don't know the average reported distress level in this population, but I imagine that feeling that your body betrays your identity must be a difficult experience.\n\n2. Is the behavior deviant? - This isn't meant to sound judgmental, although the word \"deviant\" is laden with some unfortunate implications. What we mean when we ask this is \"is this what we expect MOST people, on average, to be experiencing, given their unique life circumstances\". For gender dysphoria, the answer is no. Experiencing this dysphoria is not something most people report. Gender dysphoria should also be differentiated here from men or women wanting to blur gender roles and do things that the opposite sex would do. Men do feminine things and women do masculine things, this is not indicative that someone has gender dysphoria. Gender dysphoria is really about a complete transition and bodily transformation from one sex to the other, and then adopting all those gender roles as a full-time member of that sex. People take hormone replacement drugs, get surgeries, get voice training, adopt a whole new name and wardrobe, etc. The transformation changes everything as much as possible, sometimes permanently.\n\n3. Is the behavior causing dysfunction? - Does this person wake up in the morning, take a shower, go to work, eat normal meals, have satisfying social relationships, pay all their bills, and generally tend to the demands of daily life? For people with gender dysphoria, the answer is probably mixed. If the dysphoria is intense and it creates feelings of depression, worthlessness, anxiety, and suicidality, then perhaps they aren't functional. They might be using drugs as a means to cope with the frustration, or they might seek pleasure in other unhealthy ways as a means to self-soothe and find some measure of satisfaction. People with gender dysphoria who transition and live full-time as the opposite sex however are certainly able to be happy, normal, and tend to their lives just as anyone else.\n\n4. Is the behavior dangerous? - This one is very straightforward. Behavior is abnormal when it puts ones self or others in harms way. This criteria is not often used, although it something to consider. People with severe eating disorders, psychotic disorders, and sometimes even major depressive disorder can pose a risk to themselves and others. For people with gender dysphoria, the answer is likely that they aren't dangerous to themselves, however, the distress caused by the dysphoria could enable other more dangerous behaviors (drug taking, aggression, suicidality, etc).\n\nSo considering the 4 D's, I think that its fair to say that gender dysphoria is a mental illness ONLY WHEN the experience of the dysphoria causes distress, dysfunction, and is dangerous to the person. It is a given that most people experience their sex as their gender, so it is obvious that this is deviant from the average human experience. That does not mean that it is less good or less psychologically healthy than congruent experiences of sex and gender. People can live very happy lives post-transition, but the disorder described is really about focusing on people who are suffering because of the dysphoric sensations.\n\nIn the United States (where I will be practicing) we refer to the DSM-IV-TR as the standard manual for diagnosing and defining the parameters of what constitutes a mental illness. The DSM-V is coming out sometime next year (supposedly), and the website that the American Psychiatric Association has created to watch the changes being made between the two editions is worth checking out. Here is the page that references the new criteria for gender dysphoria: _URL_0_\n\nI hope this helps!\n\n[Edited for terminology and sensitivity.]","label":0,"model":"human","source":"reddit","id":2792}
{"text":"Most answers here are generally correct, but to be more precise, this an old convention in the world of TV.  In the old days, when there was no cable, no recording, only 3 antenna channels, and very few content creators, the networks  had to think about how to fill all their timeslots and get viewers.\n\nSo there are 52 weeks a year.  Do they want to show something new always?  New is better, but they know people are going to miss episodes from time to time, and the episodes don't really have a strict order, so why not give people more chances to tune in? So they compromise.  26 episodes and the contract with the content creators would allow the network to air each episode twice in the year (afterwards it would be open to syndication).  So, to have a new season every year, they traditionally made 26 episodes a season.\n\nThis became known as a full order.  The first run would begin in the fall, the second run in the spring\/summer.\n\nThen, networks learned how hard it was to know if a show would be a hit.  So they managed risk by only ordering half-orders (13 episodes) with an option to extend the run.\n\nAs shows got more expensive, complex and serialized, networks learned that in some cases half orders were more reasonable for some hits going forward.  Making 26 hours of GoT would be a Herculean feat.\n\nIn the last few years we've also lost the practice of the second run as networks do new summer programming, especially cheap reality shows.  Survivor was the big step in popularizing this.  So now, without more abundant programming and no second run, networks don't show a full set of reruns and just skip low viewership weeks (holidays, big sporting events).  This has lead to full orders being 20-24 episodes and halfs being 10-12.\n\nNetflix is really just following tradition\/convention with their season sizing.","label":0,"model":"human","source":"reddit","id":2793}
{"text":"I know more about this than a lay-person should, but that's perhaps because I'm considering starting a business in this area. On this, you can AAMA - I know quite a bit.\n\nI also know this is _way_ more info than you actually care about, and nobody is going to care about this, but I enjoy talking about it, so, here goes:\n\nThe system you're talking about in tennis is called [Hawk-Eye](_URL_0_) and the first time I saw it in use was during a cricket match being shown in the UK where the broadcaster had brought it in to understand very subtle decisions almost impossible to judge by eye (cricket has a rule called \"LBW\" which in particular, is hard to get right sometimes by sight), and it quickly got engrained into the top tier of the game.\n\nThere are other systems as well. Ever wondered how \"they know\" your favourite soccer player has run 18.2km this game? That's almost certainly from the [ChryonHego](_URL_1_) Tracab system.\n\nWhen it comes to pass completion, shots on goal, etc. that data is probably coming from another system called Opta. That relies on a mixture of camera tracking and human event classification (they have people at games saying \"that frame was a pass, that frame was an attempt at goal, etc.\"\n\nIn Rugby the players are often wearing positional tracking devices that also track heart rate and letting coaches know how players are performing in real-time. I believe NFL does this too.  Basketball has a mixture of systems, and in cricket, they have even more toys (I'll come back to that). \n\nYou are probably also aware that in motorsports, the objective has become \"a driver moving as large a collection of sensors as possible around a track as quickly as possible\", because the more sensors you have, the better you are at managing the data, the more likely you are to win. Hilariously, just this weekend, one F1 team has shown that their [positional analysis is easy to fool and can lead to stupid decisions](_URL_2_).\n\nThe positional tracking systems all have similar concepts: film something, then analyse each frame. In the case of Tracab and Opta, they're happy with 25 frames per second, the same as most broadcast TV systems. I would argue it's a little too low for accuracy.\n\nHawk-Eye is a much higher frame rate (500-1000 frames per second) that should aid with the accuracy. Each frame is broken down in terms of object recognition: here are the lines, here's the ball. Now look at the relationship between them and produce a data frame. The data frame will have x, y, z co-ordinates of a ball in relation to a line or some other aspect you care about, and can be fed into a piece of software that can call \"foul\" or \"goal\" or whatever you need.\n\nNow, why has it not all replaced officials? It's obviously already used to augment officials. In some tennis competitions you can actually hear Hawk-Eye make a noise on fouls that is effectively taking the place of a linesman. In EPL games, goal line technology using something like Hawk-Eye (not ChyronHego, I believe), is used to let a referee know the ball just crossed the line in case it's not obvious to them.\n\nFor some scenarios then, it is being used to replace linesmen. There are three reasons it hasn't yet succeeded in removing them completely, I think.\n\nFirstly: cost. It's not cheap. There are a lot of problems they have to take into account in getting this far. Wind, light, shadows, floodlights causing multiple shadows, they all need to be dealt with. That has led to R & D costs being quite significant.\n\nThey are proprietary systems that cost a fortune to develop, and it's a bit of a closed-shop monopoly. Setup requires a fair bit of work and there is normally a team of people running it behind the scenes at each game where this technology is deployed.\n\nThat cost is not a problem with the top tier, but the top tier of every sport normally knows its future lies in lower rungs and \"grass roots\" forms of the game. It's important that even the second division (where these systems can't be used), look and feel like the top tier. And ideally, it should be possible to play the game on a Sunday afternoon down your local park without it seeming to be futile.\n\nHuman judges aid with that. They signal \"you can do this\". The sports in which that's not possible (F1), still have alternatives, and there are some measurement systems (timing, speed, etc.) that are accessible to teens with a desire to soup up their Corolla.\n\nSecondly: not all in-game events can be tracked using these systems. Hawk-Eye can track a ball near a line being foul or not, but it can't easily tell whether a player's foot is over the line it should not be when serving, for example. \n\nEven in cricket where Hawk-Eye first made its reputation, detection of foul balls (when a bowler steps over a line) has to still be done by umpires and video review. Whether the ball hit a bat or not is done by audio detection (snick-o-meter), and heat-sensitive camera (as the ball brushes the bat, the friction warms it up enough to show as white hot on an appropriate camera). The technology isn't quite there yet to make it possible for these events to be done through camera analysis alone.\n\nThirdly: accuracy. It's pretty damned good in some scenarios but less so in others. I competed in the Manchester City hackday in July 2016 (was in the winning team, too) where we got access to Tracab and Opta raw data from some historical games. In some of those frames we were seeing the football travelling - according to their systems - at 1,500m\/s - which is about Mach 5. It clearly didn't actually do that (I mean, I think Yaya Toure is _great_, but he's not _that_ good), so where did the error creep in? \n\nThat casts doubt, and to my mind, just enough to not allow it to be determining the outcomes of games on which millions of pounds and entries in history books are determined.\n\nHawk-Eye's higher frame rate is likely to make it much more accurate, so you can see it making more inroads into official calls. It makes most sense in contexts of positional review (cricket, tennis, goals in soccer), and ideally in phased-play games where there is a lot of stop-start and reviews can be done without disrupting play.\n\nTennis lends itself to all this perfectly, so the only honest answer I can give you as to why it's not everywhere is quite simply cost and the fact some in-game events can't be done through hawkeye so they need the official anyway. They'll defer to humans allow Hawk-Eye on review where it has the time to churn the data rather than in real-time.","label":0,"model":"human","source":"reddit","id":2794}
{"text":"Oh man, I just read something about this not too long ago. I will see what I can recall...\n\nBasically, Americans in 1776 did have British accents in that American accents and British accents hadn\u2019t yet diverged. That\u2019s not too surprising.\n\nWhat\u2019s surprising, though, is that those accents were much closer to today\u2019s American accents than to today\u2019s British accents. While both have changed over time, it\u2019s actually British accents that have changed much more drastically since then. Between the American revolution and now, the lower class began modeling their speech more closely to that of the posh upper class aristocracy. The main difference being a thing called \"rhotacism\". Rhotic speakers pronounce the \"r\" in words like \"water\" and \"butter\" hard. So, \"wat-urr\", and \"but-turr\". Non-rhotic speakers would pronounce the r \"soft\", as in \"wat-ah\" and \"but-tah\". \n\nAnd as it were, non-rhotic speakers are still very common in America, especially in cities on the east coast (think of a New York or Boston accent).\n\nTL;DR American and British accents were quite similar at one point, but the British accent has changed more over time to reflect the speech of aristocracy.","label":0,"model":"human","source":"reddit","id":2795}
{"text":"Ag student here. This is a subject that obviously gets discussed quite a bit amongst my peers. \n\nFirst of all, I see way too much confusion over the difference between:\n1. Plant\/animal species that we've been artificially selecting for (intentionally or not) over thousands of years (NOT GMOs), and \n2. Transgenic crops that are usually created by using a 'gene gun' to insert DNA sequences into plant embryos over thousands of trials, usually with lackluster results.\n\nWith respect to transgenic crops, I agree that they are not known to be inherently dangerous, and can be a more economically attractive (and operationally simple) option for farmers. They also definitely promote monocultures that can lead to decreases in local biodiversity, which can tend toward putting short-term yield increases over long-term ecosystem\/soil biota health. \n\nMy personal hesitancy toward completely accepting GMOs is for a different reason that I don't see mentioned often. IF, decades later, we discover an issue with any of these genes that has a truly harmful effect, as in the case of DDT in the early 70s (and it's happened with several other previously-issued classes of chemical controls), we can't just stop using them and expect their presence to diminish over a certain half-life like with pesticides. The gene will persist in any population that isn't prevented from reproducing, and if it spreads it'll be difficult to control or even identify.\n\nI'm expecting a lot of negative feedback to this comment, hence the throwaway, but I can assure you it's not necessary. I'm not blind to the potential positives of GM crops; they can lower herbicide\/pesticide use, slowing the ability of weeds\/pests to evolve resistances. Bt crops have been, as far as I can tell, a very positive development. I think the real crux of the discussion lies in the proper ongoing education of farmers without the whole thing being politicized by the rampantly pro- or anti-GM groups.","label":0,"model":"human","source":"reddit","id":2796}
{"text":"Former game dev here,\n\nIt's not impossible, there's just no profit in it for the amount of work involved. The studio would be selling to a tiny niche who would want such a thing, and they can't possibly charge enough to turn a profit at a price the market will tolerate.\n\nOn the technical side, game code for a console is incredibly specific to that console. What you suggest would require a near complete rewrite from scratch of these games, and recreation of all art assets - which would require a full scale dev team to port to a modern platform. That, and not only would the code have to be ported, but redesigned to work with modern graphics on this other, modern platform. And then you would of course require full scale play testing to find and fix bugs and ensure the game experience is faithful to the original.\n\nIt costs ~$10m to make a low end AAA title, and these ports will cost at least as much, likely ~$20m per title. And if you're going to improve the video quality, it's inadequate to go with an incremental improvement, you might as well go all the way, so the effort required would be demanding, escalating the price. Anything less than what is expected for an exalted title on a modern platform would be a complete market flop. And again, this is for a niche market. Buyers don't buy the same experience again, unless you're Nintendo.","label":0,"model":"human","source":"reddit","id":2797}
{"text":"Flash being slow is usually the result of a poorly programmed animation, menu, etc.\n\nAdobe has to have its team of programmers support many browsers, patch security holes, and solve bugs, leaving them spending a lot of time on something that does not make them money directly. Hence their decision to drop mobile support. (the subtext is that they might not care about quality that much since it's already on the out, or they don't have the resources invested in it)\n\nHTML5 is considered to be the upcoming alternative to Flash, and since it's an open standard, it's up to Chrome, IE, Firefox to implement it (and the quality of the implementation is now up to them instead).\n\nBack when flash was introduced, it represented a significant advancement in web technology since one plugin would act the same in all browsers (with many competing standards, each acting differently in every browser). In the present, it's kinda out dated, especially compared to its alternatives.\n\nEdit: spelling  \nEdit Note:\nTo clarify, HTML5 is not a replacement for flash currently, it represents a possible alternative that might be viable in the (near) future. Flash works for its __supported__ systems. Thanks to the many who mentioned it below.","label":0,"model":"human","source":"reddit","id":2798}
{"text":"Not sure if this counts, but my ex's father was the CEO of a hospital.\n\nSee it like this:\n\nThe buck doesn't stop anywhere else.\n\nHis day started at 5 am (so he could make the commute and get to work on time, which was at 8 am). He wouldn't leave work till around 7pm most days if he was lucky. Most of the day consisted of responding to e-mails, making phone calls, meetings, composing presentations (he never delegated this to any of his administrative staff). Like a lot of management positions it consisted of his daily schedule then taking care of major problems as they arose. \n\nHe was really great at letting me shadow him whenever I went out to visit him. I wanted to follow his career path (he started out as an RN, got his MBA and worked his way up to CEO). \n\nAn example when I hung out with him for a day:\n\nThe hospital he was CEO over was part of a system of hospitals in the area. The morning started out with him meeting with the COO of the system along with the other CEO's of the separate hospitals at a hospital across the major urban area these hospitals were located in. An hour meeting that wasn't BS in the least. They spoke of new state policies, construction to hospitals, and the organizational issues. All in all this meeting was an hour and a half when it was supposed to be an hour. They discussed strategies on how to soften the blow of expenses incurred by these new policies in depth.\n\nHe carried two cellphones and a pager. His pager went off during the meeting. It was the Chief Nursing Officer. The hospital was all out of beds and the ER was full. Ex's dad said he would call the Medical Director of the hospital and see if the medical staff could push some discharges. 10-15 minute call to Medical director. Medical director mentions some issues he would like to discuss. Make appointment in blackberry to meet with medical director.\n\nDrive from meeting place back to hospital. 5 phonecalls he has to return. Meeting with medical director in 30 minutes. First phonecall takes 45. Discussing bids with a contracting company for renovations where the hospital was expanding. Leave message with admin staff to return other 4 phone calls unless emergent. Go to emergency meeting with medical director. Medical director has chiefs of different medical departments present. Ask if attendings and residents can try to discharge patients. Medical chiefs and directors urge ex's dad to go on divert (hospital cannot accept any ambulances to the ER, they go to other hospitals in the area, only happens if there are multiple hospitals within a certain mile radius).\n\nMeeting with house staff and CNO to discuss divert. CEO and COO of system are teleconferenced in. Discuss financial ramifications of divert. Medical staff pushes for it. Administrative staff, especially the CEO of the system say no. ER patients can go to hallways, medical staff can find discharges.\n\nLunch. He orders an amazing meal to be delivered from this traditional mexican food restaurant and pays for it (he never admits to what he makes, but he works as a hospital CEO in the second biggest city in the US, I know its at least 7 figures). CNO and medical director attend lunch to discuss how to solve the overflowing bed situation.  Ex's dad answers e-mails on his laptop\/blackberry. Returns those 4 other phone calls after. Two of them require teleconferencing. The other two he puts on hold to make other phone calls. \n\nAt this point he's 4 hours behind his daily agenda. And its 1 pm.\n\nTry to make headway on catching up with meetings. Meetings with CNO and floor managers regarding patient satisfaction scores gets delayed. Makes it in time for teleconference phone call with CEO and COO with contractor about renovations and expansions. Lasts an hour. 2 pm. 3 hours behind. \n\nCNO pages while we're on the way to meet with the risk management department. Needs to speak with him about a \"situation\". Meet with risk management and CNO about a patient case that may be brought to the civil court that involves unsafe practice by an employee of the hospital. Discuss that case and the original reason for meeting.\n\n4 pm. Supposed to be the end of his day. Now 3 hours behind. Impromptu teleconference with legal team and CEO discussing potential case. An hour goes by. 5 pm. The rest of the executive team has gone home as well as his secretary.\n\nRespond to any phone calls he received that weren't pertinent to respond to by e-mail. Work on responding to other e-mails. \n\n7 pm. We go through the drive through of Del Taco to eat on our way back to his two bedroom apartment with a patio that faces the coast. He has a ton of take-out\/doggy bag carriers in his refridgerator and no actual food. The freezer looks like its stock from the day he bought the refridgerator.\n\nHe goes to sleep which involves passing out on his couch while watching half a TiVo'd episode of his favorite sci-fi show and is back at work the next day at the same time and doesn't get home till 10 pm.\n\nOn the weekends he returns pages and phone calls as well as e-mails. We go to a vacation spot that is a 3 hour drive away from the urban area he works in and he's answering pages and phone calls the whole time.\n\nTL;DR: A ton of meetings. But a ton of \"putting out fires\" with meetings. 16 hour + work days. 80+ hour work weeks. Working from home a lot.\n\nIf you read my posting history you'll see that I'm a pretty liberal guy when it comes to politics. But from the experience I have with being up close and personal with the CEO of an organization, in my opinion he earned every last dime of those 7 figures, and I'm pretty sure a lot of other CEOs do too.","label":0,"model":"human","source":"reddit","id":2799}
{"text":"In the very beginning fish had one hole (called a [cloaca](_URL_0_)) that they did everything from (number 1, number 2 and spawning). \n\nThere may not have been any reason for this, evolution relies on chance to add new features and then natural selection to optimise them, so if by chance the various parts were added to the existing hole instead of a new hole then this would stick.\n\nNow while mammals were evolving the part that does both pee and sex happened to start to grow out. As it is an advantage to place sperm further in the female this grew into the penis. Again it just happened by chance that the pee and sex tube grew out together rather than separately, and because it's not really a disadvantage it was never changed.\n\nIt's like how animals use their mouth for both breathing and eating where it would be a better \"design\" to keep them separate to help avoid chocking, it's because the fish that had air filled swim bladders (which evolved into lungs) empty into their mouth and not a different part of their body.","label":0,"model":"human","source":"reddit","id":2800}
{"text":"Okay, I work for one of the three largest airlines in the world, have for a while in several capacities around the company and this is the best I can do here.\nThere are (4) parts to this equation that all have separate solutions that work in concert to recover from Irregular Operations (IROPs in airline speak), these are:\n\n(1)\tAircraft\n(2)\tCrews (Pilots and Flight Attendants)\n(3)\tPassengers\n(4)\tFacilities (kind of)\n\n(1)\t Aircraft\nAircraft in the modern day system do not always do what we call \u201clinear\u201d flying (flying Dallas-Austin-Dallas) etc all day from the same Hub.  Often they might go, Chicago-Austin-NYC-LAX-Chicago or some other combination of airports.  Instead of cancelling all of these flights, people who are called \u201cAircraft Routers\u201d will attempt to isolate the impacted airplanes to flights solely at the impacted Hub by re-routing aircraft throughout the system in a manner to pick up these other flights, and dropping other flights into the impacted area, thus mitigating the cascading effect of cancellations throughout the system.\nThis allows the airline to have airplanes ready to go either in the Hub airport, or in the outstation (the destination city) ready to resume operations as soon as the issue is resolved (weather, technology outage, etc).\n\n(2)\tCrews\nObviously, Pilots and Flight Attendants are people too.  They are also nearly all union employees.  Each union has a contract that specifically governs how folks are scheduled to work during IROPS.  All airlines keep \u201creserve\u201d crews at their hub airports in case of delays\/cancellations to other flights, so that if a particular crew were supposed to change aircraft and were late, it would not impact the onward flight.  However, during IROPs there are too many flights for this to be a meaningful number.  Often times in the case of extended delays, flights will be pushed to cancel because the crew \u201ctimes out\u201d aka, they are no longer legally allowed to work the flight because after a certain point, they no longer have enough time in their legal \u201cduty period\u201d to complete the flight.\nIn winter operations the impact of these legalities is mitigated by cancelling flights early, and then rerouting the crews in the same manner as the aircraft.  By cancelling the flights, instead of the crews burning their \u201cduty time\u201d at the airport waiting on a delay, they can report at the later, rescheduled time to operate their flights with little\/no worries.\n\n(3)\tPassengers\nThere are several methodologies that airlines use, but here is ours.  We have what we call an \u201cauto-rebook\u201d tool.  It takes passengers from the cancelled flights, looks at their Origin and Destination, and then puts them onto the next available flight\/flights between those points.  For example, if you had a ticket for a connection from NYC-Chicago-LAX, but the NYC-Chicago flight was cancelled, you may well be automatically put on the next direct flight NYC-LAX.  However, the reverse could also be true and the program could put you on a connection instead of waiting for the next available direct flight.  The order you rebook passengers varies by airline but generally it would be to rebook higher status passengers first, then the rest of the passengers either by A) the amount paid for the ticket, or B) when you checked in for your flight.\n\n(4)\tFacilities\nI just needed to throw this in here briefly, for passengers and crews stranded overnight, airlines generally hold blocks of hotel rooms at many nearby hotels that they pay for in advance to have available in times like this.  However, and this is important unless your flight cancellation is the fault of the airline and not due to weather\/government etc, then they are not obligated to put you up in a hotel, but often they can get you a discounted rate.  Obviously, crew members are always supplied with a hotel on the company.\nIn addition, while during IROPS runway constraints do play a factor, it\u2019s not usually a significant factor in the recovery of the operation because it limits both arrivals and departures, so it forces ongoing forward cancellations if you cannot get the airplanes into the airport.\n\nFinally, I\u2019d just like to say that it is really only in the last 5 years or so that airlines have really gotten all the technological tools in order to properly manage this whole process and it should only get better for all involved going forward.\n\nTL:DR = Airline employees work small miracles getting people to where they need to go.\n\nEdit: Thanks for the gold my anonymous friend!","label":0,"model":"human","source":"reddit","id":2801}
{"text":"Think of the fat as the food you store in your home bomb shelter. Your bomb shelter is there to keep you safe when you are in danger and the food you store in that place is only to be used up when you cannot get other food.\n\nNow you could always snack on the food in the bomb shelter whenever you like, but more than anything you want to keep a hold of it in case an emergency comes along. \n\nWhen your body sends you hunger signals it is telling you your pantry, in this analogy this would be the food you ate recently, is completely empty. Your body now wants you to go to the groceries and pick up more food. If it turns out nuclear war has broken out and you can't get to the grocery store, then it's okay to eat the food you have stored in your bomb shelter. However the body does not think this is the best of idea, since the best option would be to keep these emergency supplies and get more food too. So your body keeps saying \"Hey! Go get the groceries! I don't want to have to keep using up what's supposed to be there for an emergency!!!\"\n\nAs for *How* the signal works:\n\nOne of the major signals that tells you to eat is Leptin. Leptin is a hormone - a small thing that is released into the blood which tells other body parts what's going on and what to do. Leptin is released by fat when you have eaten food to tell the body that you are full, this tells the body that you don't need to eat any more. When you don't eat, the fat doesn't release this 'hormone' and so you feel hungry.\n\nThe other major signalling hormone is Ghrelin. Ghrelin is released by the pancreas - a part of the body that is important in telling you how much sugar is in your blood. When you have not eaten there are changes in the blood which then cause the pancreas to release ghrelin, which makes you hungry by affecting your brain.\n\n**Edit:** Note that leptin's action is more concerned with the long term when compared against cholecystokinin. Cholecystokinin (CCK) is released by the duodenum (the part of the digestive system that comes after the stomach) in response to the sensation of fat\/protein contacting it. I think the mechanisms behind how it inhibits appetite are not certain.","label":0,"model":"human","source":"reddit","id":2802}
{"text":"**Socialism**  \nSocialism is a big word that actually covers a VERY LARGE variety of political ideologies. Socialism can be ran by the state or anarchic, it can be national or a small community, it can be communist or have markets in it.  \nThe IMPORTANT part, which frankly no \"socialist\" country has actually achieved, is that the Means of Production are owned not by any one individual, by by the communities themselves. Some forms of socialism are merely means to implement communism too, which is a very specific type of socialism.  \nSo yeah, socialism is a huge over-arching term that covers a lot.\n  \n  \n**Democratic Socialism**  \nSo one of the first fracturing points in the socialist ideologies is HOW a society is going to implement socialism. You have some camps (Leninists) who advocate violently wrenching control of the state from the capitalist overlords and using it to implement socialism, and eventually communism.  \n  \nIt is now that I would like to point out most socialists, and ALL communists, think this is stupid as hell. You will scarcely see any of us advocating for a recreation of the USSR.\n  \nNow, Democratic Socialism is simply socialism that intends to implement itself by playing the governments rules. In the U.S.A. this would mean electing DemSoc politicians who will attempt to lay the groundwork for a socialist society. Democratic Socialism also likes to \"Band-Aid\" the current capitalist system by helping the disenfranchised and marginalized through welfare.\n  \nHowever, this is still a socialism that is ran by the state, and you have whole armies of socialists who think this is absolutely silly and will just lead to more Authoritative State Socialist bullshit.\n  \nAnd, for the record,  \n**SOCIALISM =\/= GOVERNMENT PROGRAMS**  \nThat so completely misses the point that it hurts...","label":0,"model":"human","source":"reddit","id":2803}
{"text":"Shortest answer:  Economics\n\nMedium answer:  Algorithms have been created that allow digital music (like that found on CDs) to be compressed up to five times or more in size, but with the penalty of a varying loss in sound quality - but that loss in quality is not sufficient to bother *most* music consumers.  Apple wants you to be able to store as many songs as possible on your device so they can sell you more songs, so they compress the songs.  Smaller songs also take less time to download and don't take up as much storage on iTunes's systems.\n\nLonger Answer:  Medium answer + Consumer demand for the highest quality music is so small that iTunes sees no financial gain in servicing this demand.  Think of iTunes like the McDonald's for music.  Cheap and tasty enough to keep you coming back.  Most people think the food is just fine, a few only go there when there's no other options, some will never lower themselves to eat McDonald's food.  McDonald's knows they will never capture the \"food purists,\" and frankly don't care because they're such a small percentage of all diners.  iTunes feels the same way about music.","label":0,"model":"human","source":"reddit","id":2804}
{"text":"To everyone on this thread debating the safety of LSD:\nEveryone's mind reacts differently to hallucinogens so let's not try to pretend like there is one answer for everyone concerning the whether or not it's safe. For example, I have dropped acid somewhere over 40 times (ballpark) and it has had no lasting effects on me. However, the very first time a very close friend of mine tripped, it changed him forever. Not only did he have an extreme reaction during the trip but now (almost 3 years later) he's schizophrenic. \n\nI want to be clear that the reason it affected him so adversely is because he was predisposed to developing schizophrenia and LSD was just the straw that broke the camels back. However, there no way to know the mental predisposition that a person might have most of the time. \n\nAnytime try a hallucinogen you roll the dice because there's no way to tell how you'll react or what the lasting affects might be. Don't believe the people on here who say it's completely safe or that it's super harmful because the truth is it's just a gamble.","label":0,"model":"human","source":"reddit","id":2805}
{"text":"Ah, that \"Jeff smell\".  This has remarkably little to do with showering routine and cleaning products, and almost everything to do with your personal sweat.  In addition to water and electrolytes, sweat also contains small amounts of waste products, like urea, and smellier things like mercaptans easily pass through the skin.  So diet plays a role, natural bacteria in the gut and on the skin play a role, organ health plays a role, density of apocrine glands, percentage of sugar in sweat (which, in turn, allows for increased growth of yeasts which produce their own smell), degree of keratin production (mostly because this can plug pores), natural hormones, and natural pheromones, among other things.  Obviously, frequency of bathing will diminish the scents on people, and perfuming plays some minor role, but if you recognize your friend's scent on their pillow, for example, it's mostly due to what I just wrote.\n\nIn addition to this, there is the influence of breath, which people don't normally think about.  There are a lot of reactive species that get exhaled that have a scent, many of which will linger in air, including ketones, alcohols, and volatile organic compounds.  This isn't even including situations of poor oral hygiene or stomach upset, the latter of which can smell like HCl or sulfur, depending on the nature of the problem.","label":0,"model":"human","source":"reddit","id":2806}
{"text":"Because it's not something that you inherit. Straight people can have gay kids. Gay people (at least those who do have kids), can have straight children. \n\nHuman genetics are FAR more complicated than most people think, and being gay seems to involve more than just genetics. The fact that identical twins, who have the same genetics, can have different sexualities seems to prove that. There's some evidence that while some people may be genetically more disposed towards homosexual or bisexual activity, some of that may also be determined through epigenetics, from conditions during development as a fetus that affect brain growth. (Basically, imagine that quirks and oddities while a fetus could impact the brain in a way that makes someone gay.)\n\nThere continues to be a lot of investigation into this, but it's also worth noting that exclusive homosexual behavior is relatively rare in the animal kingdom. Plenty of animals are gay, and some animals have homosexual behavior being more common than heterosexual, but as long as some animals procreate, the species is able to survive.","label":0,"model":"human","source":"reddit","id":2807}
{"text":"The main difference is that your legs are vertical in a car or plane. It takes a lot of effort to move blood from the legs to the heart, so there are little valves in the returning blood vessels to make sure that moving blood doesn't go back down when it stops moving.\n\nWhen you move your legs, the muscles help to squeeze the lower blood vessels and force the blood back up through the valves. Without the help of moving muscles, the blood hangs around and pools where it is. When you're lying down, your legs are at a similar height to your heart, so considerably less effort is required to pump blood back.\n\nWhich raises the question, why is pooled blood so bad?\n\nOur blood has a few checks and balances to make sure that blood flows properly -- the blood needs to stay liquid when it's in the body, but needs to lock up quickly (or clot) when it leaves the body. To get this to work, a clotting protein is always present in the blood, so that the blood can clot strait away when it needs to. Ordinarily, these clotting proteins are held in check by anti-clotting proteins (also in the blood). When a person bleeds, the anti-clotting proteins degrade, allowing the clotting proteins to do their work and clot the blood. The anti-clotting proteins naturally degrade in blood as well, but are usually replenished by the flow of blood through the body. Pooled blood is blood that is not being replenished properly by fresh blood, and after enough of the anti-clotting proteins degrade naturally, there's a high risk of a clot forming in the pooled blood.\n\nOnce you stand up and\/or pump a few muscles, the clot gets pushed through the valves, up the body, through the heart, and gets wedged in a small blood vessel in the lungs. This blocks blood flow and causes a rapid unscheduled disassembly of the blood vessel as blood keeps entering in from the heart and having nowhere to go.","label":0,"model":"human","source":"reddit","id":2808}
{"text":"Well, bobblerable is mostly correct except for the distinctions between satin and sateen. I'll try to make the following as simple as possible but it can be hard to explain without using the terminology for weavers. But here we go:\n\nSatin is just simply a weave structure-- meaning, it's just a specific way to weave. There are three very basic weaving structures that (most) all other weaving is based upon. Those three are: plain weave, twill weave, and satin weave. (I'll get to satin in a minute)\n\nPlain weave is really basic. Every culture on earth that has weaving has plain weave in there somewhere. [This picture](_URL_1_) shows basic plain weave. \n\nThe fibers running up and down are called the warp. The fibers running from left to right are the weft (think weft rhymes with left).\n\nAs you can see in the picture, the weft (left to right fiber) goes over one fiber then under one fiber over and over again. And each row alternates so no fiber is always jumped over or slid under. You don't want to leave anyone out. See that? That's what locks those loose fibers together into one sheet of fabric. Without them locking together like that, all you'd have is [this](_URL_3_) and that's not fabric.\n\nThe second weave style, twill, is very similar to plain weave. The only difference is that instead of the fibers going over one, under one, it kind of jumps. So, it can look like [this](_URL_4_) or [this](_URL_0_). These are both very basic twills. In the first one, the white weft jumps over two fibers then slides under two fibers. In the second one, the darker weft jumps over two fibers then goes under one fiber. But they're both twills. \n\nTwill fabrics are generally stretchier than plain weave 'cause the fibers can move around a bit without pulling out or falling apart. They're flexible where plain weave is very sturdy and doesn't like to move at all. Twill fabrics have to be careful, though 'cause if the weft tries to jump too far, the fabric can become weak. Those weft fibers can snag easily if they're too long. And that can make the fabric fall apart, which isn't good at all.\n\nNow, the third main type of weaving is the satin weave, which is like a brother to a twill but is more daring. Satin jumps over four or more fibers, goes under one, then does it again. It looks like [this](_URL_2_). See how the blue weft jumps over many threads there? That's what makes satin a satin. \n\nSatin is shiny because there are so many fibers floating and the light is reflected more evenly from them. It also helps that modern satin is made from shiny fibers like rayon and silk. Often, the weft fiber will be a shinier fiber than the warp so the shininess is more obvious, but it's the way the fibers are woven that make it a satin; not the fiber types.\n\nYou could make a satin-weave scarf out of wool. It would be shinier than a twill-weave scarf but it wouldn't be as shiny as a silk satin. \n\nWhich brings us to the difference between satin and sateen. Satin, as I said above, is a weave structure like twill and plain weave. Sateen, though, is just a satin fabric made from cotton (or some other short staple fiber). Generally, cotton sateen is made when the weft goes over four fibers and under one fiber as anything longer than that and the cotton makes it weak. Cotton is not as strong a fiber as wool or silk. \n\nSatin is wonderfully shiny and slippery but it is not as durable as plain weave or twill. Even when made of silk (the strongest fiber), the jumping weft means it lacks stability. \n\nIf you have any further questions about weaving-related things or if any of the above terms weren't clear enough, please let me know.\n\nSource: I'm a weave structures and print pattern textile designer and a weaver myself. I focus on twill-variants, damask, brocade, and shadow weave. I also have a very strong love for pictorial tapestry weaving.\n\nedit: For a little more non EILI5 info: there are literally hundreds of variations of pretty much everything you can weave, even plain weave. I picked two twill examples but there are many others and twill alone has many sub-categories of weaving styles. Like random walk, corkscrew, shadow weave, and a bunch of others. Satin, too, can jump more than four warp, so there's a lot of variation there as well. These three types of weaves can be combined to make damask, brocade, and overshot, to name a few. The above is just the most basic I can make it.","label":0,"model":"human","source":"reddit","id":2809}
{"text":"First, a point of clarification. The rest of the world **has not** abandoned it. Norway still actively participates in whaling and so do many inuit tribes in Canada and other northern hemisphere countries. \n\nBut why does Japan do it:\n\n1. Culture - Some Japanese believe that killing one animal (e.g. a Whale) is more humane than killing thousands of animals (e.g. a school of fish or a ton of shrimp). This somewhat relates to Shintoism and the concept of everything having a \"spirit,\" but again it's more cultural than religious.\n\n2. \"Research\" - This is a controversial issue, but let me see if I can take an unbiased approach to it. The International Whaling Commission has an exemption for whaling that relates to medical research. So a Japanese organization known as the Institute of Cetacean Research conducts whaling operations under this IWC exemption. However, there is also a law in Japan that states that when a whale is killed no part of the body may be discarded as waste. So the ICR packages the whales (after they have done their research) for consumption\/sale back in japan. This is where organizations like Sea Shepherd and Greenpeace argue that Japan is using the clause in the IWC charter to engage in commercial whaling, which should technically be illegal. \n\n3. National Pride - Japan has had a long tradition of strong national pride, where they do not appreciate being dictated\/told what to do by other countries. There is a fringe within Japan that sees whaling as something that only the Japanese should have a say in, and do not want to be seen as bowing to pressure from western countries.\n\n4. Commercial Value - This hasn't proven true in 5\/6 years, but there is a market for whale meat. Were Japan able to haul in as much whale meat as they had intended, there is enough interest in the value of whale meat to fund the operations needed to hunt it. That being said, the folks from Sea Shepherd have had an enormous economic impact on commercial whaling in japan.","label":0,"model":"human","source":"reddit","id":2810}
{"text":"CG Supervisor here.  Simple answer, it's because of wages and overhead.\n\nWorking with a studio is usually a lot more expensive than working with freelancers.  A couple of the important factors is that studios will guarantee or insure a delivery,work more professionally, and the overall process is much smoother especially when a client is involved and they're hands need to be held during the whole process.\n\nWorking with a freelancer comes with their own rates (usually way cheaper), but they can be flaky, work with illegal software, unprofessional and make the whole process very messy.  It's usually less kosher this way as sometimes the client or agency takes time or doesn't pay them at all for their work and the relationships in the freelancing realm is usually a bit rocky and a lot of walls are put up to avoid either party getting screwed.\n\nDetails about the process of CGI and why it's so expensive working in the studio as I mentioned above is because of the wages and overhead costs (equipment, licensing of software and plugins, and operational running costs).\n\nA team of CGI Artists consist of people with very specific skillsets.  First we have the CG Supervisor, Art Director, and Technical Director.  These would carry the higher paying salaries of roughly $80,000 USD up to $150,000 USD, depending on your experience and the studio you're working for.\n\n* CG Supervisor: Responsible for managing the team's schedule, works with budgets and quoting for jobs with the producers, involved in meetings and presentations for the production.\n* Art Director: Responsible for the quality of the team's work.  Makes sure the work is going in the right direction as per clients' and agencies' expectations.  Guides the artists with concepts, style frames, mood references, anything that will give the rest of the team the vision they need to get to the end product.\n* Technical Director: Responsible for bringing new technology to the department (scripts, plugins, etc) to lift the process and complexity of our quality to a higher degree.  They essentially concentrate on research  &  development while assisting the team in finding solutions to problems that they the artists are facing.\n\n\n\nFinally you have the Artists, who generally will be involved in their own specific task in the CG workflow.  You will also have Generalists who makeup everyone's skills, but only at the least at a competent level, while specialists would be more inclined at delivering their tasks at a much higher level.  Depending on the studio and their experience, and Junior, Mid, or Senior level, these artists can make a salary of anywhere from $20,000 USD to $100,000 USD\n\nWithin the artists you have Leads who will usually be in charge of a certain stage in the CG Pipelines (Modelling, Animation, Shading, Compositing).  These team leads will be at the very least Senior level that possess a leadership trait and can help guide the team through the process.  Sometimes You will need a team of Modellers to create a robot, or you will need a team of rendering artists to light a few shots in sequence.  These leads will be there to micro-manage them so that everything comes together as one.\n\n* Concept Artist: Responsible for drawing \/ painting references for mood, styleframes, concepts for characters, monsters, environments.  This can also include storyboarding which tells the story of the animation in frames drawn by hand to visualize the composition and camera movements that compliment the action of what's happening in each vignette.\n* Modeller: Responsible for bringing the concepts to life.  They would bring the 2D Concepts or references in to the 3D world and \"sculpt\" them so that we can use them as assets in the application of shots.  They can be characters, environments, assets like trees, rocks, stones.\n* Animator:  Responsible for using the storyboard and assets created by the modeller to create the shots in 3D.  This involves camera move, and character animation - which can get technical as they need to be rigged so that they are able to have all the controls needed (posing, facial expressions, etc).\n* Shading \/ Rendering:  Responsible for taking the shots from the animators and making them look realistic or whichever direction the project is going.  It could be cartoonish, or it could be photoreal.  They would set up the lights and make sure the materials on objects react the correct way (metallic, rubber, subsurface, translucent, etc).\n* Compositer:  They will take the sequence rendered from the rendering artist and polish the final look of the shots.  They will add or remove passes from the shots (more reflections, ambient occlusion, motion blur, depth of field), play with the warmth or coolness of the overall image, add effects, and run a final grade through them.\n\nDuring this whole process there will be milestones and that means there will be rounds of feedback given by the client and agencies.  This will overall slow the process down and add more buffer time to the schedule.  So paying a whole team of artists for months of work can start getting extremely expensive, while keeping up with licenses, plugins, and software packages that are now all at subscription models.","label":0,"model":"human","source":"reddit","id":2811}
{"text":"According to [a fusion scientist that did an IAMA a while back](_URL_0_), the #1 hold back is funding - funding required to build the test reactors, needed to test the physics and engineering theory.\n\nWhile he mentions \/ elaborates on this answer in multiple posts; this is arguably the main one in which he talks about it:\n\n_URL_0_c32ravg\n\n---\n\nELI5:\n\nSay you lived in a really windy place, on a farm away from town, and you've always wanted a tree fort. Unfortunately, because it's so windy, when you try to build one, it gets blown out of the tree.\n\nTo try and make it stay up there, you try a bunch of things, like using new things to hold your tree fort in place; you know that you could use rope, or glue, or nails or something else. \n\nYou have a go at using some thin rope that your parents didn't want, but it's not enough, and it rips when a strong gust of wind comes along.\n\nYou try some glue, but your clag glue doesn't really work.\n\nYou try some small nails that you found, but they won't go through your planks of wood, into the tree.\n\nTo figure out how other people do it, you watch some grown up shows that your dad watches, and they use special glues, stronger ropes or chain, and they use big nails.\n\nYou ask your dad if you can get those things to build your tree house, and he says he will, but not until next week when he gets paid and makes the trip to town. You dad does get paid, but he forgets to buy your things. Next week he says. In the mean time, you find some larger nails that work, but you don't have nearly enough of them, and still your tree house isn't complete.\n\nHopefully, next week, your dad will remember to buy them, or at least save enough to warrant \/ afford a trip to town for your things...","label":0,"model":"human","source":"reddit","id":2812}
{"text":"It actually is ice. \n\nRockets store their fuel as liquids, so they can be pumped around easily and take up as little space as possible. Many fuels have a boiling point below room temperature, so they need to be cooled down and pumped into the tanks that make up the structure of the rocket itself.\n\nThese fuel tanks need to be as light as possible so that the rocket can carry more useful weight to space as payload so the sides of these tanks are *very* thin. When moist air meets the outside surface of the rocket, water condenses just like on a cool drink glass. Because the fuel is so cold the water turns to ice, and ice forms on the side of the rocket.\n\nWhen the rocket is ignited it shakes violently, causing the ice to break off.\n\nIt is odd that you choose the Space Shuttle as an example, [as this is one example that *does not* have ice falling from it](_URL_1_). The external tank is insulated and the boosters use solid fuels. [The Saturn V rocket is a famous example of the ice falling](_URL_0_).\n\nNB: 'Fuel' in this explanation includes oxidiser. There are plenty of rockets that don't use cooled fuels but this is a simple explanation.\n\nEDIT: I is not ice, as it turns out.","label":0,"model":"human","source":"reddit","id":2813}
{"text":"Why shouldn't it be? Speed limits are an inherently crude and not particularly effective solution to a complex problem: how to encourage people to drive *safely*. I see two main problems with speed law:\n\n1) If there were a maximum \"safe\" speed at which to drive down a certain road, it wouldn't be a constant--it would vary wildly depending on road conditions, visibility, traffic volume, etc. Driving over the limit on one day can easily be safer than driving under the limit the next day if conditions were better on the first day.\n\n2) Although speed is a major factor in how much damage there is in a collision (not to mention that it reduces the reaction time available to the driver), it is really only one aspect of road safety. Driver skill, maturity and awareness, as well as car maintenance are also very important (I would say more, at least to a point), but they're far more difficult to enforce so they don't get much emphasis. Personally, given a choice between being on the road with other drivers who are attentive but going 10 mph over vs. drivers who follow the speed limit but are spaced-out, I'd choose faster and more attentive any day.\n\nTo answer your question, I think there are enough people that feel safe enough going 10-20% over the speed limit that it has become socially acceptable. This is enabled by the fact that in some jurisdictions, the cops usually won't bother you at this speed unless you're doing something stupid.\n\n(Edit: forgot a couple words)","label":0,"model":"human","source":"reddit","id":2814}
{"text":"I'm going to have to come at this from the other angle:\n\nThe reason that poor people tend to get screwed by the Legal System is because they are saddled with a Public Defender. \n\nPublic Defenders aren't bad Lawyers, they're usually *shockingly* competent at their jobs. *However*, due to their small numbers, Public Defenders are required to take on an *insane* number of cases. Public Defenders *usually* don't have much time to prepare for a case. Most can't afford to spend more than 15 minutes on a single case, and are *insanely lucky* if they can afford an hour of preparation for a single case. \n\nIt's virtually impossible to win a case without time to prepare. You need to research the relevant Laws, and **all** of the Case Law that is binding in your Jurisdiction. Then you need to review the Prosecution's Evidence, and get Evidence that was illegally acquired or questionable thrown out. You also need time to coach your client on how they should behave in court.\n\nIt's *physically impossible* to accomplish **half** of the preparations required in an hour, much less 15 minutes. As a result, most Public Defenders recommend that *most* of their Clients just take whatever plea-deal is offered by the Prosecution. It'll be a lighter punishment than they'd receive if they go to court and lose, and the odds of not losing are *basically zero*.\n\nThe exception to that *most* are clients who *can* realistically be defended. This generally means that the Case against the Client is full of holes to the point where the *Jury Members* would probably catch the procedural errors, or relies upon evidence that's *blatantly* inadmissible in court.\n\n---\n\nMeanwhile, Wealthy People can afford to hire a Private Lawyer. These Lawyers can control their case-load, ensuring that they have **all** the Preparation Time that they need. They can take their time, do their research, coach their client, and pick apart the Prosecution's case.\n\nEven if the client is blatantly guilty, the Lawyer can actually hold the Court to the Letter of the Law regarding sentences... but this is going to require some explanation.\n\nMost Judges in the United States, outside of the Federal Circuits, are *elected*. Because of our current culture, it's best for a Judge to maintain a public image of being \"Tough on Crime.\" If you pass down a relatively light sentence on someone, you can bet your ass that your Opponent in the next election will plaster it all over their campaign ads. As a result, Judges are incentivized to be as harsh as possible when it comes to sentencing.\n\nThis trickles up to the Federal Circuit as well. Federal Judges are generally drawn from State Judges, and getting promoted up in the Federal Circuits requires you to be Nominated to that position by the President of the United States. *Being* a Candidate for a Federal Bench (which you can hold for life) requires you to hold onto your State Bench. Moving up in the Federal Circuits requires the President to move you up... and the President has incentives to put Judges that are \"Tough on Crime\" onto the Federal Benches.\n\nHowever, a Good Lawyer can hold a Judge's feet to the fire when it comes to sentencing. Excessive Sentences can be used as a justification for Appeals, and having your judgements *overturned* by a higher court hurts your chances for advancement **even more** than not being \"Tough on Crime.\" A State Judge up for re-election can spin not being \"Tough on Crime\" as being \"Merciful,\" but it's difficult to spin \"Incompetent Judge's Decision Overturned by Appellate Court\" into something vaguely positive.","label":0,"model":"human","source":"reddit","id":2815}
{"text":"Native Mandarin speaker here. I have some knowledge of other dialects like Cantonese as well.\n\nEven though Chinese is a tonal language these tones come secondary to the melody of the song. However, speakers will still be able to understand the exact meaning of the song by the pronunciation of the words even without the tonal information. Here, let me give you an example: \n \n\nTake a look at [this song](_URL_0_) (thanks for the help!) \u6ca1\u90a3\u4e48\u7b80\u5355\u3002 The first five words of the song should be pronounced Mei2 na4 me jian3 dan1 in Mandarin Chinese - but if you take a quick listen you'll realise that this isn't the case in the song!! \n\nA poster here mentioned context - I would respectfully like to clarify what this context involved means. These five words combine to form a _completely unambiguous_ meaning, even though every single word taken individually is a total homophone. For a more Anglo-centric example, imagine the song lyric \"I can't BEAR with you anymore\" - no English speaker would confuse that with the four legged furry animal!! Similarly, five very ambiguous individual words come together to provide a clear an unambiguous meaning.\n\nThat's how speakers of tonal languages distinguish meaning without tone! The first top level reply by \/u\/kamiyamato is Not Completely Accurate since there is no REQUIREMENT nor is it necessary for the tones to roughly approximate spoken tones or end higher than they begin in order for speakers to infer the correct and unambiguous meaning. \/u\/kamiyamato is also completely erroneous in claiming that most words are made up of only one character, because there are literally hundreds of thousands of words which are made up by joining two or more characters together forming a word with a completely different meaning :)\n\nHope I helped!","label":0,"model":"human","source":"reddit","id":2816}
{"text":"3 key reasons:\n\n1. North Korea, while technologically and numerically inferior to South Korea, do have a stranglehold on SK; in the shape of an estimated 10,000 artillery pieces (mobile and not), all pre-dialled onto South Korean population centres and military bases. At a moment's notice, North Korea is capable of unleashing a notable amount of destruction upon South Korea. North Korea is also obviously nuclear - while their missile technology is a bit lacking, it is significantly easier to fire a few hundred km than a few thousand. Additionally, through tunnelling, smuggling or even possibly via a shell, there are alternative methods to using a rocket to get a nuke to it's target.\n\n2. The population of North Korea is indoctrinated, and North Korea is the most tunnelled-under nation on Earth - whilst we don't know the full extent of either of these, we do know that North Koreans are basically taught that the western world (mainly the US) is evil and pig-like, etc. Trying to win a nation's population over, by destroying their military and killing their peers, is a pretty difficult thing to do, let alone to ones whom firmly believe that there aren't better alternatives to their lifestyle. As for the tunnels, think of Vietnam; the North Vietnamese hired North Korea tunnellers and planners to set up some of their networks. With it's mountains as well, North Korea will have difficult terrain to battle across - lots of cover for enemies to hide behind, and lots of tunnels and shortcuts for enemies to flank you with, or to use for explosive-laying.\n\n3. Even if you take over North Korea with minimal damage done to either side, you have another issue; looking after ~20 million impoverished and uneducated people - North Korea gets by by not caring about it's people; they want them to survive, but they'd rather risk a mass-starvation than lose valuable funds for nuclear research, etc. Unless you want to be just as ruthless, you're going to have to work out hundreds of billions of dollars, if not trillions, trying to build \/ rebuild a nation, *and it's population* (in terms of education and health) from scratch.\n\n---\n\nAlso, as far as China goes, China won't do anything other than place token economic sanctions on the US, give words of disapproval, etc - Wikileaks revealed that some Chinese officials had said that China would welcome a peaceful, SK-run united Korea.\n\n---\n\nEDIT2: [I know it's a wiki link, but that paragraph covers a number of examples of why China wouldn't support North Korea in war.](_URL_0_)\n\n---\n\nEDIT3: Just a reminder:\n\n > please, no arguments about what an \"actual five year old\" would know or ask!\n\nYes I know there are definitely words in there that a 5 year old wouldn't understand, but with this being a topic of such a misunderstood and sensitive issue, it's better to answer accurately at the expense of subreddit ruling than to put across the wrong message.","label":0,"model":"human","source":"reddit","id":2817}
{"text":"Shampoos are all essentially a soap. The only noticeable difference are colour-protecting shampoos - while most shampoos are based with Sodium Laurel Sulfate (soap), good color protecting shampoos will be sulfate-free and use a different soap base to protect hair colour. The reason behind this is, sodium laurel sulfate is a fine detergent molecule that binds well with grease, but also with hair colour, especially semi-permanent hair colour that isn't embedded deep in the hair. It also lathers really well and makes a lot of bubbles. Color protecting shampoos use a different molecule that doesn't bind with hair colour as well but still washes the grease out of your hair and lathers, just maybe not as extremely well.\n\nMy guess as to why there isn't a shampoo that helps with everything is this - shampoo that helps with frizzy hair and gives \"smoothing\" qualities is going to have silicones and natural oils to smooth and weigh the hair down. People who want volume because their hair is fine and limp aren't going to benefit from this - they're going to benefit from a shampoo that's light and helps clean all the gunk off their hair. People with thinning hair are also going to wait this, and they're also going to want something that stimulates the hair follicle and scalp to increase blood flow, which in theory, helps stimulate hair growth.\n\nShampoo for dandruff is self-explanatory. People with dry scalp\/hair will want a moisturizing shampoo\/conditioner where as people with oily hair will want the opposite. With that said, most drugstore shampoo will have negligible difference, whereas professional\/salon brands will usually have 4-5 varieties that when used with the corresponding conditioner, will show fairly noticeable results.\n\nIt's kind of like how there's different face wash for people with different types of skin\/skin problems.\n\n** Edit: holy shit, gold. Thanks kind stranger! Day has been made.","label":0,"model":"human","source":"reddit","id":2818}
{"text":"Actual opera singer here.\n\nThe same reason why anyone gains weight when they are in a high-stress job that requires a lot of travel. It's a stressful job with intense hours during production periods, and it gets really easy to neglect exercise and just eat crappy foods, especially because you usually celebrate after performances with your friends\/colleagues\/family. You often don't get a lot of sleep, and that can also mess with your overall health.\n\nAs far as actual singing goes, there's actually no evidence that being overweight makes your voice bigger, lets you sing louder\/over an orchestra, or anything like that at all. If anything, vocal scientists are finding that being in better physical shape actually IMPROVES vocal stamina and quality, just as being in shape improves performance in any physical activity.\n\nHOWEVER, being fat has some \"advantages\" when learning to sing classically. Some of the most common phrases to singers is that they need \"to be grounded,\" \"sing from their diaphragm\", \"have low-breath,\" etc. When you're carrying around a lot of extra belly weight, you have a constant tugging-down feeling. You have extra weight physically pulling the breath mechanism downwards. It can help in learning to sing.\n\nAlso, until recently, it didn't matter if you were fat and unhealthy. If you had a god-like voice, you could look however you want and still perform and get good work. Opera used to be (and, IMO, SHOULD still be) about the voice and music over anything else.\n\nNow that there is a big push for opera singers to LOOK like the roles they play (nobody wants to see a 300 lb Juliet anymore), many professional singers have taken to losing weight in unhealthy, crash diets. They lose weight so quickly, they are unable to feel the changes that happen in their body and take time to make proper adjustments. They no longer feel \"grounded\" because they don't have the weight actually \"grounding\" them. So their technique suffers, their voice gets \"lighter\", they have an existential crisis, say they can only sing when at a certain weight, and eat themselves into oblivion again.\n\nThose who lose weight responsibly and healthily rarely, if ever, have any problems singing.\n\n**TL;DR It's a big, mental game. Some singers think they can only sing at a certain weight and size, and actively try to stay large.**","label":0,"model":"human","source":"reddit","id":2819}
{"text":"There is a whole group of computer Operating Systems which are called 'linux' because they all use the linux kernel, which basically just means they all work in roughly the same way.\n\nThe main thing that differentiates linux from the other main operating systems (Windows and Mac OS X), is that it is \"free software\". With other \"proprietary\" software, nobody except for the company that made it is allowed to modify it, make and share their own versions, or even really see how it works. If there's something you don't like about a piece of proprietary software, the only thing you can do is ask the company that made it to change it, and if they don't want to, you just have to put up with it. If you find a problem with the software, even if you are a programmer and know how to fix it, you aren't allowed to.\n\nFree software doesn't have those restrictions, so linux is worked on by thousands of programmers all over the world, many of whom are not paid to do it, they just want to make the software better. Since so many people are making improvements and fixing bugs, linux is a very stable, fast and reliable operating system.\n\nSome advantages:\n\n- You can change, tweak, customise or replace anything about your system if you want to. If you don't know how, chances are someone else has wanted the same thing as you and figured it out, so you can just use their solution.\n- The type of people who work on linux tend to be a little paranoid, which is a good thing because it means the code they write is very secure. Combined with the fact that fewer people use linux, this means that linux basically doesn't get viruses.\n- Linux and much of the software that runs on it is free, as in you don't have to pay any money for it. Apart from being cheap, this is neat because it means installing and removing software is really easy, there are no keys or authorisation systems or whatever. You can casually install some big complicated program that would have cost $100 proprietary, and if you get bored of it uninstall it 5 minutes later.\n- There are loads of people who love linux and like to help people learn how to use it and help solve their issues\n\nSome disadvantages:\n\n- It doesn't run software that was written for Windows or OS X (well, it often can, but not without a little bit of work). This is mostly an issue for gamers.\n- Getting the most out of it, using the more advanced features, modifying it etc, requires you to know about computers\n- Because so many different people are working on different programs, there are a lot of options and variations. I think it's good to have choice, but it can be confusing.","label":0,"model":"human","source":"reddit","id":2820}
{"text":"**This is a long answer, which I think might be a little more satisfying than the others here to anyone who's interested in this question.**\n\nI'm not a physicist, so take all this with a cup or two of of salt; but I'm not really satisfied with the answers so far, so I'll give my own try. The answers so far seem to get at some of the important points to your question, but don't tie things together in a very understandable way I think. So here goes. \n\nThe first thing is to get some basic conceptual vocabulary down. The most important concept you'll need is the concept of a space-time manifold. Prior to Einstein, the world was used to thinking of space as this fixed three-dimensional grid that we move through. Think of a gigantic box that contains the whole universe. And we thought of time as a linear track that we were on, moving forward into the future, away from the past. So imagine this big box that contains the universe (space) now being set on a set of train tracks, and moving forward in a straight line, at a constant speed. \n\nOn this picture, you can move around in space all you want, but it's just a bare fact about the universe that it's all moving forward in time at the same rate. *Why* you can't go back in time is sort of an unanswerable question. It's just a *basic fact* about the universe that everything's moving forward through time at a constant rate. You can't change that any more than you can change that matter takes up space, protons have positive charge, etc. And there's no more explanation for that fact than \"well, that's just the way things are.\"\n\nEnter Einstein. Einstein (and others, but we'll keep the story simple) suggests a very different picture of space and time. Rather than being two separate things - space being the 3d grid we move through, and time being a linear track we're going down - Einstein suggests that space and time are really a single, unified thing. \n\nJust as you can have a two dimensional space (a flat plane - a grid with only length and width, like a sheet of paper), you can add an extra dimension to it - height - to have a three-dimensional space with length, width, and depth. \n\nEinstein suggests that time is just a fourth spatial dimension that we add to the three-dimensional space we're used to thinking about. This is hard to picture in the way you picture a third dimension being added to a 2d space, but picturing it visually isn't very important so just stick with me. All you need to keep in mind is the *idea* that time is just like an extra spatial dimension. And, just as the addition of a third dimension to a flat plane creates the three-dimensional manifold we call real space; the addition of this fourth dimension to real space creates a four-dimensional manifold that we call space-time. \n\nSo, we used to think of time as a track that everything moved down at a constant rate. We can now think of every object as being situated inside this 4D manifold called space-time. And, further, we can think of every object as having a constant speed that it's moving at. Why do objects move at a constant speed? That's an incredibly complex question of physics, and we'll leave it aside. Just take it as granted that we've discovered, empirically, that objects all seem to move at a constant rathe through this 4D manifold of space-time.\n\nBut, you don't *feel* like you're moving, right? You're probably sitting still. And you can *control* the speed at which you move too. You can run, drive, walk, sit, etc. So how is it that you have a constant speed through space-time? \n\nWell, to understand this, [look at this image.] (_URL_0_)\n\nImagine that you're that dot, and that the forward (y) axis is time, and the horizontal (x) axis is space. You're moving at a constant speed through this space-time manifold, and there's nothing you can do about that. The speed is fixed. What you *can* change, however, is the *direction* you move through space-time. You can move more in the x direction or more in the y direction, though you'll stay at the same constant speed, no matter what you do. \n\n This sounds funky, but the basic meaning is this. When you move through space, you are diverting some of your space-time speed away from movement in the forward (time) direction to movement in the horizontal (space) direction. So, imagine you're sitting, and then you get up to go to the kitchen. When you were sitting, all of your space-time momentum was moving you through time. You weren't moving through space at all. When you got up and walked to the kitchen though, you diverted some of the speed at which you were moving through time to your movement through space. You moved a bit more slowly forward in time, so that you could move through space. \n\nNow, if instead of walking slowly to the kitchen, you had *sprinted* to the kitchen, you would have diverted even more of your space-time speed into movement through space, and thus would have made even less progress forward through time during your trip to the kitchen than if you had walked. \n\nThis illustrates the fact that the faster you move through space, the slower you move through time. And, conversely, the slower you move through space, the faster you move through time.\n\nSo now, remember that constant speed I said you (and everything else) were moving at through space-time? That constant speed that everything is moving at is just one of many 'constant' facts about the universe. Other constants are things like the mass of an electron, the charge of a proton, etc. These constants are basic facts about the universe that we've discovered. We're not sure *why* these facts are the way they are, but they seem like basic, unalterable facts about the universe. \n\nThere's a name - or at least a symbol - for this constant speed we're all travelling through space-time. It's notated with *C*. But just how fast is this speed we're moving through space-time? Well, it's the speed of light. That's what *C* is. When you sit down, you're moving at the speed of light (186,000 miles per second) through space-time. And all of that motion is in the forward (time) direction. When you get up and get a coke, you divert a tiny, tiny bit of that speed (say, two miles an hour?) to movement through space. And when you sit back down, those 2mph are returned to movement through time. \n\nSo, remembering that this speed at which we move through space-time is a *constant* fact of nature, much like the mass of an electron, we can ask your question anew: Why can't we travel through time? \n\nWell, the answer is now very complex. We obviously *can* move through time. We're doing it right now, at nearly the speed of light. And we can alter the speed at which we move through time by moving through space at different rates.\n\nBut your question is presumably whether we can know with confidence that we can't go backward or jump forward in time. It *seems* that we can't. The reason is that, just in the way we can't make electrons that have different masses than they naturally do, because the mass of an electron is a fixed, constant fact about the universe; neither can we make objects that move through space-time at any different rate than they naturally do, because the speed at which all objects move through space-time is fixed at *C*. *C* is a constant that we don't know how to manipulate (and are currently most tempted to think is not manipulable). \n\nWhat we can do, however, is divert more or less of our speed through space-time in the space direction. If we diverted half our speed through time to speed through space (by travelling at half the speed of light) we would move through time half as fast. If we diverted all of our speed through time into speed through space (by travelling at the speed of light), then we would cease to move through time at all. Time, for us, would 'stop'. But what this really means is that *we* would stop moving through *time*. \n\nSo, I think you've asked a question whose answer will be unsatisfying in direct proportion to how short it is. The short answer is that we know we can't travel back or jump forward through time because it seems that the speed of light is constant. The long, more satisfying answer is contained in a thorough understanding of relativity theory. I've tried to give something a little better than the first, and far, far short of the second.","label":0,"model":"human","source":"reddit","id":2821}
{"text":"Game Genie's worked a little different depending on each system because each system treated cartridges a little different.\n\nIn general a Game Genie was designed to sit between the console and the cartridge and when the console asked the cartridge for data the game genie could secretly change it before passing it back to the console.\n\nSo for example you have a game where you start with 3 lives. That number 3 exists somewhere in the data or code on the cartridge. Let's assume the number 3 is stored at the 5000th byte of the cartridge's data bank. On the Game Genie you'd enter a code like \"50 00 99\". This would tell the genie that every time the console tried to load the number from address 5000 to send back a value of 99 instead of what was really there. Now when a new game starts you have 99 lives because that's the number the console recieved.\n\nWhile that code is obvious in its meaning the genie usually used scrambled codes in a known way, so for example you might actually enter the code \"90 05 09\" and it would get unscrambled into the more meanful code.\n\nDifferent consoles had different ways of working. On the NES the cartridge was linked directly into the CPU bus in such a way that it could control *all* memory, not just the cartridges (this allowed NES cartridges to enhance the original hardware, not just provide game data) by routing any memory access through the cartridge pins first. This means that the cartridge could even override the data in the consoles built in ram. \n\nSo Game Genie codes for the NES might do things like \"hold\" a byte. What this means is that it essentially kept a value in RAM locked - attempts to change it wouldn't work. So you have a place in ram where health was stored and when health reached 0 you are supposed to die. The game genie could just hold that value at 99 and now you are invincible.","label":0,"model":"human","source":"reddit","id":2822}
{"text":"Muscle contraction occur when electrical impulses trigger the release of chemicals (sodium and calcium, along with some other things) that react with the muscle fibres causing them to contract. The contraction ends when the chemical are cleared away stopping the reaction. A \"knot\" in a muscle is an area of muscle fibre that has remained contracted while the rest of the muscle has relaxed. By pressing on the knot for an extended period of time (technical term is digital ischemic pressure) the blood in the area is forced away. When you then lift off, fresh blood floods in and this helps to clear away the remaining chemicals that were still bound to the muscle fibres, allowing them to relax.\n\nEdit: Looking through the comments there are 3 main ideas being thrown around, and I had addressed 2 of them in comments so I thought I should put the info here too.\n\nGood news is, all 3 ideas are relevant. I provided the answer above As my primary response as it relates directly to the question of pushing into the muscle, but as a remedial therapist treating soft tissues, a combinations of techniques are needed for effective treatment. \n\nStimulation of the Golgi tendon organ is one of those treatments and is used mostly on long muscles, like calves and hamstrings, when tension is being felt along the length of the whole muscles as opposed to an isolated area of contraction \"knot\". Most effective way of stimulating the Golgi organ is by bowing the tendon as this effectively makes it freak out thinking the tendon is about to snap, telling the muscle it has to relax so this doesn't happen.\n\nAdhesions are another issue where trauma to the muscle fibres over react while healing causing them to become stuck to the fascia  (muscle casing), this results in the muscles not being able to move smoothly and will cause disfunction of movements. This is commonly treated with friction as it helps to tear apart the adhesions.","label":0,"model":"human","source":"reddit","id":2823}
{"text":"The only natural disaster that concrete and brick houses are better at dealing with are fires. \n\nWith tornadoes most damage is done by winds so strong that they dismantle concrete, brick, or stone either directly or by throwing debris into them only slightly more slowly than they do wood. You can make a tornado bunker that is above ground with 4 foot thick walls and steel reinforcement. Homes do not do that, even in places that they make concrete homes. \n\nHurricanes tend to do most of their damage with flooding. Concrete and Bricks flood and mold just as easily as wood. When they are washed away the ground itself is washed away so they break just as easily too. For the extremely powerful hurricanes we have the same wind issues as tornadoes. \n\nFor earthquakes the concrete and brick homes are far worse than wood. They are too rigid unless very expensive tech and building practices are used and so they just crumble when an earthquake happens as they cannot flex. \n\nAnd finally they are 3-10 times more expensive than wooden homes. Chances of you actually losing your home in your lifetime are low, and you get insurance to protect against it. So it is far better for most to spend what money they have to build a larger nicer home. \n\nAlso you seem to think drywall is a structural weight bearing material. It is not. It is the interior finishing of the wall. Wood is the structural component and wood\/fiberglass is the outdoor wall slat.","label":0,"model":"human","source":"reddit","id":2824}
{"text":"Most TV shows have someone on staff called the Music Supervisor. It is their job to acquire the songs you hear on a show, negotiate their rights, etc. Sometimes there's a specific song the show wants for a scene, something real famous, but most of the time the producers\/writers don't have the time to nitpick through and pick all their music, so they say \"we want an uptempo song here\" and the Music Supervisor goes to work.\n\nThe Music Supervisor is generally someone in the know about indie music (a lot of the shows that are produced in LA use the DJs from the highly influential KCRW radio station), whose job it is to find the hot new bands before they're hot. If a show has a certain tone or feel to it (eg Veronica Mars and its neo noir ambient soundtrack), the Music Supervisor will quickly get a handle on the groups that fit the show, and as the show builds traction they might even begin to get a ton of demo CD's from the various labels who want to increase their band's visibility by putting it on the hot new show. \n\nRecord labels know who the Music Supervisors on the tastemaking shows are, they develop these relationships as the shows license more music from them, so if a label has a new act they want to get out there they'll reach out pre-emptively and say \"hey, we got this great new act, here's their album, wanna put it on your show?\"\n\nThe other thing that might happen sometimes (although this seems to happen less and less these days) is that record labels will attach unknown acts to known ones. You want that Radiohead song that you love? That'll be 50,000 dollars, PLUS you have to use these 5 other songs from some of our up and coming artists in your show as well.\n\nSource: worked in the sound dept of a major TV studio for 4 years, saw this kind of stuff on the regular.","label":0,"model":"human","source":"reddit","id":2825}
{"text":"I'm actually reading a book right now that talks about this: \"Oxygen: The Molecule that Made the World\" by Nick Lane\n\nYes, fire did burn differently because of excess oxygen fuel.  During the carboniferous period, 300-350 million years ago,  oxygen levels reached as high as 35%, compared to today's ~20%.  This allowed fires to start in places which we wouldn't normally expect (places of high humidity, during rain storms).  Today we find fossilized charcoal in coal beds that arose from peat bogs, which grants credence to the idea that fires did burn much more easily during this period.  You might ask \"how did plants survive the constant danger of fire?\" Adaptation thanks to random evolution.  For example, plants from this period expressed deeper roots\/tubers than their modern day counterparts, and the leaves of trees were higher which could help avoid catching fire from a stray brush fire.  \n\nThe reason so much coal (a large part of which is oftentimes fossilized charcoal) remains from this period is due to two factors, as other posters have mentioned.  First The structural material of plants, lignin, doesn't burn rapidly, (paper is often stripped free of lignin which is why it burns so easily) rather it tends to smolder (i.e. logs in a fire), so fires would leave large amounts of charcoal rather than their usual main byproducts: carbon dioxide,carbon monoxide (at lower burning efficiencies) and water.  Secondly, lignin is very hard to digest, even fungus\/bacteria today have a hard time digesting it.  300 million years ago, the decay rate was practically zero.  In summary, pretty much all of todays coal comes from plants which, millions of years ago, died and were buried without ever going through the process of decay.\n\nWhat's even cooler though is that during this time gigantic insects roamed the earth _URL_0_ which is believed to be related to the faster metabolic rates that high oxygen content allows.\n\nAnyways, its a good book if you're interested in the topic.","label":0,"model":"human","source":"reddit","id":2826}
{"text":"Better question is why it worked on Earth but not Mars or Venus.\n\nEarth is larger and from that fact alone, our core, assuming similar composition, can get denser and hotter. Also the additional gravity allows us to hold our atmosphere better.\n\nEarth's moon is also abnormally large compared to the moons of other planets(when compared to the mass of the moons' host planets). Meaning our moon is bigger than normal for our planet size. The gravitational effects of our moon affects our planet more than a smaller moon would. The tidal effects that affect our oceans also tugs on our mantle keeping it in motion and also fluid.\n\nThe active core generates our magnetosphere that shield us from solar winds that can strip away atmosphere. \n\nIt's still debatable, but a geologically active core might be a prerequisite.\n\nVenus is closer to Earth's mass, but is geologically inactive. (correction : Venus is geologically active, but lava hasn't been observed. It does have a young crust, so that is a sign of geological activity)","label":0,"model":"human","source":"reddit","id":2827}
{"text":"Hi there, I work in pulmonary rehab and respiratory therapy.\n\nThe truth is, they don't. Your lungs don't have regenerative properties. The damage that's been done, will always be there. Pulmonary rehab can't heal the damage that's been done (but instead can help improve your body's utilization of the reduced pulmonary function).\n\n**BUT**, as others have mentioned, the shit that's been accumulating in your lungs has a chance to be pulled out by your body's immune system. Smoking regularly has the dual effect of suppressing your immune system and layering on more shit for it to get rid of. Quitting lets your immune system recover and stops bringing in more foreign substances to damage your lung tissue.\n\nOver time, even without rehab or therapy, a former smoker will notice a significant improvement in respiratory and pulmonary function. This is thanks to the removal of the accumulated tar, nicotine resin, and other chemicals (depending on the brand of cigarette). Others who have developed conditions such as COPD or pulmonary hypertension will always have those diseases, and that's where rehab comes in.","label":0,"model":"human","source":"reddit","id":2828}
{"text":"First off, everyone has a different amount of wax production, so some have way more of a buildup than others. It can be a serious problem for some people. Secondly, it exists to protect your tympanic membrane  (ear drum) from foreign substances. It does this by being sticky and catching everything that goes in. If you stick a lot of things in your ear, spend time in the sand, in dusty areas or outdoors or with pets or anywhere where the environment can get inside your ear canal, more things are going to be sticking to your ear wax, thus causing a bigger build up! Shoving q tips will just shove the wax further and further back, making it more impacted and harder to fall out. Thirdly, Our ear wax is actually produced in a way where wax is continually being pushed out of the ear naturally so some people literally never need to clean out their ears! The ear is a strange thing. \n\nPro tip: if a bug crawls in your ear, your ear canal is so sensitive that you'll know it's there  (no need to worry about bugs crawling in without your knowledge) and they rarely get stuck in the wax, but all you do is turn off the lights and shine a flash light or bright light near your ear and it'll crawl out!","label":0,"model":"human","source":"reddit","id":2829}
{"text":"The Director of a film (or a play, or a television show episode) is the person responsible for the creative vision of the piece. They create a concept from the script (which may or may not be something concretely found in the script, it may be metaphorical or tangential) and from the concept lead the design and production team towards a collaborative vision. Once rehearsals\/filming have begin, the director blocks the piece (i.e. tells actors where to move), provides objective and subtextual support to the actors (i.e. tells them why they are saying the things the writer wrote) and ensures that the visual style and setting are within the original vision or concept parameters.\n\nIn film, they also work closely with the DP, first story-boarding the script, and then, once on set, making sure that each shoot is framed, blocked and shot per their vision. Including ALL design aspects, from the color of the walls to the type of purse a character might wear.\n\nIn essence they are the Captain of the ship. A lot of my notes below can also be laid at the feet of bad writing, but in film (less so TV and theatre) directors have a great deal of oversight on the writing, so they are typically held accountable if the writing is terrible.\n\nA film which has been directed badly will usually (but not always, the problem with a collaborative art form, which is what film is, is that there are many, many chefs in the kitchen. However, since the director tends to get the credit when everything works, they also tend to get the blame when it doesn't)--usually show the following flaws:\n\n1. Incoherent story telling. You don't know what is happening. Or why it is happening. Or who it is happening to. Sometimes things are just blatantly implausible.\n\n2. Cliche or trope ridden dialogue\/shots\/events. You feel like you've seen all of these things before. All the characters are stereotypes, all the plot points unfailingly predictable. Note: cliches, tropes and stereotypes can all be used well. But bad directors tend not to.\n\n3. Bad dialogue. Dialogue that is forced and unnatural. Dialogue that is too on-the-nose. People telling other people things instead of doing things. People explaining how they feel ad nauseam. Dialogue spoken only to allow for the plot to push forward, leading us to:\n\n4. Coincidental plotting, or plots hole you could drive a freight train through (not the small inconsistencies that almost every movie has, but HUGE giant massive oh-my-god-this-movie-is-broken plot holes). Coincidental plotting is when everything that has to happen for the plot to move forward does, without any effort on the part of the hero (or the bad guy).\n\n5. Bad acting. Directors are responsible for getting a performance out of their actors, so even if the actor can't act (one reason why casting is important) the director is still the one people are going to hold responsible for any painful moments on screen (this is less true in TV and theatre). \n\n6. Over or under designed. Over designed is when the concept\/vision of the piece becomes more important then any other element. Think 300: Rise of an Empire or Sin City: A Dame to Kill For (not-at-all-oddly, both Frank Miller graphic novel adaptations, where the look was where the design team started with). Tim Burton is also a well-known director who can go to far with his vision\/design to the point of over balancing the movie. Under designing is when there is a lack of design and the production feels (usually) cheap or not-thought-through. Good design elevates the narrative, supports the characters and provides visual clues to the audience about what is happening--excellent design can comment on and complement the action, enhancing the entire experience.\n\n7. Movies\/TV only: bad editing. Either because there were technical difficulties during filming and the needed shots weren't gotten (or a director wasn't prepared and didn't get the shots they needed), and therefore the editor is attempting to make up for missing and\/or bad shots; or because the editing itself is just bad. Odd cuts, odd shots going back to back, odd audio issues. Various other things.  While most early directors at a studio on a movie won't have any say over the final cut, most editing issues are from a lack of footage (which is the director's issue), not bad editing. OR a director who does have final cut approval and shouldn't, which is where you got a three-hour movie that should have been 2 hours and 10 minutes max.\n\n8. Poor production value. An overall feel that the movie wasn't cared for (this isn't about money, this is about time and support). Usually shows in bad lighting, bad audio, bad set dressing, bad costumes--just an overall sense that these things weren't considered important or there wasn't time to pay attention to them.\n\nA film, tv show or theatrical play is an immense, multi-part beast, and the Director is the one that tries to tame it. To varying degrees of success. Every director probably has one (or many) bad movies to their name, as its how we all learn. The more telling test is not if they directed a bad movie, but if people wanted to work with them again. And, sometimes, the love of the thing they are creating can shine through the worst movie and make it, somehow, good (think Sam Raimi's original Evil Dead).\n\nHope that helped!","label":0,"model":"human","source":"reddit","id":2830}
{"text":"How he won, by scoring points within the rules of boxing, period. You get points for landing hits on the face and body cleanly. And he landed more, he always does, and if you analyse the tapes you can see he landed about 75% more punches. In other words, he completely dominated this fight. And the judges saw that and awarded him between 8 and 10 of the 12 rounds.\n\nNow punches landed, that goes into scoring. But it's difficult to see when a punch takes a fraction of a second. So subjective things like pace, aggression, poise etc all play a non-official role in scoring. Here we see Mayweather dictate the pace of the fight and showing ring leadership. We perceive aggression from Pacquiao because he comes forward more, the key way in which aggression is measured. But aggression can also be measured in punches thrown, although it's less striking as you can punch while backing up (like Mayweather does), and here we surprisingly see that it's Mayweather who threw more punches by a very tiny margin. While Pacquiao was clearly more aggressive, he threw nowhere near the normal rate he usually does, which gets him the win.\n\nSo why not? What prevented Pac from throwing volume? Mayweather is a master of defence, and has the physical advantage of length and more reach. This allows him to hit at a distance where Pac can't hit him, requiring Pac to lunge in and punch from a relatively less stable position. Mayweather can anticipate and counter, or move away. When he did get pinned down on the ropes, he carefully timed his exit and pivoted around Pac towards the center of the ring, where he can dictate the range of the fight. If Pac came in with too many angles preventing Mayweather from escaping, he'd go in for the clinch and pivot. After they break up, he's center ring again. By doing this, Pac's offence was neutralised.\n\nThat's mostly it. There are details, but that's the gist of it. \n\nMost people don't like watching Mayweather fight, they want to see a slapfest while Mayweather plays chess. Mayweather barely does combinations because combinations put you at risk of getting hit. Instead, he takes potshots, controls distance, his stamina, his position in the ring etc. That's why May's KO percentage is relatively low and why many consider him to be a boring fighter. The people that watch him do so because 1) he is unbeaten and they want to see if he'll get defeated or worse, KTFO 2) some are starstruck by his earnings and think he must be interesting to watch 3) he's a very complete and tactical boxer. Number (3) is pretty rare among mainstream people who watch one or two boxing matches a year, but it's the reason he is considered the pound for pound best fighter active today.\n\nAt the end of the day this is boxing, a sport with certain rules, within which he thrives. He's not the most exciting or powerful fighter, not the one who brutally beats people up. He is unbeatable by today's fighters within the parameters of the sport of boxing, but loses out within the parameters of most spectators.","label":0,"model":"human","source":"reddit","id":2831}
{"text":"My Dad was a TV weatherman, 1985 to 95 BBC South Today.\n\nFirst off I'm in the UK so things here may be different. He was on national radio for around 10 years then local TV for another ten years or so. My dad worked for 40 years for the Meteorological Office. A government job, we call that civil service here in the UK. When he joined the Met Office it was part of MoD Air (Ministry of Defence), I'm not sure if the Met Office is still an MoD department. He spent most of his early career working at military air bases(RAF). The end of his time with the Met Office was spent working at the Southampton Weather Centre.  A bit like a serviceman in the military we moved a lot. \n\nSo a working day would be spent in the office doing whatever weather forecasters do everything from wave height in the north sea (oil industry) special shipping forecasts for maritime industries, temperature data for the power companies. Special weather forecasts are real important to many commercial customers. The Met Office was one of the few governmental organisations to be profitable at the time in the UK. The BBC was another customer.\n\nAbout an hour before his shift ended he would go to the TV studio. I know he made notes to be sure he had a good idea of what he wanted to say. However it was unscripted and there was no teleprompter. So that's unscripted live TV, an expert talking.  It could never be scripted because he never knew how much time he would have on the live show. His slot was at the end of the newscast so if something over-ran he got less time. Sometimes the hosts would want adlib a bit, this would cut into his time. Sometimes he would have plenty of time because the show was running short and have to pad a bit. The cut off time was an absolute because the end of the show would be when the local TV went back to the national feed. \n\nSo if you are from the UK and watching BBC the weather presenter is a civil servant and is almost certainly unscripted. Quite what will happen in coming years is unsure as the Met Office has lost the BBC contract. For the first time since 1922! Over 90  years the BBC has had Met Office staff giving us our weather forecasts. yes I'm a bit salty about the whole thing, the BBC is going to rat s**t...","label":0,"model":"human","source":"reddit","id":2832}
{"text":"In American sign language, you use signs to show the other person what you're talking about, and then they use signs back to show you the answer. Grammatically, you normally describe something after you've stated what it is. So, adjectives typical follow nouns. In English, adjectives precede nouns. For example, in English we would say \"that big, brown dog is barking a lot\". In ASL, it's more like, \"that dog, big and brown, is barking a lot.\" This makes sense in a visual language because it gives you details after you have an idea about the subject. \n\nASL is like spoken English in that some nuance comes from words, but most comes from inflection, tone, and body language.  Asl is also more about describing and showing things, rather than telling. So, 'furious' is 'really mad', as a concept, right? The signer can explain that by signing the same sign differently (bigger gestures, more emphatic movements, more dramatic facial expression, etc). I find asl to be a lot more expressive than spoken English. When someone signs a story in asl, they often convey information they arent explicitly 'saying'. So, a story about an angry customer might portray the customers part as drunken and unsteady. In English, we could do this by putting on voices and acting out the story, but that's very uncommon. Otherwise, we can only convey this by saying it. \n\nIn ASL, much of the nuance (and even grammar!) is facial expressions. In fact, facial expressions are so important that when you watch someone sign, you watch their face and catch the actual hand gestures in your peripheral vision. And the grammar of the face can be pretty complex. For example, when you ask a question where someone picks between two options (as in, \"which do you prefer, hot chocolate or coffee?\" Which is signed more like \"you like coffee or hot chocolate, which?\") you lower your eyebrows and slightly narrow your eyes. But when you ask a question that's open ended (like, \"do you have any pets?\"), you raise your eyebrows. That all sounds very complicated, but it doesn't feel forced once you get the hang of it. \n\nAnd, no, there is not a sign for every English word. When there isn't a sign for a word, or the signer doesn't know if there is or not, they'll use fingerspelling to spell out the word. Fingerspelling is a system where every letter has a handsign, and you literally spell out the word. This would be time consuming and slow, except you get used to recognizing how certain syllables look in very quick motion (common endings like 'ing', 'ain', etc), as well as common letter combinations ('ly', 'st', 'sh', 'er', etc), and you start seeing groups of letters as movements instead of individual letter shapes. Some people are able to 'read' Fingerspelling as fast as you can read normal writing. In fact, a lot of people are only limited by the speed of the person spelling.","label":0,"model":"human","source":"reddit","id":2833}
{"text":"Egypt contrary to what the media would have told you shortly after Mubarek was taken out, was frail. They were already hurting from lack of basic supplies, gas, food, money...and when the revolution ended and Morsi stepped up to plate they expected real change. The people had thought they had forced the hand of the government and showed them they would not stand for inequality and an overall lack of livelihood.\n\nIn comes Morsi, and nothing has changed. In fact it was looking rather scary as Morsi was pushing through laws that left him all but a dictator. The Egyptian people took to the streets for a second time and were successful once again in taking out their own leader.\n\nHowever, the control is now in the army's hands. The single most powerful entity in Egypt. Think the United States army with free roam to do whatever they want without having to take orders from the President. Also the Egyptian army has what some are calling a monopoly on jobs and job creation in Egypt. The bad part about this is those working in these factories are part of the army and aren't getting paid anything to be there. Essentially it's like Apple being owned by the US's army and the soldiers are the ones designing the next iPad but not getting paid an engineer salary, just the base army pay. \n\nA law was just passed to stop any form of protest, or gathering from happening and it seems like another revolution, the \"final\" revolution will take place in Egypt. Hopefully, for their sake and the worlds, this will be the final necessary step to truly change the country for the better. If they are successful, they could become the single most important country when it comes to influencing people in westernized countries to take to the streets and create the change they want to see. \n\n**EDIT I posted this answer when this thread was pretty much empty and since then there's been some awesome responses that go more in depth into the situation that I tried to do with this one. I implore everyone to read deeper into the thread and read some of the more detailed answers if your looking for a longer summation of what occurred**","label":0,"model":"human","source":"reddit","id":2834}
{"text":"The royal family plays a largely symbolic role and most of their duties involve putting on social functions and entertaining esteemed guests. Technically they do hold some political power though. For instance, the Queen must give royal assent to a bill before it passes and becomes law. In that sense, the monarch has the power to veto proposed legislation similar to the president of the United States. Britain has its own Prime Minister, however, who holds a position that much more closely approximates the role the US President (as compared to the Queen). \n\nIt's also worth noting that other countries like Canada also require royal assent for new legislation and this is provided by the Governor General who is appointed to represent the British monarchy and to act on the Queen's best interests (side note: the Queen is referred to as the 'Queen of Canada' when she visits Canada). This relationship is largely a symbolic gesture however, the Queen (or rather her Governor General representative) would be very unlikely to veto any Canadian bills and Canada would probably just declare full independence from Britain if the Queen ever tried to over-step her boundaries and actually exercise any political control over the country.","label":0,"model":"human","source":"reddit","id":2835}
{"text":"People like to think that it's an animal trait to like to stay alive, and protect and defend themselves, and plants are just totally inert. But that isn't true. Unlike animals, plants can't run away and escape danger, but plants are every bit as opposed to being killed and eaten as any animal is. Instead of running, plants engage in physical warfare: spikes and tough exteriors and all kinds of other things, and chemical warfare: releasing a number of different chemicals in response to being attacked by an herbivore. These responses fall into three main categories:\n\n1. Direct defense. Some chemicals released by plants are intended to directly harm the predator eating it. Many plants, such as clover for example, use cyanide as their poison of choice. Sometimes, to prevent poisoning themselves by accident, they'll even compartmentalize their cyanide into a two-part weapon system, storing a harmless, nontoxic cyanide precursor inside their cell cytoplasms, and storing an enzyme in their cell walls that breaks down that precursor into active, deadly cyanide. Getting munched on by a herbivore breaks the cell wall and mixes these ingredients, poisoning the predator. Plants can also harm their herbivore attackers indirectly too, through things like producing an analog of the mating pheremones of the herbivore's natural predator.\n\n2. Local repair. Some chemicals that plants release when they're damaged, such as jasmonic acid, serve as plant hormones that signal the rest of the plant to brace and prepare for damage. Plants constrict their water channels to avoid losing water through their damaged parts, produce saps and sticky coagulants to block off the damage, produce antibacterials and antifungals to protect against infection, increase cell replication to heal faster, and start producing bitter, foul-tasting molecules that discourage herbivores from continuing to eat them, as well as enzymes that block digestion, making itself less nutritious.\n\n3. Remote signaling. Many of the same chemicals that direct plants to start repairing themselves, such as jasmonic acid, are also highly volatile, and signal neighboring plants to start bracing for impact and preparing themselves as well. In response to distress signals given off by nearby plants that are being eaten, plants will produce bitterants and digestion-blockers, making themselves unpalpatable to their herbivore predators. In fact, this is the reason that giraffes have to be nomadic creatures: you never see a giraffe herd strip a tree completely bare, because after munching on a tree for some time, the tree becomes bitter and inedible, and depending on wind conditions, other trees for miles around become so too. So the herd has to keep moving, trying to stay ahead of the chemical cloud of anguished screaming their leaf-munching inspires, in order to keep finding new trees which are still delicious and haven't yet hardened themselves.\n\nOf course, plants can't tell the difference between an animal's teeth and a lawnmower's blade, so against us, all their chemical screams, poisons, and distress calls don't do them much good, and make a pleasant summertime perfume for us instead.","label":0,"model":"human","source":"reddit","id":2836}
{"text":"Well, the reason why it's called the \"Third Reich\" is that Hitler was modeling it after the first two reichs. That's a German word that means \"empire.\" \n\nThe [Second Reich](_URL_2_) was run by an emperor called a Kaiser, which is a German word that means \"Caesar.\" \n\nThe First Reich sprang from the original Caesars themselves - the Romans, who (more or less) unified Europe into one body, which then carried on through the [Holy Roman Empire](_URL_1_). (If you want to get technical, the Holy Roman Empire was the First Reich - it merely inherited a sense of self-importance or entitlement from the original not-so-holy Roman Empire.)\n\nWhat's important to understand for all this is that \"Germany\" didn't really exist as Germany for most of its history - it was a collection of often warring kingdoms, duchies and principalities with names like \"Saxony\" and \"Prussia.\" There were also enclaves of ethnic German aristocrats who ruled towns or counties across Eastern Europe where most of the people were Slavs or Poles or something similar. \n\nSo one of Hitler's main political aims was to bring all these scattered places together into one country - Greater Germany - and to get the world to accept that these enclaves and bits and pieces of land that were nominally other countries actually belonged to Germany. That's why when Germany annexed the [Sudetenland](_URL_0_), there was enough international confusion that not everyone accepted it as an invasion. \n\nSome people there spoke German... and some of those German people were traditionally \"in charge.\" \n\nThe big plan was to create and consolidate a \"Greater Germany\" that would become the most powerful political body in Europe - a new European Empire (or something empire-like) that would rival the former world-controlling empires (like Britain, or Rome) or the upstart world powers (like the United States).","label":0,"model":"human","source":"reddit","id":2837}
{"text":"A lot of them are standard things used in farming -- pesticides, preservatives, etc. Others are intended to make the cigarette \"safer,\" by attempting to extinguish it if it's left unattended. Others are to make it burn at a certain rate, or to change the flavor profile. \n\nQuick point of contrast -- American Spirit cigarettes, which are promoted as Additive Free take 2X-3X longer to smoke than a regular commercial cigarette. \n\nBonus fact: I'm 240 days tobacco-free today!  \n\nEDIT: To answer OP's follow-up question -- This issue has been brought to the fore by electronic cigarettes, which use naturally extracted nicotine in a less harmful base. This is because the \"tar\" that accumulates in the smoker's lungs is naturally occurring in tobacco, and harmful despite being perfectly natural. Your assumption is correct though -- on its own, nicotine is about as harmful as caffeine. \n\n**Edit 2** A bunch of people asked for my How to Get Started with E-Cigs email, so [here's a link to the GoogleDoc](_URL_0_). \n\nAlso, I just want to say that I *loved* my cigarettes, and quitting tobacco this way has turned out to be much more enjoyable, less expensive, and all around cooler. You can *totally* do it. That said, if vaping turns out to give you some other kind of cancer, well, then I'm really very sorry about all that.","label":0,"model":"human","source":"reddit","id":2838}
{"text":"You may think morphine is easy and painless, but it ain't. The current cocktail of drugs for a lethal injection does three things: It causes unconsciousness, it decreases respiration, and it stops the heart. No breathing and no heart beat, and the condemned is pretty sure to be dead in a short amount of time. \n\nAn overdose of morphine isn't as reliable. Look at some of the famous people who have died from a heroin overdose: Janis Joplin, John Belushi, Chris Farley. Vomiting, convulsions, and a inexact method to determine how much of the drug will be needed to kill someone make it less than ideal. \n\nIt ain't about making the death pretty. It is about making sure that it works without causing undue suffering. Morphine overdoses aren't reliable and add to that you have the risk of someone not quietly passing, but choking to death on his own vomit, and you will see a lot of lawsuits filed claiming that the death was especially cruel. Get a lot of those lawsuits before the Supreme Court, and you see a suspension of the death penalty. That kind of defeats the purpose.","label":0,"model":"human","source":"reddit","id":2839}
{"text":"Here is the full explanation from The Economist: _URL_0_\n\nGROWN-UP SUMMARY:\n\nIn the aftermath of the plague, the royalty wanted to demonstrate to the public that it was cutting costs.\n\nIn the 1600s, British monarchs declared a very early version of the suit as the official dress of the court. It was halfway between what fancy people wore at the time and what commoners wore at the time. It was an attempt to make them seem down-to-earth.\n\nThis new attire was too tight for military use, so they made it progressively looser for riding and shooting. They also added a neckerchief, like modern soldiers, to keep out dirt and wear over their mouth and nose. \n\nThe military version influenced the civilian version, making it looser but still form-fitting. For instance, the neckerchief became the neck tie, and the soldier's coat became the blazer.\n\nELI5 SUMMARY: \n\nThere was once a terrible sickness. Everyone got sick and died. The Kings and Queens wanted the people of the land to know they cared, so they stopped wearing fancy clothes and instead made their old clothes look like the clothes of the commoners. The soldiers had to change their clothes too, for the King and Queen, but still needed to be able to fight. So they made their new clothes look the same but not so tight, and they added a neckerchief to keep the dirt out. The Kings and Queens liked the soldier's clothes so much, they made fancy versions like theirs. After doing this back and forth for hundreds of years, the \"fancy\" version of commoners' clothes now looks like a business suit.","label":0,"model":"human","source":"reddit","id":2840}
{"text":"It comes from a particular function : the exponential function.\n\nOk, so the exponential function is pretty unique. It is the only function other than the \"always-zero\" function that is always equal to its own gradient. That means that a lot of random things follow its shape, things where the speed of the process depends on the size of the process (chemical reactions, atomic ones, bacteria populations who double every fixed amount of time, etc etc.) Basically, it's the function that fits phenomenon that have a feedback loop built into them : the bigger it is, the faster it goes, and the faster it goes, the bigger it becomes.\n\nNow, e, the number, is just the value of this exponential function for an entry value of 1. It pops up a *lot* because it makes writing the exponential function very, very easy : exponential of x (exp(x)) can be written e^x\n\nPlus, the \"always equal to its gradient\" bit means that the exponential function is the cornerstone of every technique we have to solve differential equations, so it pops up in a lot of abstract math too. \n\nAnd it can also simplify a lot of work on complex numbers, which are a powerful tool to work on a whole other class of problems","label":0,"model":"human","source":"reddit","id":2841}
{"text":"They're working on impossibly hard problems like:\n\n - Is every even number the sum of two primes?  \n\n - Start with any natural number x.  If it's even, go to x \/ 2.  If it's odd got to 3x + 1.  If you do this the cycle 1, 4, 2, 1... repeats forever.  Are there any other cycles? \n\n - ~~A Pythagorean triple is three natural numbers so a^2 + b^2 = c^2.  Are there any four numbers such that a^2 + b^2 + c^2 = d^2 ?~~  (Oopsies.  I did *not* think this one through.  Hat tip to \/u\/Usion and \/u\/RelentlessPessimist)\n\n - Corrected: A Pythagorean triple is three natural numbers so a^2 + b^2 = c^2.  If you make a triangle with lengths a, b and c, it will be a right triangle.  Now put two of those together to make a rectangle.   Can you make a cuboid (brick shape) from rectangles like these?\n\n - Pick any number N, no matter how big.  If you start adding up the fractions 1\/1 + 1\/2 + 1\/3 ... the sum will eventually get bigger than N.  This is called divergence.\n\n-  When the gaps in a sequence 1, 3, 5, 7 ... are all the same size it's called an \"arithmetic sequence\".  For any arithmetic sequence, the sum of reciprocals (1 + 1\/3 + 1\/5 + 1\/7) also diverges.\n\n- Corrected: If the sum of reciprocals of a sequence diverges, it seems that you can always find arithmetic sequences inside (at least three numbers with equal gaps between them).  Is this true?  If it's true, can you always find longer sequences?  How long? (Hat tip to \/u\/Spetzo and \/u\/FUZxxl )\n\n- When sequences do not contain an arithmetic sequence (e.g. 1, 3, 9, 27, 81...), the sum of the reciprocals do not seem to diverge.  Are there any exceptions to that rule?\n\n - Pi is irrational and e is irrational.  Is pi * e rational or irrational?\n\n - What is the [largest sofa](_URL_0_) that can fit around a 90-degree L-shaped corner between two hallways of equal width?\n- We know you can fit one that's 2.2 times bigger than the width squared.  And we know that 2.83 times is too big.  \n\n\nRecently solved very hard problems include:\n\n - Imagine you are coloring countries on a 2-dimensional map.  How many colors do you need to keep the shapes distinct?  You usually need three and sometimes you need four.  Do you ever need five colors?\n- No.\n\n - Go back to Pythagorean triples (a^2 + b^2 = c^2).  Finding triples a + b = c is easy, Pythagorean triples are a little harder but not too hard.  Triples that satisfy a^3 + b^3 = c^3 seem impossible to find, but are they?  What about other powers (a^n + b^n = c^n)?\n\n- There are no triples (a, b c), if n  >  2.\n\nbut the proofs are very long.","label":0,"model":"human","source":"reddit","id":2842}
{"text":"Many studies have found that dogs are able to not only read your body language but also read your facial expressions. So if you hurt the dog, but didn't mean to do it, then he will see your compassionate facial expression and know that you aren't angry at him and obviously didn't mean to do it. Conversely, if you are angry at the dog, he will read your face and immediately react by cowering or looking down at the ground (I've seen some dogs with 'bad self-esteem' that don't receive any positive reinforcement, that pee themselves even when their owner just yells at them, without even hitting the dog :(\n\nBtw, since my dog (12y\/o Pekingese) became deaf about a year ago, I have since had to use hand gestures much, much more, and today I can simply gesture for him to 'come inside' with a hand gesture after he has peed, to get him to come inside. It's amazing that a dog of his age can still learn new things. To my knowledge, dogs are the only animals on earth that can understand gestures like pointing (without prior training), and also read human facial expressions, so I truly believe that as long as you are compassionate with your dog, he will forgive and understand if you accidentally hurt him.\n\nEdit #1: Here is a study that shows that Elephants CAN understand pointing without prior training also: _URL_0_\n\nEdit #2: Here is a study that shows that Chimpanzees CAN'T understand pointing without prior training: _URL_2_\n\nEdit #3: I've received messages asking for a photo of my 12 y\/o Pekingese, but I'll do you guys one better... Here's a vid I took just now especially for you guys: _URL_1_ (Reddit meet Ollie, he loves to talk, even-though he's deaf)","label":0,"model":"human","source":"reddit","id":2843}
{"text":"Speaking as a barrister, I can say the following. \n\nIn essence, a good lawyer is one who will do the work. They will go the extra mile and put their heart and soul into their cases. They will read all the case law, know all the facts and essentially be able to explain every factor that helps or hinders them.\n\nTruthfully, the facts will win 90% of cases. 'Bad lawyering' or as we call it, poor advocacy will lose 5% of cases. This is due to someone making a critical mistake, failing to appreciate how important a piece of evidence is or generally doing something silly.\n\nLet's take this famous textbook example of one question too far. One question too far is when you have successfully created doubt in the jury (or trier of facts) mind and you ask one extra question which destroys all the credibility you bought. For example, we want to prove that there was no way that the man could have seen the assault\n\n* Where were you standing when you saw the fight?\n100 meters away\n* Do you wear glasses?\nYes I do\n* Were you wearing them that day?\nNo I wasn't\n* How is your eyesight?\nIt isn't that good\n* What time of day was it?\nIt was night\n* So without good eyesight and at night, you made an identification of the accused?\nYes\n\nTHAT'S WHERE YOU STOP! You have totally made your point. A bad advocate gets smug and tries to rub it in. This is a cruel lesson we all learn with experience. That extra step can end up here:\n\n* So how could you POSSIBLY confirm that the person you saw all the way over there was the accused?\nBecause after he assaulted him, he screamed, \"IM RICK JAMES BITCH and came running past me within meters of my face.\n\nThis example is of course over the top, but illustrates something a 'bad' lawyer would do.\n\nGood advocacy will win 3% of cases. These are cases where the facts are not in your favor, but a smart advocate can see an avenue that nobody else thinks to look. \n\nOne of my favourite examples is when I had a case where a client stole a stop sign from an intersection. He was caught red handed, made a confession and had the sign in his possession.  All the elements of the larceny had been made out, BUT, at no stage did the prosecution show who owned the stop sign. It's a logical oversight, obviously the council owned the stop sign and an affidavit from a person from the council highlighting that fact would have proved it in short order. They didn't do this.\n\nFor an object to be stolen, it has to have an owner. By proving that this object didn't, we asserted that it had been abandoned. I got a judge who agreed with me and we won.\n\nTo sum up, I wouldn't say that I am a particularly good lawyer, but if you get a lawyer who is prepared to do the work and acts like your case is the most important one in the world (because to you, it really is) and has the skills to back it up by looking at the problem through different lenses and doing the research necessary to argue the appropriate angle, then you have a 'good' lawyer.","label":0,"model":"human","source":"reddit","id":2844}
{"text":"Unions aren't some broad\/nebulous construct, different ones do different things, and have different pros\/cons.\n\n**Construction Unions** represent individuals that would otherwise be treated as individual companies\/contractors by major construction companies. The union negotiates standard rates\/fees for various services (bricklaying, sheet-metal work, etc.) so that individual workers don't engage in a race-to-the-bottom for prices. In big cities unions will also function as a job-placement service, so that elevator repairmen\/electricians\/etc can spend their time doing their job instead of looking for a job. The major drawback has to do with contract negotiations and cost controls. For example, a corporation may want to build a new building in a city but is forced by the cities rules to hire a certain number of union construction crew. This reduces their flexibility in being able to hire people at a lower cost. Union construction workers also have pretty stern regulations and limitations as to what they can\/can't do. Much of what the unions fought for in the latter half of the 20th century, such as workplace safety, has now been made law. However, there are a plethora of rules\/regulations that can frustrate contract managers. For example, a pallet of brick-laying material may sit idle for weeks on a construction floor because only a union employee and move it or work with it.\n\n**Teachers Unions** serve to negotiate broad\/standard contracts with local school districts or school boards. They help standardize pay and benefits for all teachers, and act as a firewall against aggressive school leadership that may want to fire a teacher without cause. The downside is that most teachers unions have adopted an \"innocent until proven guilty, but still innocent anyways\" approach at protecting their members. This has frustrated many jurisdictions, most recently Washington DC. They attempted to fire a slew of teachers they considered \"bad\" for a variety of reasons, but the Union took them to court and won.","label":0,"model":"human","source":"reddit","id":2845}
{"text":"An image is recorded as a high number of still frames per second, which are then played at high speed to give the cognitive impression of movement.\n\nThink of it this way: \n\nImagine that you've got Bruce-Lee like reflexes. Someone tells you that they are going to take 4 photos of you in one second, and they want you to raise your arm to shoulder height, and then lower it back to your hip as fast as you can.\n\nBeing so fast, you can do this exactly four times per second.\n\nThe photographer shouts \"Go\" and your hand raises to shoulder height - the first photo snaps.\n\n1\/4 of a second later, your hand has gone back to your hip, and back up to shoulder height. The camera snaps again.\n\nThis happens 2 more times.\n\nNow, to anyone who looks at those photographs, it will for all the world, appear that you stood there with your arm at shoulder height for a second. They will never know that you lowered your arm four times.\n\nSo, lets replay this experiment, except you can only raise your arm **three times** in one second.\n\nSo, you raise your arm, the camera snaps *just before* it gets all of the way up. \n\n1\/4 of a second later, and you're lagging behind even further. The picture is taken, but now your arm is slightly lower than in the previous picture.\n\nThe next one, even lower.\n\nAfter the pictures are developed, it will look like you have taken one second to lower your arm from just below shoulder height, to about 3\/4 of the way to your hip. They will not know that you actually made the complete up-down cycle 3 times in that second.","label":0,"model":"human","source":"reddit","id":2846}
{"text":"The person is unconscious not asleep. Asleep is a pattern in neurons firing rate, direction and cycle in specific regions off the brain, like the ascending reticular system. \nAnesthesia uses 3 bases, unconscious, analgesia and muscle paralysis. For gas anesthesia, we actually don\u2019t know how it works in molecular levels. In total venous anesthesia we do know. For this kind of anesthesia the most common drugs are Propofol for\nUnconsciousness and it works on chlorine channels in neurons making them hard to fire upon a stimulus, like auditory or visual stimulation. For analgesia we use remifentanil working on opioid receptors to make the neurons hard to fire on noxious, pain, stimulus. And for muscle relaxation we use rocuronium that blocks the sinapses between neurons and muscles connecting in the receptors in the muscle end on the sinapses \n\nThe gas anesthesia with sevoflurane we really don\u2019t know how the gas block the pain, give unconsciousness and some muscle relaxation. Several theories speculate that it modify the neurons in some way that make it hard to fire on stimulus. There are a beautiful example destroying this theory on YouTube when scientists uses gases to numb a plant that contract its leaves on touch. Since plants don\u2019t have neurons, how it works remains a big mystery\n\nI\u2019m an anesthesiologist","label":0,"model":"human","source":"reddit","id":2847}
{"text":"Even though the typical English-speaker doesn't think about it, there is a typical order in which adjectives are listed that \"sounds right.\"  There is no logical reason for the order, but it's something that an English-speaker would be familiar with and hear regularly.    [Here's a website](_URL_0_) that describes the \"standard\" order for adjectives to be included:\n\nDeterminers \u2014 articles and other limiters. (e.g. a, the)\n\nObservation \u2014 postdeterminers and limiter adjectives (e.g., a real hero, a perfect idiot) and adjectives subject to subjective measure (e.g., beautiful, interesting)\n\nSize and Shape \u2014 adjectives subject to objective measure (e.g., wealthy, large, round)\n\nAge \u2014 adjectives denoting age (e.g., young, old, new, ancient)\n\nColor \u2014 adjectives denoting color (e.g., red, black, pale)\n\nOrigin \u2014 denominal adjectives denoting source of noun (e.g., French, American, Canadian)\n\nMaterial \u2014 denominal adjectives denoting what something is made of (e.g., woolen, metallic, wooden)\n\nQualifier \u2014 final limiter, often regarded as part of the noun (e.g., rocking chair, hunting cabin, passenger car, book cover)","label":0,"model":"human","source":"reddit","id":2848}
{"text":"Conservative republicans in the house of representatives are demanding a budget that cuts all funding for planned parenthood, because they feel that it subsidizes abortions even if not directly paying for them.  A lot of it is that Tea Party anger, and trying to make a stand.\n\nBoehner is worried about the Republican Party in general, as a whole and not just about the Tea Party wing which is more conservative.  He's afraid that a government shutdown, and a refusal to extend the federal borrowing limit will make the Republicans look bad, and possibly contribute to them losing the upcoming presidential election.  (they lost support after the last attempted shutdown)\n\nIf he can keep up the bipartisan cooperation that used to be the norm for the U.S. government long enough to pass a \"clean\" budget that doesn't de-fund Planned Parenthood, and maybe pass a debt ceiling extension he will hopefully save the GOP from looking really bad to the general population as we gear up for the 2016 elections.  Doing this will also really piss off the conservative wing of the party (Tea Partiers, etc) and he's just sick of dealing with their shit.  So he's ready to drop the mic and walk away (or run for a different office) but he's going to try to save the party's image first","label":0,"model":"human","source":"reddit","id":2849}
{"text":"You know how in action movies, when a guy is talking and looks down to see a little bright red dot moving around in the middle of his chest -- suddenly realizing somebody is about to shoot him?   \n\nNow, let's say that dot was big, as big as the guy himself -- the guy looks down and sees his entire body is red, meaning he's about to get shot.\n\nNow, let's say instead of red light, it's *invisible* light - infrared or radar.  The guy has been shot too many times to ignore the invisible light, so he wears a sensor on his hat that detects if the invisible light is ever pointing at him.\n\nCertain missiles illuminate their target with invisible 'light' to stay on target -- and planes have detectors on them to tell if they're going to be shot by detecting if they're being 'lit up'.\n\n*Edit:*  Jesus christ, everyone is hung up on my imaginary example.  When I use the words 'infrared' and 'invisible light' I am describing a guy with a sensor on his hat trying to not be shot by an unseen sniper who has a targeting illuminator.  *It's not real, it's an example that I figured a five-year-old would understand*.","label":0,"model":"human","source":"reddit","id":2850}
{"text":"The water your dehumidifier produces is actually drawn from the air around it. The air contains water vapor that moves into the air from the earth around the walls and floor of your basement. Concrete absorbs water fairly easily. The water moves through it and eventually evaporates into the air on the other side.\n\nYour dehumidifier uses a cold coil of metal that attracts the water vapor in the air to condense onto it (Similar to how a cold glass of water gets wet on the outside of the glass on a warm humid day). As the condensation builds up, it drips down into the catch basin.\n\nIf you did not have a humidifier, the water vapor would remain in the air (resulting in higher humidity), and would likely condense over time onto other items in your basement. The dampness would promote the growth of mildew and mold (both things which like to grow in damp areas) and over time become a health hazard, not to mention also possibly damaging anything stored in your basement.\n\nAs an aside... Most dehumidifiers have an outlet that you can attach a hose too. I have a hose attached to mine, which then run to the basement drain. Assuming you have a basement drain, this might be a good way to avoid having to empty your dehumidifier bucket every day.","label":0,"model":"human","source":"reddit","id":2851}
{"text":"So there is a good reason to keep certain flavors out of tobacco products, namely because they make them more addictive. Take the cooling refreshing flavor of menthol...menthol is an TRPM8 receptor agonist which triggers a cooling sensation. This cooling sensation reduces the pain associated with inhaling smoke. However menthol has other pharmacological properties aside from taste. It actually blocks the metabolism of nicotine as it uses the same enzymatic pathways for breakdown, thus keeping the nicotine in your system longer and at higher levels.\n\nELI5: The flavor menthol makes you taste refreshing minty flavors but it lets you inhale more deeply and gives you a longer bigger high than just nicotine alone.\n\nLevulinic acid was added to cigarettes to add a nice caramel flavor to them. However this and other acids reduces the pH of cigarette smoke and desensitizes the upper respiratory tract allowing you yet again to breath in more deeply. Even more interestingly however it also binds to the nicotine receptor and synergistically increases the binding of nicotine to the receptor increasing the effective dose of nicotine you experience.\n\nELI5: The flavor caramel (Levulinic acid) makes nicotine way more effective in your brain and lets you breath in more smoke giving you a stronger high.\n\nMany many chemicals placed in cigarettes called flavorants act this way all relating increasing the amount of nicotine in your system and making it more addictive. The tobacco companies were able to test these things not only on thousands of rat studies but millions of people for decades. That is an awesome control group, so they found many really cool ways to increase the dose of nicotine under the guise of flavor enhancement.\n\nTL;DR: Lots of flavors actual make nicotine more addictive and thus keeping them out and not checking every single one to see if it is bad for you was a safer course of action.","label":0,"model":"human","source":"reddit","id":2852}
{"text":"The answer requires accepting that according to quantum mechanics, atoms and molecules can only have specific energies, called energy levels. You can be in energy level 0, 1, 2, etc, but higher energy levels require the atom to take in energy from the surroundings such as from heat. At absolute zero there is no heat, so every atom or molecule will be in energy level 0 at absolute zero (we call this the ground state). But the funny thing about quantum mechanics is, it teaches us that everything has what is called \"zero-point energy\". This is the energy of the ground state, and you can never have less energy than this. For a very light atom like helium, the small amount of zero-point energy is actually enough to make the atoms move around a lot. This is called zero-point motion. (All things experience zero-point motion, but for bigger atoms or molecules these motions may be too small to make any noticeable difference). For helium, these zero-point motions prevent the atoms from coming close enough together to solidify, so even at absolute zero it exists as a liquid. \n\nHowever, if you compress the liquid helium enough, you can form a solid,  but only at pressures above about 25 atm (25x atmospheric pressure) for helium-4, the most common isotope. Even in the solid form there are still large zero-point motions, so it doesn't behave like a classical solid where every atom has an exact fixed location. Because of this, solid helium is referred to as a quantum solid.\n\nSource: 5th year graduate student using quantum calculations to model solid helium.","label":0,"model":"human","source":"reddit","id":2853}
{"text":"**As people have pointed out, this explanation is only a part of the story. There are many factors causing aging through stress, and many factors causing stress, I am basing this explanation on what I know. If you need more information I will do some more research and try to post a more in depth explanation taking into account as many factors as possible. I do not intend to mislead anyone**\n\nOur body is made up of trillions of cells, which in turns carries chromosomes. At the end of each chromosome, there is something called a telomeres. \n\nEvery time I cell divides, the length of its telomeres become shorter, making its life a little shorter too. \n\nAnything that causes our cells (our building blocks) to age will in turn make us age too. So, the shorter the telomeres in our cells become, the older we get. \n\nWhen we get stressed, we overproduce our hormones. There is an observed correlation between higher hormone levels and shorter telomere length, and therefore a shorter cell life. \n\nBasically when we produce too many hormones through stress, our cells age a little quicker because the telomeres become even shorter after dividing, and when our cells age quicker, so do we. \n\nSource: took a human biology class last year","label":0,"model":"human","source":"reddit","id":2854}
{"text":"[It's hardwired](_URL_1_). Whether there's anyone there or not our brains get that \"sixth-sense\" that we're being looked at when we're feeling exposed, around people whose eyes you can't see, or even paintings whose eyes are aligned just right. You can look at it as an evolved \"early warning system\" that makes us more aware of our surroundings, but more study is needed to determine if it's always been there or if it's a more recent development. Walking by a large bush at night and getting an eerie feeling would make you more prepared if something did actually jump out at you. And to make it even more interesting, as the article states: someone with autism has a harder time knowing if someone is looking at them even when, to a non-autistic person, it's perfectly obvious, or someone with social anxiety is positive they're being watched (not necessarily looked at) when, to a non-social anxiety person, it's obvious they're not.\n\n*Edited Disclaimer*: The next paragraph is mostly anecdotal (or \"bullshit\" according to some commenters =) ), about my experience with this feeling of being watched as someone with pretty intense social anxiety. At the time I wasn't expecting to be the top answer so I didn't cite more sources, I apologize. It's not completely understood by neuroscience, whether it's learned or genetic, and is treated as part of social anxiety. [_URL_2_](_URL_0_) has many, many threads about different people's experiences.\n\nSimilarly, there's a disorder called [Scopophobia](_URL_3_), \"an anxiety disorder characterized by a morbid fear of being seen or stared at by others\". While I'm not crippled by it as a phobia demands, I do have social anxiety about having too much attention drawn to myself, and get intensely uncomfortable around cameras. I used to take very long (3-4 hours) midnight walks and it took me a lot of practice to stop being hyper-aware of bushes or people's houses, living in a crowded city is not always pleasant, so being unseen for a while is the only way to really relax.","label":0,"model":"human","source":"reddit","id":2855}
{"text":"Unlike people in this post are saying, it's not because it's \"more efficient\" or \"because it actually works\". It's due to a lot of historical events. Capitalism is global because capitalism countries won the ideological war against the other systems, to put it simply.\n\nThe Bourgeoisie won over the French Revolution and changed the world's politics because of that. They adapted the previous representative system that kings used to listen to people into the modern concept of representative republic (more on it in this video: _URL_0_ ). In the process, they also obtained control over the means of production (such as lands), and the system they devised also excluded most of the population from the political process.\n\nHaving control over the means of production gives the controllers A LOT of power over other people's lives. Economic power and political power are directly correlated, and capitalism favors the concentration of economic power in the hand of a few. That creates a vicious cycle, where people with more power can acquire even more power. If you try to overthrow them, you'll find yourself fighting against the monopoly of force. It's beneficial to the people in power for the system to continue operating, and that's why it still operates, and why there's so much propaganda on \"it working properly\".\n\nI know people will come and say \"ok, so if communism is better why didn't it won over capitalism on the USSR?\". That also has some historical explanations: Marx himself believed that capitalism made industrial development a lot more efficient, and when he talked about implementing communism he was talking about doing it in fully developed industrialized countries. Russia was an agricultural country back at the times of the revolution (and yet, in just some years, it was about as industrialized as the rest of the world, in a much shorter timestamp). Nevertheless, communism is also the control of the means of production by the hands of the *workers*. USSR had the means of production in the hands of a *representative republic*, which can be easily be controlled by private interest. The actual workers were still alienated from the value of their work. That is, USSR's communism is not that far away from the capitalist system, and some social scientists, such as Noam Chomsky, call that system a \"State capitalism\".\n\nWhy do I talk about propaganda? Because capitalism doesn't \"work\". It just generates value in the hands of a few and drives industrial progress towards that goal, but that by no means is inherently good. We're all seeing the effects of the industrialization on the environment. We all see that people still die of hunger every day. Unemployment rates are getting to an absurd point, because industrialization is driving automation for efficient profit, and that has as a consequence that less people need to work.\n\nI don't wish to imply communism is the solution for such problems. I think my point is that a good economic system should be fit for people in general, and not for those in power. Communism tries to address that, but it has its own set of criticism among other socialist authors (such as Bakunin, Kropotkin, or Bookchin).\n\nRojava has an interesting experiment in a truly democratic society, inspired by the work of Bookchin, where economy is planned to benefit people in general, not just private interests. It is working well, even if you consider they are in a state of war against the daesh.\n\n\nEDIT: I'm having to argue over and over and over and over again on how socialism doesn't imply central planning, and I'm tired of it, so please, PLEASE, read about more socialism models than the USSR model. Please. This is an example: _URL_1_\n\nIt's by no means the only one.\n\nEDIT 2: Thanks for the gold, anonymous stranger! I believe I could have worded this answer a lot better if I had more time for research, but my point is that most capitalist apologists completely ignore both the moral grounds for capitalism (which Weber did a great job on writing about it) and the historical reasons on why it became so pervasive (which Marx and Chomsky also wrote very well about).\n\nEDIT 3: while I consider myself an anarchist (not a communist or marxist - although I do like Marx's historical analysis), I find it funny that, even though I explicitly stated that I don't wish to imply communism is the solution for the problems of capitalism, most capitalism advocates are still insisting in pointing that \"communism failed and capitalism is better\". So... thank you to prove you have not read the post, I guess?","label":0,"model":"human","source":"reddit","id":2856}
{"text":"We're approaching the point where we have to reevaluate some of our very fundamental notions about work.\n\nSomeone in the thread mentioned trucking?  We are going to have fully automated trucks before much longer.  Retail is already slowly dying as Amazon and others chip away.  Eventually Amazon will automate their entire supply chain, including their warehouse workers.\n\nSomeone else mentioned welding, or steel work?  Robots are already chipping into a lot of those jobs.  Sure, they might require human oversight for now, but they won't in 30 years.\n\nEventually we're going to have a society where virtually everything that's currently done by lower and middle class workers will be replaced by what are effectively computer\/robot slaves.  And only a very few percent of those people they replace have what it takes to even learn how to write software or design robots.\n\nUnlike previous generations, where \"buggy whip\" workers just retrained as automobile workers, we're just slowly eliminating the need for \"work\" entirely.  And that's going to have profound effects on our sense of ethics, economic policy, and self-identity.","label":0,"model":"human","source":"reddit","id":2857}
{"text":"I was reading or watching somewhere that laughter is actually an evolutionary survival response to stress. \n\nAs in; Caveman 1 hears a rustle in the bushes, Caveman 2 and 3 wait with spears ready. Caveman 1 investigates, and discovers that it's two badgers humping. Laughter. \n\nLaughter evolved as an evolutionary \"pressure release\" from stress that is easily interpreted as *all clear, no danger here* Laughter is a feeling of perceived safety from a stressful situation. That's why it's easy to laugh when the guy on the tv gets kicked in the nuts. Ha Ha, I'm safe and so is he, really. He might hurt, but he's safe. And why we don't (normally) laugh when we see videos of people doing handstands in the street and getting demolished by an 18 wheeler. It's not funny anymore because the sensation of \"safe\" no longer exists. (some people have distorted this feeling, and that's when we say they have a \"sick sense of humor\" because they CAN laugh at people who faceplant into concrete when their bungie cord breaks. It's a over-selfishified feeling of \"safe\")","label":0,"model":"human","source":"reddit","id":2858}
{"text":"Wow. I don't feel these answers really address some of the issues. \n\nOne of the most numerous reasons that English has silent letters has to do with how languages change over time. It's like, when you are younger, you play with a lot of toys, but as you get older, some of those toys disappear, or maybe you left them at your neighbor's and now he's playing with them. Or maybe you just gave up the toy because you didn't like it anymore. \n\n\nEnglish actually works like that. Over time, as more and more people pronounce the language, there are different mechanisms that are in play to change the sounds of words. Sometimes it's education of the poor. As they become educated, their way of spelling begins to enter the realm of writing and influence how words are pronounced. Sometimes, one group of people stops talking one way (Maybe the dutchess has a lisp, and in order to be more like her, people start saying their ~~F's differently.) So one group is saying F's like you and I might expect, but another group is saying them like V's.~~ - BassNectar pointed out I have an error due to F and V not being affected by lisps. So I'm correcting with the following scenario: \nInstead, imagine the Dutchess has a lisp and it changes the way she pronounces her S's. People who look up to her might imitate her, where her neighboring dukedom will retain the older non-lisp pronunciation. \n\n\nOver time, sounds can deteriorate. In the case of Knight, which was at one point pronounced with an actual K sound. In fact, English had a lot of words with interesting beginning sounds. Kn, Gn, Hn, and a few other sounds were all pronounced. But English has gone through some sound shifts, and while these sounds are shifting, sometimes people just stop pronouncing sounds. K-Nickt was a more accurate representation of what Knight once sounded like. The definition has also changed. Knight first meant \"Serving boy\" but over time, it changed to mean something like \"soldier who serves a king\". A similar word in meaning \"Wench\" once meant serving girl. It still kind of does, but it's also got a touch of sexual immorality tied to it. \n\nSometimes people just decide sounds are too hard without thinking about it. Consider when you say to your friends \"Iunno\" or \"whatcha doing?\" In these instances, we've sacrificed the D and clear T sounds. As we say certain things often enough, we start to make them easier. Our language is evolving to fill these phrases to have slightly different meanings than their completed counterparts. Or their completed counterparts just kind of disappear. Many words ending in \"ly\" (Godly, womanly, etc) started as compound words with the \"ly\" being \"like\". Godlike, womanlike, etc. But over time, they shortened it to just an ly sound. Spelling changed to reflect that. Now, a few centuries later, we've forgotten, and have started using words like \"Godlike\" without realizing \"Godly\" is exactly the same thing that fits the same function. \n\n\nBasically, English has silent letters because the language is constantly changing for various reasons. As the language change, letters get left behind, and we just learn to stop saying them, even though we see them in words. Sometimes they stay to help show the difference between a homophone (word that sounds the same as another, but is spelled differently). Sometimes they stay because it's easier to leave letters than to try to change the way people already know how to spell.","label":0,"model":"human","source":"reddit","id":2859}
{"text":"Tax exempt status is allowed for nonprofit organizations that are operated exclusively for religious, educational, scientific or other charitable purpose. There are a few more rules, but those are the two big requirements: (1) nonprofit and (2) religious, education, scientific, or charitable.\n\nAs you can see, churches don't get special treatment over other nonprofits. However, you might still ask why \"religious\" organizations are included at all. One reason is that it helps maintain separation of church and state. As the Supreme Court noted in one of its earliest decisions regarding a national bank, the power to tax is the power to destroy. By not taxing churches, the state removes itself from interfering in church business and also makes sure the state doesn't start relying on church taxes for its operation.\n\nAnother reason is that historically churches were important charitable and social centers. That's still true for many churches today, but it was even more apparent when the tax laws were first being made.","label":0,"model":"human","source":"reddit","id":2860}
{"text":"To cats, petting feels like grooming. Cats groom by licking themselves to stay clean, they self-groom to comfort themselves when they're upset (displacement grooming), and they also groom each other as social contact. Cats aren't pack animals like dogs, but they are still social creatures. Mother cats groom their kittens, and friendly cats will groom each other to bond\u2014especially nice-feeling because they can wash the other cat's head\/neck\/ears, which are very hard to lick clean properly on your own body! (A lone cat will lick a paw and use it as a face washcloth.) The mutual grooming also mingles the cats' scent, which makes them feel comfortable; \"This is my space \/ my community, we smell like each other.\"\n\nSo to a human-friendly cat, being stroked\/petted by a friendly human: reminds the cat of being groomed soothingly by its mother; mimics social grooming and gives the cat a nice scratch on itchy head-places it can't reach; and spreads the cat's scent on you  &  vice versa, putting you together as a community.\n\nNote that petting the cat doesn't really properly clean it and leaves human skin oil all over the fur, which is why a cat will go into ecstasies at being petted and then promptly lick smooth all the fur that hands touched. This is also why cats hate being strokes against the fur \"grain.\" Also also, when a cat really likes you, it may return the grooming favor, with it's rough sandpapery tongue on your tender skin. Ouch!","label":0,"model":"human","source":"reddit","id":2861}
{"text":"It is a combination of three factors. First one is anatomical.  Unlike animals whose vocal folds are found at the larynx, the equivalent organ on the birds, called the syrinx, is found on the trachea fork and spreads to the both bronchus branches. This gives the birds the ability to produce multiple sounds at the same time( in a way they have stereo sound production compared to mono mammals). Due to this, parrots have the anatomical ability to mimic human voice\/words without requiring a human like larynx or lips or tongue. \n\nSecond one is intelligence. Parrots are very intelligent birds. Intelligence allows an animal to be more \u201cbehaviorally plastic\u201d and let them behave in ways that they dont necessary do in the wild. Plus, parrots can comprehend the meaning of words they speak. So it is not pure mimicking. They, to some extend, know what they are saying and can express them selves. This makes the difference between mimicking and talking. Alex the African grey parrot is the only non human animal to ask a question. He was trained on identifying the colors, numbers and shapes or certain objects. One day he asked his question what color he was and according to the story learned he was gray after it was ~~related 3 times~~ repeated 6 times.\n\nThe third and last piece is being social. In the wild parrots live in large flocks. It is important for a parrot to memorize and mimic the unique calls of its flock. This is how members of the flock can find each other and remain together. When we humans become their flock, they have an instinctive urge to memorize and mimic our unique calls, same way they do for their flock. Anyone who owned a parrot knows they love talking when you shout at them from another room, but they tend to be much quieter when you are in the same room. In nature the multiple flocks can share the same area, calls unique to each flock allow each individual to find its flock. When a parrot can\u2019t see their owner, they make sure that the flock-mate can still hear the unique calls of your flock so two of you won\u2019t lose each other.\n\n As a side note, parrots are not the only birds that can talk. Many corvid species can also speak, although their words are not as clear as, or easy to understand compared that of parrots.\n\n\n**Edit;Thanks for the upvotes and the gold guys. Its great to see people are interested to learn about these great birds.**\n\nFew little corrections; as some comments pointed out, Alex is the first animal to ask an existential question. This basically means that he asked a question about its own existence. I am sure there are other animals that asked questions, mainly in the form of request, permission etc, but Alex is the only animal to ask a question concerning himself. It is often considered as an significant event because it indicates the existence of \"theory of mind\". Simply, Alex not only Alex showed a high level of self consciousness, he also showed an understanding that other concision beings exist and they might now the answer of a question that he didn't know. I must point out that there is some criticism about how much the question Alex asked was sincere and how much was it him just repeating what he was trained for all his life (Alex was the subject of an experiment where he was trained to identify the shape, size, number and color of objects). Although Alex's ability to ask questions is documented many times.\n\n\nSecond point I want to make is about I might have went a bit overboard with the sentence \"Plus, parrots can comprehend the meaning of words they speak\". A better way of saying this is they can use words with context. They at best have a very limited understating of grammar and even when tif they can form sentences, the grammar is often broken. But Alex did use some simple, grammatically correct, expressions. He would use the term \"wanna go\" and completed it with where he would like to go, like wanna go back, and he would say \"wanna banana\" when he wanted a banana. He could use personal pronouns and spoke differently when referring to himself or others. He also showed the ability to generate words of his own. He was not familiar with apples, when he was introduced to apples, he named the apple \"banerry\". A mixture of banana and cherry that he was familiar with. \n\nIt is important to note that Alex was an exceptional smart parrot. He was probably a genius in gray parrot standards. Since his death 10 years ago, we couldn't find another gray parrot that had an equivalent level of abstract thinking. Wiki page for alex gives a good summery of his accomplishments, some of which are very impressive;\n\n_URL_0_","label":0,"model":"human","source":"reddit","id":2862}
{"text":"To reduce the \"BANG\" of a gunshot, you have two things you have to fight:\n\n1. The explosive gasses\n2. The supersonic crack of the bullet\n\nTo reduce the sound of the explosion, the suppressor has a series of chambers that slow the expansion of the explosion. As the gasses hit each section, the gasses have to squeeze through restrictions then expand again on the other side... This takes the energy out of the explosion in steps.  Imagine the difference in sound between popping a balloon with a pin and letting the air out slowly. Same volume of air is moved, just less violence. \n\n & #x200B;\n\nNow that still leaves the supersonic crack. Picture a whip, that crack is the tip of the whip breaking the sound barrier. So anything moving fast enough will create that crack. You can eliminate or reduce this by slowing the bullet down. Example, a 9MM round that has a 115 grain projectile with 6 grains of powder will send the bullet faster than the speed of sound. So you may reduce the sound of the explosion, but you still get the sharp crack. To avoid this, you would load a 9MM round with a 147 grain projectile and about 3.5 grains of powder.  For a semi auto it is a balance between enough powder to cycle the action and not too much to avoid the supersonic crack. Too little powder and your semi auto acts like a bolt action, way to little powder and the projectile might not clear the barrel.  As a bonus: 45 cal is already subsonic.","label":0,"model":"human","source":"reddit","id":2863}
{"text":"1. Product differentiation - They look distinctive so that consumers can tell at a glance that these are *exercise shoes* and not *regular shoes*.\n\n2. Marketing - The shoes are trying to grab your attention at the point of sale, and they *look like* they have \"advanced exercise tech\" integrated into them.\n\nA similar analogy would be electric\/hybrid cars and why they're all arguably ugly. Part of it is for actual function reasons, but a lot of it is because consumers who buy them deliberately want to look futuristic, stand out, and appear *different* from regular cars.\n\n-----\n\nEdit: Some people are replying about the hybrid vehicles part. I assure you this is a thing. There's been studies that (particularly when the hybrid\/green trend began) \"green\" vehicle owners want to be acknowledged about being green. Here are some links:\n_URL_1_\n_URL_0_\n >  In fact, more than half of the Prius buyers surveyed this spring by CNW Marketing Research of Bandon, Ore., said the main reason they purchased their car was that \u201cit makes a statement about me.\u201d\n\n >  \u201cI really want people to know that I care about the environment,\u201d said Joy Feasley of Philadelphia, owner of a green 2006 Prius. \u201cI like that people stop and ask me how I like my car.\u201d\n\n >  \u201cI felt like the Camry Hybrid was too subtle for the message I wanted to put out there,\u201d Ms. Gatch said. \u201cI wanted to have the biggest impact that I could, and the Prius puts out a clearer message.\u201d\n\nHowever, this is just a strategy used by some manufacturers. Other brands like Tesla are trying to do the exact opposite, which is to appeal to a crowd that want a luxury tier green vehicle that looks sleek and normal. As green vehicles are becoming more commonplace, there will be less of a consumer need to stand out from the crowd.","label":0,"model":"human","source":"reddit","id":2864}
{"text":"Fox Hollywood, doesn't have contradictory interests politically than Fox News. What 20th Century Fox produces is a lot of movies, and yes some do go against the grain of the political views of much of the Fox News shows. But the simple fact is they produce these movies, for the same reason Fox News itself exists: Money. Fox News came about because Murdock (I'm using him here as a figurehead here, not saying the idea was his or of that sort) believed that the media was biased, and he wanted to create a completely unbiased news network (read: a conservative news network [Note before those few on the right come to attack me: There is nothing wrong with having a conservative news network. It doesn't have be \"Fair and Balanced\" as it tries to say, it isn't, and it won't ever be. Just like MSNBC, it has a sway to it, and it isn't balanced, it is the opposite of MSNBC, and exists as that reason]). Now what this did was shift a huge portion of the CNN crowd who didn't like being told that maybe their side does some wrong (they all do) and created a huge load of wealth for News Corp. \n\nWhat 20th Century Fox does (and Fox Broadcasting Company does with a lot of it's more political liberal [Simpsons, Family Guy, Glee, etc.] shows) is it ties in the other side of the spectrum. You now are being paid a huge buttload of money by pissed off conservatives, who then piss off a bunch of left wingers, who then use your movies and tvs as escapes into a better more liberal minded world, and you're getting money from every single aspect. It's money over philosophy, money over convictions. \n\nSo they exist, not because they are fighting internally against each other, but because they are able to fill in the needs that every side wants. And if you produce good enough movies, and funny enough tv shows, than those who don't know squat about politics will also tag along, so you get the best of all worlds.","label":0,"model":"human","source":"reddit","id":2865}
{"text":"Pretty much every answer in this thread is wrong.\n\nDominance theory has been disproven for quite a long time now. Dogs know that humans are not dogs, thus a \"pack structure\" resembling dogs or wolves cannot be established. If Fluffy is a hell hound that is destroying your house and property, chances are that he's untrained, under exercised, and under stimulated. Dogs are not lawn ornaments, if you bought a Belgian Malinois expecting him to be the best behaved dog in the world because you saw one \"on that tv show\", chances are that dog is gonna end up in a shelter by the time it's 9 months old. There are tons of Siberian Huskies in shelters around my area because people buy them because \"they're pretty\", not realizing they are a high maintenance breed that require more than a 5 minute walk around the block.   \n\nDominance theory was on it's way out until someone called Caesar Milan made it popular with the masses again, setting us back 20 years when it comes to dog training. \n\nRecommended reading: \n\n[American Veterinary Society of Animal Behavior's stance on Dominance Theory](_URL_7_)\n\n[De-Bunking the \"Alpha Dog\" Theory](_URL_8_) \n\n[The Dog Whisperer Controversy](_URL_7_)\n\n[Dog training methods: their use, effectiveness and interaction with behaviour and welfare\u0016 \u0007\u0014\u0004](_URL_7_)\n\n[An evaluation of dog-training techniques: an assessment of efficacy and\nwelfare](_URL_8_)\n\n__________________________________\n\nTo answer OP's question: No, dogs have been selectively bred and domesticated to bond and prefer the company of humans rather than other dogs. They will not become \"unhappy\" or \"depressed\" because there are no other dogs in the house.\n\nHere are a few articles: \n\n[Link](_URL_8_)\n\n[Do Dogs Love People More Than They Love Other Dogs?\n](_URL_8_)","label":0,"model":"human","source":"reddit","id":2866}
{"text":"For the same reason Fox News is.\n\nOnce you sort through all the nonsense arguments people level about bias and the like there really are only a few problems with both networks.  First, they tend to overwhelmingly cover news that is anything *but* newsworthy and so both networks constantly bombard the audience with endless coverage of stuff like a missing airplane even though most of that coverage consisted of nothing but \"We don't have any information to give\".  The second problem is that both networks are frankly *lousy* at distinguishing between editorial segments and news segments and as time passes what line may have existed has largely vanished.\n\nThe problem, then, is that both networks are fundamentally failing at the basic function of presenting the news while both actively and obviously push an agenda.  The disrespect thus comes from two sources.  The first is from people who actually want actual news reporting and the second is from people who sit on the opposite side of the ideological divide between the networks.","label":0,"model":"human","source":"reddit","id":2867}
{"text":"The answer involves the fact that it *does* initially come out as one mashed up language.\n\nLanguage acquisition involves a lot of repetition and trial-and-error. Kids in bilingual households do mix their languages quite often, but they quickly learn that a mixed sentence can cause confusion.\n\nThink of every speech as an attempt to get the listener to do something -- if they don't do it, it's annoying and you have to try a new way to express what you need. Enough 'WTF face' reactions to sentences\/questions that mix together two languages will gradually teach the child to use one or the other at a time. (This takes quite a while and mainly happens outside of the home where most listeners speak only one of the languages). It helps that languages sound significantly different.\n\nSecondly, within a given household or even a whole community, languages **do** get mashed up together. When two people speak two different languages and are in an environment where there is no particular reason to favour one or the other language, they will switch often, including mid-sentence, and even combining words from the two languages. One example is [Taglish](_URL_0_), which is very common in the Philippines. You hear it on TV, and it is even an option on many ATMs.","label":0,"model":"human","source":"reddit","id":2868}
{"text":"When your brain wants you to do something, it releases **dopamine** to force you to do it. The more dopamine your brain releases, the more you feel compelled to do whatever it is your brain wants you to do.\n\nIn other words, dopamine is your brain's \"gotta have it\" chemical. It is what makes you crave things to varying degrees. And the more dopamine your brain is outputting, the more uneasy and uncomfortable you feel until you act on its urges.\n\nThen when you finally complete that task that dopamine has forced you to act on... and you've gotten whatever \"prize\" was waiting at the end of that task... your brain releases a rush of **serotonin** to match the dopamine it just outputted.\n\nSerotonin is your brain's \"got it\" chemical, shutting off the fountain of dopamine that was compelling you to seek out whatever you wanted.\n\nBut, more importantly, the **brief combination of high dopamine and high serotonin in your brain** acts as a reward mechanism. It gives you a feeling of immense pleasure.\n\nIt's nature's way of rewarding you for taking a specific behavior.\n\nThink about a time when you were a little hungry... then really hungry... then completely starved. It was dopamine being released in varying quantities in your brain that caused to feel those growing levels of hunger.\n\nAnd think about how relieving it felt when you finally ate. You likely gorged yourself and you felt INCREDIBLE. That was the combo of high dopamine and high serotonin in your brain.\n\nYou experience the same thing when you build up the nerve to ask a girl out... and then she says yes. Or when you go on a long run and then make it home, safe and sound. And when you jump out of an airplane (followed by landing safely on the ground). And when you watch scary movies (then go back to \"safe\" reality).\n\nIt's all dopamine making you feel uneasy (escape! eat that food! make it to safety! get out of this awful situation!)... followed by serotonin when you'd achieved your mission... followed by pure unadulterated bliss in the brief moment when your brain was flooded with both chemicals at once.\n\nLong story short, **nicotine \"artificially\" creates this rush of serotonin in your brain**. So if you're stressed (i.e. you have extremely high dopamine levels from something being incomplete or scary in your life), cigarettes temporarily create the surge of serotonin that turns off dopamine -- and leaves you feeling extreme pleasure.\n\nBut, since they're not *actually* solving the issues causing high dopamine for you, the dopamine just comes back... but eventually comes back at higher, and higher, and higher levels. It acclimates to your \"artificial\" serotonin source.\n\nSo the pleasure and calm you get from cigarettes (and their \"artificial\" serotonin hit) increases with time -- but your addiction (and stress) also grows.\n\nSource: [The Craving Brain](_URL_0_) by Ronald Ruden","label":0,"model":"human","source":"reddit","id":2869}
{"text":"'Eat myself to death' isn't a specific medical thing. It could mean any number of things. You can, for instance, rupture your stomach, but it is pretty difficult, since your body will attempt to stop you. You can simply have an unhealthy diet to the point where the cumulative effects of weight gain kill you, such as heart disease or stroke, you could be eating a diet unbalanced to the point that you die of malnourishment. \n\nIt's simply a vague term, not a specific cause of death.\n\n\nedit:\n > I mean eating food continuously until I die :)\n\nThis is, oddly, the second time this has been asked in 2 days. Did I miss a weird news story?\n\nYour body will try to stop you. Your appetite will disappear, you'll attempt to vomit. You might choke to death, depending on your tenacity shoving stuff into your vomming throat. If you can shove enough food down to stretch out the muscles sufficiently, then vomiting may be incapable of pushing food back out. If you cram upwards of 4 liters of material into your stomach, it will eventually overcome its ability to stretch, and rupture. At that point you are very definitely in a life-threatening situation.","label":0,"model":"human","source":"reddit","id":2870}
{"text":"There's a lot of debate about this, and the answer is unclear. \/u\/SleeplessinRedditle quoted a study by Chen, who is an economist and not a linguist, but he makes several very basic errors: for example, he claims English has a \"strong\" future tense and German has a \"weak\" future tense, but in fact both languages express the future in almost exactly the same ways (he claims that English always requires a future marker except in very specific cases which he then sets aside, but this is completely false). His hypothesis also fails to explain why savings rates in different countries with the same languages vary so much: Ireland is near the top of the chart in the linked article, Australia near the middle, and the US and the UK right at the bottom.\n\nThe basic idea that language can affect how you think is known as the Sapir-Whorf hypothesis (although neither Sapir nor Whorf actually formulated it), and has been floated around and discussed since 1929: it's more correctly called \"linguistic relativity\". It comes in two basic versions: linguistic determinism says that the language you speak necessarily restricts the range of possible thoughts you can have and so dictates how you perceive things; linguistic influence says that the language you speak can have some effect on how you think and perceive in some ways, but doesn't dictate everything.\n\nChen appears to lean towards determinism; if you've seen *Arrival*, the scriptwriters basically took the determinist model of Sapir-Whorf and ran with it. Obviously, learning a new language can't literally rewire your brain to the extent that you can see all of time -- it makes a great movie, but scientifically it's bunk -- but the consensus among linguists at the moment is that there is no evidence at all to support the idea of linguistic determinism.\n\nBasically, linguistic determinism says that before you can think, you must have language. People who speak languages that use the same word for \"blue\" and \"green\" can't tell the difference between those two colours.\n\nHowever, research suggests the opposite: that we are all born able to think, and then we learn a language that enables us to express those thoughts. Somebody who speaks a language that makes no distinction between \"green\" and \"blue\" can still tell the difference between them: they just use the same label for them.\n\nAnecdotally, this chimes in with my experience. Russian has different words for what we call different shades of blue: I can still see the difference between them, but I take longer than a native speaker of Russian to decide which word goes with which shade.\n\n[This study](_URL_0_) suggests that the language does make a bit of a difference -- just not a big one.\n\nHaving lived in Germany now for nearly 25 years, I can say that learning the language hasn't made me better at saving money, or changed my perception in any meaningful way. There are some things I can express better in German than in English and vice versa: I don't *think* differently, but I am able to express a slightly different range of thoughts.","label":0,"model":"human","source":"reddit","id":2871}
{"text":"So lets say you own a company that sells clothes.  You get all the proper paperwork saying that your company is authorized to operate in your home country.  So you are free to sell it's clothes within your own country.  While there might be some minor regulation differences between regions, normally countries take steps to make these \"domestic\" sales as easy as possible. \n\nBut when you want to expand that company into a new country, you'll face a series of problems.  First is that shipping the product itself into the new country will likely incur a kind of tax.  This is known as a tariff or duty, aka an import tax. \n\nThe next problem has to do with regulations.  Different countries have different regulations on products.  So my home country might allow the use of red die #1357 but the country I'm trying to expand into might not.  Just different rules for different countries.\n\nA long time ago the people of Europe saw some problems with this system.  European countries tend to be fairly small so companies were always looking to expand into the country next door, and due to the above 2 things this was a giant pain in the ass. \n\nThe idea of the \"single market\" was that they would do away with the import\/export taxes.  Second they should take steps so that a product that is legal to sell in Germany is also legal to sell in Spain. This makes it easy for companies to expand their customer base from one country to another.\n\nIn a related way, the same idea was applied to the movement of people.  A, Employee is just someone who sells their labor and they should be allowed to sell it in Germany, Spain or the UK. Just like any company can sell it's goods, any person can sell their labor. \n\nSo what does leaving this do to the UK.  It means that the citizens of the UK will require special permission (a visa) to live and work in other European countries.  So a Londoner wanting to work in Berlin would face the same challenges as a Londoner who wanted to work in New York. \n\nNext, UK companies would face additional challenges when it comes to selling their products to the rest of Europe.  This would likely mean the prices of those products raise, and sales fall.  This is expected to cause some economic problems for the UK.\n\nForeign companies would also face higher costs when selling to the UK.  So a product made in Germany would cost more.  This would make it easier for local companies to compete with those foreign companies. So in general there would be a few more jobs, but the costs of almost all imported goods would raise so everyone would be spending a little more of their income on those same goods. In general it is expected that the economic boost from the new jobs will be fairly minor, wheres the damage from increased prices would be fairly large, so a net negative.","label":0,"model":"human","source":"reddit","id":2872}
{"text":"A better way to word this would be, \"Why *would* it be possible to imagine a hue that is completely outside one's experience, to the point where the neurological circuitry to process such a hue may not even exist?\"\n\nAnything you can imagine or visualize is necessarily going to be informed by and at least in part based on things you've previously experienced. By the time a person is old enough to ask a question like \"Why can't I imagine a new color\" their brain has probably already been exposed to every possible color or combination of colors available to them (especially nowadays with such widespread and easy access to video games, television, and digital artistic tools like Photoshop and Paint Tool SAI) and has adjusted itself to recognize and process them.\n\nYou can't imagine a new color because there's nothing to build it from; it would necessarily have to be derived from a color you already know, or a combination of those colors. Where would you even start? It's as if someone asked you to build a house out of a brand new, never before seen material, but without using any currently known or even hypothetically possible substances or combination of substances whatsoever to create it. You'd need to invent new physics and chemistry before you could even begin.\n\nEdit: I want to clarify that I don't necessarily think it is impossible for a person to *experience* a new color, either because they weren't exposed to it before, or because of drug use, injury, or some other phenomenon. What I don't think is possible is for a person to *purposefully imagine* a brand new color.","label":0,"model":"human","source":"reddit","id":2873}
{"text":">  What are they used for and how?\n\n**Video games!**   \nVideo games are hard but cool math. If you ever asked your teacher \"What do I need this for?\", the teacher should have said \"Video games!\".\n\nNow, Quaternions can be used in video games as a mathematical means of describing the orientation of an object in three dimensional space, for example the current world of the level. Most of objects are controlled by using matrices (Matrix, but not the movie), but a considerable part of generating these matrices is done using Quaternions.\n\n & nbsp;\n\nTo understand why Quaternions are helpful in video games, I need to introduce you into two common problems that will help you understand how rotations depend on each other.\n & nbsp;    \n & nbsp;    \n    \n**Issue number 1: Rotation order.**    \n\nImagine a plane on a runway. The plane has three axises: X-axis the one the plane rotates around when it takes off (From wingtip to wingtip). Y-axis is towards the direction of travel (When the plane does a barrel roll, it rolls around the Y axis). And Z-Axis is the one it rotates around when it is on the ground driving around the airport.\n\nNow I give you the following instructions for a plane on a runway facing north:\n\n1. Rotate the plane around its X-Axis by 45 degrees. (This looks like the plane has just taken off)    \n2. Then rotate the plane around its Z axis by 90 degrees. (This looks like the plane is taking off, but looking towards the right of the runway)    \n    \nIf I reverse these instructions, you get a different outcome:\nThe plane is now going off the runway towards the East. And it is still facing up correctly at 45\u00b0. No weird sideways flying here.\n\nThe more axes you add, the more erratic the result will become if you do not follow the rotation order.\n & nbsp;    \n & nbsp;    \n    \n**Issue number 2: Gimbal Lock**    \n  \nYou are inside this plane and it keeps going up at 45 degrees. But you sit in a cool pilot chair that will always keep you upright so your drink never spills. This chair will counteract all the rotations the plane currently does to move you back to a level position where your drink does not spill.\nNow the plane has kept pulling up and is now going exactly straight upwards. (X = 90\u00b0)\n\nThe chair compensates for this well, as it only applies the opposite rotation of the plane to your seat. But remember the chair is still fixed to the plane, so the chair itself is now at 90\u00b0 like the plane.\nThen, suddenly, the plane is grabbed by a wind gust and shaken around, and it is now flying straight up AND 45\u00b0 around its Z axis towards the left.\n\nThe Pilot chair cannot help you counteract this, as it is not able to do the opposite rotation here. The rotation order would need to be first 45\u00b0 around Z, and then 90\u00b0 around X. It cant do this and your drink will spill.\n & nbsp;    \n & nbsp;    \n    \n**Fix: Quaternion**    \n  \n\nTo fix the chair, the engineers will need to cheat and allow the chair to turn around an extra axis to fix any rotation-order mess that may have occurred. This fourth axis is a joker, and it will need to be perpendicular to one of the three axes of the chair to help out when needed. \n\nAnd when taking this extra joker axis into account, we no longer have a three rotations, but four. The construct of dealing with this is called Quaternion and it is some pretty tough math(s) which others have tried to explain here.\n\nNow, in video games with characters, each arm and leg is made up from virtual bones that let the character bend the knee or hold weapons. These virtual bones are exactly like the special pilot chair in the plane. That is where quaternions are used.\n\n & nbsp;\nedit: man fuck reddit formatting...","label":0,"model":"human","source":"reddit","id":2874}
{"text":"There is *some* evidence that shows that the bioburden (amount of bacteria) on uniforms is higher on scrubs that have been home laundered versus those laundered by a hospital facility. This is a topic that has been going round and round with Infection Control, the American Medical Association (AMA), the American Nurses Association (ANA) and Association of peri-Operative Registered Nurses (AORN) for years.\n\nScrubs get contaminated at work. They get contaminated on the way home from work. They get contaminated on the way into work by sitting in the car in which you drove home from work the day before in your contaminated scrubs. \n\nWearing them out and about after work can *potentially* spread bacteria\/fungi picked up at the hospital. \n\nPatients in the hospital are there because they're already sick. Getting infected by a hospital acquired \"bug\" is a huge issue and leads to significant morbidity and mortality. In the interest of decreasing *risk*, it's been recommended by Infection Control, the AMA, the ANA, and the AORN to change into clean, hospital laundered scrubs on arrival to the hospital (usually this is done in the OR suites--restricted and semi-restricted areas) and to leave the dirties behind before you exit the facility.\n\nThis does nothing to account for the staff who wear their own uniforms in other areas of the hospital to work and back home. They home launder their uniforms and are commonly seen out and about running errands in their personal scrubs. \n\nOne study found that the hospital's laundering facility was spreading a nasty fungus because the equipment was dirty. So, there's another place where contaminants need to be monitored. \n\nThe evidence that is available shows that it's probably better to not wear them outside of the hospital.","label":0,"model":"human","source":"reddit","id":2875}
{"text":"Your body feels the transfer of heat not really temperature. Diarrhea is mostly water and water has a high convection coefficient, meaning it transfers heat faster. Think of it the same effect as biting into a really hot pizza, the cheese burns you more than the crust. They're both at the same temperature but cheese transfers heat faster. Also, water has high heat capacity, so for every degree of temperature, it has a lot more heat. Regular feces doesn't have as much water in it so it doesn't transfer as much heat as quickly, that's why diarrhea feels hotter. Really, no matter what comes out of you, it's at body temperature.\n\n**TL;DR: Diarrhea is the same temperature as regular feces 98.6F\/37C but it transfers heat quicker because it's mostly water so it feels hotter.**\n\nEdit: for clarity.\n\nEdit2: Everyone keeps mentioning acidic diarrhea causing this sensation. What I describe above is why it feels *hotter* not *burning*. The burning feeling is from digestive enzymes, though completely different and distinct when compared to the warmer feeling of shooting liquid out your behind. Also, one thing I neglected to mention, mostly for simplicity, is that it's not just the convection coefficient of water that helps the transfer of heat happen quicker but also the fact that it is liquid vs solid. Liquids cover more surface area, therefore can transfer that heat quicker.","label":0,"model":"human","source":"reddit","id":2876}
{"text":"Cop here. What most people mentioned is right with regards to the methods. There are tons of ways. \n\n & nbsp;\n\nWhen police take a report of it, we do so with the knowledge that: you're most likely going to be reimbursed, the CC company will do their own investigation if they want, and it's nearly impossible to find the person if they know how to work the system.\n\n & nbsp;\n\nThis isn't a movie; we don't have the tools to get their IP address, trace a package, or essentially do anything to locate the thief unless they provided true info about their identity at some point in the process. Oh, and everybody involved has to help us. Thieves know all of this. CC fraud is an inherently deceptive crime. \n\n & nbsp;\n\nThe CC company won't work with us \"beat cops\" without a search warrant, and even then they only provide the bare minimum information. Same for many stores in general due to the sensitive information we need to know.  Thieves know this. After you're reimbursed, the CC company essentially is now the victim, and their lack of willful cooperation means the investigation is terminated. CC company won't even do their own investigation unless it meets their guidelines. One BIG one is that it can't cost them more than the money they stand to gain from a successful investigation. I've had people tell me that could be upwards of 100k. Thieves know this too. \n\n & nbsp;\n\nI'll add one interesting method no one else has mentioned. A CC thief in Argentina set up a fake \"Quality Control\" company in the US and advertised it online. Eventually people like my victim respond, get \"hired\", and are being sent packages for quality control inspection. These packages have other people's names and different people's addresses on the invoices, etc. The victims are then instructed to repackage and resend the item to various addresses. Always to a different address than the last, some to other countries. Basically what this thief did was buy CC numbers, use their \"employees\" personal info to eventually open new CCs, buy more stuff online, and send it through all of their fake employees to end up back to them in Argentina. \n\n & nbsp;\n\nBy that point, no cop has the means to contact numerous victims across various states in a mail fraud fueled by rampant online CC theft, let alone do the investigation that ends with an international crime, all while still taking calls about your neighbor's dog pooing on your lawn. So no. With regard to crimes like that, there's little we can do. Tomorrow they'll have changed their name, the address the packages are sent to, and the victims they use.\n\n  & nbsp;\n\nGood question though. Try to ignore some who are simply bashing cops out of bias and ignorance of the investigative process.","label":0,"model":"human","source":"reddit","id":2877}
{"text":"Animals that live in the wild will spend all their life foraging or hunting for food.  They work hard every day to find the nutrients they need to survive, and most are lucky to find enough nutrients to nourish themselves.\n\nHumans have moved out of the wild and created a safe haven to live in, so we don't need to forage or hunt anymore.  We can go to the store to buy the food we need, and we can buy as much as we want.\n\nThe problem is that some foods, if eaten a lot, will cause your teeth to rot away.  Finding them in nature is rare enough that wild animals won't consume enough of them to ruin their teeth.  But humans can buy as much of it as they want, right away, without having to work hard for it.  And it's very tasty, so we can help ourselves most of the time.  Our bodies aren't made to consume so many sugary treats, so our teeth aren't protected against them.\n\nWith domesticated animals like a house dog, the dog food you buy is designed to give your dog all the nutrients he needs. Some dog foods come with nutrients that are good at protecting and cleaning your dog's teeth just by eating it.\n\nYour dog's choice in food is limited by what you give him, so he has to eat the healthy stuff no matter what.  But as an adult, you'll be able to make your own food choices.  And most adults aren't able to make wise choices for their health, so they brush their teeth so they can continue eating unhealthy, yet tasty foods.\n\n**TL;DR - Living in a society gives humans access to more sugary foods that will destroy teeth.  These types of foods aren't easily available to animals, unless humans feed them to animals.  So humans brush to keep their teeth clean while wild animals never have to brush.**","label":0,"model":"human","source":"reddit","id":2878}
{"text":"One of my favorite subjects.  Let\u2019s take a look!\n\n\nFirst off, let\u2019s define \u201cworried about guns\u201d in drier terms.  The worry comes because of a continuing battle at most levels of government (notably state, with some federal legislation occasionally introduced) between the \u201cLet them own and carry their weapons\u201d side and the \u201cGuns are for killing and we don\u2019t need any more of that\u201d side, commonly abbreviated as pro gun and pro gun control.  \n\nNow, let\u2019s look first at the pro gun side of the argument.  \n\nThere are folks out there who believe the federal government is gearing up to send the browncoats.  They\u2019re gonna take our guns, take our money, take our freedom, and trample our liberty.  It\u2019s imminent.  These folks have fallout shelters stocked with MREs and lots and lots of ammo.  Often these are the folks open carrying ARs to see how the police will react.  They\u2019re probably misguided and wrong, but believe it or not they\u2019re statistically harmless.  They\u2019re not really part of the discussion as much as they wish they were.  \n\nNext, there\u2019s the group of people with whom I identify.  Some of us hunt, some of us target shoot or even compete.  Lots of us are armed in public, though you\u2019d generally never know it.  We\u2019ve got a varied education but a unified respect for the responsibility that comes with carrying a weapon.  Lots of us also took a history class or two.  \n\nHere\u2019s a high level history of the last 100 years from the viewpoint of gun control, shamelessly copied and pasted to avoid extra typing but easily verifiable: \n\n1.\tIn 1911, Turkey established gun control. From 1915-1917, 1.5 million Armenians, unable to defend themselves against their ethnic-cleansing government, were arrested and exterminated.\n\n2.\tIn 1929, the former Soviet Union established gun control as a means of controlling the \u201cmore \ndifficult\u201d of their citizens. From 1929 to the death of Stalin, 40 million Soviets met an untimely end at the hand of various governmental agencies as they were arrested and exterminated.\n\n3. After the rise of the Nazi\u2019s, Germany established their version of gun control in 1938 and from 1939 to 1945, 13 million Jews, gypsies, homosexuals, the mentally ill, and others, who were unable to defend arrested and exterminated.\n\n4. After Communist China established gun control in 1935, an estimated 50 million political dissidents, unable to defend themselves against their fascist leaders, were arrested and exterminated.\n5. Closer to home, Guatemala established gun control in 1964. From 1964 to 1981, 100,000 Mayans, unable to defend themselves against their ruthless dictatorship, were arrested and exterminated.\n\n6. Uganda established gun control in 1970. From 1971 to 1979, 300,000 Christians, unable to defend themselves from their dictatorial government, were arrested and exterminated.\n\n7. Cambodia established gun control in 1956. From 1975 to 1977, one million of the \u201ceducated\u201d people, unable to defend themselves against their fascist government, were arrested and exterminated.\n\n8. In 1994, Rwanda disarmed the Tutsi people and being unable to defend themselves from their totalitarian government, nearly one million were summarily executed.\n \n\nSo look at it this way.  I know we\u2019re not a third world country.  We don\u2019t have the financial strife that lead to fascist Germany.  We haven\u2019t got the sociopolitical, economic, and geographic climates that led to Stalin\u2019s rise.  We don\u2019t have the racial, religious, or regional strife that lead to some of these other conflicts.  And we probably won\u2019t.  But if you watch 8 people from different areas of the world repeatedly crash their bikes into ditches and break their skulls, wouldn\u2019t you put a helmet on?  None of these genocides would be remotely possible against an armed populace.  \n\nI will refrain from making any detailed arguments on the topics of personal defense, the prevalence of mass violence in gun-free zones (movies, schools, etc), the recent violent acts with (Finland) and without (the rest of Europe) firearms, and the various other soap box points of the pro-gun crowd.  You didn\u2019t ask; I won\u2019t bore.  Suffice it to say that guns exist and, on a small \/ individual scale, those who want them will always be able to get them.  Taking firearms away from the law abiding public may reduce overall firearms incidents but it skews the violence towards law abiding citizens every time.  \n\nNow, the other side:  Let\u2019s look at pro gun control by the numbers.  Assume that none of the scary genocides will occur \u2013 they probably won\u2019t, not here.  Assume also that the control is somehow executed in such a way that sportsmen are not impacted; hunting prevails and skeet ranges keep booming, accuracy competitions carry on, and life is good, but we somehow do eliminated handguns from the general populace.  It\u2019s a best case (impossible) scenario, but let\u2019s look at what we\u2019re really trying to accomplish.  \n\nI like to do that in terms of deaths. \n\nStraight from the (_URL_0_)[CDC]:  3.5 homicide deaths per 100,000 yearly, or approximately 11,208 per year in all of the USA.  That includes accidents, police, mass shootings, and of course, individual violent crime.  We\u2019ve excluded the 19,000 suicides by firearm; that\u2019s another debate.  I\u2019m on the \u201cdeath finds a way\u201d bandwagon \u2013 better to work on treatment and availability of help than worry about whether they jump off a bridge or pull the trigger, but that\u2019s just me.  \n\nMight be interesting to break that down.  Who\u2019s killing who with guns in this country?  The story is [_URL_1_](here) if you don\u2019t want my summary.  According to Wikipedia, the actual number of deaths (vs. the calculated rate) in 2012 was 8855.  Well, the police have knocked down 400 people so far this year.  That\u2019s not something gun control is going to address.  So, we\u2019re down to 8500, assuming steady state.\n\nI\u2019m not a professional researcher so you\u2019ll have to forgive the patchwork here, but we\u2019re going to get close with these next numbers \u2013 the number of firearms homicides in a given year in some of our major metro cities.   \n\nCDC gives us some interesting data to work with.  For example, 60% of firearms homicide occur in just 62 cities and within just 50 metros.  Why is this?  Population density plays a part \u2013 more opportunity for conflict.  Gang violence plays a HUGE part.  Pull up the wikipedia page on US gun violence linked above and take a look at the age breakdown.  18-24 year olds are doing a vast majority of the killing and the dying, and they\u2019re doing it in DC, LA, Detroit, Chicago, etc.  You might say we have a gang problem more than a gun problem, if you were so inclined.  That leaves about 3500 firearms murders outside of our major cities.  You\u2019re pretty safe out there with the rednecks, contrary to popular (international) belief. \n\nSo what\u2019s killing Americans?  Heart disease is number one, with nearly 600,000 of us dropping yearly.   Suicide is number 10, with almost 40,000 victims yearly.  Wait, you say \u2013 firearms aren\u2019t even in the top ten?  Homicide isn\u2019t even on the chart, much less the subset involving firearms.  You\u2019re 70 times more likely to die from overeating than from a gunshot wound, and that climbs to 176 times more likely if you stay out of the inner city gang turf.  Car accidents, kidney problems, alzheimers, diabetus type two, cancer, medical mistakes \u2013 these are the killers.  Firearms deaths are a dot on a lemma on a theory on a whim when it comes to significance and risk, in the scheme of staying alive in the USA.\n\nSo, why then is it such a debate?  Why does the media cover 30 people losing their lives in a shooting but not the 56,000 people who die every year from the flu?  Why does the government spend $1.2 BILLION yearly on the BATFE, making sure our guns are regulated, don\u2019t shoot too fast, don\u2019t make too little noise when the damage those guns are doing is so infinitesimal and there are SO many other low hanging fruits out there, so many ways that money could actually save lives?  \n\nI don\u2019t honestly know.","label":0,"model":"human","source":"reddit","id":2879}
{"text":"Here's something you might not have thought of:  It's actually very easy to kill Ebola, or any other pathogen.  Fifteen minutes with a flamethrower, for instance, will get rid of anything.\n\nBut obviously, that doesn't do the patient much good either.  So most of medical science is about finding things that will kill the disease, but leave the host alone.  Much easier said than done.  All sorts of clever tricks have been invented; some of which work, some of which don't, and some which work but are so dangerous to the patient that they can't be used except in extreme cases.\n\nI don't know about Ebola specifically, but that's why finding a cure for cancer is such a pain in the ass.  Cancer cells are not foreign invaders; they're your body's own cells turning against you.  Much of cancer research goes into methods of diagnosis -- just being able to find the cancer cells, and figuring out what makes them different from the healthy cells.  This is called \"differentiation.\"  When a doctor says that a particular case of cancer is \"not well-differentiated,\" that's a bad thing.  It means that it's hard to \"tell the good guys from the bad guys.\"\n\nOne form of treatment is chemotherapy; which in this sense is the tactic of \"shoot them all and let God sort them out.\"  Basically, it's poisoning the patient in the hopes that the cancer cells will suffer the most, and the healthy cells will be able to bounce back.  Again, sometimes it works, sometimes it doesn't, and sometimes it's just too dangerous to even try.  Chemotherapy techniques have improved greatly over the past few years,  mostly through improvements in drug delivery -- making sure that the poison reaches only the cancer cells, and damaging as little healthy tissue as possible.  But it's still often a case of the cure being worse than the disease, and it's a very difficult decision whether to even try to treat a patient's cancer.\n\nWARNING:  BADLY STRETCHED ANALOGY AHEAD\n\nBack to vaccines, and things like Ebola.  Ebola is a kind of virus.  Technically, viruses aren't even living organisms.  They're more like molecular robots than anything else.  They're unable to reproduce on their own; they can only survive by invading a host cell and \"reprogramming\" it to reproduce the virus (and killing the cell in the process).  Vaccines are meant to prepare your body's own immune system to fight the virus should you actually catch it.\n\nThink of a bouncer at a nightclub, where there's a private party going on.  If you want to get into the party, the bouncer checks your name against a list to see if you're invited.  Your body's immune system is like a bouncer, but works in reverse; it checks a list to see if you're *not* allowed in.  If you're not on the list, you can go on through.\n\nIf, once inside, you turn out to be an asshole, then the bouncer chucks you out and adds your name to the list, and you'll never be allowed back in again.\n\nDifferent viruses will cause different amounts of trouble.  Your typical cold or flu virus will just get drunk, try to hit on everybody, and will get thrown out before causing much damage.  But some viruses, like Ebola, are really nasty and will smash up the furniture and completely wreck the joint before they're thrown out -- if they even *can* be thrown out; the bouncer might not even be able to handle them.\n\nVaccination is where a \"deactivated\" form of a particular virus is injected into the body, where the immune system can discover it and destroy it without it causing any damage.  In this analogy it's like taking an undesirable, beating the crap out of him, then showing him to the bouncer and saying, \"See this guy?  Don't let him in.\"\n\nBut viruses are (relatively) very simple structures, and thus change or mutate frequently and rapidly.  So the guy we showed to the bouncer changes his clothes, gets a fake ID, and the bouncer lets him in anyway.  That's why you need to get a new flu vaccination every year, because the virus that you were vaccinated against last year has turned into something else since.  And even then, you can still catch a common cold.\n\nAIDS is a kind of virus, and its particularly evil scheme is to kill the bouncer.  Then just anybody can get in, and nothing can be done about it.  Even though we've identified the AIDS virus, and can give a patient medicines to combat it, a cure is hard to come by because a simple vaccine won't work if the body's immune system has been disabled by the virus itself.  So for now at least, the best way to deal with AIDS is to not get it in the first place; which means doing some of the work of the bouncer yourself.  Always carry a condom, and don't be afraid to use it.  Which is a good idea when going to a nightclub, anyway.","label":0,"model":"human","source":"reddit","id":2880}
{"text":"If you are asking about the United States, citizens can claim self-defense against officers.  The legal standard varies from state to state- are you interested in one in particular?\n\nIn some states there is an elevated standard if the assailant is an officer, requiring actual imminent danger rather than a reasonable fear or belief (the standard for self-defense). Here are a few examples I found in responding to a similar question:\n\n*State v. Kraul*, 90 N.M. 314, 318 (N.M. Ct. App. 1977) (\"The right of self-defense is not barred simply because the other person in the affray is a police officer.\")\n\n*State v. Hutchinson*, 959 P.2d 1061 (Wash. 1998) (self-defense instruction in case involving shooting of two police officers)\n\n*Brown v. Commonwealth*, 497 S.E.2d 527, 530 (Vir. 1998) (\"It has long been held in Virginia that where an officer attempts an unlawful arrest, the officer is an aggressor which gives the arrestee the right to use self-defense to resist so long as the force used is reasonable.\")\n\n*Boyd v. State*, 406 So. 2d 824 (Miss. 1981) (reversing conviction for assault on a police officer because the trial court failed to instruct the jury on self-defense)","label":0,"model":"human","source":"reddit","id":2881}
{"text":"* Its not the \"rawness\" of the food that is bad for you. Your body can digest all raw foods with no issues. As a species we have a well functioning digestive system, nothing has changed in this regard.\n\n* It is the increased risk of contracting a foodborne illness that is the problem with raw food.\n\n* Foodborne illnesses are typically caused by a bacteria (e.g. E. coli), a parasite (e.g. tapeworm), fungi, or toxin. For example, toxins can be found in some mushrooms, in shellfish (red tide), or the Fugu fish (puffer fish) toxin.\n\n* ALL raw food has the potential to make you sick, including veggies, fruit, fish, and meat. For example, *E. coli* outbreaks on veggies like tomatoes and spinach are fairly common. Foodborne [*botulism*](_URL_0_) is caused by a toxin produced by a bacteria that is often found in improperly prepared canned\/preserved foods. Raw fish might contain parasitic tapeworms - even sushi carries a level of risk.\n\n* Cooking food includes many processes such as boiling, baking, peeling, washing, freezing, or grilling. These process kill or get rid of potential sources of contamination.\n\n* While our modern food industry takes every precaution to prevent a foodborne illness from getting to you, its not 100% perfect. Some raw foods have a higher risk (like raw chicken) and cooking using proper and safe techniques is highly recommended to reduce risk to yourself.\n\n* Humans can also get a waterborne illness from drinking contaminated water. This includes people who are living near the same untreated water source their entire lives. For example, you (and your dog) can get [giardia](_URL_1_) from drinking untreated water from rivers\/lakes in the USA\/Canada. \n\n* **Wild and domestic animals also get sick from eating contaminated food and water sources.** This is true for carnivores, omnivores, and herbivores. Some animals are susceptible to the same things we are, others have unique diseases to their particular species.  Some species are better equipped to handle foodborne illnesses, like the carrion eaters (vultures) but even they are not 100% perfect. For example, in a wild population of monkeys every single individual was host to a couple of species of tapeworms. Generally, animals and humans can withstand a certain parasite load (foodborne illness load) without compromising health but it is difficult for the old, young, weak, or sick to withstand heavy infestations or reoccurring exposures.","label":0,"model":"human","source":"reddit","id":2882}
{"text":"I don't know the definitive answer to this, only speak to my own experience and its 2 reasons. \n\n1) when recordings a song, I have played it probably hundreds of times before goi g into the studio at a tempo I find most comfortable and pleasing and best suited to the song. When it comes to work out the tempo chances are it won't be a a round number. I'll go in to work it out thinking it's somewhere around 110, and after a mess around with the metronome find that 108 is a much more comfortable and natural feel for the track. \n\n2) it may not be consciously noticeable, but if a song is around 120, recording at 119 or 121 will give the rhythm the sense of just holding back or just pushing forward. That will be a very deliberate choice and as I say, one that's only perceptible to the listener subconsciously, but you'd be amazed what a BMP or 2 here and there can do to a track and the different feeling it can give\n\nThere's another way of doing the above, by having the drummer play behind or ahead of the click, but on electronic music (and even for live bands) it's probably a hell of a lot easier just to drop the BMP back a beat etc","label":0,"model":"human","source":"reddit","id":2883}
{"text":"In the beginnings of the automobile era, Michelin, a tire company, created a travel guide, including a restaurant guide. Through the years, due to their high standards and very strict anonymous testers, Michelin stars have become very prestigious.\n\nNo one but Michelin knows exactly the criteria, but they have gone out to say that a decision (either adding or removing a star) requires several different anonymous testers's testing the restaurant at several points throughout the year.\n\nMarco Pierre White, one of the very few chefs to at one point have 3 Michelin Stars, said that you can get one or two stars for having amazing food. But to get the third star, you need an amazing experience overall\u2014the appearance of the restaurant, the waiters, everything must be of the highest quality. You can't get it with food alone.\n\nedit: Interview link: _URL_0_ \n\nThe rest of boiling point is worth a watch as well if you're interested in the inner workings of a (later) 3 star Michelin restaurant. As well as a very young, passionate, and angry Gordon Ramsay.\n\nedit:\n\nJust to add, Michelin stars are no joke. They are incredibly coveted. Gaining just one can change your life; losing one, however, can change it as well. A french chef, fearing from just rumors that he may lose his third star, committed suicide. That's how much a star means to chefs.\n\n_URL_2_\n\n3rd edit:\n\nHere's what a [Michelin star actually looks like](_URL_3_). [And here is it in the actual book](_URL_1_). \n\nA bit strange to think that that little squiggly clip art-esque star printed next to your restaurant's name in what looks like a Microsoft word table is worth more than any medal or award you could give to a chef.","label":0,"model":"human","source":"reddit","id":2884}
{"text":"The hypothalamus is the control or master gland of the body controlling all the other endocrine glands of the body, including the testes in boys and ovaries in girls.\n\nAt birth there is normally a mini puberty lasting about 6 months where the level of a hypothalamus derived hormone called GnRH (or LHRH in some countries) is very high, which causes the pituitary gland to release two hormones called LH and FSH which in turn act on the testes and ovaries.\n\nLevels of GnRH then fall away to practically zero, as do levels of LH and FSH until the time puberty is due.\n\nThere is a gene called GPR54 and this is thought to be a puberty regulation gene as it helps to set the time of puberty by releasing a hormone called Kisspeptin which tells the hypothalamus when to release GnRH to start puberty.\n\nThe timing is set so when the body is big enough to be able to become fertile, puberty will be initiated by allowing the release of Kisspeptin. This timing can be influenced by external and genetic factors where puberty can start early or not at all (as in in Kallmann syndrome). Timing of puberty can be influenced by poor diet or excess fat in the diet which affects the levels of certain hormones or is can be influenced by excessive exercise.","label":0,"model":"human","source":"reddit","id":2885}
{"text":"Edit one point that I've remembered having trouble with when I first learned this stuff *many* moons ago: all of this is only about how we write a number down. It makes no difference at all to the value of the number. six plus one is seven regardless of whether you write is in base 10 as 6+1=7 or in binary as 110+1=111. It's like writing a word in a different language, a spoon is still a spoon.\n\nSo, base 2 binary) and base 16 (hexadecimal):\n\nEvery four digits of a binary number (after you add zeros to the left side to make it a multiple of four long, eg binary '110' needs to be '0110') is the same as one hex digit 0-F.\n\nIt doesn't work out as nicely in Base 10 because the binary 1,2,4,8,16,32... columns don't match up with the decimal 1,10,100,1000.\n\nThat's why it's easier to work in hex when doing things related to bits on computers \n\n\n    Base 10:|10|1 | Base 16:|16|1 | Binary:| 16| 8 | 4 | 2 | 1\n    --------|--|--|---------|--|--|--------|---|---|---|---|---\n          ||0 |      || 0 |        || 0 | 0 | 0 | 0\n          ||1 |     ||  1 |        || 0 | 0 | 0 | 1\n          ||2 |      || 2 |        || 0 | 0 | 1 | 0\n          ||3 |      || 3 |        || 0 | 0 | 1 | 1\n          ||4 |      || 4 |        || 0 | 1 | 0 | 0\n          ||5 |      || 5 |        || 0 | 1 | 0 | 1\n          ||6 |     ||  6 |        || 0 | 1 | 1 | 0\n          ||7 |     ||  7 |        || 0 | 1 | 1 | 1\n          ||8 |     ||  8 |        || 1 | 0 | 0 | 0\n          ||9 |     ||  9 |        || 1 | 0 | 0 | 1\n         |1|0 |    ||  A |        || 1 | 0 | 1 | 0\n         |1|1 |    ||  B |        || 1 | 0 | 1 | 1\n         |1|2 |    ||  C |        || 1 | 1 | 0 | 0\n         |1|3 |    ||  D |        || 1 | 1 | 0 | 1\n         |1|4 |    ||  E |        || 1 | 1 | 1 | 0\n         |1|5 |    ||  F |        || 1 | 1 | 1 | 1\n         |1|6 |   | 1|0  |       |1| 0 | 0 | 0 | 0\n         |1|7 |      |1|1 |       |1| 0 | 0 | 0 | 1\netc...\n\nYou can see how the rollover points are much better aligned for hex and binary than decimal and binary. That makes it much simpler to convert binary to hex than to decimal, and vice versa.","label":0,"model":"human","source":"reddit","id":2886}
{"text":"It's important to recognise the difference between the chemicals called \"octane\" and what the \"octane rating\" of a fuel is.\n\nOctanes are chemicals which contain carbon and hydrogen in the ratio 8 carbon to 18  hydrogen. These are widely found in refined petroleum products. There are many different octanes, with slightly different properties. When it comes to determining \"octane rating\" of a fuel is, a specific octane called \"2,4,4 trimethylpentane\" is used.\n\nThe octane rating of a fuel is a measure of how easily a fuel ignites in a gasoline engine. In a gasoline engine, you need the fuel to be difficult to ignite. When an gasoline engine works, air and gasoline are sucked in, then the mixture is compressed, after it is fully compressed it is ignited, and the heat and pressure from the burn makes the engine go. \n\nBecause the process of compressing the air\/fuel mixture creates a lot of heat, it is important that gasoline does not ignite during this process. The mixture must only ignite when a very hot electrical spark is generated - at the same time, when the spark goes, the fuel must ignite gently, and not just explode - because the force of the explosion can damage the engine. \n\nOctane rating measures how resistant the fuel is to pre-ignition (self-igniting during compression) and detonation (exploding when hit with a spark). An \"octane rating\" of 100, means that the fuel has the same resistance to ignition as pure \"2,2,4 trimethylpentane\". An \"octane rating\" of 0 means that the fuel has the same properties as pure \"n-heptane\". An \"octane rating\" of 90, means that the fuel burns similarly to a 90:10 mixture of 2,2,4 TMP and n-heptane. It is possible to have ratings higher than 100, for fuels that are even more difficult to ignite than the reference \"octane\".\n\nThere are lots of additives which were used to improve \"octane rating\" - most called \"organometallic\" compounds. The most notorious was tetraethyl lead (usually just called lead) which is a very strong fuel ignition stabiliser. This was banned because of toxicity. Many oil companies replaced it with a different organometallic called methylcyclopendienyl manganese tricabonyl (MMT) - but this is now mostly banned, because it damages catalytic converters. These days high octane fuel is made by adding lots of \"aromatic\" hydrocarbons produced from advanced refining methods, and\/or adding ethanol. \n\nGasoline engines are designed to take fuel of a specific octane rating. The amount and speed with which an engine compresses a fuel-air mixture is fixed in the design, and nothing can change that. This means that if you have an engine which uses very strong compression (e.g. a highly tuned sports car engine), and put low octane fuel into it, the fuel may \"detonate\" during compression - causing the engine to \"knock\" or \"ping\/pink\". The knocking noise is the sound of the fuel exploding - as the engine is not designed to take that pressure, it will suffer severe damage.\n\nThere is also the issue of detonation when the spark hits. If the spark comes soon after compression, the fuel is very hot and more likely to detonate - but igniting early gives the best burn time, and therefore best engine power and best fuel consumption. Igniting late allows the compressed fuel to cool down, but wastes burn time and produces less power. Older cars would be manually tuned to a specific \"ignition timing\" by a garage - so you could tune a car to work better on higher octane rating fuel by adjusting the timing. \n\nModern cars use electronic ignition and timing control. The car has a microphone bolted to the engine which listens for the sound of the burn. The engine computer analyses that signal and uses it to adjust the ignition timing. If it hears detonation starting to occur, then it delays the timing - if everything seems to be going OK, then it'll try advancing the timing. The computer tends to be programmed with limits based upon typical fuel ratings in the country the car was sold in. So, if you put 110 octane race fuel into a corolla, the computer won't super tune the ignition timing, it'll move it up to where the manufacturer expects premium fuel to be, but go no further.\n\nIf you put low octane fuel in, the computer will try to delay the ignition timing - but that can only do so much - and if the problem is that the fuel is igniting during compression, there is nothing the computer can do, except put on the check engine light and hope that you stop before you blow the engine.","label":0,"model":"human","source":"reddit","id":2887}
{"text":"This is a really fascinating question. There's a book I highly recommend reading that discusses this very topic, called \"Guns, Germs, and Steel\" by Jared Diamond. For the sake of answering your question, here's a couple of the reasons the book posits.\n\nFirst and foremost, the relative ease of domesticating wheat compared to maize. The type of wheat that was grown by early civilizations was genetically almost identical to wild wheat. Maize required extensive artificial selection to become even the rather mundane crop that it was when Europeans arrived in the Americas. Wheat could also spread equatorially, which means fewer adaptations were required for surviving in different climates. Maize was domesticated in South America, had to adapt to survive the much hotter Central Americas, and then adapt again to survive the cooler North America. Early human's artificial selection to make maize a proper cereal crop took millenia, whereas domestication of wheat occurred quickly. Wheat also has a lot more protein in it than maize, which means it requires less supplement from other foods. All this stuff seems little, but the presence of a robust and nourishing cereal crop was one of the foundations of every early civilization. Settling down means you can't forage, and providing surplus food allows for people to specialize in things like \"making tools\" or \"writing things down\" instead of hunting all the time. These advances occasionally lead to improved farming techniques, and consequently a larger food surplus.\n\nThe lack of domesticated animals in the New World was also likely a contributor. Humans brought protodogs over the land bridge from Russia during the last ice age, but by then we were already superpredators. Almost every large land mammal that existed in the Americas was extinct within a few thousand years (bison and llamas being two of very few exceptions). Species like horses, pigs, cows, and sheep had evolved alongside humans and had developed means of avoidance or cooperation. The humans that migrated to the Americas were almost like an invasive species. Of the few large animals that the Native Americans left alive, only one ever was domesticated, the llama, and its use was quite isolated. Domesticated animals (especially the horse and cow) allowed Eurasian civilizations to benefit from nonhuman labor, pulling plows, turning mills, all work that is backbreaking for humans and... somewhat less so for other animals (I'm not going to pretend that humans were nice to these animals, but having four legs is an advantage in pure traction). Living in close proximity to such a diverse group of animals also bred powerful new diseases. Tuberculosis, for example, is believed to have jumped to humans from animals. These diseases grew more powerful in the many densely-packed cities that formed in Eurasia, and ravaged the defenseless Native Americans when the two cultures met.\n\nThese advantages basically gave Eurasian people a couple thousand years or so head start, and as I mentioned before, the production of surplus food allows specialization and technological innovation, which often compounds the surplus and drives further growth. But I do highly recommend that book if you're interested in the development of ancient cultures.","label":0,"model":"human","source":"reddit","id":2888}
{"text":"The theory that I have heard is that it relates to signaling. The ability to build up fat stores is indicative of success.  It means that you have whatever combination of brains and brawn and social skills needed to accumulate more than you need to just survive.  \n\nSo why the butt? Well, for our ape ancestors, who walked on all fours, the butt was the best place to put something like that.  Putting it on your torso would mean it was hidden much of the time, and just by the nature of how primates are constructed, putting it on the back would be too difficult.  \n\nIt also connected to birth, in the sense that it gave an illusion of wider\/bigger hips.  So you could imagine a sequence like this: males that are attracted to females with bigger hips produce more offspring, since birth is easier. Females \"imitate\" bigger hips with fat deposits. Males who go after females with either wider hips OR more fat deposits tend to have more off-spring. This cycle would only become more pronounced as humans began having babies with bigger and bigger heads. \n\nInterestingly, at least according to Jared Diamond, the most likely reason that men are attracted to breasts is that, as we began to walk upright, it became harder and harder to put the extra fat deposits on the butt, and so females shifted it to the chest, where it was even more visible on a biped.  It would make sense that, despite this shift, males would still have some residual interest in the butt as well.","label":0,"model":"human","source":"reddit","id":2889}
{"text":"The names stands for *Islamic State of Iraq and Syria*. Strictly speaking, they've changed their name to just *Islamic State*.\n\nThe organization actually dates back to 2003, when it was founded to fight against the U.S. invasion of Iraq. After the Syrian Civil War began in 2011, ISIS became one of the most successful factions fighting Assad. In fact, ISIS took over much of Syria's territory. Then, just this year, ISIS crossed the border into Iraq. Although the U.S. had been training them for years, Iraq's military collapsed like a house of cards and ISIS swallowed up much Iraq's territory.\n\nISIS has now declared itself and the territory it controls to be a new country, which it calls simply \"the Islamic State\". ISIS says that it is the new caliphate, a claim accepted by very few Muslims apart from themselves. Life under ISIS really sucks if you're not a extremist Sunni Muslim like them. Also, if you happen to visit ISIS's territory, it's strongly recommended that you try not to be gay or a woman.\n\nEDIT: Okay, ISIS hasn't simultaneously banned being gay and female. I was using this thing called \"sarcasm\" to comment on their treatment of gay people and women. Apparently, some people had trouble with this.","label":0,"model":"human","source":"reddit","id":2890}
{"text":"As for the answer to the question - yes. \n\nI am going to use this question as an opportunity to clear up some misconceptions that people have. Almost no layman correctly understands electricity. People usually think, that electricity is some magic substance, say electrons, that is \"produced\" in the power plants, transported through the wires into your house where your appliances \"consume\" it. Similar to gas or water. **But that's exactly how it doesn't work.**\n\nThe best analogy for understanding the AC system, is to imagine that instead of an electrical socket, a rotating axle is sticking out of your wall. Actually, in the early days of industrialization, that's how factories worked. They had a huge steam engine in the basement which was connected to the belt, gear and axle system. This system distributed the rotating movement around the building and all the machines on the factory floor were hooked to this single steam engine. \n\nSo in a totally same way, your little axle in the electric socket is connected all the way to the power station that has a huge rotating engine inside. Whenever you want to power some appliance, you just hook it up to the axle and draw the rotating motion out of it. \n\nSo the electrons are not the electric energy itself, they are the \"axles and belts\", that just carry the force (the rotating motion) produced at the power plant, right to your home.\n\nThis analogy is very precise. It explains all the things about the AC system like:\n\n**Why does the production of an electric power always have to equal the consumption?**\n\nWhat happens when you reach the top of the hill in a car, and keep up the constant press on the accelerator pedal? You will start speeding up! Your engine suddenly produces more power than the car needed when going uphill, so it has to go somewhere. On the contrary, If you reach another, steeper hill, and don't press the pedal enough, you will eventually lose your speed and stop. \n\nLiterally the same thing happens to the AC electrical system. All the power plants rotate at the same rate of 50-60 revolutions per second (depending on your country). If not enough consuming devices \"brake\" the common axle, it will really start to rotate faster and faster. If there is too much load, it will eventually bring all the system to a grinding halt. \n\n**What happens when I cut the wires that go from a power plant?**\n\nThe plant engine will suddenly lose all the load and will start spooling up like crazy, just like your car when you lift the wheels up from the ground. The operators will have to enact some emergency procedure of reducing the power quickly, like for example, release the steam that drives the turbine.\n\nOn the other side of the system, all the rotating engines in all the other power plants will start slowing down, because the load is higher than they can produce. If the network operator does not have a backup engines that are ready fill the missing power quickly, the easiest solution that saves the whole network, is just to sacrifice some part of it and \"cut off\" some of the load.\n\n**What power plant am I drawing power from?**\n\nIn a sense, from all of them. All of them are hooked to the same \"axle\" and all consumers are drawing the rotating motion from it. You cannot point to a single power plant that powers you right now.","label":0,"model":"human","source":"reddit","id":2891}
{"text":"Like you're Five:  On the day you get your allowance, you buy a bag of candy.  The next day, you want more candy, but you spent your allowance, so you ask your brother if you can borrow his allowance, and pay him back with your next allowance. You buy another bag of candy.  The next day you ask your sister if you can borrow her allowance, and promise to pay her back when you get your allowance. You buy another bag of candy.\n\nWhen you finally get your allowance, you realise you're in trouble - you can't pay your brother and your sister.  You get so worried about it that you go buy a bag of candy instead.  When you get home, you get in a big fight with your brother and sister about it. \n\nWhen your Mom asks what you're fighting about, your brother and sister tell her that you borrowed money and you won't give it back.  She asks you why not, and you say that you spent all of the money on candy, and you don't have any money left.  She sighs, and makes you give all the candy you have left to your brother and sister.  They want to know when they get their money back, and she tells them the money is gone, and they need to stop fighting with you and forgive you.  They say that that isn't fair, and she says that it really isn't, and that they should remember this the next time you ask them for money.","label":0,"model":"human","source":"reddit","id":2892}
{"text":"(Throwaway cause I'm admitting to shady stuff in this post)\n\nI know quite a bit about this scene because I used to run a tracker (it was a tracker dedicated to anime, nothing as big as thepiratebay). We didn't run any advertisements in our website, what we did was just ask people to donate for hosting, we only asked for the amount that we needed for hosting. We did this like every 6 months and in a few days we'd find somebody that'd pay our server bills.\n\nWe did this because we didn't want to profit from it, not only for ethical reasons but also in the hopes that showing we didn't profit would give us some legal protection\/leniency if we got caught.\n\nThe vast majority of trackers and sites like that, however, were run for a profit. Thepiratebay was among those 100%-for-profit enterprises that turned a huge profit while portraying themselves as uninterested martyrs. The truth is that bandwidth is hilariously cheap if you don't have specific stability concerns for it. For a tracker, it's irrelevant to have an uptime of \"just\" 98% or a packet loss of 2% (too much for enterprise) or a latency that is 50ms higher than it should, but this kind of bandwidth is really, really cheap. I made calculations once and the piratebay could have been paid for by showing a single banner one day a month. But they had a shit-ton of banners all month long, all that was profit.\n\nThe reason there was a conviction is that as soon as numbers were presented in court it became clear to everybody present that the pirate bay was a cynical cash-grab and not an ideologically-driven internet community.\n\nEDIT: Here is the verdict in English, _URL_0_ in pages 53 and 58 you can read how the commercial nature of the operation was indeed a factor regarded in the severity of the penalties.\n\n >  It has been confirmed that the operation of The Pirate Bay has generated advertising revenue\nwhich, during the period indicated in the indictment, has amounted to at least SEK 1,200,000.\nOn this basis alone, the District Court can conclude that the operation was carried on as a\ncommercial project. This conclusion is confirmed by the correspondence between the\ndefendants and the fact that the defendants have investigated and discussed various corporate forms which may have been applicable to the continued operation of The Pirate Bay. It has,\nconsequently, been a question of an operation carried on in organised form. The\ncircumstances mentioned here also indicate that an increase in the penalty may be\nappropriate.","label":0,"model":"human","source":"reddit","id":2893}
{"text":"I can speak to the evolution of USB.\r\n\r\nUSB started out in the mid 90s when PCs ran at maybe 33MHz.  Microsoft was pushing Windows 95 with Plug and Play where devices could be hot-plugged., discovered and described themselves enough so that the OS could find a driver and get it running without a reboot.  Laptops were big and bulky largely because people wanted an external mouse port, a video port, power, a printer port, and sometimes a joystick port.  Also external storage would be nice but we were still buying slow drives with megabytes back then.  \r\n\r\n\r\nAll of these connectors were different and large.  So Intel and Microsoft a few other companies developed  USB to kill two birds with one stone, *cheaply.* USB got rid of all those connectors except video and power, and made everything self-describing for Plug and Play.  \r\n\r\nTo keep it cheap, they kept the signal rates low but good enough for printers, Ethernet adapters, CDs and floppies, and there was a super-cheap option for mice and keyboards, so cheap that you couldn't have a detachable cable because they were so bad that they couldn't guarantee interoperability.  The connectors were designed to be *tough* and cheap and well-shielded., so what if they weren't reversible they were a bazillion times better than a DB25. They had different plugs at each end to prohibit peer-to-peer connections, such as plugging one PC unto another, which wouldn't work and presented power conflicts.\r\n\r\nAnyway, fast forward a few years and USB took off and was a huge hit, especially thumb drives, (which came out of Israel and really upset the storage market -- sayonara, floppies and CD-R!).  Also people were hooking up many, many devices, so they needed more speed but the industry didn't want new cables and connectors.  It turned out that the good, certified 12 Mbit cables were over-engineered enough to support 480 Mbits if the chips at both ends were changed, so they invented USB 2.0 which could run all the old hardware, use existing cables, and run new hardware 40 times faster at 480 Mbits, on par with Firewire at the time.  USB could challenge the digital video market at last but could not yet take on displays.\r\n\r\nAbout this time they created some thinner connectors for the portable market. This made it the default phone connector (except Apple) but as a side effect also made it the default charger, which it was *not* designed for.  More on that later, but a *lot* of cheating happened to get more power from these fast but underpowered ports.  They also invented Wireless USB which never caught on because of that damned Bluetooth...\r\n\r\nFast forward a few more years.  PCs are now running in GHz and drives are now running GBytes.  More speed and more power, please! But we still want the old connectors... So they added five more wires to the new connector, right at the end of the PC connector, but had to make the device side plug bigger.  But, they did it in such a way that old plugs would still fit in the new sockets, and the new host plug would fit in an old PC and still work at the old speeds,.  The new signals supported 5 GBits, alongside the 480 Mbits on the old wires.  They also bumped up the power budget by 80% for new devices.  USB 3.0 was born.  However, they intentionally over-engineered the cables and connectors for the future, up to about 25 GBit capacity.  USB was finally able to run displays and an entire company was built around making USB3 display adapters: DisplayLink.  Say what you will about their drivers, but they made quad monitors on a laptop possible.\r\n\r\nStep forward a few more years, and they did the expected USB thing, they doubled the speed to 10 GBits with USB 3.1.\r\n\r\nBut now it was time to take out the last plugs on the laptops: Power and audio.  (WiFi had already killed the Ethernet jack for them.)  To do this, they needed a new plug that could handle the power.  So, they also took on the biggest complaint about the plugs: Different plugs at each end of the cable, and you couldn't flip them.  Along came Type C and USB Power Delivery.  Type C fixes the connector issues, and Power Delivery offers  power up to 100W, and can go either direction, so your laptop can power a portable display, or you can charge your laptop through it.  This was attractive enough to bring Apple to the table as a significant contributor.  \r\n\r\nBut there's a bonus: Type C has twice as many high speed conductors as the legacy cable and half of them aren't used because they support the flipping feature.  So, they added Alternate Modes where the device and host can negotiate other uses.  One mode is an analog audio mode (Adios headphone jack...), another is DisplayPort (Hello, dual 4K!) and finally, another is Thunderbolt.  Thunderbolt takes over the whole cable to send a fat 40+ GBit pipe carrying both DisplayPort and PCIe to a Thunderbolt dock chip at the other end, that can then support USB3 controllers, video cards or whatever. And finally, Type C is **still** compatible with 20-year old mice via a cheap adapter.\r\n\r\nWhich brings us to today.  \r\n\r\nTL;DR So, USB has evolved as needs grew, costs fell, and chips got better.  They kept compatibility with old hardware for two decades, which cannot be said of any other PC technology except maybe DVI.\r\n\r\nType C is One Plug to Rule Them All.\r\n\r\nSource:  Me.  I was one of the authors of the USB standard and have been in it from the beginning.\r\n\r\nEDIT:  Many thanks \/u\/jimmy5462 for the gold!!  Never confuse gilding with gelding!\r\n\r\nEDIT2:  For all the gratitude, you are all very welcome, it has been quite rewarding.  However, USB is the work of hundreds.  The specs were authored by dozens, and dozens more contributed to class specs, test methods, marketing, administration and myriad other things to make USB what it is today.  Thank us by buying certified products, pushing non-compliant crapware out of the ecosystem.\r\n\r\nEDIT3; A second, anonymous gilding! I didn't know such a thing existed. Many thanks to the anonymous Swede.  Well, I assume it was a Swede because the inbox message looked a lot like  opening credits to Holy Grail...","label":0,"model":"human","source":"reddit","id":2894}
{"text":"Domestic dogs (and cats) have been preferentially bred for being non-aggressive (amongst other things) for thousands of generations. As a result they are psychologically a weird; they act alot like immature wild dogs (and cats). Wild dogs don't bark, but wild dog pups do. (Again, same with meowing in cats)\n\nThis is why adult domestic dogs love to play, while adult wild dogs rarely do. 'Fetch' games seem like a version of the kind of play wild pups might do, chasing 'prey'.\n\nMany domestic dogs were also bred for hunting, and the ability to fetch the duck you just shot (or conked with a rock) would be a behaviour that would be favoured. \n\nI don't know if such a behaviour could be 'hardwired', and dogs can't inherit a learned behaviour, but I'd think that dogs who had a natural tendency to 'retrieve' would be favoured, and bred more than the dogs that eat the duck, then run round in circles while your neighbours laugh at you. Like my dog... (won't fetch, thinks he's above it. Or maybe he's just too smart)","label":0,"model":"human","source":"reddit","id":2895}
{"text":"Damaging health claims are nonsense. No reputable studies have ever been produced linking health concerns with GM crops. It's just the same kind of tenuous link people make because the rise of cancer diagnoses and the use of GM crops roughly correlate (like autism and vaccines, or ice cream and polio).\n\nAs for environmental concerns there are some concerns that are reasonable and some that are not. GM crops are often bred to be resistant to pesticides, which can lead to pesticide overuse. However the bigger problem as it turns out isn't that it kills off most of the plants in the area, but actually that it encourages resistance to pesticides, which is not good for agriculture. They may also produce pesticides. The genes that allow the plants to do this can escape into the wild and negatively impact insect populations. GM crops may also exacerbate monoculture issues, which are issues that result when lots of plants are genetically similar, making them vulnerable to disease. Part of GM research is involved in the introduction of apomixis to crop plants, which is a form of asexual reproduction that makes plant progeny identical to the parent.\n\nThe stuff about Monsanto being terrible is pretty much nonsense as well. They haven't actually done a lot of what they're accused of doing, and really they seem like a normal company.\n\nFinally, GM crops are useful in that you can increase yields and encourage plants to grow in otherwise hostile areas. However, a lot of the world's food problems come from logistical problems and corruption, so it's overly optimistic to say they'll solve everything. They're a useful and interesting method that should yield some good results though.","label":0,"model":"human","source":"reddit","id":2896}
{"text":"Because they claim to be calling out the worst parts of Reddit on ignorance on bigotry but in fact may be the most ignorant and bigoted subreddit in existence. They invent conflict and intent where it does not exist, they hypocritically make extremely disturbing and unfounded accusations and generalizations that are more fucked up than anything they complain about (I saw a post where an SRSer claimed that most males on reddit \"probably have raped someone, but they don't want to admit it to themselves\") and then claim that it's \"all part of the really funny circle jerk and you just don't get it\". Which is ironically the logic they so viciously berate Redditors for using to defend slightly off-color comments and jokes.\n\nBut the biggest reason is that they instantly perma-ban anyone who dares to question the raging mob of self-aggrandizing inoffensiveness. People in that subreddit will call out individual users by name, and say how that person is a pathetic, sexist, racist, virgin who has probably raped someone and then instantly ban anyone who claims otherwise, especially the person they are so gleefully tearing to shreds. They are taking the worst parts of people like the extreme Christian Fundamentalists who wrap themselves in a shield of God's love and then tell every normal person they are going to burn in hell for all eternity because they are a sinner and pure evil, and take any dissent as an all-out attack and respond like it's war. Anyone who disagrees with SRS's militant justice must be pedophiles and neo-nazi's. They are the most fucked thing I have ever seen on this site. \n\nEDIT: Oh lord, they found me. This post has taken about 80 downvotes today. Last night it stopped moving at about 400, but this afternoon every time I check a reply it's less and less. Not a downvote brigade my ass.","label":0,"model":"human","source":"reddit","id":2897}
{"text":"For your information, this is a much bigger problem in LCD\/LED TVs than it is in plasmas. In fact, high end plasmas will not have this problem at all unless for some reason you have motion interpolation turned on (The feature is called something different from every manufacturer i.e. Panasonic is IFC while LG is TruMotion). Just turn it off and poof, the problem disappears.\n\nLED\/LCD on the other hand has much more motion blur than plasma, so they have to \"interpret\" what is there and create new frames to \"smooth\" out the picture, which tends to be great for sports, but terrible for anything that was filmed. \n\nTo answer the question more directly though, most movies and TV shows are shot at 24 frames per second, but because of these added frames for \"smoothing\" it tends to look more like it was shot with much more frames per second than that. Not so coincidentally, cheaper productions such as soap operas shoot at 60 frames per second, which is what this interpreted video looks like, and hence the term for it being the \"Soap Opera Effect\"","label":0,"model":"human","source":"reddit","id":2898}
{"text":"If you were hurt somewhere, the part of the body that's hurt sends a signal to the brain saying \"I'm hurt!\".  It uses things called \"nerves\", which are like little wires, to send that information to the brain.\n\nThere's a lot of nerves, which is why you know exactly where you've been hurt - the tip of the finger, the back of the hand.  They all travel together as a \"nerve bundle\" all they way up to the brain.\n\nThat bundle is called the \"Ulnar Nerve\", and when it travels through the elbow, it is exposed between your skin and your bones.\n\nIf you bump the elbow, you're crushing that nerve bundle between whatever you bumped into and the bone. This sets off all the nerves, and it's not just one nerve saying \"I'm hurt!\".  It's all of them, all at once.  Just like when people shout together, it's louder than if just one shouts, this makes the pain seem stronger than it should be.\n\nNot only that, but there are other nerves that don't send signals of pain.  They send all sorts of signals - hot, cold, touch, and others.  At they same time as the nerves should \"I'm hurt\".  You've got all the other nerves saying \"I'm hot!\" \"I'm cold!\" \"I'm being touched!\".  \n\nThe brain doesn't understand all of these contradictory signals, so you get a \"tingly\" feeling in addition to the pain.  It takes a little bit for the nerves to recover from the bump, which also contributes to this sensation.","label":0,"model":"human","source":"reddit","id":2899}
{"text":"I'm not sure France has the *best* health care system in the world, but it's certainly a good one. And their health outcomes are demonstrably better than ours.\n\nBut a few observations that help answer your question.\n\n1. First of all, the United States is much more center-right than center-left, politically. There are a huge number of reasons that might be the case, but it's undoubtedly true. We are, politically, significantly to the right of almost any part of Europe.\n\n2. Second, the word \"socialism\" in the US has a historical stigma. When Americans think of socialism, they don't think of Sweden or Norway, they think of Stalin.\n\n3. With respect to health care specifically, most Americans who have insurance are actually *extremely* satisfied with the quality of the care itself. The availability of care is the problem.\n\n4. We're seldom satisfied with the quality of service we receive from government agencies. Take the Post Office vs. FedEx. Walk into any Post Office and you see long lines, metal countertops, no working pens. Walk into a FedEx Kinkos and there's seldom a line, service is better, there's wifi, etc. Same applies, we at least imagine, to health care. There's a world of difference between the leather couches and complimentary bottled mineral water at my fiance's OBGYN and the amenities and service you find at a free clinic run by the government. I'm not a fan of our health care system, but for those who do get care and don't have to fight with their insurance companies, care isn't just good; it's often luxurious.\n\n[typo]","label":0,"model":"human","source":"reddit","id":2900}
{"text":"Well, there are a few variables to consider, \n\nE.coli depending on the strain can survive in a whole slew of different types of environments for prolonged times. Some are simple soil bacteria while others are like the ones in the digestive tracts of animals including humans. Most of the ones that make people sick involve variants that live and the digestive tracts and feces of other animals. \n\nNow, flour also has a relative humidity of its own. That is, it is not a perfectly dry product and various strains of coli-forms can survive in fairly low humidity conditions for prolonged periods of time. This relative humidity is also dependent on the moisture of the surrounding air to some degree.\n\nAfter all of that, the big question about the recall would revolve around trying to figure-out the source for the e.coli contamination. Assuming its a strain of e.coli that makes people sick... How did something that largely lives in fecal material find its way in to flour products? What else is there in the product that should be of concern if consumed raw or when looking at potential sources of cross contamination to other subsistence? \n\nEdit: a word","label":0,"model":"human","source":"reddit","id":2901}
{"text":"I eat meat from factories that treat animals in ways I would not treat my worst enemies. I wear clothes made in factories that children suffer in. I pay taxes to a government that uses half of them to commit murder in my name. I have enough so that I might feed others that are starving and I don't. The coffee, tobacco, and fast food I eat are slave labor products that are produced in ways harmful to people and the planet. I use so much gas and electric that I would need an army of slaves to maintain my lifestyle where those resources unavailable. From an outside perspective I'm an aristocratic monster, or just a typical American. If I think about it too much I get depressed. I'm going to go drink more coffee and work on my portfolio. Oh god you can make a lot of money investing in banking and oil. I hope that those I have caused to suffer might forgive me, for I do not choose this lifestyle out of malice but instead convenience, and though that excuse is pathetic it is the only one I can think of, and I will not change.","label":0,"model":"human","source":"reddit","id":2902}
{"text":"It isn't well known the reason yet exactly, but there are a few theories about why we cry. First of all, tears come from our tear ducts whose primary function is to cleanse the eye. Scientists in the field have found that humans produce three distinct types of tears:\n\n1. Basal tears meant to moisten the eye (after blinking usually)\n2. Reflex tears that are caused from irritation of the eye in order to flush the eye. \n3. Emotional tears that result from sadness, pain, and even joy. \n\nNumbers 1 and 2 have very clear purposes, but number 3 is a tough question to answer, and many theories have been developed to try to explain why crying may have been a beneficial reaction to emotion. Here are a few I have found.\n\n**Communication:** When we are unable to speak to each other, maybe as infants or even before humans had developed the ability to clearly communicate, crying may have served as a form of telling others that something is wrong and needs to be addressed. An example that is easy to relate to is a crying baby. Usually when the baby cries, this means that he needs to be addressed for one thing or another, even if we might not know exactly what it is. \n\n**Stress Relief**: \"Studies have shown that emotional tears contain more manganese, an element that affects temperament, and more prolactin, a hormone that regulates milk production. Sobbing out manganese and prolactin is thought to relieve tension by balancing the body\u2019s stress levels and eliminating build ups of the chemicals, making the crier feel better\" [Source](_URL_0_)\n\nSo adrenocorticotropic hormon (ACTH) is also found in tears. ACTH is released from the pituitary gland and in turn increases our levels of cortisol. Cortisol is known as the \"fight or flight\" hormone, similar to that adrenaline rush we are all familiar with. It increases our blood pressure and blood sugar preparing the body for activity.\n\nWhen in situations that are stressful however, cortisol isn't needed because there isn't any physical labor required. So ACTH is builds up but no cortisol is being made. Crying releases this ACTH via tears. \n\nMany animals have been found to cry as well, such as elephants who mourn the loss of a relative, and certain apes also express this characteristic. \n\nUltimately, the reason why we actually tear up is unknown but all of these theories give good insight in why that may be. I hope that helps you understand somewhat better. \nEDIT: added info","label":0,"model":"human","source":"reddit","id":2903}
{"text":"Because like any multinational, Nestle has gotten up to some shady shit.  \n\nThe one that seems to rustle jimmies the most was Nestle aggressively marketing baby formula to new mothers in developing countries.  Saying things like it was better for the baby than breast milk.  The new mothers would be given free formula while they were in hospital but after they were discharged, they had to buy it.  The problem now is that those mothers had now stopped lactating since they didn't breast feed the baby so they are forced to buy expensive formula they can't afford to feed their babies.  Even if they could afford it, water in those countries isn't exactly safe to drink, exposing babies to more risk.  Babies died.  \n\n\nThere are more controversies Nestle was embroiled in.\n\n* Getting the definition of access to drinking water changed from a 'right' to a 'need' with the CEO quoted as saying \"access to water should not be a public right\".  \n* Attempting to strong arm Ethiopia to repay $6m in debt when the country was going through a famine   \n* Child labour use in their cocoa suppliers  \n* Price fixing of chocolates in Canada   \n* False advertising claims","label":0,"model":"human","source":"reddit","id":2904}
{"text":"I think the depth of knowledge required of today's society is far greater than what it was in the renaissance; for example, a modern engineer is far more knowledgable than Da-Vinci, and so are the biologists and painters. If someone reached Da-Vinci skill levels in everything he did, but in a modern context, they would just be someone who is quite good at a lot of things, but far from being notable in any of them, as they would be competing with a global industry of people who have been training their whole lives in just that one thing.\n\nIt takes far too long and too much specialisation to reach the level of skill\/knowledge on something that would make you globally notable these days. \n\nAs mentioned by others here, what makes them notable is their *relative* knowledge levels compared to the rest of the world. They didn't know much, but jeez, compared to everyone else back then, they knew everything. And they got there without the internet, supportive government and societal infrastructure, and without a lot of the inspiration we have now.\n\nThat sort of knowledge and skill monopoly concentrated in one person is unlikely to happen again, due to the sheer amount of time and effort it takes to be the best at something these days; to be great at something means sacrificing the necessary time to be good at something else. Also back then they didn't have electric lights, appliances, modern transport, communications and information infrastructure we have; just daily life was so time consuming that there was far less time for creativity and learning. The standard of skill and knowledge among everyone else was far lower. The amount of free time we have now allows people to specialise in a skill and reach levels of proficiency in months what would have taken years for someone many years ago, there are now people with after-work hobbies that are better skilled at them than people who were once considered the best in the world in that field.\n\nEDIT: A lot of people have taken my view of Da Vinci as that he was not an incredible genius with equally or even more incredible drive and focus. This is not my view, he was indisputably an unprecedented genius who hardly slept and worked incessantly. If Da Vinci were alive today, he would almost certainly be still doing very notable things. The same goes for other classical geniuses: Newton, Einstein, Galileo, Bach, etc. However,  my point is that in today's context, they would be doing truly amazing things in one or two fields only.","label":0,"model":"human","source":"reddit","id":2905}
{"text":"Imagine you had a phone booth full of water somehow sitting on the surface of a calm lake, and suddenly its walls disappeared. The water would rush out of it and create a wave that shot out in all directions until the lake evened itself out again. This is similar to how all of the air squeezed up inside a balloon pushes out and creates a pressure wave that you can hear when it bursts and equalizes with the air around it.\n\nPeople are pointing out that the balloon pop itself isn't really all that loud, it's mostly the acoustics of the room around it. To understand this, imagine you have the same phone booth sitting at the middle of a circular swimming pool. When the water rushes out, the resulting wave will bounce off of the outer edge of the pool and rush back inwards. It can then combine with itself and cause even bigger waves! Where the water in the lake will soon just have a small wave spread over a large area, the pool water will still have really big waves for a while after as they bounce around and interact with each other. If you pop a balloon in an 'anechoic chamber' (basically like a pool whose walls absorb waves completely and don't let them bounce back) then you'll only hear that first smallish wave of pressure.","label":0,"model":"human","source":"reddit","id":2906}
{"text":"In medical school I worked for a month in an ALS clinic. This topic came up once or twice. The attending (a pretty big deal in the ALS world) said that while ALS that progresses and stops is not totally unheard of, it was quite unusual. He actually thought that Hawking was mis-diagnosed and what he actually had was Spinal Muscular Atrophy Type IV (adult onset SMA). It's been years since that experinece, but if I remember correctly, the difference between the two at presentation would be if there were signs of hypertonia or hypotonia, that is if the knee-jerk went up too much or didn't go up enough. SMA IV can progress for a while and then stop, leading to a clinical picture identical to Dr Hawking's.\n\nIn some ALS, with variable presentation, the hypertonia (knees up too much) is pretty mild, so the distinction can be difficult, especially without nerve or muscle tissue and special stains. I think the SMA IV entity was only first recognized as different from ALS relatively recently, so he may actually predate that, too.","label":0,"model":"human","source":"reddit","id":2907}
{"text":"From _URL_0_ \n\nHow Dick became a nickname for Richard is known and is one of those \u201cknee bone connected to the thigh bone\u201d type progressions, somewhat similar to how the word \u2018soccer\u2019 came about.  Due to people having to write everything by hand, shortened versions of Richard were common, such as \u2018Ric\u2019 or \u2018Rich\u2019.  This in turn gave rise to nicknames like \u2018Richie\u2019, \u2018Rick\u2019, and \u2018Ricket\u2019, among others.  People also used to like to use rhyming names; thus, someone who was nicknamed Rich might further be nicknamed Hitch.  Thus, Richard - >  Ric - >  Rick gave rise to nicknames like Dick and Hick around the early 13th century.\n\nWhile few today call Richards \u2018Hick\u2019, the nickname \u2018Dick\u2019 has stuck around, and of course has come to mean many other things as well.  Its persistence as associated with Richard is probably in part because around the 16th century Dick started to be synonymous with \u2018man\u2019, \u2018lad\u2019, or \u2018fellow\u2019, sort of a general name for any \u2018Tom, Dick, or Francis\u201d (which by the way appears in Shakespeare\u2019s Henry IV, written in the late 16th century, with Dick at this point firmly established as an \u201cevery man\u201d name).  It may well be that this association with \u2018man\u2019 is in turn how \u2018dick\u2019 eventually came to mean \u2018penis\u2019.","label":0,"model":"human","source":"reddit","id":2908}
{"text":"**Actually explained like you're five.**\n\nYou know those flip-books you make?  Imagine if you made a flipbook of a man doing five jumping jacks.  You draw still pictures on 24 pieces of paper, and when you flip through them it looks like he's doing jumping jacks.\n\nNow your friends at school start making their flip-books more detailed.  Some of them even make flip-books that look like they're in 3-D when you cross your eyes (they are very smart kids.)  But you think there's a better way to make the man doing jumping jacks look more real.  So you make a new flip-book of a man doing five jumping jacks, but this time you use 48 pieces of paper.  That means you have to draw twice as many pictures.  But you flip through it in the same amount of time (the paper is moving a lot faster now, and you are seeing more drawings, but the man is moving at the same speed as before.)  This one looks pretty cool.  It looks super-real, almost like the guy could walk right out of the paper.\n\nThen you show it to all of your friends and they think it's weird and annoying.  It looks too real, and that ruins the illusion.  It was never about looking real, it was about using your imagination.","label":0,"model":"human","source":"reddit","id":2909}
{"text":"British humour is very much aimed at making people cringe. It looks at the deeply woven social fabric and finds loose threads or inconsistencies. Then it tugs at those threads to see what would happen, it all its glorious embarrassment. We laugh because we've all trodden that minefield, we've all made those fuck-ups, and by watching the pratfalls and faux pas we can relate to those tragi-comic moments of our own lives and our own failings.\n\nIn contrast, Americans are almost utterly incapable of self-deprecation. There's no element of introspection that says \"look at us - we act like dicks and losers all the time and never do anything about it\". Instead, it's about oneupmanship - the idea that any man can walk into a scenario and conquer it with his comedy. It's an open rejection of intellect, or industry, or social grace - the man who deliver the scathing comeback will always best the engineer or the businessman or the aristocrat. In that way, comedy gives the little guy, the working Joe, something to relate to.\n\nThe British comic is the Lovable Loser; the American is the Defeater of Losers.\n\nAs to *why* - I'll get downvoted to hell for saying it, but America has an identity complex. All of this posturing and patriotism and oneupmanship is still very much the hallmark of a young nation that hasn't grown into its skin yet - hasn't come to terms with its wrinkles and battle scars and beer belly. It's still the young buck proudly swaggering, offering the handshake and pulling it back at the last minute to thumb its nose at the unsuspecting victim. There's always a fall guy, and it sure as hell ain't America. Britain, on the other hand, has fallen into disgraceful old age and is more than comfortable being the butt of the joke if it means everyone else gets to have a good time.\n\nIn terms of language skills I don't think it's fair to say that the Americans are blunter or the English wittier - both sides have their share of clever comedians as well as lowest-common-denominator hacks. But I do see the Americans resorting to brute force (sarcasm and shouting) a lot more than the Brits, letting the volume of their voice and the strength of their swears compensate for not having anything clever to say.","label":0,"model":"human","source":"reddit","id":2910}
{"text":"Riding a bike relies on something called implicit memory. It is a kind of learning that you are not consciously aware of. It is the same type of memory that you used to learn to walk, make the strokes when you brush your teeth, make all the movements required to drive a car, etc. Implicit learning is very simple and fundamental: if you get rewarded (the pleasure of staying on the bike) you tend to do whatever got you there more often, if you get punished (falling off) you tend not to do that movement as often. Although it takes a while, once you've learned it it lasts a long time. This is very old in terms of evolution, most animals can do it, and it is basically re-shaping your brain to run in a new way.\n\nTechnically you don't \"forget\" implicit memory the way you might forget a name, piece of information, or some other verbal code (this is called explicit memory, and you are consciously aware of it). However, it is possible to \"un-learn\" it. For example, if you trained yourself to ride some type of bike that only worked if you leaned way off to the right, after learning that it would be difficult to transition back to a normal bike. In fact, this is why it is annoying to type on someone else's keyboard, or super annoying to switch from pc to mac. You fingers have learned implicitly where to make the keystrokes (rewarded for correct keys, punished for incorrect ones) and it takes some time to unlearn that and learn the new setup.","label":0,"model":"human","source":"reddit","id":2911}
{"text":"Say you have a bucket of water and a few friends who all want a glass of water. You charge your friends $0.10 per glass because one bucket is pretty easy to bring. At first everyone can take as much as they want because there's plenty in the bucket. As more and more people show up who want water you have to go buy another bucket to start bringing more water. Soon you have more people who want a glass than you can carry in two buckets.\n\n\nAt this point you can't carry more water. You only have two hands. You can hire someone to help carry water, but that costs money and eats into your nice water profits. \n\n\nSo you start charging people $0.15 a glass. As the price goes up people get a little angry about how much their spending on water, but they need the water. You start telling people how they can get by on a little less water. You up the cost to $0.20 saying that so many people want water that it's getting hard to get and we should all try to conserve water. People seem satisfied to start slowly buying less and less water to offset how much they're paying. At the same time more and more people are coming over to buy water.\n\n\nSo as a savy business person, you've not had to hire anyone to help you carry water (transmission lines), nor bought any new buckets (power plants) by making twice as many people buy less water individually but at twice the price. So people feel they're doing good and saving money. Meanwhile you just doubled your profits and added nothing to your expenses.","label":0,"model":"human","source":"reddit","id":2912}
{"text":"The Koch brothers-- having inherited vast wealth from their father-- engage in self interested advocacy under the rubric of \"libertarianism,\" seeking to pass the cost of their industrial businesses onto the citizens\/taxpayers using numerous schemes, including regulatory capture.  \n\nThey are known for ignoring health, safety and pollution regulations in the instances they have been unable to capture a given regulatory agency, including poisoning whole communities with cancer causing benzene:  \n\n_URL_2_\n\n_URL_4_\n\nComparably, the Koch brothers have illegally sold arms Iran simply for the profit:\n\n_URL_0_\n\nContrary to one of the responsive posters here, they are the largest funder of climate change denial:\n\n_URL_3_\n\nAs mentioned in the Bloomberg link above, one former Koch Industries employee called the practices \u201cThe Koch Method\u201d \u2014 with managers showing subordinates \u201chow to steal and cheat.\u201d\n\nAs the article provides: \n\n\"Koch Industries units \nhave also rigged prices with competitors, \nlied to regulators and repeatedly run afoul of \nenvironmental regulations, resulting in \nfive criminal convictions since \n1999 in the U.S. and Canada.\"\n\nThe issue most redditors know about is their wholesale poisoning of American politics for personal financial gain: they are creators and funders of the Tea Party, though they have tried to hide this fact.  \n\n_URL_1_\n\nIn short, the Koch brothers are sociopaths who, having inherited vast wealth from daddy, have never had to toil in uncertainty, have never had to look in a mirror.  Their general political scheme is to turn citizens against citizens, white against blacks, Christians against non Christians, heterosexuals against gays, class against class, etc-- to control them.","label":0,"model":"human","source":"reddit","id":2913}
{"text":"Because the overwhelming majority of tax breaks that they take advantage of are not specific to their industries. For example: \n\n\"Section 199 is part of the domestic production activities deduction that was included in the American Job Creation Act of 2004, which passed with strong bipartisan support, especially in the Senate. It currently provides a 9% tax deduction from net income for businesses engaged in \"qualified production activities\" in the U.S. Those activities include manufacturing a product, selling, leasing or licensing it, and engineering and software activities related to that production. The deduction was intended to encourage domestic manufacturing, and in the hope that the tax break could provide a slight competitive advantage against foreign competition.\n\nThe oil and gas industry, especially in its extracting and refining, is heavily involved in U.S. manufacturing. Congress already penalizes the industry by only giving it a 6% deduction, rather than the 9% that other industries receive.\"\n\nSource: _URL_0_\n\nThere's more at the article, but a big thing to remember is that when you vote for new corporate tax loopholes to go towards things like industrial productivity, you're opening yourself up to people complaining about how you \"Gave millions in tax breaks to the oil industry\", though they won't mention that those same breaks go to their own favored industry, like auto manufacturers. \n\n**5-year-old version**: Just because your best friend brought in cupcakes for everyone in class, and even gave one to the kid that you really hate, doesn't mean that he really likes that one kid and hates you.","label":0,"model":"human","source":"reddit","id":2914}
{"text":"First, remember that every field has tradition associated with it -- it's why we wear caps and gowns when we get degrees -- tradition.\n\nBack in the day, dentistry wasn't a medical profession. Dentists weren't doctors with degrees who went to school for 6 years and then apprenticed for another 6. Dentists were your local barber or blacksmith. Basically anyone who had the tools to pull a tooth from your head.\n\nBecause of that 'tradition', dentistry is a completely separate medical profession from everything else. A dentist is a D.D.S -- a medical doctor (cardiologist, dermatologist, endocrinologist, GP) is an M.D.\n\nAll of that carries over to insurance.\n\n\nEdit: Didn't think this would go anywhere. Sorry for those who don't like the history of the profession and why things are the way they are. It's pretty much impossible to wipe that sort of stuff away, especially in such a short period of time. 'Dental insurance' is a pretty recent thing (less than 50 years). There are plenty of \"stupid\" traditions we follow that are *way* older than that.","label":0,"model":"human","source":"reddit","id":2915}
{"text":"My criminology\/criminal justice professor used to be a police officer and he did something similar to this when he was hired out by Mothers Against Drunk Driving. He was one of the few highly educated officers who knew Sir Robert Peel's principles of policing, the first principle being that the main goal of the police is to prevent crime. If you can prevent crime, you don't need to control or arrest anybody. \n\nSo for an entire month he went to the area of town with the most bars and drove around in the police cruiser with the lights on, drawing as much attention to himself as possible. Because he did this, everybody would walk out of the bars, see the police officer, and find a way of getting home without drinking and driving. He made no arrests that month because he prevented all drunk driving. \n\nMothers Against Drunk Driving never hired him again because they really didn't want to stop drunk driving, they just want to see people being arrested for it. \n\nI realize this kind of a digression but thought it might be interesting in this thread.","label":0,"model":"human","source":"reddit","id":2916}
{"text":"Pre WW2 most people slept on double beds and then manufacturers started trying to upsell  by selling them as \"for $1 more you can get an inch bigger bed\". As the WW2 economic boom was happening people started buying larger beds for their homes and the manufacturers were making advertisements comparing the old smaller beds to cribs, so naturally people bought into it because people are easy to manipulate. As time went on the now popular bigger beds were selling, the best sellers mostly fell into 3-4 size categories which we now know as twin, full, queen, king (matress height is not standardized in any way). They aren't truly standardized, these just sold the best. Beds also grew as average human heights have, naturally taller people need bigger beds. There are also oddball sizes like Cal Kings, Alaskan Kings, Wyoming Kings, Eastern, twin xl, full xl etc. I believe in Europe double beds are still somewhat standard. I worked in a big name matress factory for 5 years and the marketing and mark up is insane. I will never buy a mattress at  retail price and I suggest the same for others.","label":0,"model":"human","source":"reddit","id":2917}
{"text":"The Bell 230 and the 212 are mediums that have two blades.\n\nBoth those were replaced by 4-bladed versions (the 430 and the 412).\n\nThere are many factors that determine the number of blades. But in general, the trend you see in looking at the results of many development programs is this: smaller, cheaper, and slower helicopters have the fewest blades. Bigger, faster, and more expensive ones have more blades.\n\nNotice that the 412 (4 blades) has a max speed about 20 kt faster than the 212 (2 blades).\n\nIn general you can say the following:\n\nFor hover, low disk loading is most efficient. That means longer blades and therefore fewer blades.\n\nBut for speed, more blades are better. The blades turn slower and the tip speed is slower, allowing higher airspeed before the advancing blade tips approach supersonic.\n\nThis is illustrated by the 212\/412 helicopters.\n\n-The 212 has two blades with a diameter of 48 feet and max weight of 11,200 lb. 100% rotor revs are 324 rpm.\n\n-The 412 has 4 blades with diameter 46 feet and max weight of 11,900 lb. 100% rotor speed is also 324 rpm.\n\nDisk loading \n212 = 6.19 lb\/ft^2\n412 = 7.16 lb\/ft^2\n\nTip Speed\n212 = 813 ft\/s\n412 = 779 ft\/s\n\nVmax\n212 = 120 kts\n412 = 140 kts\n\nSo the 212 has lower disk loading, it should perform better in hover (I don't have data on that). The 412 has lower tip speed and higher airspeed.\n\nEdit: re-wrote it with some numbers to illustrate.","label":0,"model":"human","source":"reddit","id":2918}
{"text":"Korean dude here.\n\n**Sorry this turned into a long post.  Wall of text incoming**\n\nI am not all that familiar with how bowing works in Japan, but I'm pretty sure we are pretty similar...  Korea and Japan (countries) have been a part of each others history for a long time now  and a lot of our cultures\/language\/food have mixed together.\n\nFor Koreans, the bow can be pretty casual or very formal.  It all depends on duration of the bow, how far you bow and also the context in which you are bowing.  \n\n1) The most formal standing bow would be a full 90 degree bow that you hold until the person you are bowing to has left the room.  That is some hardcore shit meant for like royalty or shit like that. \n\n2) The second formal bow would be the type you would do during your first time meeting someone important.  For example, meeting your new Korean Father-in-law for the first time is pretty formal.  You bow closer to 70 degrees and hold the position for a second or two.  Usually the person receiving the bow would give a verbal queue on when to go back to regular standing position.\n\n3) Then most other bows after this become more and more casual.  The most common type of bow would be when meeting someone of similar \"status\" (age, social position, work position, etc...) you can be more casual.  The bow is roughly 30 degrees and there is no need to hold the bowing position. \n\nFor all cases, your hands should be at your sides and it would be advisable that you do not maintain any sort of eye contact during the bow.  I've found it useful to look at the persons shoe when bowing.\n\n4) There are other bows that we traditionally do as part of yearly rituals that involve getting down on your knees and fully bowing down on the ground with your extended to your head.  It's mainly done on New Years and is the ultimate form of respect that you show to your family elders when wishing them a healthy and lucky New Year.\n\n\n**In your case, you're going to be dealing mainly with bow #3.**\n\nFrom what I've seen you shouldn't worry too much about how you bow.  When White people or other foreigners bow, it's definitely not held to the same standards as when we bow within our own.\n\nAs long as it's not done mockingly, when we see foreigners bow (bow #3)* and say the customary \"an-nyoung ha sae yo\", it's pretty endearing and will get you points. Even if you don't have a firm grasp of the language, just give it an honest try and no one (normal) would be offended by it.\n\n\n*I would imagine that the Japanese would feel the same way.*\n\nSome people like to mystify Asian culture like it's some sort of sacred thing. But in the end the Japanese are human, just like you.  They appreciate it when a person is trying to learn their culture.\n\n*edited #4 to #3","label":0,"model":"human","source":"reddit","id":2919}
{"text":"I'm a neuroscientist, and someone who likes beer probably more than he should, so I think I'm in a perfect position to answer this.\n\nYour question is perfect. What it demonstrates is that a hangover cannot *just* be dehydration and lack of sleep (let's forget \"mild alcohol poisoning\" because that doesn't make much sense for a bunch of reasons).\n\nLet me take this in what seems like a different direction: Did you know that people with epilepsy can often suffer from seizures the morning after heavy drinking? Why might that be? Also, very heavy drinkers often have seizures when they stop drinking. Why might that be?\n\nThe answer to both of these questions comes down to one fact, and one general principle of biology. First: Alcohol makes the main \"inhibitory\" neurotransmitter in your brain more effective. What does that mean? A neurotransmitter is a chemical that is released from one brain cell to talk to another. Generally speaking, neurotransmitters make the cell that receives the neurotransmitter either MORE likely to release neurotransmitter themselves, or LESS likely to release neurotransmitter themselves. A inhibitory neurotransmitter is the last one. An alcohol makes it work better.. making it even more potent at telling brain cells stop releasing neurotransmitter.\n\nThe general principle in biology I was talking about is called \"homeostasis\" and it means that if you exert some change on the body, the body tries to reverse it. While it doesn't always happen, in this case it does: alcohol makes the inhibitory neurotransmitter more effective, so the brain does a variety of things to reduce the effectiveness of the inhibition.\n\nThe problem then comes when the alcohol is removed from your body. Your brain has turned down the level of inhibition, and with no alcohol floating around, this lowered level of inhibition does not make you feel great. In people prone to seizures already (i.e. epileptics) it can be enough to trigger a seizure. In non-epileptic people, it just makes you feel gross (and is why, I guess, that sometimes after drinking, you end up waking up earlier than normal, and can't sleep, even though you feel terrible).\n\nBy resupplying a little bit of alcohol, you are turning the level of inhibition back up a bit, giving your brain time to readjust it's inhibition back to normal levels.\n\nI know I've gone a bit above ELI5 levels. Sorry about that.","label":0,"model":"human","source":"reddit","id":2920}
{"text":"To answer your question, the only way to know how long light has been traveling is to know how far the source was.  Since we already know how fast it travels (300 000 km\/s), all we need is a method to evaluate the distance, which is then converted into the convenient \"light year\" units.  \n\n[There is many techniques](_URL_4_) to determine how far objects are in the sky, but the preferred method will depend of many factors (types, range, etc). There is hundred of variants, special cases, or corrections, but it usually come down to the following methods:\n\n1. For \"close\" objects (the closest 1% of our galaxy), we use [Stellar Parallax](_URL_2_).  This technique is a direct application of right triangle geometry where one side of the triangle is the distances between two points located on Earth's orbits and the angle between this line and the star is measured with rudimentary astronomical tools.   If you want to imagine what's it's like, just stare at a fixed point on your wall, then move one meter to the right.  Because you know how far you moved, and how many degree your head had to turn to continue staring at that point, you can easily use geometrical equations to find the distance that separate you from the wall. Now, repeat the same experiment with a star, except that instead of moving 1 meter to your right, you wait  half a year to be carried millions of kilometers away from your point of origin. Obviously, we don't wait 6 months anymore and use satellite to help, but the concept always remain the same.\n\n\n2. The second method we use is the [apparent magnitude](_URL_3_).   Because there is hundred of stars that can be studied and classified using the first method, we're able to come up with [a chart that compare the absolute brightness of a star to its type](_URL_0_). With this chart in our hand, it become child play to calculate the distance that separates us from a star by looking at its type (it's color to put it simply), while knowing that intensity goes down with distance at a rate of a 1\/r\u00b2. As long you can tell what kind of stars you're looking at, you should be able to tell its distance with a decent accuracy.\n\n3. Because it become very hard (if not impossible) to see a star in a distant galaxy, we have to resort to something much brighter. Thankfully, supernovae is exactly the kind of brightness we needed for that. It get a bit more complicate at this point since there is various type of supernovae, but essentially, one of them (Type I) generate a relatively constant luminosity that allow us to use the phenomenon as an accurate measurement. From that point, we can easily guess how far the explosion happened by looking at its apparent brightness and comparing it to its absolute brightness just like we did with the second method.\n\n\n4.  But that alone is still not enough to determine the distance of every objects in our Universe. To go even further, we need to resort to our knowledge of the expanding universe, or more precisely, the [redshift](_URL_1_) .   It become harder to visualize, but what is important to understand is that light become \"stretched\" ([or redder](_URL_5_)  as it travels through expanding space.   By using the light signature of various known objects (galaxy, super nova, common element like hydrogen)  in a way that is analogous to the \"star catalogs\" from the 2nd method, you can determine by how much the light was redshifted and find out how long it traveled.\n\n\n\n\n\n\n[edit]\n\nI've no idea who sent me that gold, and I don't feel like I deserve it, but this is much appreciate. Thank you!","label":0,"model":"human","source":"reddit","id":2921}
{"text":"Short answer: Because we figured out how. \n\nCooking meat allows us to extract about 15% more calories from it because of the denatured proteins. Similar for being able to mechanically digest it. \n\nSo ground beef that's been cooked to medium has about 25% more effective calories than the same weight of uncooked roast. \n\nThat lets humans be the efficient ape. Compared to Chimps (our closest relatives), we've got about the same amount of gut and about half as much jaw for twice the ape. \n\nCooking food kicks ass. \n\nAnd this is even skipping over the huge gains that we get from our ability to take \"questionable\" (read, not really questionable, but almost certainly full of Trichinosis) meat and make it fully safe by sitting it over a smoky fire for a few hours. \n\nReally, it may not be necessary to cook your meat, but it's so much better that it's no wonder the thermivore ape kicked the ever loving shit out of the omnivore apes when it came to competition for environment suitable for living\/fucking\/fighting.","label":0,"model":"human","source":"reddit","id":2922}
{"text":"Gonna actually try a balanced LI5 explanation:\n\nSkrillex is a guy who makes electronic music. He is known for making modern\/american dubstep (also known as \"brostep\"), as well as electro house music, and other dance music that emphasizes the use of bass. (Since you're 5,) Bass is the low rumbly sounds in music that you can feel more than just hear.\n\n[ELI5 for dubstep](_URL_0_)\n\nA \"drop\" is a specific part of dubstep music, that is one of the main reasons everyone loves it. It's the main part of the song, the most moving part, where there's lots of bass, and all of the other elements of the song.\n\nThe structure of a dubstep song, most generally is:\n\nIntro, (buildup), drop, break, (build up), Drop, outro\n\nAssuming you are referring to the skrillex post about aphex twin, this is ironic\/painful because:\n\nGenerally (GENERALLY!) dubstee fans who only know about Skrillex are obsessed with only dubstep, and know little of other music.  Aphex Twin is an IDM producer, which is a much more complex form of electronic music, and one would argue takes more sophistication to enjoy.\n\nThings get less balanced ahead:\n\nThe irony of the situation is that all of his fans expect him to post a dubstep song, and instead he posted (subjectively speaking) a beautiful, complex song (as opposed to formulaic dubstep), and all of his fans are uncultured such that they end up insulting the music he posted because it's not dubstep, making them look like idiots\n\nBarring subjectivity for a moment, it's like uncultured people looking at a famous piece of art (Think mona lisa, starry night, or artists like picasso, dali, etc)  and thinking \"Wow this sucks, it looks nothing like the cartoons I watch\/videogames I play\".","label":0,"model":"human","source":"reddit","id":2923}
{"text":"Paper money, aka bills or notes, needs to be redesigned every so often. Most of the time, it's because doing so allows the Reserve to add new security features to fight against counterfeit, but sometimes, it's also to change the designs to be more aesthetically pleasing, or to add features that make it easier for the visually impaired to use (if all notes were the same size, how would a blind man know if he's getting the right change back?)\n\nA while back, someone proposed removing Founding Father, and founder of the United State's national bank, from the $10, in order to put a woman onto a note for the first time. The proposal didn't sit well with fans of Mr. Hamilton, as nobody's more fitting to exist on american currency as he, especially when there are people like Jefferson or Jackson on other bills that would have been more fitting to remove, considering Jefferson was a slaver and misogynist that tried to dismantle Hamilton's national bank, and Jackson was an old corrupt, racist, anti-Semitic, misogynist, who removed Native Americans from their land, and also tried to dismantle the national bank.\n\nIn the end, they decided to remove Jackson (since people barely even see the $2 bill Jefferson is on) in order to add Harriet Tubman to the front of the widely-used $20. Harriet Tubman was a very important agent of the Underground Railroad, leading many former slaves to freedom in the north, as well as being the first woman to lead an army while freeing slaves at Combahee Ferry.\n\nHarriet Tubman was selected to be on the $20, as she's done more good for more people in America than Jackson. Even Jackson's claim to fame, the Battle of New Orleans, wasn't even done by his own merit; he won due to British incompetence, and he frequently ignored orders, facing several court marshals.","label":0,"model":"human","source":"reddit","id":2924}
{"text":"A roadway is placed on top of compacted soils. Those soils are usually only tested for proper compaction once per 100 linear feet. \nUsually the technician testing the soil chooses a relatively compacted ( dense) area to test so they get a passing number.\nAfter the soil\u2019s are compacted they place rebar and concrete on top of the soils. \nOnce cars are driving on the roadway they cause vibrations that carry through the concrete and into the soil beneath.\nThose vibratios, after a while,  cause The soils to subside in some areas once subsidence occurs the weight of the concrete does the rest.\nGravity pulls the concrete down to fill the hole left by the soil and voil\u00e0 a pothole is created!\n\nTo remedy this problem a contractor will usually remove a workable section around the pothole (usually 3 to 4 ft.\u00b2 around the pothole) they will remove the soil and replace it. Have a technician tested for proper compaction and then they place the rebar and concrete back on top.\n\nSource: I\u2019ve been a project manager for TxDoT\n\nEdit: removing apostrophes","label":0,"model":"human","source":"reddit","id":2925}
{"text":"The real answer is tradition. Shingles are traditionally coated with asphalt, which back in the Victorian era only came in one color: Black.\n\nNowadays you can get shingles in all kinds of crazy colors including white. But it's been ingrained in our culture that a light colored house (white, beige, robin's egg blue, whatever) and a dark roof (black, brown, maybe dark red) \"looks right,\" so that's how we do it.\n\nIf you go to a country where there is no cultural predisposition to the typical western style house -- peaked roof, shingles, etc. -- or where they don't *use* shingles, you'll find that a lot of roofs are indeed white or some other light color. Check out lots of places in Africa and the Middle East, for instance. In most of those places they don't use shingles, either.\n\nThere are roof coatings in use even in America and Europe specifically designed to do what you're proposing. They're meant for use on flat roofed buildings like stores and office blocks, and most of them are silver, made with metal particles that are even more reflective of heat than white paint.","label":0,"model":"human","source":"reddit","id":2926}
{"text":"It was cheap because Napoleon needed cash for his wars a lot more than he needed a big hunk of real estate in North America. The US desperately wanted to secure free passage in the Mississippi (something that had been a problem, as the Spanish had closed New Orleans to American ships for years). There was a lot of interest in settlement of the Ohio Valley, but without access to the Gulf through the Mississippi, that would severely hinder any growth and commerce.\n\nAs for stolen, other than the rather obvious issue of the land being taken from the Native American inhabitants, there was the issue that much of it was Spanish territory for years, and had fallen into French hands shortly before the French sold it to the US. But at that point, Spain was all but a satellite power to France, so the transfer of Louisiana from Spain to France likely wasn't on fair terms to the Spanish.\n\nI remember reading that when people in St. Louis found out the territory was American, they hadn't even heard that it had been French. They thought it was still Spanish.","label":0,"model":"human","source":"reddit","id":2927}
{"text":"The sound barrier is how fast sound moves in air. But sound is just waves moving in the air, so really it's how fast the air \"likes\" to move. Which also means it's how fast the air \"likes\" to move out of the way if you're moving through it (just like moving in water, you have to push the air out of the way). If the speed of sound is how fast the air \"likes\" to move but you're moving faster than that, you have to forcibly \"push\" the air out of the way faster than it naturally wants to move. This \"push\" requires extra energy, and pushes some of the air together (kind of like an air compressor pushing air into a tire, but instead of rubber surrounding the air it's just surrounded by other air that doesn't \"want\" to move as fast as you're pushing it). \n\nOnce you've moved past, the air wants to find a way to decompress, and now that nothing around it is moving faster than the speed of sound it decompresses by pushing the surrounding air out of the way. Much like you'd pop a balloon (or a bicycle tire filled with compressed air) and the balloon makes a \"pop\" as it releases air; the air that was \"compressed\" by something moving through it faster than the speed of sound makes a \"pop\" as it decompresses behind the object.","label":0,"model":"human","source":"reddit","id":2928}
{"text":"Essentially, these animals are 'programmed' by their genetic code to do these things, and those 'programs' were developed by millions of iterations of trial and error, where the bad programs were erased (died out) and the good ones continued.\n\nNow, a better question is why don't we humans have the same 'programs' in our brains?  Well, we do to some extent (suckling instinct, diving instinct, etc).  But because our brains are more powerful than an ant or bird, we actually can do better *without* fixed programming, because we can develop better solutions based on the available information, learning from others, etc.\n\nTo put it into a computing analogy - the birds and ants are very old, very slow computers with very limited RAM.  They can do what the first computers could do, simple mathematical operations and other tasks, one simple job at a time.  They are fixed in their programming, once they're programmed to do something, that's all they can do, and they can do it very well.  Us humans on the other hand are very advanced computers that can 'learn'.  We aren't programmed to perform every little task, but rather can learn how to do *any* task given enough time, training, and input.  Your washing machine doesn't need to learn how to wash clothes, it's programmed to do that from the time it's created.  If you developed a very advanced domestic robot, it might not know how to wash clothes when it was 'born', but with an advanced processor and the right learning algorithms it could definitely learn.","label":0,"model":"human","source":"reddit","id":2929}
{"text":"Double clutching is a downshifting technique that promotes smoother transitions and lower transmission wear. It is useful for road racing, prolonging transmission life, and giving you an overall smoother ride.\n\nIn normal driving, with modern cars- you don't need to double clutch... ever. When you shift, these neat little devices called \"synchronizers\" or \"synchromesh\" (or whatever other name you want to give them) in your transmission help your shifting by matching the rotational speeds between meshing parts. Why do you need to match the speeds between transmission parts when you shift? Simple- they won't go together unless they're all traveling the exact same speed. Your synchros take care of this, so you don't have to worry about matching revs much in normal driving.\n\nSo the question now is... why the heck do you need to double clutch? It's useful in racing, it's required for non-synchro transmissions, and it's just a damn cool racing skill to master. Think of your transmission as being separated into two functional halves. One half is connected to your engine, and the other half is connected to your drive axles\/wheels. The split between the two halves is right at your gears.\n\nLet's say you're driving down the street in 5th gear. Assume that your gearing is 1:1 all the way though, just for simplicity's sake. Your engine is turning 3000rpm, and so are all the parts in your transmission. You want to downshift to get higher up in your powerband to pass someone, so you mash the clutch pedal, shift to 4th gear, then lift off the clutch pedal. If your 4th gear ratio is twice what your 5th is, your engine is now spinning at 6000rpm (along with the \"engine half\" of the transmission) while the \"driveshaft half\" of your transmission is still spinning at 3000rpm. Your car is still moving at the same speed, but you're higher up in your engine's powerband. Now you have more power to pass the person in front of you.\n\nWhat normally happens when you downshift and don't match revs? You feel the car lurch some while the transmission forces the engine to a higher rev level. The synchros grip against each other to match the gear speeds, the gears mesh, and when your clutch grips- it pushes the engine higher... and you feel the rough transition. To smooth this out, you can raise the rev level of your engine and the \"engine half\" of the transmission so the synchros have less work to do, and so your transmission isn't pushing the engine around.\n\nHow do I double clutch? I never thought you'd ask.\n\n1. Push clutch pedal down\n2. Shift to neutral\n3. Lift clutch pedal up\n4. \"Blip\" throttle to raise engine speed, \"engine half\" transmission speed\n5. Push clutch pedal down\n6. Shift into lower gear\n7. Lift clutch pedal up\n\nShamelessly copied and pasted from _URL_0_\n\n**Edit**: I ride street bikes on the reg, and I do something quite similar to this...  but all i do is pull in the clutch, blip my throttle, and kick down a gear.  The same result is achieved with less work and its faster.\n\n**Edit 2**: kodemage complained this was not ELi5 enough for him, and that I used too much jargon.  As my response will probably be buried, I rewrote this for a five year old:\n\nInside your car, there is an engine. This engine sends power to your wheels. But in order to go fast, you need a box full of spinny thingys called a gearbox, otherwise your engine would have to work too hard and probably explode! Nobody wants that!!! Well sometimes, when you want to slow down, you need to change from using one spinny thingy in the box to another. You want to make sure the two spinny thinginys are moving at the same speed, so you have to push pedals and do complicated adult stuff. I am sure I you don't need to know this for another eleven years anyway. Go have some ice cream.","label":0,"model":"human","source":"reddit","id":2930}
{"text":"This was ten years ago...\n\nI temporarily located to Lafayette, LA, though I was open to staying long-term if the right opportunity came about.  At that point, I already had a college degree, and had completed 2\/3 of a Master's Degree.  Once there, I started applying for any available job fitting my experience and education, but *no one* would give me an interview, or an explanation as to why I wasn't getting one.  \n\nAfter two weeks or so, with rent on my mind, I started looking for part-time work.  I applied at eight places, and *one* called me back--a Godiva chocolate store.  I interviewed, and was hired.\n\nAfter getting the job, I relayed my job-seeking odyssey to my new manager.  She laughed and said \"Of course!  You're a Yankee academic!\"  Apparently, everyone distrusted me because of my education and origin--the assumption being that I wouldn't work hard.  She also told me the only reason she gave me a second call was because she herself was not a native Cajun, and knew very well that no one else would give me a chance.\n\nIn the seven weeks I slung overpriced (but amazing) chocolate products, I had several locals accuse me of carpetbagging my way into someone else's good-paying job.\n\nI made minimum wage.","label":0,"model":"human","source":"reddit","id":2931}
{"text":"There are these things running through your body called 'nerves'. When something is affecting you (heat, cold, pain, itching, anything at all), a thing in your skin called a 'receptor' recognises it. It wants to tell your brain what is going on, and it does this by sending a message through the nerves to the brain.\n\nIf you're sitting in a certain position for a long time, you can put pressure on a nerve that supplies part of your leg. Since the nerve is all squished, the information transmitted from the body part becomes all jumbled up, and the messages it sends to the brain becomes strange and confusing to interpret. \nSome nerves may not transmit any information at all!\nOther nerves may start to send messages erratically and randomly! \n\nThis is what causes you to feel the pins and needles \"tingling\" sensation.\n\n\n\nEDIT: \nThanks for all the interesting replies to my comment, lots of cool stuff to read :). Obviously there is a lot more depth to this than what I replied with, but i just tried to keep it as simple and compact as possible. I kinda just lumped the tingling\/pins and needles you get with the numbness\/falling asleep, because I'm not actually sure what happens! \nI'm only a medical student, and I'm replying on the basis of what a lecturer mentioned randomly for 15 seconds. I'm sure people doing degrees in neuroscience or something like that would have a more accurate answer to give.","label":0,"model":"human","source":"reddit","id":2932}
{"text":"There are  already a lot of well-intentioned but misinformed responses here.\n\nThe little-known fact is that the famous E = m c^2 is universal, not at all just confined to exotic particles.\n\nAs a result, any and all conversions of energy convert *some* (usually tiny) amount of mass into energy:\n\n >  Another example is hydroelectric generation. The electrical energy produced by Grand Coulee Dam's turbines every 3.7 hours represents one gram of mass. This mass passes to electrical devices (such as lights in cities) powered by the generators, where it appears as a gram of heat and light.[35] Turbine designers look at their equations in terms of pressure, torque, and RPM. However, Einstein's equations show that all energy has mass, and thus the electrical energy produced by a dam's generators, and the resulting heat and light, all retain their mass\u2014which is equivalent to the energy. The potential energy\u2014and equivalent mass\u2014represented by the waters of the Columbia River as it descends to the Pacific Ocean would be converted to heat due to viscous friction and the turbulence of white water rapids and waterfalls were it not for the dam and its generators. This heat would remain as mass on site at the water, were it not for the equipment that converted some of this potential and kinetic energy into electrical energy, which can move from place to place (taking mass with it).\n\n_URL_0_\n\nEven burning a candle converts matter into energy. We've all been influenced so strongly by science fiction in movies that we think of exotic phenomenon, but what Einstein discovered is even more remarkable precisely because it also applies to the mundane.","label":0,"model":"human","source":"reddit","id":2933}
{"text":"It's fantasy wish fulfillment from the safety of the couch.\n\nTwilight touches on a lot of buttons that, to be *really sexist for a moment*, a lot of women have. Women are, in the US at least, made to feel like a thing to be *valued*. A *thing* to be valued. A *thing*. \n\nFor many, many women (and men) life is a series of obstacles where they feel that their true inner self is never realized or loved for what it is. How they see themselves doesn't reflect on how others see them. They feel they have the beautiful soul of an artist, but on the outside they can only manage drab.\n\nTwilight got this right. The main character is a tedious, boring as shit person. She just.. cries. She cries about her frustrations about being unseen. This resonates with a lot of people on a lot of levels. Good or bad writing, or whatever, it hit that nerve spot on. Then, someone SWOOPS IN and sees *past all their shit* and *loves them for what is on the inside* in such a perfect way. And then *she is fought over* because she is *so valued* as a commodity. \n\nIt's fucked up emotionally manipulative garbage. \n\n50 Shades is this on steroids. In 50 Shades of Grey, the woman is similarly weak, boring, stupid, whatever. She feels all the negative things about herself.\n\nThe handsome, rich, well endowed, dominant wonderful perfect man swoops in and *sees her perfection on the inside*. Because *many, many women* feel they are things that are more beautiful on the inside, this resonates. That man then takes her forcefully *but so gently* through all this sexy time. It is a disney dream where your pussy gets fucked. It's porn. Total, straight up, hyperfantasy porn. \n\nThankfully, most people realize this. Again, to be *very sexist* women *tend* to prefer their porn more mental and verbal than they do visual and visceral. There are of course many exceptions. \n\nBut, we have a long history of books-as-porn marketed almost entirely to women. Many romance movies are *exactly the same thing* except they don't go quite as far.\n\nEdit-\n\nI should also mention that fantasy is not necessarily the same thing as desire. We all have fantasies that we actually have no desire to act upon. It's just fun, and ultimately harmless.","label":0,"model":"human","source":"reddit","id":2934}
{"text":"\"Soap opera lighting is a major reason the shows look the way they do.\n\nBacklighting, part of the three-point lighting setup often used in television production, helps \"lift\" actors out of the background. This is especially useful for productions that are shot on a lower-quality medium and in small interior sets, which soaps often are. The problem is that shooting on videotape on a small set can reduce the subtlety of the lighting technique. Actors in the foreground often wind up very noticeably backlit, something that doesn't happen on shows with larger sets, or shows that are recorded on film.\n\nSoaps and other lower-budget shows also look \"off\" because they're often evenly lit across the entire set to facilitate simultaneously shooting with more than one camera. This lighting\/shooting method means the actors can move around and the lights don't have to be reset for every shot. This allows for fewer takes and costs less, but it also means more diffuse, less natural-looking lighting in the final product.\" _URL_0_","label":0,"model":"human","source":"reddit","id":2935}
{"text":"Yes! This is a great trivia question. Here's the story:\n\nWhen the telegraph was first invented, operators needed ways of communicating clearly, correctly, and *efficiently*. They started inventing official codes as well as slang, habits, in-jokes, and all sorts of stuff that was like an early online culture.\n\n[In one set of those early codes, '73' meant something like \"best regards\"](_URL_0_) and was a common greeting. Somehow it leaked out into the IRL world and became something of a fad for people to tap out the Morse Code rhythm of 73 in all sorts of situations. [The Morse pattern for 73 is '--... ...--'](_URL_1_).\n\nBut when you're tapping, it's kind of ambiguous whether a pause is supposed to be a dash or a between-letter gap. (Telegraph operators sometimes used a double-tap to indicate a dash before systems started including continuous tones (beeps) that were easier to read.) And somewhere along the line people added another tap to round out the rhythm, changing something that sounded like\n\n    * * *** **** *\n\nInto one that sounded like\n\n    * * *** **** **\n\nAnd we've been repeating this catchy rhythm ever since, what started way back in 1859.","label":0,"model":"human","source":"reddit","id":2936}
{"text":"Well, bombs are powerful compared to what? I would argue that, at the end of the day the types of bombs used by most suicide bombers (there are notable examples) are not all that big.\n\nIf you look on places like Liveleak of video of suicide bombs strapped to people there are generally only a handful of casualties inside the same room as the bomber and the building is usually left standing. Compare this to a \"bomb\" in a car that can take out the Oklahoma city building and we are talking about really small charges.\n\nSo, one of the answers is that in cases where the bomb is small enough for this to work, people survive to talk about it. When the bomb is big enough that this doesn't work, then everyone dies and you don't know anyone tried to jump on the bomber in the first place.\n\nWe will only ever hear about this working cause the other dude got blown up. Plenty of suicide bombs would kill one or several people who jumped on top of them.\n\nThere are a lot of other reasons that bodies make good shields to blast injuries; we are almost all water, which makes an incredible shock dampener and can use a huge amount of heat energy turning into steam. The fact the you are presenting such a large surface area will \"direct\" the blast the other way, deflecting some of the compression wave (one of the several mechanisms of injury from blast injury.)","label":0,"model":"human","source":"reddit","id":2937}
{"text":"The clothes on the catwalks- generally speaking- aren't intended to be purchased whatsoever, nor are they really intended to even be worn by people. The crazy fashion you see at haute couture shows can be more compared to, say, a sculpture or fine arts piece. It's about the designer's vision, not the wearer. You're supposed to just look at and enjoy the whimsical designs and wild creations.\n\nThese designers- based on their catwalk designs- will later design haute couture pieces at high prices for rich clientele. These pieces will be INFLUENCED by the catwalk design, but made more practical. Major design houses will then hire these designers based on their successful haute couture sales to design limited, expensive lines. These in turn are imitated by pricy but more common brands to be sold at Macy's and the likes. Those will then be imitated by common brands found at places like Target.\n\nEach generation, however, moves further from the original inspiration and becomes more utilitarian. .","label":0,"model":"human","source":"reddit","id":2938}
{"text":"The time frame matters here. In the very short term (hours) thirst is easier to ignore than hunger, and I'll get to that below. In the longer term, dehydration has a much bigger impact: if you don't have any fluid or any water, you will die of dehydration (~3 days) much faster than you will die of starvation (~3 weeks).  \n\nIn the short term, why can we ignore our thirst cravings? The answer is, we don't know! From wikipedia:\n  >  However, the true neuroscience of this conscious craving is not fully clear. In general, the end-result is towards behavior of drinking for hydration, but this can to some degree be resisted, such as in voluntary fluid restriction.\n\nThirst is complicated. It has to do with how much fluid you have in your body total, and also, how concentrated with salts is that fluid. A lot of body systems are involved (excretory, endocrine, cardiopulmonary, etc). Here is some speculation: I suspect that those systems are trying to work together to find a balance that works for the organism as a whole, so maybe no one system fully takes over -compelling you to drink- until you're quite dehydrated. \n\nHunger is slightly less complicated. It has to do with the cells of your body getting the nutrients they need. Each cell is like a car that needs fuel, and that fuel comes from food. When they don't have easy access to nutrients, they have to rely on less optimal means of operation. Your body's response to that is like, \"Eat something, ya jackass\". The digestive system releases a chemical called [ghrelin](_URL_0_), which causes your stomach to contract, aka \"hunger pain\" as a reminder to eat.","label":0,"model":"human","source":"reddit","id":2939}
{"text":"Everyone here talking about radar seems to be overlooking the fact that ATC radar **cannot** determine the size of the aircraft, and literally thousands of aircraft in the US operate without Mode C transponders. If you are operating in airspace that has no Mode C transponder requirement (Class E and G. Technically class D as well, but someone would just *see* you there), in VFR (good weather) conditions, private aircraft have no legal obligation to file any paperwork with, nor ever contact, ATC.\n\nIf you took off from an uncontrolled airport away from large metro areas or busy airports, planned your route very carefully, and stayed in Class E and Class G airspace, you would likely be breaking a number of FARs, but you might just be able to pull it off. People do it with small aircraft while trying to relocate something that technically isn't 'airworthy', and stories about such shenanigans abound. It's illegal, immoral, and generally stupid, but I'd love to see sources for many of the claims being made here regarding low level radar coverage of the CONUS, as well as ATC identification capabilities. I was a radar engineer first, then went to school for ATC and commercial aviation, and I am hereby officially throwing the bullshit flag.\n\nAll this says nothing about the actual feasibility of it. You'd have to get in and out of airports to refuel with your big airplane, and many of the most remote airports wouldn't be available, as many small airports would not be able to support a commercial jet aircraft due to limited runway size. You'd have to refuel frequently as well, as you'd have a fraction of the range that the same aircraft would have at 35,000 feet, as jets aren't nearly so efficient at low altitudes. Refueling such an aircraft is also not a trivial matter, and the guy at the FBO is certainly going to wonder WTF is up. He's also going to laugh at you when you ask him to put 50,000lbs of Jet-A on your aircraft when he hasn't got a fuel truck. A large aircraft flying below 10,000 feet (5,000 feet in many areas to avoid airspace that is more carefully observed) over vast stretches of populated terrain is apt to generate a bit of interest on the ground as well, so you'd likely get reported in fairly short order to someone who'd take enough interest to track you down, and once they had an idea of where you had been and when, finding you wouldn't be all that difficult.","label":0,"model":"human","source":"reddit","id":2940}
{"text":"It's a mixture of:\n\n* **The rise in awareness of [celiac disease](_URL_3_) over the past decade**. Scientific studies in the early 2000s revealed that roughly [1% of the US population has celiac disease](_URL_2_), but only a fraction of that 1% knew they had the disease. Researchers began to label celiac disease as a public health concern (1% is a big enough number to warrant informing the public), and that caught the attention of both the medical community and the alt medicine world.\n* **Alternative medicine practitioners overextending gluten as the cause of many common diseases and symptoms**. Case in point: [\"hidden gluten intolerance\"](_URL_0_). This author attributes \"inability to loose [sic] weight, chronic headaches, migraines, fatigue, anxiety, and many other things\" to gluten, with no citations to back up their claims. Lots of alt med practitioners push gluten-free diets, because it's easy and low cost, and because there's actually a small chance it'll work.\n* **Finding gluten-free items at the grocery store became much easier after 2006, when the [Food Allergen Labeling And Consumer Protection Act of 2004](_URL_1_) went into effect**. Trusting that items you were buying at a grocery store were wheat-free prior to FALCPA was a roll of the dice. Certain manufacturers were trustworthy, but for you to figure that out, it required contacting them and confirming. A big hassle that the average consumer wouldn't bother going through.\n* **Mainstream restaurants began offering gluten-free menus in the mid-to-late 2000s**. Some entrepreneurs realized that gluten-free dieters are fiercely loyal once they find gluten-free-friendly places to eat. It's worth taking the time to put together a gluten-free menu for the amount of recurring revenue that loyalty will bring.\n* **Gluten-free diets actually work in a small percentage of people** (like the 1% with celiacs, may also make certain people feel better because these diets are usually lower in refined carbs than normal diets, which some people respond better to). When people discover a solution to one of their health problem, they will preach it to the end of the earth and back. That helps spread the word (especially if that person is a celebrity...like Elisabeth Hasselbeck) and makes otherwise normal people try the diet.\n\nCombine all these factors together, especially the overextension of gluten as the cause of common diseases and symptoms, and you got a recipe for many people to *want* to go on a gluten-free diet, and to be able to do so without much hassle. Why not, right? Won't know if it helps until you try.\n\nEdit: Just want to mention that this is an explanation for the *initial* rise in popularity of the diet. At some point the marketing machine took over and now it's a fad where the original intent is lost on a lot of people.","label":0,"model":"human","source":"reddit","id":2941}
{"text":"I think it's because nobody bothers to make an effort to standardize it - because nobody really cares about these differences.  Most of the time, writing something by hand isn't meant to be something that's important in and of itself - only the message is important.  I have college papers written during the 30s from my grandparents - even that was typed.  And on the modern occasions when handwriting *is* important, it's it's often in a more expressive or artistic setting - invitations, for example - and standardization isn't valued there, anyway.  That isn't to say that we don't, in 2017, value standardization.  It's just that when we want it, we have tools to provide it. \n\nBut if you look at handwriting examples from 100 or 200 years ago (old census records are a good source,) you'll see that much of the writing is almost identical.  Typewriters weren't invented until the late 1800s, and of course weren't widely available or cheap for some time after that.  The only way people of this era had to create consistently similar text is through strict instruction and shit tons of practice - so that's what they did.","label":0,"model":"human","source":"reddit","id":2942}
{"text":"I watched [Penn  &  Teller's Bullshit S07E05 - Lie Detectors](_URL_0_) a few years back and this explained the issue very VERY well.  I recommend you watch this.\n\n\n* If you don't have 28 minutes to kill, allow me to summarize as best I can.  \n* Lie detectors are more or less bullshit.  Most people do not know how they work - they just **believe that they work** because they don't know any better.  So this is an intimidation tactic that generally is used to incite a confession.  If law enforcement says **\"When I asked you if you stole your neighbors car and you replied 'NO', I noticed a high level of activity that could indicate that you were lying.  Is there anything you would like to add to this?\"**\n* This is a trick, it's just like if the cops were interogatting you and your friend.  They would separate the two of you and say **\"We've got your friend in the next room and he just told us EVERYTHING!  If you confess, then we'll go easy on you!\"**\n* The cops, or the person giving you the polygraph, really just **hopes you will confess**.  This way, they don't need to cite the test at all during a trial.  You confessed to a crime, you are guilty.\n* From what I understand, polygraph tests alone usually do not hold up in court.  I don't have anything to back this up, so I **could** be mistaken.","label":0,"model":"human","source":"reddit","id":2943}
{"text":"There seem to be two key theories as to why this occurs, and they are pretty loose as this is a very niche feeling to study.\n\nThe first involves seeing objects places in a novel or different way then our minds comprehend. Similar to how when you experimented with Legos or building blocks as a child and created interesting and pleasing designs you hadn't experienced prior, placing objects together in a tight configuration also triggers this feeling. The only detractor from this theory is that the novelty of making something fit perfectly may repeatedly feel very satisfying. I swear this isn't a sexual innuendo.\n\nThe second is rather simple, and comes from a more philosophical perspective- simply put we like to construct order in a world of chaos and perfectly ordering and fitting things helps satisfy that desire. You can draw all the theories you want why we seek order, from an evolutionary or biological standpoint, to a even a dated Freudian standpoint.\n\nEdit: _URL_1_\nThis article explains it if you were not satisfied with this response.","label":0,"model":"human","source":"reddit","id":2944}
{"text":"\"Unconstitutional\" = contrary to the Constitution.  \"Illegal\" = contrary to a law (including contrary to the Constitution).  To some extent both of these things are a matter of interpretation, of course, but when it comes down to it they're either decided in a court of law or not decided at all.\n\nAs for \"impeachable\":\n\nBefore Gerald Ford became President, he was the Speaker of the House of Representatives.  At that time, he made a famous, quotable saying about exactly what \"impeachable\" means:\n\n**An impeachable offense is whatever a majority of the House of Representatives considers it to be at a given moment in history.**\n\nThat is, in my opinion, actually a pretty accurate summary of the situation.  \"Impeachable\" is not terribly well defined.  The Constitution doesn't define it at all.  It does define some related ideas:\n\n* The House of Representatives is the body that has the power to impeach.\n* The Senate is the body that has the power to try impeachment cases (i.e. after the House impeaches somebody, the Senate says whether they're guilty or not).\n* Some detail about how an impeachment case is tried (for example, it's required to be heard before at least 2\/3 of the Senate (**Edit:** misremembered this - it's 2\/3 for conviction, not 2\/3 for hearing)).\n* Some detail about the limits of punishment of a conviction for impeachment (can't be more than removal from office and barring from holding future office, something like that).\n* If the defendant is a civil officer of the US (e.g. the president), and the impeachment is for \"treason, bribery, or other high crimes and misdemeanors\", then conviction will result in (at least) removal from office.\n* The president's pardon power does not extend to impeachments.\n\nBut it's silent on what a person can be impeached for.  The \"treason, bribery or other high crimes and misdemeanors\" thing is the closest it comes to doing that, but it's not really that.  It's just saying IF that's what you're impeached for, then here's the minimum punishment.  It doesn't say those are the only things you can be impeached for.  And even ignoring that, it doesn't say what \"other high crimes and misdemeanors\" means.","label":0,"model":"human","source":"reddit","id":2945}
{"text":"Biomedical scientist here. This will probably get buried under the mountain of misinformation bombarding you, but here goes anyway. I posted this as a response to somebody else who said you would simply fall in half, but with a wire *that* thin... Let's just say it's not so simple on an atomic level inside your body:\n\n\nWhile the individual molecules of your cells are most certainly held together with covalent bonds that would be separated by such a wire, your cells may actually be able to accommodate something like this long enough that the natural healing processes of your body might be able to save your life. \n\nThe lipids that make up your cell membranes, for example, are not even bonded to each other by anything but hydrophobic and hydrophilic forces. This wire passing through (depending on the speed of its passage) would probably push them to either side of it and then they would reform around it, preserving the integrity of the membrane and keeping the cells from violently rupturing. Think of it the way a soap bubble can accommodate things passing through it sometimes without popping- in fact, soap is made from extremely similar molecules that operate under the exact same principles (soap being made from synthetic lipids or lipids from animal fats). \n\nWhat happens inside the cells, though? I shudder to think. It would probably fracture their cytoskeletons and slice through any organelles along the plane of its passage, resulting in cell death. If it went through the nucleus and passed through chromosomes (which wouldn't happen to every cell but a significant number) that in itself may cause either cancer or death.\n\nSo on the outward side, you wouldn't exactly fall in half. In fact, the cutting itself probably wouldn't even hurt with a wire that fine. But you'd see maybe necrosis or cancer develop along the entire plane which could be life threatening. Also, it's hard to say if you'd be paralyzed from wherever the wire damaged your nerves and spinal cord- but I think it's likely you would.\n\nThis whole thing is a very interesting question.\n\nEdit: I forgot to mention that bone might slice cleanly, due to it being mostly mineral mass.\n\nThis question is an excellent thought experiment where you have to take into account a lot of factors. It's not quite as simple as saying what would happen if you were to run this wire through a ball of steel or a block of stone.\n\nReally the only way to know for sure would be to actually do it. Fantastic question though! Reminds me of Randall Munroe's \"What If\" blog.","label":0,"model":"human","source":"reddit","id":2946}
{"text":"No one can say.  Few people have been decapitated and reported back in.  \n\nThere are plenty of reports of discorporated heads blinking and mouthing words and variously moving for surprisingly long after being removed. \n\nAt the same time we know that fainting is often caused by a drop in blood pressure to the brain, the brain senses a problem with blood delivery and it causes a person to go unconscious and fall, because when lying down your blood isn't working against gravity to get to your head.  \n\nWhen your head is removed its kind of hard to have much blood pressure.  \n\nThen again, there's a lot of trauma involved who can say the brain exercises its manual for crisis efficiently. \n\nOnce you cross the line from most likely going to die to certain death you reach beyond the barrier that evolution cares at all.  If there are any bits of directed action and substance in that state they are not based on anything meaningful in terms of man's biology and what he has adapted for.  \n\nEvolution wants to keep you alive for reproduction and passing on your genes, once your death is assured, it has no more use for you.","label":0,"model":"human","source":"reddit","id":2947}
{"text":"Think of your brain as a processor, except instead of using electricity as power it uses glucose. When you do, say math, you use a lot of processing power, and in turn, energy to do that math. Over time your brain \"heats up\" and gets tired. Research shows that you yawn for thermoregulation, or to cool off after doing a lot \"processing\". This is why you're told to take breaks when you study (1hr study, 10min break). \n\nThink of the fatigue as analogous to when you work out--your muscles use a lot of energy and needs breaks between sets to cool off and get rid of metabolic waste.\n\n*Edit: There is some misinformation in the comments which I'll explain.*\n\n1) Glucose as Energy: Your body uses glucose as it's most available source of energy. If you eat either fats or proteins, your body has ways to turn it into carbohydrates only if you run out of carbs, then it goes to converting fats first and if you're out of fat then protein. If you want to get really scientific, we can say ATP powers your brain but sugar was the ELI5 way of explaining it. Most importantly, eating food before studying will not help you, in fact it'll more likely turn you off to studying due to Insulin and other hormones being triggered.\n\n2) Losing Weight with the Brain: The most energy intensive activity you can do is exercise, mainly because you're moving stuff around (it requires energy). If you maxed out your brain's performance capabilities, you will not lose weight.\n\n3) Cooling Systems: Your blood is the ultimate cooling mechanism. Yes, your brain is watercooled, but when you're pushing the limits of the water cooling, then you start to yawn. Keep in mind,  your temperature will not fluctuate beyond 2\u00b0C, but these little changes in your body have a tremendous impact on your mood.\n\n4) Metabolic Waste: This is pretty important. Basically, after a lot of work, certain metabolic products accumulate in your brain making you feel like you don't want to study anymore. This is mainly because too much of the metabolic product would be toxic.","label":0,"model":"human","source":"reddit","id":2948}
{"text":"The system we currently use to understand calories includes but has trouble accounting for digestion itself using energy. Imagine if I ate a given amount of ice cream, which is very fast to digest, vs. eating something like raw kale, which takes a while to digest. The amount of energy you get from either one is going also include energy used in the process of breaking it down into useful energy. \n\nBack in 2001, researchers fed adorable little mice cooked vs. uncooked meat and sweet potatoes, and at the end of 40 days there was sufficient evidence to conclude that the cooked food provided more usable energy. The furry little white mice fed raw meat and sweet potatoes needed their digestive systems to work longer and harder to extract usable nutrients from their food. \n\nCooking, thus, is the beginning of the digestive process, delegating some of the work to things like an oven or a stovetop or a microwave. The energy is still expended in digesting, but it's energy from electricity or the burning of gas or the inductive heating of magnets instead of our body's biological processes. \n\nAlso, mice can sometimes sneeze, and it's every bit as high-pitched as you're imagining right now. I mention this not because it has bearing on the topic at hand, but because I think most people enjoy cute things, including 5 year olds.","label":0,"model":"human","source":"reddit","id":2949}
{"text":"The major contributing factor is your circadian rhythm. This rhythm is basically your natural biological clock. However it does not run exactly on a 24 hr cycle, there are slight deviations naturally and other factors can contribute to altering your biological clock. \n\nThe next factor to take into account is which stage of the sleep cycle you are waking up from. There are typically 5 stages of sleep, stage 1\/2 are \"active\" sleep cycles, your brain activity is still highly active and almost indistinguishable from a wake person. In stage 1\/2, you are also more likely to respond to stimuli, such as someone calling your name. Stage 3 is regarded as \"inactive\" sleep cycle, this is the stage where your brain activity drastically drops, preparing your brain and body to enter stage 4. In stage 3, a person is much harder to wake up and waking from this stage leaves a person exhausted and generally disorientated. Stage 4 sleep is where actual rest and rejuvenation occurs. In this stage, your brain activity is picking up slightly but still in a mild manner, not too much to be called wakefullness, but enough to signal repairs in the body. Lastly is REM stage, this is where your brain is now increasing activity and causing dreams. After REM, the cycle repeats back from stage 1. \n\nBasically as you sleep, you go through this 1-4+rem cycle over and over, with each cycle lasting about 90 mins. Depending on what your body needs, stage1\/2 will shorten as you go through the sleep cycles and lengthen stage 4 for rest and recovery or REM sleep. If you wake from REM, you will feel refreshed and ready, that is why most people dont remember dreams or recall a dream suddenly disrupted by waking up. If you wake up during stage 3\/4, you will feel tired and exhausted, your brain was trying to turn down the power after all, and signal the body to recover from things like exhaustion or injuries. \n\nSo thats it, a bit long for ELI5, but its a complicated question that takes many things into account, as is common when talking about neurological issues\n\nEdit: forgot to talk about sleep deficit and how that also contributes alot, but im on mobile and cant continue forever.","label":0,"model":"human","source":"reddit","id":2950}
{"text":"Here are a few things to get you started, I'll add some more as I think of them. I got into MLM when I was 14 (38 now). Lost 4K I saved from delivering papers. Best learning experience of my life.\n\n- upfront investment.\nThis is not always a sign of a pyramid scheme but all pyramid schemes do it. It is the main source of income, theoretically, not the product being sold. That is only a smoke screen for the scheme.\n\n- signing people up.\nUnless you get hired on for a salary or an hourly wage this will be a pyramid scheme. The more people you sign up the more money you make.\n\n- making a commission off of others work. \nUnless you are being offered a sales manager position in the company you already work for, then this will be a pyramid scheme. The \"pyramid\" parts starts when you make money off of your friends friends sales. It's called your downline, the people that make you money while you are on the beach doing nothing. It doesn't exist by the way.\n\n- setup meetings, parties, events.\nA lot of more professional pyramid schemes will teach you how to throw parties and annoy the hell out of your friends.\n\n- empty offices\nIf you walk in the door and there are a bunch of people around your age in chairs with clipboards, a plant on the floor, and a framed picture from target, leave.","label":0,"model":"human","source":"reddit","id":2951}
{"text":"Imagine the soundwave\/shockwave of thunder like an invisible cylinder around the lightning you just saw. That cylinder is created at the moment you see lightning, and it grows in radius at the speed of sound (approx. 300m\/s). Now on the surface of that cylinder there are one thousand of tiny little surfers, each of the surfing in their own direction away from the lightning, and whenever a surfer collides with your ear, you hear a noise.\n\nNow, as the cylinder grows, all of the surfers naturally surf away from each other, and all the surfers grow further and further apart (or less and less energy per surface area). The less surfers reach your ear, the more quiet the thunder.\n\nThe duration of thunder is a little bit harder to imagine. Again, a thousand surfers riding the shockwave. In their path from the lightning outwards, some of the surfers hit rigid objects like walls, houses, rocks, or trees. These rigid surfaces reflect them away from their original path. But if they cross a city, they don't only hit one house, but many. They end up on sort of a crazy random walk through the city, reflected hundreds to thousands of times. But in the end they may still end up in someones ear, and that person can again hear a tiny noise.\n\nNow as I said there a thousands of surfers, and all of them are randomly reflected a different number of times, so they take a different amount of time until they can be heard. Hence the once short, loud bang (many surfers arriving all at the same time) became a long, less loud rumble (many surfers all arriving one after another).\n\nEdit: Gold? For a bunch of surfers? Thank you kind stranger!","label":0,"model":"human","source":"reddit","id":2952}
{"text":"Well, the total world wealth is about 240 trillion. There are multiple sources that give different values, but this is the newest and they're all roughly 220-240. _URL_0_\n\nThere's about 7 billion people in the world\n\nSo if we sell everything off and redistribute it, we come to the glorious total of $34,285.\n\nKeep in mind, of course, a bunch of caveats. \n\nNot all wealth is the same; it's wealth, not income. Property or a factory is an ongoing money-maker, while gold\/oil\/etc sitting in a vault somewhere isn't. A house has utility but isn't income. (Generally speaking, of course.) And so on. So we are strictly looking at cash value.\n\nThis is per individual, so it includes children and spouses. A family of four would get a little over $137,000.\n\nAnd, of course, different areas have different standards of living. 30 grand is going to get you a lot farther in Bogota than it will in Manhattan. \n\nEdit: you know, for a theoretical question, you guys are REALLY hung up on the practicality of this idea. No, I don't know how this would happen in real life because it's impossible.","label":0,"model":"human","source":"reddit","id":2953}
{"text":"The process for melting tungsten is called induction skull melting. Basically several hollow fingers of copper are used to form the crucible in which the tungsten is melted. The hollow copper fingers are liquid cooled with water to keep the copper from melting at the extremely high melting temperatures needed to melt the W. The heat to melt the tungsten is generated through induction heating. A large cooper coil surrounds the hollow copper fingers. The copper coil has an alternating current passed through it. The magnetic field generated by passing the AC through the coil generates eddie currents in the copper fingers. The natural resistance to the eddie currents in the copper fingers generates the heat. The whole process usually takes place in a vacuum because W is very reactive and would react with almost any atmosphere. Copper is used because it has both a high thermal and electrical conductivity. Due to the massive temperature difference between the molten W and the copper fingers a fine layer of solid W forms between the copper crucible and the liquid W. This layer keeps the tungsten from reacting with the copper and potentially destroying the crucible and ruining the purity of the W. This process is very energy inefficient so despite the abundance of WO pure W is very expensive.\n\nSource: I am a materials engineer.","label":0,"model":"human","source":"reddit","id":2954}
{"text":"There's an ideal ratio of protons to neutrons for an atomic nucleus to be stable.  Even when their protons and neutrons are at the right ratio, big atoms have a nucleus that is kind of wobbly and unstable to begin with.  Over time those big atoms, left to their own devices, will tend to split into fragments.  This is called *radioactivity*.  Exactly what fragments an atom splits into depends on what kind of atom it is (which is determined by how many protons and neutrons were there in the first place).\n\nWhen a big atom gets hit by a neutron, an already-unstable atom has its proton-to-neutron ratio suddenly be \"wrong.\"  Which causes it to become extremely unstable and split into fragments almost instantly.  A bunch of binding energy is released which causes the fragments to fly apart, like what happens when you cut a tight-stretched rubber band.  How fast atoms are moving determines *temperature*, so when an atom is split, it creates heat as well as fragments.\n\nLeo Szilard, a Jewish physicist who left Germany when Hitler became chancellor, realized that if we could find some atom that splits into fragments that include multiple neutrons, you could create a *chain reaction*.  In a chain reaction, one atom breaks and releases two neutrons, which breaks two more atoms, which releases four more neutrons, which breaks four more atoms, which releases eight neutrons...Szilard convinced other influential physicists, including Einstein, to advise the US government that this was possible and might be able to be used to create a powerful bomb.\n\nIf you have only a small amount of the right kind of atom, the chain reaction can't get properly started -- too many neutrons escape off the edges of the sample.  But once you have enough atoms -- a condition called *critical mass* -- the chain reaction will be able to sustain itself.  If you're just over critical mass, the sample will start to heat up and get hotter and hotter until it melts itself and its container, this is called *meltdown* (the flow will eventually spread it out enough that it stops being critical and cools down, but before that it can cause lots of damage in places that are difficult \/ impossible to safely repair because of radioactivity).  If you have way more than critical mass,  then incredible amounts of heat and neutrons will be produced very quickly, which causes an enormous explosion.\n\nOne use of this chain reaction phenomenon is to build a power plant:  Use neutron absorbing rods which can be inserted\/removed from a sample to keep it near critical mass, enough so that it heats up enough to boil water to create steam and generate power, but not to the point of meltdown. Another use is to build a bomb:  Use regular explosives to smash two samples together to create a single sample far above critical mass, which will cause a very powerful explosion.\n\nThe right kind of atom for this process, a kind of uranium, turns out to be fairly common all over Earth, but since it is unstable, a lot of it has split into fragments over the billions of years that have passed since it was formed.  The remaining amounts of useful uranium are mixed with large amounts of useless uranium that doesn't release multiple neutrons.  So the useful uranium needs to be separated from the useless uranium.\n\nNormally when scientists or engineers want to separate things, they will rely on different properties of the things they want to separate.  For example, you can melt the sample and let the liquid settle into different layers (taking advantage of different liquid density).  Or by making the sample hot enough for only one of the mixed things to boil away (taking advantage of different boiling points).  Or by adding chemicals which react differently with the things they want to separate (taking advantage of different chemistry).  But none of this works for uranium, because the two kinds of uranium are very similar and have almost identical properties.\n\nThe best method they found for separating uranium involves vaporizing the uranium into gas and then spinning the gas really fast with a machine called a *gas centrifuge*.  The two different kinds of atoms weigh different amounts, allowing the right kind of uranium to be separated and purified.  This process is incredibly expensive and inefficient, which is a good thing.  Hobbyists or terrorists can improvise regular bombs fairly easily, but can't feasibly build a nuclear bomb in their backyard.  Even for a government, in the age of satellites it's not really feasible to hide the enormous facilities needed, so countries basically know which other countries can \/ are trying to build bombs, and can pressure each other not to create weapons.","label":0,"model":"human","source":"reddit","id":2955}
{"text":"I didn\u2019t work with glue or caulking but I was a machine operator at a sex toy factory so we worked with the silicone or rubber material (still don\u2019t know what it is.) It came in buckets, texture was similar to sand or mud. The sand materiel made \u201charder\u201d toys and the mud material made more \u201cjelly like\u201d toys. The machines were set at certain temperatures to melt the material and molds would be filled. We would mix the material with the \u201crejects\u201d as well so no material was wasted.\n\nBut to answer your question, last 30 minutes of work we would turn off the machines and have to basically unclog the machine by poking it with a metal stick or else it would dry up and get clogged up. \n\nHere\u2019s 2 vids of the actual factory I use to work at\n_URL_1_\n_URL_0_\n\nEdit: should\u2019ve mention that it was very important that the material was still hot before unclogging cause once it dried up it was a pain to unclog. The next morning we would just start the machine, set it at a high temperature, feed it and let it run for a bit to get rid of any dried excess material that was there the day before. Willing to bet money they do this with glue as well.","label":0,"model":"human","source":"reddit","id":2956}
{"text":"There were and are two main techniques of planking. \n\nCarvel is where you fit the two butted sides together with a good fit and as slight a distance between planks as you can and they lay flush upon the frames and present flush to the outside world. Then, depending usually on the size of the vessel, either cotton or oakum is driven in the whole length of that seam very very tightly packed. Once the boat hits water, the planks will swell and compress the cotton which is already in a narrow channel, and the seal will be waterproof. This may take a few hours to days depending on type and state\/age of wood and cotton and also length of time out of the water.\n\nLapstrake or clinker is what you see in Viking ships. The plank above lays over the edge of the plank below. They are both steam bent then carved to receive each other before being fastened together on the boat. They are fastened with rivets which, in the act of riveting, create further compression of the wood along the very accurate fit. A watertight seal. \n\nBoth kinds have a rabbet carved into the keel and stem and sternpost (whole spine of the boat) with a complex bevel to receive the garboard (first plank), and cotton caulking along the seam of that rabbet exposed to the world. \n\nPitch is often poured into the keel at the lowest point around the sternpost so that water which will inevitably get in, from seeping or over the gunwales, will find its way there by gravity but not rot the wood as is sits waiting to be bailed. Pitch can be applied many places and often is for degrees of repairs. \n\nSource - am boatbuilder.","label":0,"model":"human","source":"reddit","id":2957}
{"text":"The kernel is a section of code within a computer's operating system that has the most amount of privilege and has direct access to the hardware. When a user has a program that needs to access the hardware, say to read from or write to a hard drive, the program must request permission to have access from the kernel. A system call is the process by which a program requests these accesses to the hardware, and the kernel grants or denies access based on permissions.\n\nIn windows the kernel is made of two layers. The upper layer is called the executive and this contains functions for object management\/retention, permission verification, memory management, etc. The lower layer is the actual kernel, which is responsible for low-level processor synchronization, interrupts and exceptions handling, thread scheduling, and recovery from power failure (power loss, hard shutdowns, etc). These layers work together to provide everything that a user may need to access the hardware and make changes to the system as they need to and are allowed to.\n\nForgive me, but I don't have as much knowledge about Linux to this level, but the kernel performs basically the same in Linux (I'm a windows instructor). Instead of being built with layers, it instead has kernel loadable modules (KLMs) which can provide additional functionality to the kernel.\n\nHope this helps.","label":0,"model":"human","source":"reddit","id":2958}
{"text":"Oil and water don't mix very well.  You can see this by looking at the distinctive layers in a bottle of salad dressing.  The kind of dirty that tends to be the biggest pain to get rid of is oily dirty.  When we get oily things on our hands, even when we rinse them with water the oily bits stay because the water can't dissolve them and wash them away.\n\nThe teeny tiny bits (molecules) that make up soap are amphipathic.  This is a technical word that means they have two parts or regions.  One of those regions mixes really well with watery stuff, and one of those regions mixes really well with oily stuff.  When you put soap on your hands, the part of the molecule that mixes with oily stuff flips around to face the dirtiness, and the part that mixes with watery stuff flips around to face out.  Imagine you have a really big magnet, an oily mess, and a bunch of really small magnets, soap.  If you throw all the little magnets at the big magnet, they'll all flip around until they face the \"right\" way to stick.  That's what's happening at a molecular level.  Now the entire outside surface of the oily bits on your hands are covered in soap molecules with their oily regions facing inwards and their watery regions facing outwards.  This means that the water you rinse with can grab onto those oily soapy mixtures and dissolve them to wash them away.","label":0,"model":"human","source":"reddit","id":2959}
{"text":"The knob is not a \"volume\" knob, which goes from zero to some arbitrary value (11?), but rather an attenuation control, which goes from complete attenuation (minus infinity) to no attenuation (zero). This more accurately reflects the way that the amplifier works internally, because the amplifier has a constant gain, and the low level signal on the input side is selectively attenuated to produce the desired signal level on the output side.  The attenuator is appropriately thought of as a sensitivity control, as opposed to a power multiplier control. The amplifier is always capable of its maximum output, but with reduced sensitivity, requires a higher input signal level to get there. This also makes it easier to track gain structure through a complicated series of components, because the attenuation control can be calibrated in dB, so if you have a source at a nominal signal level of 0 dB, you can selectively increase gain at a signal processor, reduce it at a mixer, etc., and achieve your target gain at the amplifier to prevent clipping.","label":0,"model":"human","source":"reddit","id":2960}
{"text":"There are certain things that are common in human mythology, and one of them is that humans are predisposed to be impressed by really big things. And it's easier to imagine a really big version of something that already exists than to design something completely from scratch. Hence why so many cultures have stories of giant humans running around. This applies to other creatures as well; take a reptile of some sort, scale it up to being bigger than a house, and suddenly you've got a dragon of some kind. Imagination adds other embellishments such as horns or breathing fire, or whatever the local variation is. (And they can be pretty varied; consider the differences between a standard European dragon, a Chinese dragon, and the Aztec winged serpent -- but they're all basically big lizards.) Similarly, many cultures have stories of giant birds, be it the ~~European~~ Middle Eastern (thanks productusmaximus!) Roc or the North American Thunderbird.\n\nThis of course gives rise to the question of why some animals don't seem to get the gargantuan myth treatment as often. To this I can only speculate that when one has to deal with actual bears and tigers and jaguars, stories about even larger ones aren't necessary for drama.","label":0,"model":"human","source":"reddit","id":2961}
{"text":"Hi, B.A. in psychology here.\n\nLet me ask you this: if a person with schizophrenia doesn't ever hear speech, would they still hear voices? \n\nThe thing about schizophrenia is that it falls under the categorization of psychosis, that is, a departure from reality. In order for someone to be classified as a schizophrenic, their separation from reality would be identified given an already established reality, namely, that people should not hear a person's voice talking when there is no such external stimulus. \"I can hear my own voice in my mind though,\" you might say. That is not the same since it is not a perception. It is not an external stimulus interpreted by your brain. In a schizophrenic or other psychotic individual who hears voices, the brain perceives the voice as if it is an external stimulus, or at least that is what we are able to determine based on individuals' descriptions of said voices. However, the individual is still aware in most cases, depending on the intensity of the psychosis, that the voice is in their head.\n\nThus, the issue can be anywhere in the brain areas related to auditory sensation, perception, and of course all other related mental networks, such as those of memory, speaking, language analysis, etc. The way our brain functions is that we perceive what is happening in the world outside our minds by complicated processes in our brain and body that TRANSLATE an external stimulus like sound into patterned signals that we can understand. \n\nSomething to consider - If a person DOES hear voices given they have never heard human speech, including their own, as a result of drug use or mental illness, then we can conclude that there is a very strong genetic component, something in our DNA, that \"remembers\" what human speech is. Guess what? Studies have shown that a deaf person who has never heard human speech in their life and has been diagnosed with paranoid schizophrenia, DOES describe hearing voices. Isn't that fascinating? Given this information, this means that the cause for hearing voices may not by a separation from reality... it may not have to do with external stimuli. Perhaps, it is something very deep in our DNA. No surprise there, schizophrenia is quite heritable. In the case of paranoid schizophrenia, 30-40% of siblings will share it. That is, your chance of having Par. Schz. are significant if your sibling has it. Amplified by the use of drugs.","label":0,"model":"human","source":"reddit","id":2962}
{"text":"Because the tests aren't perfect, can't detect some drugs\/treatments, and are sometimes pre-announced.\n\nOne of the 'drugs' Armstrong used was called 'blood doping' or erythrocyte supplementation. He took perfectly matched donor red blood cells and injected them until he had 2.2 times the normal human number of red blood cells. (which gave him massive endurance)\n\n2.2X regular human RBCs is the limit imposed by the international doping agency and Olympics, because 2.2X is the maximum amount of red blood cells detected in an un-doped natural human. Which is to say that some people naturally have 2.2X RBCs and they are super-men for endurance sports....so it's common practice to bring your level up by doping so you can compete with these supermen. Otherwise you don't stand a chance.\n\nAnyway, RBC supplementation is virtually impossible to detect as what you are injecting are perfectly natural normal red blood cells. You can't even use genetic testing - often they'll donate their own blood when they don't need it, collect the RBCs, then re-inject it when it's competition or training time.\n\nAlso there are rules as to how many times in a season you can be tested for drugs. Once you've hit that limit you can dope as much as you want until the end of the season and clean up in time for the next season.\n\nAnd they are coming out with new drugs all the time that may not be detectable by existing tests.","label":0,"model":"human","source":"reddit","id":2963}
{"text":"Corruption happens for lots of reasons. Think of each file as a million light switches. The order in which the light switches are on or off tells the computer what is inside the file. \n\n & #x200B;\n\nIf the light switches were stored I'm a giant steel box then whenever you went to look at them they would be in the same order. In a perfect world, someone could open that box, copy the order of switches into a new box, and then close their box for a perfect copy. \n\n & #x200B;\n\nImagine if you started to copy the switch order to a new and something interrupted you, or the box you were copying from was removed. If this was towards the end of the switches, you would think you might think you had a copy, but would be missing pieces of it.  This is how files can get corrupted while copying though most programs have a method to verify the switches were copied correctly.\n\n & #x200B;\n\nWhen something on your computer is running, it stores those switches in a temporary box that is really fast to change, but also really sensitive. Imagine if you shook the box with a bunch of rocks in it. Theres a chance that while you're shakeing the box, one of the rocks would hit a switch and change it at random. This is what can happen with data while it's in memory, sometimes background radiation can hit one of the electrons and change it. \n\n & #x200B;\n\nEvery time you save or retrieve data there is a chance that someone along the line some piece of hardware will incorrectly read the data or incorrectly write the data. Computers do a lot to check if this happened and correct it, but occasionally something will cause an undetectable issue and that is viewed as corruption.","label":0,"model":"human","source":"reddit","id":2964}
{"text":"Think of you having a chain of lemonade stands based out of Texas, but you also operate in New Mexico. In effect, you are a Texas company that trades in Texas, but you operate in other markets as well. Now, you have investors who own lemonade stand stock: Mom, Dad, Grandma, the creepy cat lady down the block, etc. They own stock in your lemonade stand. \n\nUnfortunately, you are having to deal with a shortage of lemons in Texas. It has a small effect on your business, and your investors notice. Grandma, who always loved your brother more than you, decides to save her fifty cents and sells her lemon stock. This leads to Mom noticing a drop and selling as well. This leads to the price of your stock dropping. This can have an affect on other lemonade stands in Texas, too, and soon people start selling around the state. Hell, this can lead to car wash stocks dropping. This can lead to bake sale stocks dropping! People want to save their money.\n\nNow, people in New Mexico start to take notice. You have some lemonade stand franchises open there and they've noticed the price of lemonade stand stocks dropping. Soon people in New Mexico start selling to be safe. Is it a Texas problem? Is it a lemon problem? Who knows!\n\nSoon we see a chain reaction around the world with prices dropping in everything from lemonade stands to newspaper routes. The long term effects will be some smaller lemonade stands around the globe hitting hard times. Bigger orange juice conglomerates are not going to be effected that much as they are considered a safer investment.\n\nIn short, people around the world are going to be more cautious with their investments for the time being, and we will see that as stunted growth and even a small market drop. This happens every so often as the lemonade market becomes overvalued and is known as a market correction. In other words, your lemonade was never REALLY worth sixty cents a glass, but was really worth something like fifty cents a glass.","label":0,"model":"human","source":"reddit","id":2965}
{"text":"Well, he says a lot of inflammatory things. Some are taken out of context, some might work as jokes in person but make him come across as an ass in text, some are just him trying to be an affable asshole (and failing).\n\nHe was featured pretty heavily in Indie Game: The Movie, which, depending on your reading, either makes him look like a person under a tremendous amount of pressure starting to crack, or a pretentious, whiny asshole who thinks that everything he does is golden. \n\nBut really, most of it is overblown. He says some things that rile people up a little bit, and if his work was perfect he would be able to get away with it just fine. But due to his games being flawed on a technical level (although Fez was *incredibly* ambitions from a design standpoint), lots of the internet expect him to be more apologetic towards the public, which he is not. But watch him in a more natural, unedited setting (Like [here](_URL_0_)) that isn't edited for emotion like a movie and lets you actually hear his tone behind the words and he seems much more normal. \n\nEdit: Relevant link that I totally forgot about, [Phil Fish watches Indie Game: The Movie](_URL_1_), taken right after he sees the film for the first time. \"Ed\" that he talks about is Ed Mcmillen, one of the creators of Super Meat Boy.","label":0,"model":"human","source":"reddit","id":2966}
{"text":"tl;dr\/ELI5 - It's possible that a certain part of your brain might send too many signals to a nerve in your body when it is stressed out too much by the loss of someone or rejection. But it isn't for sure.\n\n[Source](_URL_0_) \n\n > Research has shown that a broken heart hurts in the same way as pangs of intense physical pain. A 2011 study demonstrated that the same regions of the brain that become active in response to painful sensory experiences are activated during intense experiences of social rejection, or social loss generally. \"These results give new meaning to the idea that social rejection 'hurts',\" said University of Michigan social psychologist Ethan Kross, lead author of the article. The Michigan research implicates the secondary somatosensory cortex and the dorsal posterior insula. Macdonald and Leary had earlier (2005) proposed the evolution of common mechanisms for both physical and emotional pain responses, and noted that multiple languages and cultures use terms like \"hurt\", \"heartbreak\", \"hurt heart\" or \"ripped out my heart\" to describe responses to social exclusion and argue that such expressions are \"more than just a metaphor\".\n\n\n > The psychologist and writer Dorothy Rowe recounted that she thought of heartbreak as an empty clich\u00e9 until she experienced it herself as an adult. Heartbreak can sometimes lead people to seek medical help for the physical symptom, and may then be related to a somatoform disorder.\n\n\n > The neurological process involved in the perception of heartache is not known, but is **thought to involve the anterior cingulate cortex of the brain, which during stress may overstimulate the vagus nerve causing pain, nausea or muscle tightness in the chest.** Eisenberger and Lieberman showed that rejection is associated with activation of the dorsal anterior cingulate cortex and right-ventral pre-frontal cortex, areas established as be involved in processing of pain (including pain experienced in others through empathy). The same researchers mention effect of social stressors on the heart, and personality on perception of pain.","label":0,"model":"human","source":"reddit","id":2967}
{"text":"Former USAF.  The US doesn't really have any jurisdiction once you leave the borders (assuming no legal connection from job).  They wouldn't like it, but their laws pretty much don't apply.\n\nHowever, there are two big areas of legality that will get you, and one other area.  Legality first.\n\n1.  Whatever country you go to, you are subject to their laws.  This is true even for military.  Yes, the US is probably going to fight to get you back, and they might even win.  The problem is, it's up to the country you committed the crime in.  They don't have to agree to extradition, especially if that means you will be going to a jurisdiction with no applicable law.\n\nAn example would be in the Middle East.  On-base, and for standard laws, whatever happened would be taken care of by the US.  If you go off-base and break one of their laws that we don't have (something like talking to someone's wife directly as a male) then they arrest you.  You have no rights as an American in their eyes.  The US would try, but it's ultimately up to the sovereign country you are in.\n\n_URL_1_\n\n2.  The biggest issue would be your legal classification under the Law of Armed Conflict and the Geneva Conventions.  There are four main classifications of individuals: lawful combatant, noncombatant, civilian, and unlawful combatant.\n\nMilitary personnel are typically lawful combatants, but there's a few exceptions.  Medics and chaplains are noncombatants.  They are legally not allowed to fight.  Some do, but aren't supposed to unless for their survival or the survival of their patient.  POWs are a pseudo-noncombatant.  You are free to attempt escape, but if you kill a guard while doing so, that's considered illegal.  If you escape and kill an innocent, you are an international criminal.\n\nCivilians are exactly what you would expect.  If you aren't in the service, you are considered a civilian.  That changes if you start fighting.  Anyone that is not a lawful combatant becomes an unlawful combatant once they start fighting (few exceptions).  Technically, most of the fighting in Iraq and Afghanistan was against unlawful combatants.  Once you cross this line, you are prosecutable internationally.\n\n_URL_0_\n\n3.  The third complication is realistic.  If you have no logistics, no training, no support, no anything, how do you really think you'll do?  Look at Ferguson.  They have the tools, and they have the closest similarity to military training, yet they handled a ton of things wrong. Without the supply chain, training, or more bodies, you really don't stand a great chance.\n\nEDIT:  Sorry for the late edit, been busy.  I wanted to clarify something: this is a huge legally gray area.  Yes, some laws (taxation, international laws) still apply to US citizens.  I meant things that are legal in one country may be illegal in another, and vice versa.  In those scenarios, you are subject to the laws of the country where the situation happened.\n\nWhen it comes to things that are internationally illegal, the US retains jurisdiction over it's citizens.  I was discussing who has jurisdiction when the law isn't internationally accepted.\n\nJoining a foreign military may not affect citizenship at all, or it might.  It depends on whether the US has an agreement with the other country, whether dual citizenship is allowed for that combination, and what the legality of the conflict is.\n\nFor PMCs, I'm not positive, but I believe they have a certain amount of legal protection as long as they aren't the primary combatants for the conflict.  Also, any noncombat role is protected.  PMC combatants probably fall under mercenary law, but I don't know too much about that.\n\nFor POWs, it's context dependent.  If you kill a guard to escape without your life being threatened, then it's illegal.  As a POW, there are certain restrictions placed upon you.  That all is disregarded if the country isn't treating you correctly according to law or it's for survival.\n\nSide note:  On mobile, and my post has 1,2,3 but when I post they all become 1.  Anyone have any idea?","label":0,"model":"human","source":"reddit","id":2968}
{"text":"as to why nails grow how they normally do, it's because of how they're built. all of the nail you can see is dead. it's created under the skin by a strip of material called the \"root\", and it's a C shaped channel that build the nail. the nail grows forward because that's the direction the material is drawn. \n\na common cause of ingrown nails is bad footwear. the pressure pushes the skin and nail together until the obvious happens. trauma is another cause. something damages the toe and the nail gets messed up. as are infections in the toe, as infections can swell tissues. however, a large number of cases have no known cause. \n\ncommon speculation among professionals is that it's a minor defect in the nail's root. if the channel is misshapen, then the nail can push in the wrong direction. not unlike a defective printer. the rest of your anatomy has some ability to compensate, the Perionychium (the skin around your nail) can callous up and that added hardness can potentially push the nail into proper shape, but sometimes it can't.","label":0,"model":"human","source":"reddit","id":2969}
{"text":"(please don't let the wall of text scare you away, it turned out a bit long but I've made good effort to try and make it fun to read, including asking for friends' input (I make procrastination an art form). Also, please notice that I exceeded the 10k characters limit, so I'll be splitting this text in two).\n\nHmm OK.\n\nTo understand how this happens you need to have some intuition about the geometry of general relativity.\n\nThe problem is that it is a bit not intuitive, as we're not mentally equipped to imagine higher dimensions.\n\nInstead of waving around some very general descriptions, I'll try and actually explain the math behind it to some degree, I promise not to be too formal, but I will go in deep enough to emphasize all the nuances I think are crucial for a good grasp of the physical aspect of it all. Whenever I feel some clarification is borderline, I'll add it in parens.\n\nOne very basic structure in geometry is what called a *manifold*. Formally, an n-dimensional manifold is any geometrical shape in n-dimensions which is locally homeomorphic to an euclidean space.\n\nWait what?\n\nOK, so a euclidean space is a space where all dimensions are \"straight\", this will make more sense in a second.\nHomeomorphism (also known as topological equivalency) is not easy to explain, but it's basically the relation through which a [coffee cup is equivalent to a donut](_URL_0_).\n\nLimiting ourselves to a 2 dimensional manifolds, this geometrical structure can be described as an infinitely long and wide sheet of rubber. It's \"locally homeomorphic to a 2-dimensional euclidean space\", which is just a fancy way of saying that whatever point you choose on the plane, it's immediate area will look more and more like a plane as you zoom into it, which is in turn a fancy way of saying that it has no rips or pinches - it is smooth.\n\nIt's easy for us to imagine this, but what happens when we go up a dimension? Can you imagine a curvy three dimensional space? No, you can't. You see, we have no mental image of a \"curved space\", so for us to imagine a curved space we need to embed it in a straight space, as a geometrical shape. We can imagine our rubber sheet because we can embed it in a three dimensional \"straight\" space. It's hard for us to understand that this curved piece of paper is indeed a two dimensional curved space and not just some curved shape \n\nHow does this difference (between a \"curved shape\" and a \"curved space\") manifest itself mathematically, you ask? (of course you don't, why would you ask THAT?). We'll get to it later on, as it is key to really understand the answer to this question, but right now I want to get to the physical implication of what we defined so far.\n\nSo, our universe isn't curvy, now is it? Well, this is the mind blowing part - it actually totally is. Not only does it band, but what makes it bend is mass. This has been demonstrated time and time by taking a rubber sheet and placing a heavy metal ball in the center. The rubber sinks in to create a round mold. Now, if you take a small marble, place it at the edge of the mold and give it some momentum in a direction tangent to the circumference of the mold, it would spiral around the edges of the mold, which means it will be in orbit around the heavy metal ball much like the stars orbit around the sun. The similar effect is how gravity effects us. You think the earth is pulling you down becaue of some force, but as far as general relativity goes, this \"force\" is actually you sliding down a four dimensional hole in space creating by placing there a big ass rock called Earth.\n\nNow let that sink in. If you toy with this concept a little bit you might reach some surprising realizations. For example, if you were able to control this curvature, you could bend the space on both ends of a starship in order to make it slide in which direction you choose. This will generate a \"gravity field distortion based propulsion system\" which is akin to placing some mass in front of your spaceship which moves forward with it while giving it acceleration. Other cool example is that of negative mass, or the fact that you can accelerate your own center of mass without any external forces (bashing yet another one of the eternal truths of Newtonian mechanics).\n\nSo we now understand a little better the geometry of earth, but that still doesn't answer our original question - why do big concentrations of mass curve light beams.\n\nRemember a while ago, when I blabbered about the mathematical difference between a curvy shape and a curvy space? This is where this kicks in! You see, all geometric shapes we talk about are actually *metric* spaces. That means that the space is defined with a metric. A metric can be thought of as a black box which gets two points in the plane and returns the distance between them (for the math inclined, a metric is a two variable function which gets two points on the plane and returns a non negative number), and it satisfies some other properties there's no reason to get into. This sounds trivial but it's actually impossible to define a metric (that is, a distance between two points) on most spaces (just take my word on this one, I really don't want to define what constitutes \"a space\". I know this sounds silly, but I'm afraid I've already said enough within this pair of parens to raise a mathematical shit storm).\n\nWhat differs a curved shape from a curved space is how we take distances on it.\n\nTake, for example, a two dimensional sphere, that is, the shell of a three dimensional ball, yes, three dimensional. This dimension classification might seem cumbersome, but it is for a good reason, you see. The two dimensional sphere is called so even though it's in three dimensional space because it constitutes a two dimensional curved space.\nNow, we want to define how we tell the distance between two points on the sphere.\n\nWe can take the length of the straight line beteen them. It's quite an intuitive thing to do. The problem is that we want the distance to have a physical meaning constricted to the sphere. The length of the said straight line doesn't express any distance of any trail we can take between the two poitns **on the sphere**. So instead, we define the distance to be the length of the shortest path on between the two points which doesn't go out of the sphere.\n\nThis is a good place to stop and smell the roses. This definition makes sense, but it also makes the dependency in a \"straight\" space obsolete. We can now view the sphere as a metric space of it's own, and not as a body embedded in a three dimensional space. That same way, the universe manifold should be viewed as an independent three dimensional curvy object, and not as a three dimensional manifold embedded in a bigger dimensional space [mental exercise: would four dimensions neccessarily be enough to embed this three dimensional manifold in?).\n\nSo now, remember the idiom which says that light goes in a straight line? Well, according to general relativity this is actually **wrong**. Light goes in **the shortest** path. Thing is, that when the distance the light travels is not very and the mass is not very large (the Earth, in this proportion, has negligable mass), a straight line makes for a pretty decent approximation. So good, actually, that when Einstein wanted to devise an experiment to test this theory in practice, he had to wait for a solar eclipse so he could measure the deviation of stars in the sky caused when the sun gets close to the path the light takes between the stars and earth. The experiment was almost blown due to weather condition, but when it was finally conducted it did in fact show that when a star appears in the sky really close to the sun it's position in the sky is actually slightly altered by the sun's mass. This drove the point home, the point being that **mass curves space, and so a curvy line might actually be shorter than a straight line**. That's all I really wanted to say, but from my life experience - saying only that much will only lead to confusion which will then lead to all of the above being exerted. This effect is sometimes refferred to as *gravitational lensing*.\n\n**TL;DR - Have you ever seen a modern physics book? The entire comment is the fucking TL;DR!**\n\n\n**A cool real life example of all of the above**: Been delegeted to a comment due to the 10k characters limit.\n\n**EDIT**: Just wanted to thank you all for all the supportive comments and PMs, really made my day :)","label":0,"model":"human","source":"reddit","id":2970}
{"text":"Newspaper columns are justified on both the left and right, which means that the left margin and the right margin are both straight (see [this image](_URL_0_) for a visual explanation).\n\nThere are a couple of techniques you can use to get both margins straight, and usually you use a combination of both: you can use hyphenation to break up longer words, and you can adjust the spacing between words, or between individual letters.\n\nIn most publications you barely notice it, but newspapers typically use very narrow columns. Because they use such narrow columns, it's harder to get the text to justify correctly, and often words or letters are spaced out so far it becomes noticeable.\n\nIn the days before computers were used to set type, this was done by hand, and it's a very difficult thing to get exactly right: newspapers had to be printed very quickly, so text justification was often done poorly. With modern technology, the process can be automated, but now the problem is getting the software to understand what looks neat and professional to a human, and what doesn't.","label":0,"model":"human","source":"reddit","id":2971}
{"text":"Have you ever been in a pond or stream and tried to grab a fish with your bare hands? It's not easy. You can't just reach directly for what you see. That's because the water is changing the path of your vision (refracting it). Let's also try to remember when you have a cup of water and you stick a straw in it. Remember looking at it from different angles and noticing how the straw looks like it is bent right when it hits the water? That's what water does to light; once light hits water, it gets bent.\n\nNow imagine those old 3-d glasses you used to have to wear to watch 3d movies. One side was red and one side was green. When you look through the red side, everything is either dark or bright red. red things looked bright red, and things that were kind of red looked like darker red. the opposites of red (like green) look almost black. This is how color works. Regular light is made up of every color. When regular light hits a red shirt, only the red light bounces off of it and then into your eye.\n\nlet's combine these two examples. When light hits a red shirt, red light should bounce back. But what if it's wet? When things are wet, the light is bent (remember the straw in the cup). So instead of having the red light bounce directly back at your eye, the water bends that red light away and not towards your eye. And if you don't see light, you see darkness.\n\n\n(i hope i did okay)\n\n**edit**: while this is my simple 5-year old explanation, rupert1920's [comment](_URL_2_) has some corrections about refraction and is written in a sciencey way. Definitely worth reading. also his comment [here](_URL_0_). give the man some karma, for has has much more science than I.\n\nedit: Trekky0623 caught [something I messed up on](_URL_1_). I changed it.","label":0,"model":"human","source":"reddit","id":2972}
{"text":"Chem engineer here.\n\n[Take a look at this **ILLUSTRATION **and see if you can understand it for yourself](_URL_0_).\n\nWell, in most substances a raise of pressure may provoke a passage from the liquid state to the Solid state (Solidification). This happens because the pressure forces the molecules to be closer.\n\nHowever the Water molecules are further apart when they are in the Solid state compared to the liquid state. It has to do with the geometry of the H2O molecule. That's why at 0\u00baC Ice as a density of 0.9 (Water is 1).\n\nSo when you increase the pressure and force the water \/ ice molecules to be even closer they pass to a state where they can be even closer: liquid.\n\n**EDIT**: I know this is not a proper graphic. It's an illustration. Even if it had a scale it would be useless: engineers use Equations of state because we need to know partial pressure at different points of pressure and temperature (It's like knowing how much of the water goes to moisture \/ humidity. There is no 100% liquid water with air around it with 0% humidity ).","label":0,"model":"human","source":"reddit","id":2973}
{"text":"You measure the circumference of your ribcage just under your breasts, then you measure around at the fullest at your breasts. \n\nNow there are two methods of calculation. One tells you to subtract the ribcage measurement from the breast measurement, the other tells you to add 2-4 inches to the ribcage measurement. This is because decades ago when stretchy materials like Lycra didn't exist, you need to add some inches to the band in order to be comfortable. \n\nSo most modern day bra manufacturers (at least in the US) use the modern method. Let say your ribcage measurement is 34 and breast measurement is 38. You subtract 38-34 = 4. The ribcage measurement will be the band size (so it's 34) and every inch difference will be one cup up (if you get 1 inch difference you have A cup, 2 inch difference you have B cup. In our example the difference is 4 inches, so congratulations you have D cup boobs). Our bra size is 34D. \n\nIt might be a little confusing when you get to more than 5 inches in difference. Back in the 50's the biggest cup available was D (I guess they didn't foresee breast implant become popular or people getting fatter), so when they realized they need to make even bigger cup sizes, they just add an extra D thinking boobs cannot possibly get any bigger (most old ladies at the time wore one-piece corslettes, bra was something younger and slimmer women would buy). When they needed to make yet bigger cups, some manufacturers decides to make DDD and others decides to go up one letter to E. There isn't a standard on cup sizing beyond DD so some bras cups will be A-B-C-D-DD-DDD-E while other bras cups will be A-B-C-D-DD-E-F. (I've noticed DDD is more common in the US while DD-E is more common in the UK).\n\nTake note though, finding the correct sizing is only the beginning. Bra manufacturer may or may not make the bra according to the size or make bras that's just not suitable for your shape. Therefore it's very important to try several different sizes\/styles on and learn what's the best design for you.","label":0,"model":"human","source":"reddit","id":2974}
{"text":"Baby Boomers have put serious strain on the economy for a couple reasons. Screwed up is subjective, but a majority of company CEOs are of the age of Baby Boomers.    \n  \nIndirect causes by Baby Boomers:\n  \n- Baby Boomers were the largest generation of children in the U.S.. Now they are all retiring and leaving the work force. That not only lowers the amount of taxes coming in, but increases the amount of money being spent by the government and businesses to cover their expenses like social security and pensions.  \n  \n- Baby Boomers are getting sick and dying thus putting the most strain on healthcare than there ever been. Prescription drug plans, surgeries, emergency services are all under the most strain that they have ever been because of the large amount of ailing Baby Boomers. Lack of services means higher costs. Higher costs strains the economy. \n  \n- The government only reimburses a small fixed percentage of costs for healthcare to hospitals. Insurance companies negotiate fixed rates for services from hospitals. People who don't use the government or insurance to pay their bills can negotiate huge discounts on their bills. So hospitals charge massive amounts of money for services in order to get the most money they can from the governments fixed percentage reimbursement. This has compounded funding issues with government programs because of the higher prices and loads of Baby Boomers using it.  \n  \nMore direct causes by Baby Boomers:  \n  \n- Baby Boomers tend to lean conservative when it comes to economic policy. Supply Side Economics (Also known as Trickle Down Economics) has been one of the biggest economic pushes coming from the Baby Boomer generation. They tend to believe that if you remove tax burdens from the supply side of our economy, and allow the free market to be completely unchained, then the jobs and wealth will filter down to the middle class and lower class. The draw backs of Supply Side Economics is that it consolidates wealth at the top few %'s, makes it harder to move from lower class to middle class, makes the middle class have less purchasing power, and doesn't account for our immense greed.   \n  \n- Baby Boomers control our government policies\/regulations, and Baby Boomers control the big financial companies. Both laid the groundwork that led directly to the economic collapse in 2008.   \n  \nTLDR; Their generation is the equivalent of passing a basketball through a water hose. They strain our social programs and healthcare systems, are increasingly paying less into our tax burdens, and set up the government and economy so that wealth is accumulated at the top 1%, thus severely hindering economic mobility.","label":0,"model":"human","source":"reddit","id":2975}
{"text":"B.S in Biology but I'll try my best anyways. \n\nAssuming you've read everyone's explanation of how genes play a role, I'd like to explain why these genes are present in certain individuals and answer your question as to why certain individuals burn while others don't. \n\nEvolutionarily, it all comes down to how certain shades helped individuals based on their location.  It's beneficial for someone who is closer to the equator to be darker and absorb less sunlight since it's almost always readily available. In comparison to places further away from that center point where sunlight isn't as abundant throughout the day. A more pale complexion would allow you to maximize the amount of sun you take in. \n\nSo if your genes originated from a place where absorbing the maximum amount of sun is best and you're out in the sun for extended periods of time, you'll eventually end up burning simply because that's what the sun does when you have little protection. While others who's genes may have originated from a place with varying degrees of the sun, will be able to tan. That may be why we lose tans in the winter. Evolutionarily, tanning would be to increase your protection against the sun to not burn. The best catalyst for this reaction would of course be prolonged exposure to the sun which would be signaling extra protection needed. You would then lose this tan in the winter, or when you stop being in the sun as much, since you would need to absorb more. \n\nExtra information: Since Humans as a species have been fairly good at adapting as needed, this may explain why we have lighter people in places where it'd make sense for everyone to be darker toned. Things such as umbrellas, sun block and houses would allow those with little natural protection to the sun to easily live in places where it is over abundant.","label":0,"model":"human","source":"reddit","id":2976}
{"text":"People here also forgot to mention that the outside wall is where a majority of your temperature change in the room comes from, especially if it has a window.  That means that there is a temp difference from what it is outside vs inside is greatest near your window and gradually changes to meet your thermostat temp as you move inward (the rooms temperature gradient).  By dropping your conditioned air (either warm or cold) near the window, you are putting the biggest effect from that air where it is needed the most.  Then it mixes with the room air, and the air that is making it back to your thermostat is a mix of what is in the room.  The further you move the vent away from the outside wall that is causing the temp difference, the more persistent that difference will be because you are mixing conditioned air with air in the room that is not at one end or the other of your temperature gradient.  When your ac unit kicks off because the thermostat says the room is at the correct temp, you will still have a spot near the window that is either too hot or too cold.","label":0,"model":"human","source":"reddit","id":2977}
{"text":"All of these answers are good,  but they miss the mark a little. First off,  when you see carbon fiber,  Kevlar,  and fiberglass (others include boron fiber and other polymers) ,  you are seeing basically a bundle of fibers similar to a rope or yarn. Now imagine taking that yarn and pulling on it (tension),  it's pretty strong (how hard you have to pull to break it) and stiff (how much it deforms under that force). Now,  try pulling on it from the side, you instantly will pull the fibers from the bundle in that direction. If you push on it,  the yarn or rope folds, providing no resistance. These are your fibers. These can be made from extruded graphite (carbon), extruded aramids (kevlar),  or extruded glass and they will all give you different properties such as stiffness,  strength,  etc. \n\nNow,  to solve the problem of pulling or pushing on the fiber that we saw above,  what you can do is set these fibers in a matrix,  which is basically a glue that holds the fibers in place. Imagine taking your yarn,  flattening it,  and setting it in Elmer's glue. If you did this,  it would now have actual stiffness and strength in the two directions that previously provided none. This glue,  is basically the matrix of a composite. This matrix can be a lot of things,  such as thermosets (can not be separated from the fiber with heat)  such as epoxy (most commonly used), phenolic,  bmi, etc and thermoplastics (can be separated from the fiber with heat) Each of these matrices will have different properties themselves,  but I won't go into them here. \n\nSo,  when you mix a fiber and matrix,  you get a composite. The matrix and fiber both provide strength and stiffness based on their ratios,  but in general,  what fiber you use dictates most of the properties in the primary tensile direction and the matrix dictates the the other properties. This where it gets complicated. In general,  your fiber will run in one direction (or two in the case of a bidirectional weave,  but we will only consider unidirectional here).  this will be the primary direction. This direction generally has properties on the order of 5 times better than the other directions,  we call this being anisotropic or more specifically orthotropic. Here's the beauty of composites- you can stack layers of this material to get properties in the direction you want. Therefore,  you can customize the strength and stiffness based on the angle that you stack the layers (plies) and how many layers you have. This is why composites are so \"strong\",  but what is actually being referred to is it's strength and stiffness to weight ratio (specific strength \/ stiffness). This means for less weight, you can have a stronger and stiffer object than if you made it out of a metal. This why They are special.\n\n There's a lot more to get into with composites,  including applications and processing requirements,  etc but it gets complicated fast. In general,  fiberglass is relatively heavy but cheap and provides good impact resistance so you'll see it used in large quantities for boat hull,  as protective layers on other composite,  and for generally cheaper applications. Carbon fiber is very strong, stiff,  lightweight,  but is very expensive and bad with puncture loads. It will be generally used where properties and weight matter such as in airplanes,  bikes, high performance cars,  etc though for a hefty cost. Kevlar has midrange properties,  but it's claim to fame is its energy absorption properties,  specifically in ballistic puncture applications like bulletproof vests. There are other composites all around us -  specifically steel reinforced concrete (steel rebarb fiber with concrete matrix), Adobe bricks (straw fiber and clay matrix),  and even wood (organic fiber with an organic matrix). \n\nEDIT: Source: Degree in Mechanical Engineering focusing in Composites. Work in the Aerospace Composites manufacturing industry focusing on automated processes (filament winding and advanced fiber placement)\n\nEDIT 2: Mixed up recyclable properties of thermoplastics\/thermosets. Thermoplastics are able to be broken down into individual components with heat, not thermosets as I originall stated. Thanks \/u\/Maxwedgell","label":0,"model":"human","source":"reddit","id":2978}
{"text":"They also seek out wind, ie jet streams etc. Picking up the jet stream can reduce actual flying time and increase fuel savings. A line between two points isn't exactly the shortest route. \n\nImagine you had to cross a field to get to a certain point on the other side, sure, the shortest route might be straight across the field, and the longest might be walking around the edge. Now for the 'what ifs'. \n\nWhat if I told you that the field had a hill, bushes, scrub and a drain in the middle, but around the outside it was a concrete footpath, which might be quicker then. It is exactly the same for aircraft. the straight line might look quicker, but there might be hills\/mountains in the way that adversely affect weather conditions. There could be a military no fly zone in between. \n\nWhen you are flying into and out of Melbourne, AU northward. you tend to come through a cutting, all air traffic is filtered through this cutting, otherwise any traffic heading towards Sydney has to deal with issues over the Great Dividing Range which impact weather systems etc.","label":0,"model":"human","source":"reddit","id":2979}
{"text":"What is meant by this maxim is that, contrary to what you might believe, the taste buds on your tongue don't do much actual tasting. They can detect if what you're eating is sweet, salty, bitter, sour or \"umami\" (a word which describes the savoury \"meaty\" taste of a good stock, the taste that's enhanced by monosodium glutamate). Your taste buds can't do much else.\n\nEverything else associated with taste is in fact detected by the nose, using the same receptors as those which detect smell. This is why if you have a really heavy cold everything tastes bland: the mucus in the nose prevents the smell receptors from working. The tongue still works, though, so you can still tell if something (like the pills you're taking to try to stop your head hurting) tastes bitter.\n\nWhen you walk into a smelly environment like a badly-cleaned public restroom, you are of course detecting the various complex molecules that are floating around in the air. You breathe in, the nose detects these molecules and sends signals to the brain: that's \"smell\".\n\nWhen you eat, the food in your mouth releases various molecules that find their way into your nose where they trigger those same receptors.\n\nSo it's not so much that you're tasting the molecules that you're inhaling; more that you're smelling your food as you eat it.","label":0,"model":"human","source":"reddit","id":2980}
{"text":"This isn't completely understood but we know some things about how it works.  We have 100 billion neurons that any one can connect to multiple others.  Any sensory input we have can make many connections.  But any single neural connection doesn't amount to anything mentally tangible and can be undone (short term memory).  Repetition makes multiple connections that basically maps out some concept in your brain.  Then, as you repeatedly traverse this mapped concept, or map whole other concepts together, the brain also processes these signals to form shortcuts for those \"mapped concepts\" with even more neural connections, so that accessing that stored information is faster.  The brain is continuously reconfiguring these connections based on the input it receives (what you are doing) so whatever is repeated most could be retrieved the fastest.  Trying to remember something is basically your brain looking for those shortcuts and if you were providing other sensory input, the reconfiguring to optimize whatever you were attentive would have used some of those neurons. Things you remember very easily, the connections were repeated enough to make so many connections that they'd never be completely undone again in your lifetime.","label":0,"model":"human","source":"reddit","id":2981}
{"text":"A big reason is that we have lost the discerning eye of the asesthetics of Roman art and instead view it as a classic archetype which extends far beyond just a style. \n\n\nLet me put it this way: Roman architecture at its golden age was considered by artists and historians of the day as a poor copy of the Greek and Kemet style. The Greek orders of Ionic, Doric, and Corinthian were all seperate styles, all existing at around the same era, and all meant many different things as relgious and philosophical indicators. In Egypt you had high and low as well as a smattering of archtectual styles. Each of these forms were never mixed and different regions of Greece and Egypt ascribed to differnt styles. \n\n\nThe Roman style, much like the way they conquered, simply absorbed other peoples art and orders and then combined them. The Coliseum uses all three Greek orders in a single building. This was considered gaudy and ignorant by Greek art critics at the time but of course is quite impressive now. Same was true of the Pantheon which uses high and low Kemet orders in the plan and facade and many experts at the time considered this a huge mistake. \n\n\nBut Roman art and architecture turned out to be perfect for those who came after. The heirs of Roman history in Europe did not know the original Greek and Egyptian orders or styles. The originals were destroyed or lost so instead Medival kings saw ornate hodgepodges of Rome and began to associate that with the classical style. As a king of some barren stretch of land or a future emperor of all Germans, you wanted people to see in you the legacy of Rome and it's golden age might. The symbols of the ornate column and the high arch became associated with majesty and power as the original philosophical underpinnings of the Greek and Egyptian orders were stripped away. The Roman style became the classic style which became the international neo-classical archetype that carried the feelings and emotions of Rome and all the positive things Rome came to represent into the modern age. \n\n\nNow a reasonable art historian today can point out the flaws in the Roman style. Mainly that it does not mean much more then the modern idealization of Rome. However the Roman Style as a form of  neo-classical architecture is still very powerful and nearly ubiquitous around the globe. \n\nThe Neo-classic style survives so readily because it is not an artistic style, it does not have aesthetic value, rather it is timeless because it has only emotional power, it dominates the viewer in monumentalism, it frames the society that builds it in legacy. The Neo-classical style is a political style not an artistic one, as the artistic framework it once belonged to has been torn away like ripping a child from the womb and now we raise that child to ascribe to our modern views and to legitimize our modern politics and nations.","label":0,"model":"human","source":"reddit","id":2982}
{"text":"The problem with these kinds of questions is that the top answers usually are what most people think sounds like the right answer. We know about the autonomic nervous system (ANS) and fight or flight, so if we see an answer with these terms, it sounds right and is upvoted. The actual answer is probably quite different.\n\nI don't think that the top posters of this thread actually understand how the autonomic response work. The ANS is for acute stress only!  Cortisone and other long-term stress hormone is responsible for long-term stress response. When you are stressed, you don't feel the \"fight or flight\" high, you feel depressed because stress hormones (cortisol et al.)\n\nAnyway, I don't think we will get the right answer in this thread, because the ENS (enteric nervous system) is not very well understood. Hopefully an expert will swoop and and educate us on the effect of serotonin deficiency in the ENS as a response to stress hormones.\n\nEdit: For further details on the puppeteers of cortisol, please see CRH and ACTH. Its scope is beyond a 5 year olds. I love reading ELI5 because they educate me. Except when it's a medical thing, then I realize that if the top answer is ELI5, then the result could be a 5 year old's understanding.\n\nBTW, if fight or flight is the cause of all your ills then the logical solution is to take an adrenergic blocker like propranolol or clonidine. Unfortunately these don't work for chronic stress. They are great for performance anxiety though!","label":0,"model":"human","source":"reddit","id":2983}
{"text":"Short answer: they (usually) want to make money. Some of them have bigger fish to fry. Depends. Warning: The ELI5 explanation may not make sense at some points.\n\nQuick terminology guide: \n\n**malware** = all-encompassing term for viruses, adware, scareware, rootkits, anything that wants to inconvenience you. I use 'malware' and 'infection' interchangeably below.\n\n**virus** = any code that will replicate itself, whether by sending out infected emails, reaching out to networked computers to infect them, or transmission through any other vector. Thanks to the media, 'virus' and 'malware' are more or less interchangeable in society's eyes. They shouldn't be - not all malware is a virus, but all viruses are malware. \n\nThe rest is explained below.\n\n**ELI5**:\n\nYou've got four main types of malware. Scam infections, spyware, trojan horses, and rootkits.\n\nSay you're counting your money. You're doing so openly because you know the police (antivirus companies) will protect you if anyone tries to take your money.\n\nMost malware attempts to take your money. Some, like the scam infection, may dress up like a police officer and tell you that you're in a lot of danger, so you should give them your money. But they are not actually a police officer, they're just disguised as one! \n\nOthers are sneakier. Spyware will follow you to the bank without you seeing it and watch you give your money to the teller. Then it will disguise itself as you and ask the teller for your money.\n\nSome malware (trojans) doesn't even want your money. They want to use you for their own means. They'll put a saddle on you and make you butt heads with the bigger kids - and all the other little kids you know have saddles on them as well, and the viruses hope to use these numbers to cause damage to the big kids. *(edit: this is referring to botnets - I can't think of any real way to illustrate this to a five-year-old, sorry if it sounds stupid)*\n\nAnd then some malware, you can't even see! They're invisible. But this malware (rootkits) is usually backed by very powerful and evil men, and you won't even know they're there until they've slipped into your pants and given you a wedgie.\n\nFor anyone looking for something more comprehensive: \n\n**Scareware, adware and keyloggers\/spyware**\n\n\nContrary to what the media\/Hollywood would like you to believe, most of the malware that everyday computer users face is not the overnight work of some bespectacled nerd with a taste for chaos sitting in a dark room lit by dozens of CRT monitors. Rather, most malware nowadays is small-time, cheap exploit code that is aimed at doing one thing: making money. It does this by either:\n\n- getting you to outright pay them yourself. There are many infections that will act as fake antivirus programs (known as '[scareware](_URL_2_)') in order to get your credit card information; they establish themselves on a computer, start wreaking havoc, then bring up a window saying that the fake antivirus has caught some nonexistent issues. However, the scareware is always a free trial, and you have to buy the 'real' version for it to 'clean' your computer. Other infections will just lock you out completely until you enter credit card information.\n[Here is what your typical scareware looks like.](_URL_8_) Generally, these will have lots of spelling mistakes, horrible grammar, and one giant button that tells you you're in trouble and to buy the 'full version'. Also, note that it's finding viruses (these are fake entries) in all the places, accompanied with bogus or mismatched virus types. Most people can see through this, but the seniors or technologically-stupid of the world may not.\n\n- sitting behind the scenes and sending your information to others. This malware - generally referred to as 'spyware' - is often based around 'keyloggers', which will record your keyboard's keystrokes and upload them to a human controller; then, when the controller has information they deem useful (e.g. your online banking password), they can take your money.\n\n*Why?*\n\n\nAs mentioned above, these are almost purely profit-motivated infections. These types of malware rarely attempt to spread themselves to other users' computers. They want to raise as little suspicion as possible - you may not be as inclined to give out your credit card number to a message on your computer if your friends start calling you and telling you you're sending them spam emails. So, really, they are not viruses, but just infections - they are almost always contracted as a result of downloading something infected, be it an email attachment, bad file, or something from a shady torrent\/peer-to-peer site. \n\nThese infections can range from severe, like the two examples above, to mild, like most '[adware](_URL_6_)', which just spams your computer or changes certain links to lead to shady websites which try to sell you stuff. Adware, scareware and keyloggers are usually the easiest to get rid of, and comprise the brunt of infections that plague the world today. \n\n\n   **Trojans\/botnets**\n\n\nThat isn't to say worse things don't exist. Heavier infections, such as [trojan horses](_URL_4_), serve to compromise a user's control over a computer for various reasons, usually by making security holes (back doors) for malicious code to run through. Some trojan horses are deployed for the purposes of creating a [botnet](_URL_9_) - if many computers are infected with the same trojan, they become [zombie](_URL_0_) machines with which many things can be done. If you've ever read about 4chan's infamous DDOS attacks, for example, a botnet works in much the same way - large amounts of computers generate junk signals to overwhelm a target and bring it down through sheer brute force. \n\n   *Why?*\n\n\nThese types of infections are generally tooled towards causing chaos, and may be used to attack large websites or organizations by using the controlled computers to flood web servers en-masse in a [distributed denial of service attack](_URL_3_). They may also be used to farm bank information through a combination of trojan doorways and keyloggers. Botnets are rare, as they are nowhere near as easy to deploy as simple scareware, and operating a botnet is a high-profile digital offense, whether it's for DDOS purposes or harvesting information (see [here](_URL_1_) for an example of counter-botnet efforts).\n\n*Generally*, infections that exist to make their operators money are not run by skilled users. Those infections are mass-produced templates that are sold on the market to whoever wants to run them; they're shabbily-coded, often very easy to see through if you have the slightest clue about computers, and have a short lifespan (as antivirus programs will just update to defeat them after they're released). On the other hand, trojans, especially those used for botnets, take heavy-duty coding, coordination, and are usually run by more notorious groups. (relevant note: botnet controllers are generally known as herders or botherders)\n\n\n   **Rootkits**\n\n\nThe ultimate viruses - and this is where we start approaching Hollywood territory - are [rootkits](_URL_7_). These viruses are very hard to combat for one reason - they are able to actively hide their presence from the rest of the computer. Without going into excessive detail about the layers of an operating system, think of it like this: your computer is composed of two major parts, the hardware (physical, tangible box containing all the circuitry and whiz-bang that makes a computer run) and the software. These two parts act as a sandwich for a multitude of smaller layers that gradually fill the gap between reality and the digital world of an operating system, all for the purpose of taking a user's actions and translating them down to machine level so that the computer can do something with them. Rootkits can run beneath the top, or application, layer of the operating system, effectively cloaking themselves or making themselves impossible to remove without advanced techniques. \n\n\n   *Why?*\n\n\nRootkits are some of the most malicious code out there, and are developed by the best hackers in the industry. They are extremely rare, and most users will not run into one unless they're really unlucky. Due to the skill involved in making a rootkit successfully, few hackers know how to do so, and, if they manage to make a competent rootkit, antivirus companies will immediately start releasing protective updates to prevent them from taking hold on machines. \n\nRecently, we saw rootkits being used on an international scale for electronic warfare - see [Stuxnet](_URL_5_). Rootkits can be very, *very* complicated - Stuxnet was actually able to physically manipulate machinery. \n\nAnd then, lastly, some people just write viruses for fun, but this is a very small percentage.\n\n   **Addendum**\n\n\nIt should be noted that malware types are not mutually exclusive. Scareware can incorporate a trojan, a rootkit can incorporate scareware, etc. - generally, they stay exclusive because it's easier to do things that way (you want a rootkit to be as inconspicuous as possible, for example), but there's no hard and fast guide or 'Viruses 101' that says only one type of infection can be deployed at once, or that certain types can't contain elements of other malware. \n\n\n\n*EDIT #1*: added a bunch of Wikipedia links for further reading, expanded a bit on some sections, separated sections into virus definition and 'why'? for clarity, added introductory definitions.","label":0,"model":"human","source":"reddit","id":2984}
{"text":"i am a lawyer and will take a stab at this (although i am not a criminal lawyer or even a litigator). everyone let me know if i get something wrong.\n\na written document (such as a diary) written voluntarily is not \"testimony\", it's \"evidence\". testimony is when you talk in court or to the authorities, and are cross-examined. if you are forced to do this, they are forcing you to testify against yourself.\n\n a diary is evidence just as much as a gun they found in your house. the right not to be forced to testify against yourself is more usefully understood as a \"right to silence\". otherwise, would the court not be able to consider your bank records or phone records or a tape recording of you talking to an undercover agent?\n\nthe admission of a diary DOES potentially violate the rule against \"hearsay\" which states that out of court statements (whether oral or written) are not admissible for the truth of their contents.\n\ngenerally speaking, statements made by the defendant ARE admissible for the truth of their contents. they fall into an exception to the hearsay rule. if the statements are not true, the defendant can waive his right to silence and tell the truth.\n\n_URL_0_","label":0,"model":"human","source":"reddit","id":2985}
{"text":"It's due to levels of neurotransmitters in your brain changing due to large variety of physiological, environmental, and emotional factors. There are likely a lot of neurotransmitters in play that have a huge impact on motivation, but currently the most well-known is one called \"Dopamine\". \n\nPeople who have disorders involving dopamine regulation dysfunction (such as Parkinson's, some forms of Depression, MS, etc) have severe disruption in motivation. The lower the dopamine production\/uptake, the lower the motivation. They lack what would be considered a normal motivation drive, and typically see improvements in this motivation when undergoing pharmaceutical therapy to increase levels of dopamine.\n\nSo what can you do to influence dopamine? Well, the field is still very young and is full of unknowns, but it seems that one way to increase dopamine in healthy individuals is to have them partake in activities they **enjoy**. Exercise also has an effect on dopamine levels and as a result may increase levels of motivation. \n\nSo if you're feeling unmotivated, perhaps it'd be wise to spend some time among friends or go for a quick jog\/walk\/bike ride before returning to your work. It could have a very positive impact.","label":0,"model":"human","source":"reddit","id":2986}
{"text":"It's counter intuitive, because you'd think everything would be easier that way, but the current way is actually easier on instrumentalists.\n\nTake the trumpet for example.  The most common trumpet is pitched in Bb.  It has a nice, open, brassy sound.  But the C trumpet is a bit brighter, and is often used in orchestral music.  So a well-rounded trumpet player would have to learn to play the instrument twice.\n\nWe avoid that by writing the music differently.  So a trumpet player sees the note F, on the bottom space of the staff.  If he's playing a C trumpet (concert pitch), he holds down the first valve and plays an F.  If he's playing a Bb trumpet, he holds down the first valve and plays a concert Eb, which he calls an F.  Either way, he uses the same fingering and similar embouchre.  Same with middle C - one line below the staff.  On both trumpets, you'd see the note and play an open fingering - on a C, it's a C, on a Bb, it's a Bb.\n\nSo because of that, the trumpet player only has to learn the instrument once.  Even if he's never seen a C trumpet before, he can pick one up, read the music (which is in C - concert pitch), and play it perfectly... it'll just sound a whole step higher than if he'd played his Bb trumpet.","label":0,"model":"human","source":"reddit","id":2987}
{"text":"*Not* only is there still the strong anti-Castro pro-Republican contingent lobbying hard to keep the embargo AND no motivation to lift it, but it is a matter of continued doctrine, principle, and preparation for the demise (of old age, probably) of the Castro brothers.\n\nIn Cuba, a single strong dictator confronted us directly both ideologically and (granted, re-actively) militarily by allowing launchers so close to US soil. We lost face in the Bay Of Pigs incident (long forgotten except in Miami) and then cut a deal with the Soviets (which continued with the Russians, btw) that we would not invade militarily. So basically the U.S. government took the position that we will win this confrontation the easy way; through attrition and time. This is actually a good tactic because the U.S. believes that the only thing that holds the Cuban nation together at this point is the Castro family.   Once Raul is gone, the system will *likely* fall apart within a couple of years.  A large (and rich) ex-pat community will then pour much needed capital and political leadership into the island nation.\n\nFidel, though many have wished him dead of old age for years now, is still hammering away ideologically from the sidelines, but it is Raul that is the military strongman now. Behind him is...nothing. Well, at least nothing with their force of personality or the Castro family name-brand attached to it.  In short, within a few years of Raul's death, no matter who takes over, the island will revert to where it was before Communism took over and we will not have needed to fire a shot.\n\nThe embargo keeps their economy weak and prepares the way for the wave of Cuban-heritage politicians and business people from the U.S. that will provide the next government and economic base once the Castro brothers die off.  These 1st  &  2nd generation Cubans have *vast* wealth. Others have political power  &  experience in local Florida as well as national-level politics. Let us not forget that we have THREE U.S. Senators that are of Cuban heritage. Senators. [One of those, Marco Rubio](_URL_0_), will likely be a Republican nominee for PRESIDENT OF THE UNITED STATES as well. That is some serious pro-embargo political clout at the highest levels of our government.\n\nSome ex-pat Cubans have something even better... money AND political power.  Combined with the very generous hand-outs and recommenced trade from a newly friendly U.S. government, these people will probably turn Cuba from one of the poorest nations in the Caribbean to one of the wealthiest in less than a decade.\n\nTLDR: The embargo keeps them weak  &  paves the wave for 2nd generation Cubans in the US to transform it almost over-night into a rich  &  (more importantly) US-friendly ally once the Castro brothers die.\n\nEdit: I seem to have drawn the ire of Mtl_dood.  I am not trying to justify the embargo or predict the future; I was simply answering the question by explaining the US rational for keeping it in place.","label":0,"model":"human","source":"reddit","id":2988}
{"text":"From _URL_0_\n\nIn 1896, Halsey W. Taylor lost his father to an outbreak of typhoid fever caused by a contaminated water supply. This personal tragedy led the young Halsey Taylor to dedicate his life to providing a safe, sanitary drink of water in public places. \u2026 The historic Double Bubbler projector [spouter] was designed by Halsey Taylor himself, and still ranks as the most important innovation in the industry\u2019s history. It projects two separate streams of water, which converge to provide an abundant `pyramid\u2019 of water at the apex of the stream. This gives the user a fuller, more satisfying drink.\u201d\n\nThe folks at Halsey Taylor are being polite here. What they mean is that the Double Bubbler enables you to take in more water and less air when you drink. As a result, you don\u2019t burp. Think of all the delicate social negotiations you\u2019ve been involved in that have gone awry because of an ill-timed eructation (that\u2019s belch for you dropouts). Had you been drinking from a Double Bubbler, that fat contract (job, babe, whatever) might have been yours.\n\nThe Double Bubbler serves other purposes as well. You get less spraying, presumably because the water slows down when the two streams merge. The double streams also act as a sort of pressure regulator. If the water pressure is unusually strong one day, a single-stream fountain might give the unwary sipper a shot in the eye. When the twin streams of the Double Bubbler meet, however, their upward momentums tend to cancel out no matter how high the pressure gets.","label":0,"model":"human","source":"reddit","id":2989}
{"text":"An installer has a checklist of things to do. Written simply, it will simply report what percentage of the list is complete. Problem is, it doesn't know how long each item on the list will take until it's done.\n\nImagine it this way. You are a robot who has been given a shopping list.\n\n1. Bananas\n2. Sandpaper\n3. Beer\n4. 1,000 kg of sand\n5. The remote control for the TV\n\nYour master sends you out to grab all the things he needs for the weekend. It takes you 20 minutes to go to the supermarket and buy some bananas, then you text your master \"1 of 5 items bought, or 20% done.\" Then it takes you 20 minutes to go to the hardware store, buy some sandpaper and say \"2 of 5 items bought, or 40% done.\" The liquor store is on the other side of town, it takes you 40 minutes to get there to buy the beer, then you say \"3 of 5 items bought, 60% done.\"\n\nIt took you an average of 26 minutes to get each item so far, and there are two items left, so you naturally estimate that it'll take you 26\\*2 = 52 minutes to finish the work.\n\nOnly you can only carry 100kg of sand at a time, so it takes you 10 trips to get it all, or 5 hours total. You finally bring it all home. Now it's taken an average of 95 minutes an item, so you say it'll take 95 minutes to get the remote. But the remote is just under the couch, it takes you 10 seconds to get it.\n\nThis is how progress bars work. Computers don't know how long work will take until they've actually done it. The items on an installer's to-do list can vary in type and complexity, and it's just telling you how *many* items have been done, not how long each item takes.\n\nThe very last item on the list might be \"Double check that every file we've copied is there and the right size, no incomplete files.\" If you installed a big game this might take a minute or two as it checks tons of big files. If it's the last item on a 100-item list, it'll say 99% done while this is happening. Just how things work.","label":0,"model":"human","source":"reddit","id":2990}
{"text":"When  the onion turns translucent, the cell walls are breaking down. But more  important to the difference in flavor between raw and cooked onions are  sulfur compounds floating in the cell fluid and sulfur-reacting enzymes  stored in vacuoles (basically closed storage compartments) inside the  cells.\n\nWhen you cut or chew on a bit of  raw onion, these vacuoles are ruptured, and the enzymes inside react  with the sulfur in the cell fluid, creating strong, irritating compounds  (intended, of course, to discourage animals from eating the plant). In  particular, onions, shallots, and some related plants, when sliced,  produce a compound called 'lacrimator', which is both light and  volatile. It enters the air and first acts on the nerve endings in your  eyes, causing some direct pain, and then breaks down into tiny amounts  of sulfuric acid, both of which cause you to tear up in defense.\n\nThe  process of cooking onions denatures these enzymes, stopping the process  of converting the intracellular sulfur compounds into these defensive  compounds, which removes the harsh flavors, leaving just the sweet, sort  of meaty flavor that we all know and love.\n\nSweet  or Vidalia onions, which are grown in particularly low-sulfur soil,  don't have many of the sulfur precursors in their cells, which is why  they're so much less harsh when used raw.\n\nYou can read all about the process in Harold McGee's On Food and Cooking.\n\n & #x200B;\n\nSource: [_URL_0_](_URL_1_)","label":0,"model":"human","source":"reddit","id":2991}
{"text":"One thing I would note (12 years F-class competitive shooting here) is just how many things go into a shot in order to hit a target at distance. Most urban tactical shooting is considered sub 300 yards due to common line-of-sight scenarios and the historical precedent (or lack therof) for long range assassination attempts. So locking this (0-400y range) down covers a lot of your steps, for beyond that range see below: *Assuming 400-1000y for common cartridges (5.56, 7.62, 30-06), 400-1500y for specialized (your AI's, 6.5 SAUM, 7 RUM ect) and 400-2000y for your big boys (338LM, 50 BMG, Cheytac's)\n\n- Hitting a moving target at a range much beyond 700y is basically out of the question if you plan on taking one shot. \n- If the target is stationary for a long time, likely behind a bullet proof podium. Energy at range is severely lessened and bullet proof material would be a problem.\n- Despite what you see in the movies, no one targets the head unless it's a hostage situation. Having to aim at something other than center-mass is very difficult. it's smaller and moves more than center mass.\n- You have to factor among other things: bullet ballistic coefficient multiplier, bullet velocity, scope height, barometric pressure, temperature, altitude and then past 1000y things like spin drift, Coriolis effect, direction you're facing in relation to the earth's spin. All to hit your target on the first shot. But most of this can be done long before or just before you would arrive.\n- The major variable which you can't prepare for: windage. Windage correction is the hardest job a shooter has. In an urban long range scenario this would be a major bitch. I correct for 2 windage readings (muzzle, at target) for most of my shooting, others can do 3. With buildings directing wind and nothing on the ground to read it (i.e grass, dust) it would be a pain in the ass.\n\n- To give you an idea, my competition .308 with a long barrel for velocity would drift from target 18.2\" @ 1,000 yards with 2mph of wind 90 degrees to target. My flattest shooting rifle (6.5x55AI) sending a 140gr VLD as fast as humanly possible would still drift 10.7\" on a normal day at sea level. That's for only 2mph of wind you have no idea how to read.\n\nEdit: Coriolis not Cornelius, sorry Don.","label":0,"model":"human","source":"reddit","id":2992}
{"text":"Imagine walking up to a pond and seeing massive ripples expanding in circles from a point. There\u2019s no wind, and you don\u2019t see anything else unusual, so you would infer that something (like a rock) had just fallen into the pond at the center of the circles.\n\nAstronomy is like this. We really can\u2019t see anything as it happens under normal circumstances, since things happen on enormous time scales (it took longer for Andromeda to eat a galaxy than our species has existed; probably over 100x longer). What we can do though is look at the ripples in the pond and make very careful measurements to figure out why there are ripples and the pond isn\u2019t smooth.\n\nIn the case of M32, we can measure its composition (both structurally in terms of masses and types of stars and shape) and chemically (by looking at the type of light it emits\/absorbs) and its size and position. We know it\u2019s not a normal galaxy just like our rippling pond isn\u2019t a normal pond. Things like M32 don\u2019t just form on their own. So we infer that something else had a hand in generating this weird situation. A lot of theories, a lot of careful simulation, and more extremely careful observation later, and we have come up with a pretty decent idea of what happened.\n\nWe could still be wrong. Just like it might not have been a rock thrown into the pond. But it\u2019s a really good guess. And hopefully, if we\u2019re wrong, we\u2019ll measure something in the future which proves it and forces us to come up with new theories and simulations and measurements. Scientists love when that happens.","label":0,"model":"human","source":"reddit","id":2993}
{"text":"There's a couple of different reasons but I'll try to separate them.\n\nChemically, there's 3 different 'types' of tears; Basal, Reflex, and Emotional. The other two tears are a result of irritants or just general lubricant for the eye in the form of mucus, but emotional tears are made up of stress hormones such as cortisol (which will relieve tension in your body which is argued for why you feel better after crying) and leucine enkephalin, a natural painkiller and endorphin (stuff you get while doing exercise) which also uses up cortisol in the body and makes you feel better.\n\nPsychologically, everyone cries because it's a form of non-verbal communication that you are feeling sad or might be injured, or anything that means you might need help, despite you not being consciously aware of it. You can't make yourself cry unless you think of something emotional. \n\nYou cry when you're emotional because your body treats an emotional stressor as if you have been physically injured. You might be in relatively low physical danger when told that a family member has passed away, but the overwhelming stress and emotion causes such a build up of stress hormones from the sympathetic nervous system (increases heart rate, respiration, supresses digestion, etc) that you will most likely cry or feel overwhelmed.\n\nWhen something makes you extremely happy, you remove a lot of distress from your life (if not momentarily). Looking at the GAS (Hans Selye's General Adaption Syndrome) model for stress, most people are subconsciously in the 'resistance' stage; meaning they have increased their resistance to stressors to get through their lives. When something extremely great happens to you, say a partner proposes or you win the lottery, that stress resistance level is no longer needed, so excess stress hormones that were activated to keep you resistant are released in Emotional tears.\n\nSource: A lot of shit I learned doing Psychology and Biology research in Australia.","label":0,"model":"human","source":"reddit","id":2994}
{"text":"Excellent question. A bunch of people want to communicate at the same time. Since other people make a lot of noise, you have to speak louder. Everyone else speaks louder too and eventually you are shouting as hard as you can and nobody can hear you since everyone is shouting. To make this work, you need a system.\n\nFirst comes FDMA (Frequency division multiple access) which is basically giving everyone their own frequency. This is analogous to putting everyone in different rooms so they do not have to shout due to lack of interference. We quickly ran out of rooms to use once we got more users so we abandoned the idea.\n\nThen comes TDMA (Time division multiple access) which is basically giving everyone a short time slot to transmit their message. This is analogous to giving people turns to speak so they won't interfere with each other. This is how GSM network works.\n\nCDMA (Code division multiple access) is when you encode your data into the white noise and then run the decoding algorithm to get it back. This is analogous to everyone speaking at the same time but in different languages. This helps to separate different people speaking, but when there are a lot of users, the speech still becomes impossible to differentiate from the noise. This is how 3G works.\n\nOFDMA (orthogonal frequency division multiple access) is what LTE uses and is quite more complicated but is basically separating users by time AND frequency thus increasing capacity by a lot.","label":0,"model":"human","source":"reddit","id":2995}
{"text":"Chess is slightly biased toward white, given even skill levels. All other factors are identical, it's just that white gets to move first and thereby set the tone for the match. \n\nDue to this inequality, white players tend to play for a win (+1), and black players tend to play for a draw (+0.5). Black often plays defensively, while white tries to break that defense (at the very highest levels of play). Right now, that's the Ruy Lopez variation. This means that the first 11 or so moves are pre-planned and already locked in. If a game isn't won within 30 moves or so, it'll likely be a draw (at the highest levels), which means white has ~20 moves to win. \n\nBlack's job is easier; all he has to do is trade pieces until the board is relatively cleared, making white's job much harder. White has to reposition and restrategize and constantly look for very slim edges. \n\nWhat this ends up meaning is that black is typically successful, because he has a much easier task. It's very difficult to beat a person who is around your own skill level, is playing defensively, and actively aiming to sabotage the game in order to achieve a draw. He isn't trying to win, he's just trying to not lose.","label":0,"model":"human","source":"reddit","id":2996}
{"text":"The main factor in speed zoning is supposed to be the 85th percentile speed.  Unless there are other compelling factors this is the most important component of setting speed limits.  This is based on a large sample of actual speeds traveled on the road in question during favorable conditions.  The speed limit should be at the 85th percentile of this data set of actual speeds, and it should also be an iterative process.  This is based on the common sense notion that almost all drivers have the judgement to chose a safe and comfortable speed to drive, regardless of the posted speed limit.  In reality, this is exactly what happens, speed limits, no matter how high or low they are set have a pretty small impact on the actual speeds people drive.  Government studies have proven this, and they have proven that safety and efficiency are optimized when speed limits are set using this method.  Here is a link the Massachusetts Speed Zoning manual that describes this process in more detail.  _URL_0_\n\nAs you might suspect, it sure seems like a lot of speed limits don't really conform to this standard, and you'd be exactly right!  Speed enforcement is a massive money maker for the state, municipalities, and insurance companies.  If the speed limits are set at the high end of the speed range that people chose to drive, speed enforcement would be pointless!  (Typically whatever the 85th percentile ends up being is rounded up to the next 5 mph, and Federal guidelines recommend a minimum tolerance level of 5 mph for speed enforcement, the net effect being a virtual 100% compliance rate and very small fines when issued).  Misinformed or outright untruthful political pandering about making roads safer, and persistent misinformation of tired old lines like \"speed kills\", as well as the fact that odds are generally in your favor for avoiding a ticket, keeps the populace complacent enough that they get to keep bilking us out of our hard earned money.  It's not right, and artificially low limits make roads less safe by promoting less than optimal traffic flow.  So there's the way it should be, and the way it is, and they aren't the same thing!","label":0,"model":"human","source":"reddit","id":2997}
{"text":"I teach a class in sleep physiology and dreaming. The theory I like best is called the \"threat simulation\" theory of dreaming, which proposes that a major function of dreams is to rehearse potential responses to likely threats that we might encounter in daily life, both minor and major. Some points:\n\n- the majority of dreams are negative in emotional tone. This is evaluated by waking research subjects up whenever the EEG indicates they're dreaming, and asking them what they were dreaming about. More often than not it was a stressful or unhappy dream. Nightmares are just an extreme case. A regular dream isn't usually that scary, but there's often something negative: anxiety, job worries, classroom worries, social fears. Very often dream plots involve common problems (you miss your bus, you're late for work) or social problems (you're unexpectedly naked when nobody else is) or even sometimes a grand challenge (you have to go cross-country for some reason).  The idea is that your brain is thinking through scenarios: \"What could I do if X happened?\" \"What could I do if Y happened?\"\n\n- also, many dreams show an interesting tendency to gravitate toward 3 types of threats that were common in our evolutionary past. Chronic nightmares in particular show a really interesting tendency to change over time to incorporate plot elements that include: natural disasters \/ extreme weather, groups of strangers chasing you, or large predators. For example someone in a car crash might have a series of nightmares about the car crash, but the dreams start to change to incorporate a natural threat like a storm or flood or some strange people running around. The theory is that those 3 types of threats are the ones we faced most often in the past and that we still instinctively try to \"rehearse\" what we could do if those things happened. \n\n- also a general point; Dreaming involves elaborate neural circuitry designed to fool the conscious brain, with very convincing hallucinations, into believing that you are somewhere else. The neural circuitry involved in this is so precise and the hallucinations so detailed that it seems likely it evolved for some reason other than just  \"incorporating  the previous day's memories\" which is the rather vague reason usually given for dreams. That theory (incorporating memories) doesn't!5 really fly for me because it now appears memory consolidation occurs in other stages of sleep, not during dreams. The famous \"Tetris effect,\" for example, where you see fragments of the day's events just as you're dropping off (like, if you've been playing Tetris all day, you'll see Tetris blocks just as you're falling asleep), occurs in stage 1 sleep, not during dreams. Most dreams actually do NOT feature events of the day that just happened (this is known from those dream-topic experiments mentioned before).\n\nThe idea is that by rehearsing \"threats\" in your sleep, you are able to rehearse \"solutions\" that will enable you to react more rapidly in real life if a real threat occurs. PS - we often don't remember dreams, but just rehearsing the solutions would help anyway. It's already known (from other research on the awake brain) that you need not remember a solution consciously for it to help you react more quickly when you need to.","label":0,"model":"human","source":"reddit","id":2998}
{"text":"We had a speaker come in to our school once and he explained it like this:\n\nThe first thing you have to understand is that the Roma are different. They have a different culture, they have a different way of thinking, they have a different idea of how life should be lived. Part of the problem is that it clashes with other peoples ideas on how to live.\n\nPartially, its a problem of perception. The Roma see mainstream society as something separate from themselves that they want little part of while society can't understand why the Roma desire separatism. \n\nMost Roma live in Europe and mainstream European society is very different to how they live. Mainstream European society says you have to go to school, go to work, pay taxes, buy a home, respect the law but most importantly, with respect to the Roma at least, respect some fundamental authorities.\n\nThe problem is, the authorities are generally perceived as the enemy by the Roma, because they try to force the Roma to integrate, which many don't want to do.\n\nSo the encounters they have with the authorities and mainstream society goes like this:\n\n\"You can't live here. This is our land. you're not allowed.\"\n\"Where are you kids? Why aren't they in school? We don't care about you ways. This is our land and while you're on our land, you have to send you're kids to our schools to learn our ways!\"\n\nSo the Roma end up marginalised and end up perceiving mainstream society as an enemy wanting to destroy them and so it's OK to steal, its ok to disobey the rules and break the laws. Especially since some think it's the only way to survive.\n\nmeanwhile, everyone else is saying, why the fuck would you want to live your way? our way is better! we're richer and more powerful, look at all the things we have that you don't. and all you have to do to get them is be like us. Your way is stupid and outdated. Accept it and move on.\n\nApparently part of this is that huge amounts of Roma culture have effectively been destroyed by generations of persecution. So to most Europeans, they just look like us but poorer, clinging to stupid ways while to the Roma, they're the last generations desperately fighting to preserve themselves as a people.","label":0,"model":"human","source":"reddit","id":2999}
{"text":"We present the results of an optical spectroscopic survey of galaxies in the redshift range 0.3 <z<0.7, carried out with FORS2 on VLT-UT1 (Antu). The sample consists of about 1000 objects selected by their photometric properties to be early-type galaxies or QSOs. We have measured redshifts for 97% of them using multi-object spectroscopy. In addition we obtained high-resolution imaging data in B,V,R,I bands for all targets observed spectroscopically. This dataset is used to study scaling relations between galaxy luminosity, size, velocity dispersion and black hole mass as well as correlations among these quantities themselves. Our main findings are summarized below:  -The M BH \u2212 \u03c3 relation evolves very little over this redshift interval.  -There is no evidence that the slope of the M BH \u2212 L bulge relation changes significantly with redshift; however there may be some evolution in its zero point.  -The scatter around both the M BH \u2212\u03c3 and M BH \u2212L bulge relations increases towards higher redshifts.","label":1,"model":"bloomz","source":"arxiv","id":3000}
{"text":"We present new constraints on the merger rate density and mass ratio distribution for luminous red galaxies (LRGs) using data from the Sloan Digital Sky Survey Data Release 7 (SDSS DR7). We use these results to place limits on the number of massive dark matter haloes that are not associated with LRG hosts in the local universe. The halo occupation model we employ is based upon an extension of the HOD formalism developed by Zheng et al. (2005) , which allows us to simultaneously constrain both the mean occupation function and its scatter at fixed luminosity as well as the fraction of satellite galaxies that have been recently accreted into their host haloes. Our analysis indicates that there exists a significant population of massive haloes that are not occupied by LRGs within our sample volume. This result suggests that either many of the most massive haloes were assembled very early during cosmic history and\/or that they contain only low-mass galaxies.","label":1,"model":"bloomz","source":"arxiv","id":3001}
{"text":"We present the results of an X-ray study of supernova remnant (SNR) G299.2-2.9 using data obtained with Chandra and XMM-Newton observatories. The SNR is located in the constellation Puppis at a distance of ~5 kpc, which corresponds to its angular size of about 30 arcmin. We find that the spectrum of this object can be described by two thermal components with temperatures T1=7\u00d710^6 K and T2=2\u00d710^6 K. In addition, we detect non-thermal emission above 10 keV. Using these parameters, we estimate the age of the SNR as t=4000 yr. This value agrees well with the characteristic time for the expansion of the shell into the surrounding medium. Based on our analysis, we conclude that the observed morphology of the SNR is consistent with the model of a spherical explosion expanding into a uniform interstellar medium.","label":1,"model":"bloomz","source":"arxiv","id":3002}
{"text":"We consider the statistical properties of nonstationary random acoustic and electromagnetical waves in terms of their correlation functions, power spectra, and probability density functions (PDFs). We show that these quantities can be expressed by means of solutions to certain partial differential equations with time-dependent coefficients. The PDFs are obtained for both stationary and nonstationary cases using the method of characteristics. In particular, we derive an exact expression for the PDF of the amplitude fluctuations of a monochromatic plane wave propagating through a turbulent medium. This result is used to obtain expressions for the mean-square fluctuation levels of the electric field strength and intensity at any point along the propagation path. Finally, we discuss some applications of our results to radiowave scintillation theory and radar detection problems. PACS: 42.65.Tg, 43.20 .Fx, 47.55.+q, 47.60.+j","label":1,"model":"bloomz","source":"arxiv","id":3003}
{"text":"We propose that the dark matter in our universe is composed of neutrinos with masses and mixings similar to those observed for ordinary neutrinos, but which are neutral under all Standard Model gauge groups except U(1)B\u2212L. We show how this scenario can be realized within an extension of the minimal supersymmetric standard model (MSSM), where we add three right-handed neutrinos NRi , i = 1, 2, 3, each charged under B \u2212 L . The lightest of these new particles may then serve as cold dark matter candidate. In addition, we introduce two Higgs doublets H u and Hu , one of them being responsible for breaking electroweak symmetry while the other breaks B \u2212 L .\nThe resulting spectrum contains four massive Majorana neutrinos, whose mass matrix has been shown to reproduce the pattern required by current data on neutrino oscillations. Furthermore, it allows us to explain why only three out of the six possible lepton flavors have so far been detected experimentally.","label":1,"model":"bloomz","source":"arxiv","id":3004}
{"text":"We report the discovery of massive outflows (1000 km s-1) in two post-starburst galaxies, which are thought to be progenitors of present-day elliptical galaxies. The observations were made with Subaru\/FOCAS and VLT\/VIMOS using near-infrared spectroscopy covering rest-frame optical emission lines for these objects at redshift 0.6. We found that both galaxies show strong [O III]5007\u00c5\/H\u03b2 ratios indicating high ionization parameters. In addition, we detected blueshifted high-velocity components in H\u03b1 profiles as well as broad wings on their H\u03b1 line profiles. These results suggest that there is an energetic ionized gas flow driven by AGN and\/or supernovae activity. This work was supported by Grants-in-Aid for Scientific Research from MEXT Japan (No. 20540230). Keywords: Emission-line galaxy, Galaxy evolution, Nearby galaxy","label":1,"model":"bloomz","source":"arxiv","id":3005}
{"text":"We consider the problem of learning an unknown vector $\\boldsymbol{\\theta}$ in $R^d$ by observing noisy linear measurements $y_i = \\theta^T x_i$, where $x_i$ is drawn iid from some distribution $\\mathcal{X}$ on $R^d$ and $(y_i)_{i=1}^n$ are observed with noise $\\epsilon_i$ that is independent of $x$.  We assume that each measurement has been quantized to one bit (i.e., it can only take values 0 or 1), which leads us to formulate this as a compressed sensing problem. In particular, we show how to recover $\\theta$ using OMP when the number of measurements $n$ scales linearly with $d$. Our results hold for any fixed choice of quantization thresholds, but they also apply if these thresholds are chosen adaptively based on previous observations. This work was supported by NSF grant CCF-1217225. The authors would like to thank Prof. David Donoho for his comments on earlier drafts of this manuscript.","label":1,"model":"bloomz","source":"arxiv","id":3006}
{"text":"We present new near-infrared (NIR) spectroscopy and photometry for the brightest star in the young open cluster M82F, which is located at a distance of 3.6 kpc to the south-east of the nearby galaxy NGC 3077. The NIR spectrum shows that this object has an effective temperature T eff = 13000 K and surface gravity log g = 4.0 dex. We find no evidence for any infrared excess emission indicative of circumstellar dust around the star. Using our spectroscopic results we derive a mass of about 20 M\u2299 for the central star. This value agrees well with previous estimates based on optical spectra. From the observed luminosity and assuming solar metallicity we estimate a radius R\u22c6 \u2248 5R\u2299 for the central object. Our analysis suggests that the age of the cluster is between 1 Myr and 10 Myr. In addition, we have performed a detailed comparison of the properties of M82F with those of other open clusters containing massive stars.","label":1,"model":"bloomz","source":"arxiv","id":3007}
{"text":"We present new constraints on the time evolution and duration of dark energy using measurements of baryon acoustic oscillations (BAO) in galaxy surveys, combined with supernovae type Ia data. We find that the universe underwent an early phase of accelerated expansion at redshifts z > 1.7, which lasted for more than one billion years. The current accelerating epoch began around redshift z = 0.6, and is continuing to accelerate today. This picture is consistent with standard \u039bCDM cosmology but differs significantly from other models such as quintessence or modified gravity theories. These results are based on the WiggleZ Dark Energy Survey, the Sloan Digital Sky Survey Data Release 7, and the Union2 compilation of supernovae type Ia observations. They were obtained by combining these datasets through a Bayesian analysis technique known as Monte Carlo Markov Chain sampling. \nThe best-fit values we obtain are:  $\\Omega_{\\Lambda}_0 =0.69 \\pm 0.01$ , $ H_0 = 67.3 \\pm 2.1 km s^{-1} Mpc^{-1}$ .","label":1,"model":"bloomz","source":"arxiv","id":3008}
{"text":"We present new constraints on the redshift distribution of sources contributing to the source subtracted near-infrared background (SSNIB). We use deep Spitzer\/MIPS 24 micron data in combination with optical and infrared photometry, including GALEX NUV imaging, to select galaxies at z > 1.5 over an area of 0.6 deg2 centered around the Lockman Hole East field. The resulting sample consists of 16,000 objects selected between redshifts 2<z<8. Using this sample we measure the evolution of the luminosity function out to high redshifts by fitting Schechter functions to our observed number counts as a function of flux density binned into bins of width \u2206logS = 0.1 dex. Our results are consistent with previous studies that find evidence for strong luminosity evolution up to z ~ 3 followed by little or no evolution beyond this point. \n \n We then fit models to these measurements using Monte Carlo simulations which include contributions from both obscured AGNs and normal star forming galaxies. These fits show that the majority of the SSNIB is produced by faint galaxies at low redshifts (0.3 < z < 1) while bright galaxies dominate at higher redshifts (4 < z < 6). \n \n Finally, we compare our best-fit model predictions to existing observations of the unresolved extragalactic background light (EBL), finding good agreement within uncertainties.","label":1,"model":"bloomz","source":"arxiv","id":3009}
{"text":"We have studied the polarizability and hyperpolarizability tensors for clusters Li_n (n = 2, 3) using density-functional theory with the B3LYP functional in combination with large basis sets. The results are compared to those obtained by other authors as well as experimental data available for n=2. We find that our calculated values agree very well with previous theoretical work but disagree significantly with experiment. This is attributed mainly to the fact that we use an atomic orbital basis set which does not include diffuse functions. \n \n Keywords: Polarizability tensor; Hyperpolarizability tensor; Clustering; Density-functional theory; Basis set; Diffuse functionals; Lithium cluster; B3LYP functional. 1 Introduction \n \n In recent years there has been considerable interest in studying the optical properties of small metal clusters due to their potential applications in optoelectronic devices such as lasers [1] , light-emitting diodes [2] , solar cells [3] , etc.. These materials can be used either directly or incorporated into more complex structures [4] . For example, it was shown recently that gold nanocluster films could be prepared on glass substrates [5] . It should also be noted that these materials may exhibit interesting nonlinear optical effects [6] .\n \nThe study of the optical response of clusters requires knowledge about both linear and nonlinear optical susceptibilities [7, 8] . The latter quantity describes how strongly the material responds when exposed simultaneously to two or more laser beams [9] . A convenient way to calculate this property is through the so-called sum-over-states approach [10] where one calculates the imaginary part of the third-order susceptibility \u03c7(3) (\u03c91, \u03c92, \u03c93). Here, the frequencies \u03c9i correspond to different wavelengths of incident radiation. If all three frequencies coincide then the corresponding susceptibility is called the static hyperpolarizability \u03b20 [11] .","label":1,"model":"bloomz","source":"arxiv","id":3010}
{"text":"We have performed simulations of nova explosions using the hydrodynamic code VH-1, which includes nuclear burning and convection. We find that changes to reaction rates can significantly affect the results of these calculations. In particular, we show how different choices for the 12C(p,\u03b3)13N rate lead to differences in the predicted light curve shapes.  The inclusion of this reaction is important because it affects the amount of 13N produced during the explosion. This isotope decays by electron capture into 14O, which then undergoes \u03b2+ decay back down to 14N. If there are too many electrons present at late times (due to an overabundance of 13N), they will be captured onto protons instead of being emitted as positrons; thus, less energy will be released than if no such process were occurring. Our results suggest that the current uncertainty in the 12C(p , \u03b3 )13N rate may cause errors in the predicted luminosity of up to 50%.","label":1,"model":"bloomz","source":"arxiv","id":3011}
{"text":"We report on optical and near-infrared observations made with the Very Large Telescope (VLT) of the gamma-ray burst (GRB) 051028, which was detected by Swift at 07:55 UT on May 28th 2005. The prompt emission lasted for about 20 s in the 15-150 keV energy range. We find that this event is associated to an elliptical galaxy located at z = 1.62 \u00b1 0.01. A bright afterglow was discovered in the R-band image taken one hour after the trigger time. This source faded rapidly during the first few hours following the burst but remained visible up to day 10.5 post-burst. In addition we detect two other sources within the error circle of the X-ray position. One of them has been identified as a star forming region while the second one could be either another supernova or a faint AGN. Finally, we present photometric data obtained between days 3 and 8 showing evidence for spectral evolution across these epochs.","label":1,"model":"bloomz","source":"arxiv","id":3012}
{"text":"We study the interplay between thermal percolating states and jammed states in disordered systems by using Monte Carlo simulations for dimers adsorbing onto binary alloy surfaces with different compositions. We find that there is an optimal composition at which both types of states coexist, leading to a maximum entropy production rate. The coexistence state has been observed experimentally as well. Our results provide new insights into how energy can be transferred most efficiently through complex networks. Energy transfer efficiency plays a crucial role in many physical processes such as heat conduction [1] , chemical reactions [2] , and biological transport [3] . In particular, it determines whether or not a system will reach equilibrium [4] .\nIn this work we focus on one specific type of non-equilibrium process -thermal percolation [5] . Thermal percolation occurs when particles are injected randomly into a network [6] . Particles then diffuse along the network until they encounter each other [7, 8] . When two particles meet, their energies combine irreversibly [9] . This leads to a cascade-like spreading of particle density [10] . As more particles are added, the number of clusters increases [11] . Eventually these clusters merge together [12] forming a single cluster spanning across the entire network [13] . At this point all particles have combined into a giant cluster [14] . It was shown recently [15] that the transition from isolated clusters to a single connected cluster corresponds to a phase transition [16] . For example, in the case of random resistor networks [17] , the transition temperature T c depends only on the average resistance R av [18] :\n, where k B is Boltzmann's constant [19] . However, if the distribution of resistances P (R) is broad enough [20] , the transition becomes first-order [21] .","label":1,"model":"bloomz","source":"arxiv","id":3013}
{"text":"We present new constraints on planet formation timescales by combining infrared photometry and spectroscopy with optical data to study the circumstellar environments of young stars in nearby open clusters. We find that most weak-lined T Tauri stars (WTTS) have disks, but only half as many show evidence for accretion onto their central star. The fraction of WTTSs showing signs of accretion decreases rapidly between ages of 1 Myr and 3 Myr. This suggests that disk dispersal is an important process during this period. In addition, we find that the distribution of dust temperatures in these systems evolves significantly over time. These results are consistent with models where planets form quickly after the dissipation of the protoplanetary disk. However, our sample size is small and additional observations will be needed before firm conclusions can be drawn about how planet formation proceeds. \n \n Keywords: Open cluster, Circumstellar disk","label":1,"model":"bloomz","source":"arxiv","id":3014}
{"text":"In this article, we study the properties of holomorphic functions in complex Banach spaces. We prove that if $(X, \\|\\cdot\\|)$ is an infinite-dimensional complex Banach space with unconditional basis then every norm bounded sequence in $X$ has a subsequence which converges weakly to zero (Theorem 1). This result generalizes the classical theorem due to Lindenstrauss for Hilbert spaces. In particular, it implies that any unbounded closed convex subset of $X$ contains at least one point whose distance from the set equals its diameter (Corollary 2).\nWe also show that if $(X, |\\cdot |)$ is a separable complex Banach space such that there exists a constant $C > 0$ satisfying $\\sup_{f \\in X} |f| \\leq C \\|f\\|$ then every numerical peak holomorphic function on $X$ is continuous (Theorem 3). As applications, we obtain some results about numerical range of operators acting between two Banach spaces.","label":1,"model":"bloomz","source":"arxiv","id":3015}
{"text":"We have studied the effect of masking out part of sky for CMB temperature fluctuations by using simulated data sets and found that it is important to take into account the effects of masked pixels when estimating angular power spectrum (APS) at large angles, especially for low multipoles l < 20. We also find that the APS estimated with the standard method based on spherical harmonics decomposition can be biased if there are significant correlations between masked and unmasked regions. In this work we propose an improved method which takes these effects into account. The new method has been applied to estimate the APS of the WMAP three-year data set after removing point sources detected above 5 mJy beam \u22121 . Our results show good agreement with those obtained directly from the full-sky map without any masking. \n \n Keywords: Cosmic microwave background radiation, Large angle anisotropy, Point source removal","label":1,"model":"bloomz","source":"arxiv","id":3016}
{"text":"The 12th STEPHI campaign was carried out at the Roque de los Muchachos Observatory, La Palma, Canaries Islands during August-September 2003. We observed two bright Delta Scuti stars - 7 Aql and 8Aql with the STEPHI photometer equipped with an Andor CCD camera. In this work we present results for these targets obtained by applying different methods to analyse their light curves. For both stars we found pulsation frequencies which are listed in Table 1 . \n7 Aql is known as one of the most active Delta Scuti variables showing more than 100 periodicities in its power spectrum. Our analysis revealed that it has several additional modes excited simultaneously. Some of them were not detected before because they have very low amplitudes or are located close to other peaks. \n8Aql turned out to be another interesting target. It shows only three significant periods but all of them are quite short -larger than 0.1 d. This star also exhibits low-amplitude variability on longer time scales. \n\n\nWe compared our results with those published previously.","label":1,"model":"bloomz","source":"arxiv","id":3017}
{"text":"The nullstellensatz is an important theorem in algebraic geometry, which states that if a system of polynomial equations has no solution over some field then it must have at least one solution over its algebraic closure.  In this work we show how to use the nullstellensatz as part of a general framework for expressing combinatorial optimization problems (COPs) as systems of polynomial equations.   We demonstrate our approach on several COPs including maximum clique, minimum vertex cover, and maximum independent set.   ... This research was supported by NSF grant CCF-0430020. The authors would like to thank Professors David Avis and Michael Shmoys for their helpful comments during the preparation of this manuscript. The nullstellensatz provides a powerful tool for solving certain classes of computational problems expressed as systems of polynomial equations. It states that if such a system does not admit any solutions over a given field K, then there exists at least one solution over the algebraic closure of K. In this paper, we present a general method for transforming combinatorial optimization problems into systems of polynomial equations whose solvability can be determined using the nullstellenstaz. Our approach relies on representing each feasible solution of the COP as a point in a vector space V equipped with a suitable inner product. Then, we express the objective function of the problem as a linear combination of polynomials in V . Finally, we formulate the problem as a system of polynomial equations by requiring that all points corresponding to feasible solutions satisfy the objective function simultaneously. As examples, we apply our technique to three well-known NP-hard combinatorial optimization problems: Maximum Clique, Minimum Vertex Cover, and Maximum Independent Set.","label":1,"model":"bloomz","source":"arxiv","id":3018}
{"text":"In this article, we introduce the notion of affine structures on ringed spaces in order to study schemes over rings with non-trivial units. We show that every scheme can be equipped with an affine structure which is unique up to isomorphism. This result allows us to define the category of schemes as a full subcategory of the category of affine structures on ring spaces. In particular, it follows that any two morphisms between schemes are isomorphic if they have the same source and target. Finally, we prove that the category of schemes has all small limits and colimits. The main results of this work were presented at the conference \"Algebraic Geometry\" held in June 2005 in Stony Brook (USA). Keywords: Affine Structure, Scheme, Ringed Space. 1 Introduction Let R be a commutative ring with unit. A scheme X over R consists of a set |X| together with a sheaf O_X of algebras over R such that:\n(1) For each open subset U of X there exists a faithfully flat morphism f:U\u2192Spec(R), where Spec(R) denotes the spectrum of R. (2) There exist elements x_1,...,x_n \u2208 O_X(U) for some n > 0 such that the ideal generated by these elements equals f*O_{Spec(R)}(f^{-1}(0)) = {f*r\\in O_{Spec(R)}: r\\notin m}, where m is the maximal ideal of R. \n \n It turns out that one may equivalently consider only those schemes whose underlying sets are locally Noetherian topological spaces. Moreover, one may also assume without loss of generality that the sheaves O_X are coherent.","label":1,"model":"bloomz","source":"arxiv","id":3019}
{"text":"The elastic scattering angular distributions, fusion cross sections (FCS), and total reaction cross sections (TRCS) have been measured in the 7Li + 208Pb system at energies near Coulomb barrier by using the time-of-flight technique with an active target. The experimental data are compared to theoretical calculations based on the extended optical model (EOM). In this work we use the folding potential method which is one of the most successful methods used to calculate nuclear potentials. We also compare our results with those obtained by other authors who used different approaches such as double-folding or Woods-Saxon potentials. It was found that all these models can reproduce well the elastic scattering angular distribution but they fail to describe simultaneously both FCS and TRCS. This indicates that there may be some missing physics in the present EOM approach. Finally, it should be noted that the present study provides useful information about the structure of nuclei involved in the collision process. \n \n Keywords: Nuclear reactions; Elastic scattering; Fusion","label":1,"model":"bloomz","source":"arxiv","id":3020}
{"text":"We study the geometry of generalized Gaussian distribution (GGD) in terms of its Fisher information matrix and entropy, which are closely related to each other by Cram\u00e9r-Rao inequality. We show that GGDs with different parameters have different geometries; specifically, we prove that for any fixed parameter $\\mu$, as $\\sigma \\to 0$ or $+\\infty$, the Fisher information matrix converges to zero while the entropy diverges to infinity. This is consistent with our intuition on how small variance and large variance affect the uncertainty of data points. Finally, we provide an algorithm to compute the exact value of the Fisher information matrix for any given GGD. The results presented here can be used to analyze the performance of many machine learning algorithms based on GGD such as kernel density estimation and support vector machines. \nKeywords: Generalized Gaussian Distribution, Entropy, Fisher Information Matrix, Geometric Analysis, Machine Learning","label":1,"model":"bloomz","source":"arxiv","id":3021}
{"text":"The first stars in the universe were born out of primordial gas clouds, which collapsed under their own gravity to form dense cores that ignited nuclear fusion and became hot white dwarfs.  The most massive of these early stars are now known as Population III (PopIII) stars.   In this work we present results for PopIII star formation using cosmological hydrodynamic simulations with radiative transfer calculations performed on an adaptive mesh refinement grid.  We find that PopIII stars can be formed by direct collapse of metal-free gas clouds at redshifts z > 20.  These stars have masses M * = 100 - 1000M\u2299 and lifetimes t < 10Myr.  They evolve into black holes or pair-instability supernovae after consuming all available fuel within their convective envelopes.  Our results show that PopIII stars may contribute significantly to reionization of the intergalactic medium around redshift z ~ 15.","label":1,"model":"bloomz","source":"arxiv","id":3022}
{"text":"We present an exact expression for the probability distribution function (PDF) of the number of steps taken by a one-dimensional, discrete-time, self-propelled particle that moves in a periodic potential and interacts with itself via elastic collisions.  We show how this PDF can be used to calculate the mean-square displacement as well as other statistical properties of such particles. The results are illustrated using numerical simulations. \nPACS numbers: 05.45.-a; 05.70.Jk; 05.60.Gg \nI. INTRODUCTORY REMARkS\nThe motion of many biological systems is often described as being driven by internal forces or active processes [1] . Examples include bacteria swimming through fluids [2] , cells crawling on surfaces [3] , and molecular motors moving along cytoskeletal filaments [4] .\nIn recent years there has been growing interest in understanding the dynamics of these active particles [5] - [8] . In particular, it was shown that their behavior may differ significantly from that observed in passive Brownian particles [9] - [11] . For example, while the latter exhibit normal diffusion at large timescales [12] , active particles typically display superdiffusive [13] or even ballistic [14] transport depending on the details of their interactions [15] - [17] . This difference arises because active particles have additional degrees of freedom which allow them to explore more efficiently the available space [18] . As a result they tend to move faster than passive particles [19] .\nRecently we introduced a model describing the motion of a single active particle [20] . It consists of a point-like object that performs a biased random walk in a periodic potential [21] . Its position x(t + 1) = x(t) + v t+1 \u2212 v t is determined by its velocity v t+1 = f [x(t), v t ] where f [\u00b7] denotes some deterministic force acting upon the particle [22] . Here we consider two different types of potentials V (x). First, when V (x) \u221d cos(2\u03c0x\/L) (L is the periodicity length), the system exhibits a series of metastable states separated by energy barriers [23] . Second","label":1,"model":"bloomz","source":"arxiv","id":3023}
{"text":"We study the phase diagram of a model system consisting of particles interacting via an attractive Yukawa potential, which is balanced by a short-range repulsive term. We show that this simple model can reproduce many features observed in experiments on colloidal suspensions near their glass transition temperature Tg. In particular we find that for temperatures below some critical value Tc there exists a metastable liquid state characterized by strong density fluctuations. The relaxation time of these fluctuations diverges at Tc as predicted by mode-coupling theory (MCT). Above Tc the equilibrium state consists of a disordered gas-like phase. For large values of the coupling constant between the two potentials the gas-liquid coexistence line ends in a first-order phase transition to a solid state. This transition occurs close to the MCT prediction for the ideal glass transition point. Our results are obtained using molecular dynamics simulations combined with free-energy calculations based on thermodynamic integration techniques. \n \n 1 Introduction \n \n Glass-forming liquids exhibit fascinating properties such as non-exponential relaxation [1\u20133] or dynamic heterogeneity [4\u20136]. These phenomena have been studied extensively over several decades [7\u20139] but still remain poorly understood. One reason for this lack of understanding lies in the fact that glassy systems are out of thermal equilibrium [10] so that standard statistical mechanics methods cannot be applied directly. However, it has recently become possible to investigate glassy states experimentally [11\u201313] and numerically [14\u201318]. A particularly interesting class of glass-formers comprises colloidal suspensions [19\u201321] whose slow dynamics can be probed by light scattering [22]. It was found that the relaxation times diverge when approaching the so-called glass transition temperature T g . Close to T g , the relaxation process becomes extremely slow and the system falls out of equilibrium within experimental observation times [23]. Therefore, one usually studies the dynamics of colloids well above T g where they behave like normal fluids [24]. Nevertheless, even in this regime the relaxation processes are not completely understood [25].","label":1,"model":"bloomz","source":"arxiv","id":3024}
{"text":"We present an analysis of the Galactic latitude distribution of stars in the 2MASS survey, and find that it is well described by a superposition of two Gaussians with different widths. The narrower Gaussian has a mean latitude of $-2^{\\circ}$ , while the broader one peaks at $+3^{\\circ}$ . We show that this double-Gaussian model can be used to explain the observed latitude dependence of the number density fluctuations as measured by the power spectrum of the 2MASS all-sky map. \n \n In addition we demonstrate how our results are consistent with previous studies on the subject. Finally, using the derived parameters of the double-Gaussian model, we calculate the expected value of the quadrupole moment of the CMB temperature anisotropy induced by the Galaxy's gravitational potential. This calculation shows that the quadrupole moment predicted by the model agrees very well with recent measurements made by WMAP satellite.","label":1,"model":"bloomz","source":"arxiv","id":3025}
{"text":"We introduce the notion of covering homology, which is an extension of ordinary homology to infinite graphs and simplicial complexes. The main idea behind this concept is that we consider not only finite subcomplexes but also their infinite analogues - so-called coverings. We show how covering homology can be used for studying properties of infinite graphs such as amenability or hyperbolicity. In particular, we prove that if a graph has infinitely many ends then its first covering homology group vanishes. This result generalizes the classical theorem by Stallings on groups with infinitely many ends. Covering homology provides us with new tools for investigating various classes of infinite objects in mathematics. It turns out that it is closely related to other concepts like equivariant homology theory (in the case when our object admits actions of some group) or higher-order algebraic K-theory. Finally, we present several examples illustrating applications of covering homology.","label":1,"model":"bloomz","source":"arxiv","id":3026}
{"text":"We study the critical behavior of the 3D RFIM with Gaussian distributed disorder by means of Monte Carlo simulations and finite-size scaling analysis. We find that the system undergoes a continuous phase transition at zero temperature, which is characterized by an infinite correlation length but no divergent susceptibility. The results are compared to those obtained for the pure 3D Ising model as well as other models with quenched disorder. In particular we show how our findings can be understood within the framework of the droplet picture. \nPACS numbers: 64.60.Cn, 64.60.J-, 64.60.Nz \nI. INTRODUCTORY REMARkS\nThe Random Field Ising Model (RFIM) has been introduced more than 50 years ago [1] . It describes a ferromagnetic material where each spin interacts only with its nearest neighbors via exchange interactions J ij , while it also feels an external magnetic field h i randomly oriented on different sites [2] .\nIn recent years there have been many studies devoted to this problem both experimentally [3] - [6] and theoretically [7] - [12] . This interest was triggered mainly by the fact that the RFIM shares some features with real systems such as diluted antiferromagnets or spin-glasses [13] - [15] . For example, the presence of quenched disorder leads to frustration effects [16] similar to those observed in spin-glass materials [17] . Moreover, the RFIM displays a rich variety of phases depending on the strength of the applied magnetic field [18] . At low fields one finds a paramagnetic phase, whereas above a certain threshold value H c = O(J), the spins align along the direction of the local magnetic field leading to a ferromagnetic state [19] . Finally, if the magnitude of the external field exceeds another threshold value H t > H c , the magnetization becomes discontinuous [20] . These three regimes are separated by two second-order transitions occurring at T c1 < 0 and T c2 > 0 [21] . However, despite these analogies between the RFIM and experimental systems [22] , the exact nature of the phase diagram remains controversial [23] .","label":1,"model":"bloomz","source":"arxiv","id":3027}
{"text":"The Axion Dark Matter Search (ADMX) experiment is designed to search for dark matter in the form of axions, which are hypothetical particles predicted by theories beyond the Standard Model.  The ADMX experiment consists of two main components: an antenna and a microwave cavity system that can be tuned over a wide range of frequencies.   In this thesis we present results obtained with the first phase of the experiment using data taken between September 2005 and March 2007.    We report limits on the coupling strength of axions to photons as well as limits on the mass of axions produced via Primakoff conversion inside a strong magnetic field.  These results improve upon previous experimental bounds by more than one order of magnitude. This work was performed under the auspices of the U.S Department of Energy by Lawrence Livermore National Laboratory under Contract DE-AC52-07NA27344. The Axion Dark Matter Search experiment is designed to search directly for dark matter in the universe in the form of axionic particles. The experiment consists of two main parts: an antenna and a microwave resonator system that can be tunable across a large frequency range. In this dissertation I will discuss our recent results from the first stage of the experiment.","label":1,"model":"bloomz","source":"arxiv","id":3028}
{"text":"We study information diffusion on networks with multiple communities, where each community has its own set of nodes and edges. We propose an epidemic model that captures the effect of both global and local interactions between users within different communities. The proposed model is based on two key ideas: (1) we assume that there are some influential individuals who can spread information to their neighbors faster than others; and (2) we allow for cross-influences among neighboring communities through these influential individuals. Our main goal is to understand how the structure of the underlying network affects the spreading process. In particular, our contributions include:  1. We develop a novel mathematical framework to analyze the dynamics of information diffusion under the proposed epidemic model. 2. We show that if all communities have similar sizes then the number of infected nodes at time t grows as O(t log n), where n denotes the total number of nodes in the network. 3. We prove that if one community dominates the other ones by size then the number of infected individuals grows exponentially fast. 4. Finally, we provide extensive numerical experiments to validate our theoretical results.","label":1,"model":"bloomz","source":"arxiv","id":3029}
{"text":"We study how to promote cooperation among selfish agents by introducing a reward mechanism into the spatial prisoners' dilemma (SPD). In our model, each agent is located on an LxL square lattice with periodic boundary conditions. Each agent interacts only with its four nearest neighbors at every time step. The payoff matrix for this game is given by: \nwhere R>P>0<S<0<T. We find that when T=0.5R+0.25P-0.75S, cooperators can survive even if they are initially less than 25% of all players. Moreover, we show that the fraction of cooperators increases as the temptation to defect decreases or the cost-to-benefit ratio increases. Finally, we demonstrate that the introduction of punishment does not destroy cooperative behavior but rather enhances it. Our results suggest that the proposed reward mechanism may be useful for promoting cooperation in real-world systems such as biological communities and social networks.","label":1,"model":"bloomz","source":"arxiv","id":3030}
{"text":"We consider a spherically symmetric solution to Einstein's equations in five dimensions with an extra dimension compactified on S 1 \/Z 2 . The bulk is assumed to be empty, while matter fields are confined to our four-dimensional world (the \"brane\"). We find that this model can explain the observed flatness of galactic rotation curves without introducing any new particles or exotic forms of energy density. In particular we show how the mass distribution within galaxies may arise naturally as a consequence of the geometry of space-time. This work was supported by NSF grant PHY-0456728. PACS numbers: 04.20.-q, 11.10.-z, 98.80.Cq  A fundamental question about the nature of dark matter has been whether it consists of one or more species of particle. If so, what are their masses? What interactions do they have with ordinary matter? How much dark matter does each galaxy contain? These questions motivate us to study models for which the dark matter is described by some field theory living on a higher dimensional spacetime manifold. \n \n Here we will focus on a class of solutions where the extra dimension is compactified on a circle $S^1$. Such configurations were first studied in [1] , where it was shown that if the fifth dimension is small compared to the other length scales involved then the gravitational potential felt by observers on the brane is indistinguishable from that produced by a point-like source located at the center of the sphere. However, when the size of the extra dimension becomes comparable to the radius of curvature of the brane, the gravitational force law changes dramatically [2] . \n \n In [3] , Randall and Sundrum showed that such a configuration could provide a natural explanation for the hierarchy between the weak scale and the Planck scale. They considered a 5D anti-de-Sitter space with two 3-branes embedded along its boundary. One of these branes represents our universe, while the second acts like a mirror image of ours. Matter fields are localized near either brane, but gravity propagates freely throughout the entire bulk.","label":1,"model":"bloomz","source":"arxiv","id":3031}
{"text":"We present the results of our analysis on the supersymmetric parameter space, taking into account all available experimental data including those from LHC experiments as well as electroweak precision observables (EWPO). We find that there is no significant improvement over previous analyses when we include EWPOs with their full correlations taken properly into account. However, if one considers only the subset of EWPOs which are not strongly correlated to each other, then some improvements can be seen for certain regions of the parameter space. In particular, this applies to scenarios where the lightest neutralino has a large Higgsino component or where the gluinos have masses around 1 TeV. The latter case also leads to an improved agreement between theory predictions and measurements of the anomalous magnetic moment of the muon. Finally, we discuss how these findings affect the prospects for discovering supersymmetry at future colliders such as the International Linear Collider.","label":1,"model":"bloomz","source":"arxiv","id":3032}
{"text":"We present an analysis of the first two years (Feb 2005 -Jan 2007) of data taken by the Swift satellite, which has been designed to detect and study gamma ray bursts (GRBs). We find that GRB 050904 at z = 6.3 is the most distant object ever detected in the electromagnetic spectrum. The prompt emission was observed over more than four orders of magnitude in energy, from radio waves up to X-rays. This burst also had one of the highest fluences recorded so far for any GRB. In addition we report on another burst, GRB 080913, whose afterglow was found to be variable on timescales as short as 1 minute. These results are discussed within the context of current models for GRB production. Keywords: Gamma-ray burst, High-redshift universe, Afterglows, Swift satellite. Gamma-ray bursts (GRBs), intense flashes of high-energy radiation lasting only milliseconds, have now been discovered out to redshifts greater than six [1] . Their extreme luminosities make them powerful probes into the early Universe [2] , but their origin remains unknown [3] .\nSwift [4] , launched in November 2004, carries three instruments capable of detecting GRBs across the entire electromagnetic spectrum [5] : the Burst Alert Telescope [6] detects GRBs via their X-ray and\/or optical emissions; the Ultraviolet\/Optical Telescope [7] observes the afterglow through ultraviolet and visible light; and the X-ray telescope [8] monitors the afterglow's decaying flux. Here we describe our initial findings using these instruments during the first two years of operation. \nThe Burst Alert Telescope\n\nBurst Alert Telescope Observations of GRB 050904\nOn September 5 th , 2006, the Burst Alert Telescope triggered on a bright source located at RA=05h54m36.6s Dec=-69d21'59.6\" [9] . Follow-up observations revealed this event to be a new record holder among GRBs [10] . Its peak photon count rate reached 2 x 10 4 photons s -1 cm -2 in the 15-150 keV band [11] . It lasted about","label":1,"model":"bloomz","source":"arxiv","id":3033}
{"text":"We present new observations with the Hubble Space Telescope (HST) and Chandra X-ray Observatory to study the properties of ionized gas in galaxy clusters at z ~ 0.5-0.8, where most massive clusters are found today.  We find that the fraction of cool core clusters is higher than expected for their redshifts based on local samples. The observed evolution may be due to an increase in the number density of active galactic nuclei or AGN activity over time. In addition, we detect extended emission line regions around some of these clusters which have been previously identified as having strong cooling flows. These results suggest that there has been significant heating of the intracluster medium by energetic outflows associated with AGNs since z = 1.0. This work was supported by NASA grant NAG5-9998. Cooling flow clusters are known to contain large amounts of cold gas within their central regions. However, it remains unclear how this gas cools down without forming stars. Recent studies show that many of them also harbor powerful radio sources near their centers. It is possible that such radio jets heat up the ICM through shocks and\/or turbulence generated during the interaction between the jet plasma and the ambient hot gas.","label":1,"model":"bloomz","source":"arxiv","id":3034}
{"text":"Les espaces analytiques rigides ont \u00e9t\u00e9 introduits par A. Grothendieck dans les ann\u00e9es 1960 pour \u00e9tudier la cohomologie \u00e9tale des sch\u00e9mas alg\u00e9briques sur un corps quelconque. Ils constituent une g\u00e9n\u00e9ralisation naturelle des vari\u00e9t\u00e9s alg\u00e9briques affines et projectives, mais ils ne poss\u00e8dent pas toujours de structure g\u00e9om\u00e9trique classique (vari\u00e9t\u00e9s alg\u00e9briques). Dans cet expos\u00e9 nous allons pr\u00e9senter quelques r\u00e9sultats r\u00e9cents concernant l'\u00e9tude de ces espaces en g\u00e9om\u00e9trie diophantienne. Nous verrons que certains probl\u00e8mes classiques peuvent \u00eatre \u00e9tudi\u00e9s gr\u00e2ce \u00e0 cette nouvelle approche. En particulier, nous montrerons comment on peut utiliser les espaces analytiques rigides pour obtenir des bornes explicites sur le nombre de points rationnels d'une vari\u00e9t\u00e9 alg\u00e9brique d\u00e9finie sur un corps fini. Ces travaux font suite aux articles suivants :  - J.-B. Bost, M. Chambert-Loir and C. Soul\u00e9, ``Explicit bounds for rational points'', Inventiones Mathematicae, vol. 162, no. 1, pp. 1-33, (2005) - J.-B.","label":1,"model":"bloomz","source":"arxiv","id":3035}
{"text":"We report on the optical and near-infrared afterglows of the short-hard burst GRB 050802 detected by Swift\/BAT on May 2nd, 2005 at 07:55:06 UT (T0). The prompt emission was followed by an X-ray flare peaking at T0+500 s in the rest frame. We find that both components are well described by power laws with decay indices \u03b11 = 1.2 \u00b1 0.3 for t < 10 ks and \u03b12 = 2.5 \u00b1 0.4 for t > 10 ks. A break is observed between these two regimes around t0 + 20 ks. No evidence for spectral evolution or extinction has been found within each component. Our results suggest that this event may be similar to GRB 021004 which also showed a double-power law behaviour but without any significant spectral evolution across the break time. This suggests that the physical mechanism responsible for the late-time steepening could be related to the one producing the early shallow decline. \n \n Keywords: Gamma-ray burst","label":1,"model":"bloomz","source":"arxiv","id":3036}
{"text":"We study the dynamics of superfluids with rotation, viscosity and mutual friction using numerical simulations based on the two-fluid model for superfluids developed by Tsubota et al. (Phys Rev Lett 106, 053001 (2011)). We show that the system is unstable to axisymmetric perturbations when the angular velocity exceeds some critical value $\\Omega_c$. The instability leads to the formation of vortex rings which are then advected outward by the flow. In addition we find that there exists another type of non-axisymmetric instability at higher values of $\\Omega_c$. This instability results in the generation of spiral waves which can be seen as an analogue of Rossby waves observed in planetary atmospheres. Finally, we discuss how these results may affect the spin evolution of young pulsars. Superfluidity plays an important role in many astrophysical phenomena such as the cooling of white dwarfs or the structure of neutron stars. It has been shown recently that superfluid turbulence could explain the rapid spindown of young pulsars [1] . However, despite its importance, our understanding of superfluid hydrodynamics remains incomplete due to the lack of direct experimental observations.","label":1,"model":"bloomz","source":"arxiv","id":3037}
{"text":"In this talk, we will discuss the relationship between the Thurston norm and normal surfaces in 3-manifolds.  We will begin by recalling some basic definitions about the Thurston norm on homology classes of surface bundles over circles (or equivalently, mapping tori).  Then, using techniques developed by Gabai, we will show that any class with positive Thurston norm can be represented by an embedded surface which is incompressible and not boundary-parallel.  Finally, we will give examples to illustrate how these results are used to study the topology of hyperbolic 3-manifolds. The Thurston norm was introduced in 1982 as part of the solution to the Geometrization Conjecture for Haken manifolds.   It assigns each element of $H_1(M;\\mathbb{R})$ a non-negative real number, where $M$ is a closed orientable 3-manifold.  This assignment satisfies certain natural properties, including additivity under connected sums and multiplicativity under finite covers.  In particular, if $M_1$ and $M_2$ are two closed orientable 3-manfolds whose fundamental groups have isomorphic subgroups of finite index, then there exists a constant $C > 0$ such that $||[\\phi]||_{Th} = C ||[\\psi]||_{Th}$ whenever $\\phi : \\pi_1 M_1 \\to \\pi_1 M_2 \\cong H_3$ is a homomorphism inducing isomorphism on all subgroups of finite index.    If $\\mu \\in H_1(M;\\mathbb{R};\\mathbb{Z})$ represents a fibered knot or link in a closed oriented 3-manifold, then its Thurston norm measures the complexity of the associated fibration.  For example, if $L \\subseteq M$ is a fibered torus knot or link, then $||\\mu||_{Th} = 1$ if and only if $L$ bounds a Seifert surface in $M$.  On the other hand, if $L$ does not bound a Seifert surface in...","label":1,"model":"bloomz","source":"arxiv","id":3038}
{"text":"We consider the problem of estimating an unknown field in R^d based on noisy measurements taken by randomly deployed binary sensors. The measurement process is modeled as follows: each sensor takes one bit of information about its local environment and transmits it to a fusion center via a wireless link; at the fusion center, these bits are combined into estimates for the underlying field. We assume that there exists some prior knowledge regarding the structure of this field (e.g., smoothness), which can be used to design efficient estimators. In particular, we propose two classes of estimators: non-parametric estimators with fixed bandwidths, and adaptive estimators whose bandwidths vary according to the local properties of the field being estimated.  For both types of estimators, we provide performance guarantees under various assumptions on the noise model and the spatial distribution of the sensors. Finally, we present numerical results illustrating our theoretical findings.","label":1,"model":"bloomz","source":"arxiv","id":3039}
{"text":"We study the formation and control of Liesegang patterns in an electrochemical system by applying external electric fields to modulate the local concentration gradients during precipitation reactions.  We show that, under certain conditions, the applied field can be used as a guiding field to direct the growth of precipitate bands along specific directions. The results are explained using a simple model based on the competition between diffusion and reaction rates at different locations within the sample. This work provides new insights into how chemical systems may self-organize through interactions with their environment. Chemical systems often exhibit complex spatial structures such as stripes or rings which form spontaneously without any externally imposed symmetry breaking [1] . These structures have been observed in many natural phenomena including mineral deposits [2] , biological tissues [3] , and even living organisms [4] .\nThe most famous example is the so-called \"Liesegang ring\" formed when two solutions containing metal ions react chemically [5] . In this case, the initial solution contains both cations (e.g., Ag+) and anions (e.g., Cl-). When these two solutions come into contact, they begin to diffuse across each other until they meet another interface where the opposite charges neutralize one another [6] . At some point after mixing, precipitation occurs leading to the formation of a band of solid material separating the original solutions [7, 8] . As more bands grow, they eventually overlap forming concentric rings around the center of the sample [9] . Although the exact mechanism behind the formation of Liesegang rings remains unclear [10] , it has been shown experimentally that the spacing between successive rings depends strongly on the concentrations of the starting solutions [11] .","label":1,"model":"bloomz","source":"arxiv","id":3040}
{"text":"We present an explicit expression for the irreducible form of the metric variation of the action term in sixth order gravity, which is valid to all orders in perturbation theory. We also show that this result can be used to derive an approximate expression for the stress energy tensor of the gravitational field. The results are applied to study the evolution of cosmological perturbations during inflation driven by a scalar field with non-canonical kinetic term. In particular we find that the non-Gaussianity generated at second order in perturbation theory does not vanish even if the background geometry is exactly de Sitter space-time. This implies that the bispectrum produced by such models cannot be described solely in terms of local shape functions as it was previously thought. \nI. INTRODUCTORY REMARK\nIn recent years there has been renewed interest on higher derivative theories of gravity motivated mainly by their possible role in quantum gravity phenomenology (see e.g. ), but also because they provide interesting alternatives to standard General Relativity (GR) in the context of modified gravity scenarios . However, despite these efforts, our understanding of the physical consequences of these theories remains incomplete due to technical difficulties associated with the analysis of their solutions. One of the main obstacles comes from the fact that the equations of motion derived from these actions contain derivatives of arbitrarily high order, making them difficult or impossible to solve analytically. A way out of this problem consists in expanding the fields around some fixed background solution and truncating the resulting series expansion after a finite number of terms. Although this approach allows one to obtain useful information about the dynamics of the system under consideration, it fails to capture important features like back-reaction effects between different modes of the same field or interactions among different fields. For example, in the case of inflationary cosmologies based on higher derivative gravity, the truncated perturbative expansions do not reproduce correctly the observed level of primordial non-Gaussianities .\nA more systematic method to deal with these problems involves the use of covariant techniques developed originally within the framework of GR. These methods allow us to express the equations of motion in a manifestly gauge","label":1,"model":"bloomz","source":"arxiv","id":3041}
{"text":"We consider the problem of finding a shortest path between two nodes in an undirected graph with non-negative edge weights, subject to the constraint that no node can be visited more than once along this path.  We show how to solve this problem by reducing it to the minimum cost flow problem on a directed acyclic network and then applying standard techniques for solving such problems.   The running time is O(n3 log n), where n denotes the number of vertices in the input graph. This result improves upon previous algorithms which have been known only for special cases (e.g., when all edges are unit-weight).    Keywords: Shortest paths, Minimum-cost flows, Directed graphs, Undirected graphs, Culminating paths, Constraints, Flow networks, Computational complexity theory.    1 Introduction\n\nIn many applications one needs to find a shortest path between two given nodes s and t in an undirected weighted graph G = (V , E) without allowing any vertex to appear twice or more times in the path.   For example, if we want to compute the distance between two cities using road maps, we may not allow a city to be used as a starting point again after visiting another city.   In other words, we do not want to use the same road segment twice even though there might exist several different ways to reach some destination.   Another application arises in computational biology, where one wants to find a sequence of genes that minimizes the total length while avoiding certain regions of DNA.   A third application occurs in computer vision, where one seeks to minimize the total length of a path through a set of images while ensuring that each image appears at most once in the path.   Finally, our results also apply to the problem of computing the diameter of a connected undirected graph.\n\nThe first algorithm for computing the shortest path satisfying these constraints was presented by Hassin and Rubinov [HR91] who showed how to reduce the problem to the minimum cost flow","label":1,"model":"bloomz","source":"arxiv","id":3042}
{"text":"We study the second-order correlation function for an atom interacting with two modes of light, one resonant and another off-resonant to atomic transition frequency. We show that higher order antibunching can be observed when the atom is initially prepared in an excited state or ground state superposition. The effect is more pronounced if the initial state has some population on the excited state. This phenomenon may have applications in quantum information processing. \n \n Introduction:-In recent years there has been considerable interest in studying nonclassical properties of radiation fields generated by atoms [1] . In particular, it was shown that the photon statistics of such systems are governed by the first-order coherence function g (1) (\u03c4) [2] , which describes bunching behavior at short times and anti-bunching at longer times [3] . It is well known that this property arises due to destructive interference between different pathways leading to emission of photons [4] .\nRecently, several authors studied the effects of spontaneous emission on the second-order correlation functions [5] - [8] . They showed that the presence of spontaneous emission leads to sub-Poissonian statistics [6]-[8] . However, these studies were restricted only to the case where the atom interacts with a single mode of field. On the other hand, many experiments involving atoms interacting simultaneously with multiple modes of electromagnetic field have also been performed [9] - [11] . For example, in Ref. [10] , the authors investigated the influence of vacuum fluctuations on the fluorescence spectrum of a three-level system driven by two laser beams. In addition, they found that the intensity noise of the emitted light depends strongly on the relative phase difference between the driving lasers. Motivated by these experimental results we consider here the problem of calculating the second-order correlation function of an atom interacting simultaneously with two modes of light [12] .","label":1,"model":"bloomz","source":"arxiv","id":3043}
{"text":"We study the thermal properties of symmetric and asymmetric nuclear matter using an extended Thomas-Fermi model based on a momentum dependent effective nucleon-nucleon (NN) interaction, which is derived by solving the Bethe-Goldstone equation in ladder approximation. The results show that the density dependence of nuclear symmetry energy at normal nuclear matter density changes significantly when temperature increases up to 100 MeV. In addition, we find that the slope parameter L(\u03c10), characterizing the density dependence of nuclear incompressibility K\u221e = 9L(\u03c10)(3\u03c02\u03c10\/40MeV)2, decreases rapidly as temperature rises for both pure neutron matter and symmetric nuclear matter. This indicates that the stiffness of nuclear matter becomes softer at high temperatures.  We also calculate the pressure P , entropy S and specific heat Cv of nuclear matter as functions of baryonic number density nB and temperature T .","label":1,"model":"bloomz","source":"arxiv","id":3044}
{"text":"We study the strong gravitational lensing effect by an isolated black hole and its companion star, which are described as point particles with spin in Einstein-AEther (AE) gravity. We find that the deflection angle is significantly affected by the AE vector field even for small values of the coupling constant $\\alpha$. The total magnification factor also depends strongly on $\\alpha$ through the angular separation between the two images. In particular, we show that the image splitting can be suppressed when $\\alpha$ takes large negative value. This may provide us a possible explanation to the observed absence of multiple images around some supermassive black holes such as Sgr A* at the Galactic center. \nI. INTRODUCTIO N\nThe recent detection of gravitational waves has opened up new windows into the universe [1] . One of the most important goals of future space-based gravitational wave detectors will be to detect gravitational waves emitted during inspiral phase of compact binaries [2] , which would allow us to test general relativity (GR). However, it remains unclear whether GR is valid or not beyond the weak-field regime [3] .\nIn this context, recently proposed theories of modified gravity have attracted much attention [4] . Among them, Einstein-AEther (EA) gravity [5] is one of the simplest extensions of GR [6] . It introduces a dynamical unit timelike vector field called \"aether\" whose dynamics is governed by the Lagrangian density,\nwhere $T_{\\mu \\nu}$ denotes the energy-momentum tensor of matter fields, $\\partial_\\mu V^\\mu = 0$ is required by the Lorentz invariance, and $AE_{\\mu \\nu} = \\partial_\\mu V^\\nu - \\partial_\\nu V^\\mu$ represents the AE vector field strength. Here, $c_{00} = c_0 + \\alpha c_1$ is assumed where $c_0$ and $c_1$ represent free parameters [7, 8] . Note that the first term in Eq. (1) \nII. DEFLECTION ANGLE IN EINSTEIN-AETHER GRAVITY\n\nA. Deflection angle formulae\nLet us consider a static spherically symmetric spacetime","label":1,"model":"bloomz","source":"arxiv","id":3045}
{"text":"We study the problem of finding an explicit formula for the dimension of the space spanned by all vectors in a given finite set $S$ whose Schatten $p$-norms are bounded above by some constant $C$, where $1<p<\\infty$ is fixed and $p>2$.  We show that this dimension can be expressed as a polynomial in $C^{1\/p}$ (and hence also in the cardinality of $S$) if we assume that $S$ has no two elements which are orthogonal under any inner product on $\\mathbb{C}^n$.   This result generalizes earlier results obtained independently by M. Lustig and A. Shub and by J. Bourgain and G. Bouschler. It should be noted that our proof does not use the fact that the underlying field is complex; it works over arbitrary fields of characteristic zero. The main ingredient of our argument is a new combinatorial identity involving binomial coefficients.","label":1,"model":"bloomz","source":"arxiv","id":3046}
{"text":"We show that the common origin of linear and nonlinear chiral multiplets is related to the existence of an extra dimension, which can be either time-like or space-like. We also discuss how this picture fits into the context of string theory. Theories with extended supersymmetry are known to have many interesting features such as duality symmetries [1] . In particular, theories with four supercharges (N = 4) possess remarkable properties [2] , including self-duality [3] .\nIn recent years there has been renewed interest in studying these theories due to their connection with string\/M-theory [4] . For example, it was shown [5] that type-IIB strings on AdS 5 \u00d7 S 5 background [6] correspond to maximally supersymmetric Yang-Mills theory in four dimensions [7, 8] . This correspondence allows one to study strongly coupled gauge theories using weakly-coupled gravitational description [9] . It turns out that the same idea works for other types of field theories [10] - [12] .","label":1,"model":"bloomz","source":"arxiv","id":3047}
{"text":"We present new evolutionary tracks for low-mass stars with initial masses between 0.8 and 8 M\u2299, which include convective overshooting in both core helium burning (CHeB) and red giant branch phases as well as mass loss on the AGB phase. We compare these models to observations of Galactic open clusters and find that they reproduce the observed luminosity function very well at all ages up to 1 Gyr. The predicted number ratio of carbon-rich to oxygen-rich AGB stars is also consistent with observations. However, we show that our current set of models fails to explain the existence of extremely luminous C-stars found among old open cluster members. These objects are located above the tip of the RGB but below the theoretical limit of the TP-AGB phase. They may be explained by assuming an additional mixing process during the early stages of the TP-AGB; however, this requires further investigation. \n \n Keywords: Post-AGB evolution","label":1,"model":"bloomz","source":"arxiv","id":3048}
{"text":"We present new near-infrared spectroscopy of the evolved star V4332 Sgr, which is believed to be in an advanced stage of its final red giant phase. We find that it has developed a strong infrared excess due to dust formation at temperatures between 1000-2000 K. This suggests that the object may have undergone a recent episode of enhanced mass loss on a time scale of years or decades prior to our observations. In addition we detect emission lines of hydrogen and helium indicating ongoing mass loss. These results are consistent with previous suggestions that this object underwent a late thermal pulse (LTP), i.e., a rapid increase in luminosity followed by a period of high mass loss. If so, then the LTP would have occurred within the last few hundred thousand years. Based on these findings we propose a revised evolutionary model for V4332 Sgr including a VLTP event as well as a subsequent superwind phase.","label":1,"model":"bloomz","source":"arxiv","id":3049}
{"text":"We consider universal quantization for distributed estimation over noisy channels, where feedback is available at both ends and communication rates are constrained by an average power constraint. We propose a novel scheme that achieves the optimal rate-distortion tradeoff under this setting. The proposed scheme consists of two parts: (i) A quantizer design based on the Lloyd-Max algorithm; (ii) An encoding-decoding strategy using successive refinement coding to achieve the desired distortion level while satisfying the average power constraints. Our results show that the proposed scheme can be implemented efficiently even when the number of sensors grows large. In addition, we provide numerical examples to illustrate our theoretical findings. Finally, we discuss some possible extensions of our work. This article has been accepted for publication in IEEE Transactions on Information Theory. \nThe final authenticated version is available online at: http:\/\/ieeexplore.ieee.org\/xpls\/abs_all.jsp?arnumber=6627571","label":1,"model":"bloomz","source":"arxiv","id":3050}
{"text":"We present an ansatz that allows to compute asymptotic expansions in terms of multiple polylogarithms at integer points, which are solutions of linear differential equations with polynomial coefficients and rational exponents. The method is illustrated by several examples including the computation of the large order behavior of Feynman integrals appearing in perturbative quantum field theory. We also discuss how our results can be used to obtain new exact formulas for special values of generalized hypergeometric functions. \nIntroduction\n\nThe problem we address here concerns the evaluation of certain classes of multivariate sums over integers. In particular, let us consider the following situation: Let $(a_i)_{i\\in I}$ denote a finite set of complex numbers indexed by some index set $I$ (which may or may not contain multiplicities). Furthermore, let $(b_j)_{j\\in J}$ denote another set of complex numbers indexed again by some index set $J$. Finally, let $(c_{ijk})_{(i,j,k)\\in I\\times J\\times K}$ denote yet another set of complex numbers where $K$ denotes some third index set. Then one considers the sum $\\sum\\limits_{\\substack{i\\in I\\\\ j\\in J}} c_{ijk}\\prod\\limits_{k\\in K} (1-a_ia_k-b_j)^{-1}$. This type of expression appears naturally when computing Feynman integrals in perturbative quantum field theories. For example, if one wants to evaluate the integral $\\int\\limits_0^1 dx_1\\int\\limits_0^1 dx_2\\int\\limits_0^1 dx_3 x_1x_2x_3\\exp\\left[-\\left(\\frac{1}{x_1}\\right)^{1\/3}-\\left(\\frac{1}{x_2}\\right)^{2\/3}-\\left(\\frac{1}{x_3}\\right)^{4\/3}\\right]$ then one has to deal with such expressions.","label":1,"model":"bloomz","source":"arxiv","id":3051}
{"text":"We present the results on the luminosity function (LF) and color-magnitude relation (CMR) for luminous red galaxies (LRGs) in clusters with redshift range between $z=0.2$ to $z=0.6$, using data obtained by Subaru\/Suprime-Cam. We use two different methods to select LRGs: one is based on photometric colors; another uses spectroscopic information. The LF shows that there are more bright LRGs than faint ones in all samples except for the sample selected only by photometry. This result suggests that we can obtain better statistics if we combine both selection criteria. In addition, our results show that the number density of LRGs decreases as the cluster mass increases. On the other hand, the CMRs do not depend strongly on the cluster masses or redshifts. These results suggest that the formation epoch of massive elliptical galaxies may be earlier than that of less massive ones. \n \n\n Keywords: Luminous Red Galaxies","label":1,"model":"bloomz","source":"arxiv","id":3052}
{"text":"We present the first exact solution for inflation in string theory, which is based on an explicit compactification to four dimensions with N=1 supergravity and chiral matter fields. The model contains two scalar fields, one of them being responsible for slow-roll inflation driven by its potential energy density. We show that this field can be identified as the inflaton. In addition we find another scalar field whose kinetic term has negative sign. This field may play the role of dark radiation during inflation. Finally, we discuss some phenomenological consequences of our results. Introduction: Inflation [1] provides a simple explanation for many puzzles associated with the early universe such as flatness, homogeneity and horizon problems [2] . It also predicts primordial fluctuations [3] , which are now confirmed by observations [4] .\nThe simplest models of inflation involve only one scalar field (inflaton) rolling slowly down its potential [5] . However it was shown recently [6] that there exist more general classes of inflationary scenarios where several scalars contribute to the total energy density driving inflation [7, 8] . These new possibilities open up interesting avenues towards understanding the physics behind inflation [9] .\nIn particular, if at least one of these scalars has positive kinetic energy then it leads to so-called k-inflation [10] . On the other hand, if all the scalars have negative kinetic energies they lead to so-called ghost inflation [11] . Both types of inflation were studied extensively in recent years [12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59 ].\nIt should be noted however that most of these studies assume that the background geometry is given by Minkowski space-time or anti-de Sitter space-time [60, 61, 62, 63, 64, 65,","label":1,"model":"bloomz","source":"arxiv","id":3053}
{"text":"We study the vibrational density of states (VDOS) of glassy materials using molecular dynamics simulations with anharmonic interactions between particles. We show that the VDOS can be decomposed into three main contributions: one at low frequencies corresponding to acoustic phonons; another contribution centered around the Debye frequency which is associated with optical phonons; and finally a third contribution located at high frequencies whose origin lies on the presence of unstable modes. The latter are responsible for the appearance of the so-called boson peak in the VDOS. In addition we find that these unstable modes have a strong tendency to form clusters or groups of correlated particles. Finally, by analyzing the spatial distribution of the unstable modes it becomes evident that they tend to localize near regions where there exist structural defects such as icosahedral-like motifs. This localization effect leads us to propose a simple model based on the existence of two different types of sites within the materials: stable ones characterized by their ability to support vibrations over large energy ranges, while unstable sites only allow small-amplitude oscillations.","label":1,"model":"bloomz","source":"arxiv","id":3054}
{"text":"We present the discovery and characterization of two \"hot Jupiter\" planets orbiting stars that are members of wide binaries, HD 196885AB (a = 1.8 AU) and HD 208598AB (a = 3.6 AU). The planet around HD 196885A is an inflated gas giant with M sin i = 0.88 MJup and P = 4.3 days; it orbits its primary at a distance of only 0.04 AU. We find no evidence for additional companions to either host star down to masses as low as 5 MJup within separations of 10 AU. Both systems have orbital eccentricities consistent with zero. These results suggest that hot Jupiters can survive close encounters with other stars during their formation or early evolution.  - Introduction \n \n Hot Jupiters are massive gaseous planets on short-period orbits about solar-type stars. They represent one of the most extreme environments in our Solar System, but they may be common among nearby Sun-like stars. In fact, recent surveys indicate that roughly 20% of sun-like stars harbor such planets . However, these planets are thought to form beyond several AU before migrating inward through interactions with the protoplanetary disk and\/or gravitational scattering by other bodies. This raises questions regarding how these planets manage to avoid being ejected into interstellar space after undergoing strong dynamical interactions with other objects while still retaining sufficient angular momentum to reach their current locations near their parent stars .\n\nIn this Letter we report the detection of two new \"hot Jupiter\" planets using high-precision radial velocity measurements obtained over more than eight years with the High Accuracy Radial Velocity Planet Searcher instrument (HARPS), which is installed on the European Southern Observatory's 3.6-m telescope located at La Silla Observatory in Chile. One of these planets has an extremely small semi-major axis of just 0.04 AU, making it one of the closest known exoplanets to its parent star.","label":1,"model":"bloomz","source":"arxiv","id":3055}
{"text":"We study the statistical properties of highly compressible turbulence by means of direct numerical simulations (DNS) at Mach numbers ranging between 10 and 100, using both isothermal and adiabatic equations of state for the gas.  We find that the scaling laws observed in incompressible flows are violated when the Mach number becomes large enough to produce shocks. In particular, we show that the energy spectrum exhibits an exponential decay with wavenumber k instead of the power-law behavior E(k) ~ k-5\/3 predicted by Kolmogorov's theory. The intermittent nature of the flow is also investigated through the analysis of probability density functions (PDFs). It turns out that PDF tails become increasingly fat-tailed as M increases, which can be explained by invoking the presence of shock waves. Finally, we discuss how these results may affect our understanding of astrophysical phenomena such as supernova remnants or interstellar clouds.","label":1,"model":"bloomz","source":"arxiv","id":3056}
{"text":"We present the free-fermion solution to the supersymmetric extension of the Wess-Zumino-Novikov-Witten (WZNW) model based on the supergroup $SL(2,R)\\times SL(2,R)$. The corresponding coset is given by $SL(2, R)\/SO(1, 1) \\times SL(2, R)\/ SO(1, 1)$ and we show that it can be realized as an orbifold of the bosonic coset $SL(2, IR)\/SO(2)$ with respect to its discrete subgroup $\\Gamma_2$. We then construct the corresponding affine Lie superalgebra using the method developed in Ref. [1] . Finally, we use this construction to obtain the explicit form for the transfer matrix which allows us to calculate the partition function exactly. \nThe results are compared against those obtained via Monte Carlo simulations. \n\n\nIntroduction\n\nSupersymmetry has been one of the most active areas of research over the past few decades due to its connection to string theory [2] , quantum mechanics [3] , statistical physics [4] , condensed matter systems [5] , etc.. In particular, there have been many attempts at constructing exact solutions to various supersymmetric theories [6] - [8] .\nIn this work, we consider the supersymmetric generalization of the Wess-Zuminov-Witten (WZWN) model [9] based on the supergrouo $SL(2,IR)\\times SL(2,  IR)$ [10] . This corresponds to the coset space $SL(2,\\  IR)\/SO(1,   1)\\times SL(2,    \\    IR)\/SO(1\\  ,\\   1)$ [11] . \nIt turns out that this coset can also be viewed as an orbifold [12] of the bosonic cosets $SL(2,    \\    IRN)\/SO(2)    \\    $ [13] where $N$ denotes the number of fermionic degrees of freedom. Using this realization, we will construct the corresponding affine Lie Superalgebra [14] and compute","label":1,"model":"bloomz","source":"arxiv","id":3057}
{"text":"We study the linear stability properties of coronal beams in the presence of background plasma and magnetic field fluctuations, using a multi-fluid model for ions and electrons. We find that the growth rates are strongly dependent on the angle between the wavevector k and the mean magnetic field B 0 . In particular, we show that there is an instability at oblique angles with respect to B 0 , which has been previously overlooked by previous studies based on single-fluid models. The new mode arises due to the coupling between the Alfv\u00e9nic modes associated with each species (ions and electrons). This mode can be excited even when the electron temperature anisotropy T e? \/T ez < 1, where ? denotes directions perpendicular to B 0 .\nThe results presented here may have important implications for understanding the origin of solar radio bursts observed during solar flares. Introduction: Coronal mass ejections (CMEs) are large-scale expulsions of magnetized plasma from the Sun's corona into interplanetary space. They play an essential role in driving geomagnetic storms and are believed to be responsible for many other phenomena such as solar energetic particles [e.g., Reames et al. (1998) , Kahler & Ragot (2007) ], solar radio bursts [e.g., Aschwanden (2004)] , and white-light flares [e.g., Benz (2008) ]. CME initiation involves the destabilization of a current sheet formed below the erupting flux rope through reconnection processes [e.g., Forbes & Priest (1995) ; Lin & Forbes (2000); Aulanier et al. (2010) ]. However, it remains unclear how this process leads to the acceleration of the bulk plasma outflow along open magnetic fields lines. Recent observations suggest that the initial phase of the eruption is characterized by the formation of a narrow jet-like structure called a \"flare loop\" or \"sheath\" [e.g., Liu et al. (2009a Liu et al. ( , 2009b ; Cheng et al. (2011); Jiang et al. (2012","label":1,"model":"bloomz","source":"arxiv","id":3058}
{"text":"The statistical mechanics of time-independent non-dissipative nonequilibrium states is studied in the framework of generalized entropy production and fluctuation theorem. The main results are formulated for systems with discrete energy levels, but they can be extended to continuous spectrum as well. It is shown that the probability distribution function (PDF) of the total entropy production has an exponential tail at large values of entropy production. This result agrees with recent experimental data on PDFs of heat fluxes between two reservoirs connected by a single molecule junction. In addition it is demonstrated how this approach allows one to calculate the average value of the total entropy production and its fluctuations. Finally we discuss possible applications of these results to transport phenomena in molecular junctions. We show that our theory predicts correctly the dependence of the current-voltage characteristics on temperature difference between the leads. Statistical mechanics of time independent non-dissipative non-equilibrium states is studied within the framework of generalized entropy-production and fluctuation theorem. Main results are formulated for systems having discrete energy-levels, however extension to continuum spectra is also discussed. It is shown that probability-distribution-function (PDF) of total entropy-production has exponential tails at large values of entropy-production. This result agrees with recently obtained experimental data on PDF's of heat-fluxes between two reservoirs coupled via a single-molecule-junction. In addition it is shown how this approach allows us to calculate the average-value of total entropy-production and it's fluctuations. Finally we discuss possibilities of applying these results to transport phenomena occurring in molecular junctions.","label":1,"model":"bloomz","source":"arxiv","id":3059}
{"text":"We present results on the metal cooling function for cosmological simulations, including both primordial and secondary coolants (H$\\alpha$, CII, OI, SiIII, Fe). We find that the inclusion of metals has little effect on the temperature distribution at z>5 but significantly affects it below this redshift. The effects are more pronounced when we include photo-ionization heating by UV background radiation. In particular, we show that the gas density profiles become shallower with decreasing redshifts due to stronger radiative losses near galactic centers. This leads to an increase in the central galaxy masses as well as the number densities of low-mass galaxies. These changes can be understood within the framework of our analytic model which is based on the assumption of hydrostatic equilibrium between gravity and pressure gradients. Our results suggest that the observed properties of high-z Lyman break galaxies may not be reproduced without taking into account the presence of heavy elements. \n \n Keywords: Cosmology","label":1,"model":"bloomz","source":"arxiv","id":3060}
{"text":"The calibration and data products for GALEX are described in this document, which is intended to be used by astronomers who wish to use these data sets.  The main focus of this document is on describing how to obtain calibrated images (i.e., fluxes) from raw data files produced by GALEX's instruments.   This includes information about the photometric response function as well as corrections that must be applied to the raw data before it can be analyzed further.   ... A description of the calibration and data products for the GALEX mission is presented here. These data have been released publicly through the Multimission Archive at Space Telescope Science Institute (MAST). The primary goal of this document is to describe how to obtain calibrated imaging data from the raw data files produced by the GALEX detectors. In addition, we provide details regarding the photometric response functions and other corrections necessary to produce accurate results when analyzing the data.","label":1,"model":"bloomz","source":"arxiv","id":3061}
{"text":"We report the observation of fractional quantum hall effect (FQHE) at filling factor = 1\/3 and 2\/3 in an optical lattice with two different species of fermions, which are realized by using 40 K-87 Rb mixture atoms confined to two-dimensional plane. The FQHE is observed as sharp peaks in the density distribution function obtained by time-of-flight absorption imaging technique after releasing the trapped atoms into free space. We also observe that the peak positions shift depending on the strength of magnetic field applied perpendicularly to the atomic plane. These results show good agreement with theoretical predictions based on exact diagonalization method for finite-size systems. \n \n Introduction \n \n Fractional quantum hall effect (FQHEs), discovered more than twenty years ago [1] , has been one of most fascinating phenomena in condensed matter physics [2] . In this phenomenon, electrons or holes form incompressible states at certain fractions of Landau levels under strong magnetic fields [3] . Recently, it was shown theoretically [4] and experimentally [5] that such effects can be observed even without external magnetic fields if particles have internal degrees of freedom like spin [6] - [8] .\n \nIn recent experiments [9] - [11] , we succeeded in observing FQHEs in ultracold gases confined to twodimensional planes. By applying artificial gauge potentials [12] - [14] to neutral atoms [15] - [17] , we were able to realize strongly correlated many-body states similar to those found in solid state devices [18] - [20] . \n \n Here, we present another example of FQHEs in optical lattices [21] - [23] . Instead of confining cold atoms to two-dimensional planes by trapping them in harmonic potential wells [24] , we use a deep three-dimensional optical lattice [25] to confine the atoms [26] . This allows us to study the properties of FQHEs in higher dimensions [27] - [29] .","label":1,"model":"bloomz","source":"arxiv","id":3062}
{"text":"We prove that there exists an open set of initial data such that the corresponding solution to the rapidly rotating shallow water (RSW) or Euler equations is globally smooth in space-time, provided it stays sufficiently close to some equilibrium state. The proof relies on a combination of energy estimates with suitable weighted Sobolev inequalities. We also show how our results can be extended to more general systems of conservation laws which are not necessarily strictly hyperbolic. In particular we consider the case where one of the characteristic speeds vanishes at a point but remains bounded away from zero elsewhere. \nIntroduction\n\nThe purpose of this work is twofold. First, we establish global regularity properties for the rapidly rotating shallow water system and its generalization to arbitrary dimension. Second, we provide new insights into the structure of these models by showing how they fit within a larger class of nonlinear dispersive partial differential equations. Our main result reads as follows: \nMain Theorem 1 Let u 0 \u2208 H s , s > n\/2 + 2, then there exist constants C = C(n), K = K(n) such that if","label":1,"model":"bloomz","source":"arxiv","id":3063}
{"text":"We introduce the concept of neutral networks, which are graphs that represent relationships between sequences and shapes in an unsupervised manner. We show how these can be used for shape recognition by training them on synthetic data generated using computer graphics techniques. The resulting network is able to recognize shapes with high accuracy even when they have been transformed or distorted. Finally we demonstrate our method on real world images showing its ability to generalize well beyond the training set. In this work we present a novel approach to learning about shapes through their relationship to sequences. Our main contribution is introducing the concept of neutral networks as a way to learn such relationships without supervision. Neutral networks are graphs whose nodes correspond to sequences (e.g., strings) and edges connect similar sequences. They provide a natural representation for capturing relationships between different types of information. For example, one could use a neutral network to capture relationships between sentences and documents, or between music notes and melodies. Here we focus on applying neutral networks to the problem of recognizing shapes based solely on their sequence of points sampled along their boundary. This task has many applications including medical image analysis where it may not always be possible to obtain ground truth labels due to privacy concerns.","label":1,"model":"bloomz","source":"arxiv","id":3064}
{"text":"We present new results on the kinematics and dynamics of the circumnuclear region (CNR) of NGC 1097, based on integral field spectroscopy with SINFONI at VLT\/UT4. We find that the CNR is dominated by two components: an inner disk-like structure extending to about 1 kpc radius, and a fast outflowing component along PA\u223c45\u00b0extending up to 5 kpc distance from the nucleus. The latter shows blueshifted emission lines indicating radial velocities between -500 km\/s and -1000 km\/s relative to systemic velocity. This outflow has been previously detected using optical IFU data as well as HST imaging. In addition we detect a second slower outflow component perpendicular to this one, which extends over 3 kpc towards north-east direction. Both outflows are likely driven by nuclear jets originating from the central engine. Using simple models for the gas distribution and kinematics, we estimate mass inflow rates into the CNR ranging from 0.1 M\u2299 yr-1 to several solar masses per year.","label":1,"model":"bloomz","source":"arxiv","id":3065}
{"text":"We report on an experiment to measure decoherence in a quantronium qubit using the cavity bifurcation amplifier (CBA). The CBA is used as a dispersive readout device for the quantronium, and we show that it can be operated at high bandwidths without significant back-action noise or added phase noise. We demonstrate measurement times up to 1 ms per data point while maintaining quantum-limited sensitivity. This allows us to perform measurements over many relaxation cycles of the system, which are necessary to extract information about its dynamics. In addition, we use this technique to study the effect of different environmental conditions on the coherence time of our system. Our results indicate that the CBA has great potential for future experiments studying open quantum systems. Quantum mechanics predicts that macroscopic objects such as atoms will exhibit wave-like behavior when they interact with light fields. However, these predictions have not been experimentally verified because measuring the properties of individual particles requires extremely sensitive detectors. Here, we present a new method for detecting single photons based on a nonlinear optical process known as parametric down-conversion. Using this approach, we detect single photons emitted by a trapped atom interacting with a laser field inside a high-finesse optical cavity. By coupling the output of the cavity into a second cavity containing a nonlinear crystal, we generate pairs of entangled photons whose frequencies differ by twice the frequency of the pump beam. These photon pairs are then detected simultaneously by two silicon avalanche photodiodes operating in Geiger mode.","label":1,"model":"bloomz","source":"arxiv","id":3066}
{"text":"We present high resolution X-ray imaging and spectroscopy of the central region of the galaxy cluster Abell S1063 (IC342). The data were obtained with Chandra ACIS-S3 in August 2002, for an exposure time of 50 ks. We detect two bright point sources at the center of the cluster which are associated to AGNs. Their luminosities range between 1043 erg\/sec and 1044 erg\/sec in the 0.5-10 keV energy band. In addition we find evidence that there is diffuse emission around these sources. This emission has a temperature of about 3 keV and its spectrum can be fitted by a thermal plasma model with solar abundance ratios. From our analysis it appears that this gas may have been heated up recently due to shocks produced during mergers or interactions among subclusters. Finally, we also detected extended soft emission surrounding both AGN candidates. Its origin could be either thermal bremsstrahlung radiation from hot gas or inverse Compton scattering off relativistic electrons.","label":1,"model":"bloomz","source":"arxiv","id":3067}
{"text":"We present the results on the interband, intraband and excitonic transitions for Si and Ge nanocrystals (NCs) embedded into Al2O3 matrix by using first-principles calculations based on density functional theory within local-density approximation. The NC size is varied between 1 nm to 5 nm with an interval of 0.5 nm. We find that the optical gap decreases as we increase the NC size due to quantum confinement effect. In addition, we observe that the lowest energy peak shifts towards higher energies when increasing the NC size which can be attributed to the surface states. Furthermore, our calculated results show that the oscillator strength increases significantly at smaller sizes while it decreases rapidly at larger sizes. Finally, we also investigate the influence of strain on the electronic structure of these systems. Our findings are expected to provide useful information about the design of optoelectronic devices such as solar cells or photodetectors. \n \n Keywords: Silicon Nanocrystal, Germanium Nanocrystal, Optical properties","label":1,"model":"bloomz","source":"arxiv","id":3068}
{"text":"The mechanical properties of hair are determined by its molecular structure, which is mainly composed of proteins called keratins. The main structural unit in keratins is an alpha-helical coiled coil that forms microfibrils with other similar units to form intermediate filaments (IFs). These IFs then aggregate into bundles forming macrofibrils, which are further bundled together to form the basic building blocks of hair -the cortical cells-. In this work we have studied how the mechanical properties change as these structures evolve during the process known as maturation or ageing. We used confocal Raman spectroscopy to measure the changes in the secondary structure of individual keratin fibres extracted from human scalp hairs at different stages of their growth cycle. Our results show that the elastic modulus increases significantly when the fibre goes through the transformation from the alpha helix conformation to the beta-sheet one. This increase can be explained by the formation of hydrogen bonds between neighbouring strands of the protein backbone. Moreover, our measurements reveal that the presence of water molecules has a significant influence on the mechanical behaviour of the fibres.","label":1,"model":"bloomz","source":"arxiv","id":3069}
{"text":"We present the discovery of a huge filamentary structure in the local universe (z = 0.55) using galaxy groups selected by their X-ray emission, which is traced by galaxies with redshifts measured by the Sloan Digital Sky Survey Data Release 7 (SDSS DR7). The total length of this filamentary structure is about 1 Gpc. We find that there are two peaks along its main axis; one peak lies around the Great Attractor region while another peak is located near the Perseus-Pisces supercluster. Using these galaxy groups we also investigate star formation histories of nearby galaxies. Our results show that most of the galaxies have experienced recent bursts of star formation activity within the last few gigayears. This suggests that the large-scale structures may play an important role on triggering such bursts. \n \n Keywords: Galaxy group, Large-scale structure, SDSS DR7, Xray","label":1,"model":"bloomz","source":"arxiv","id":3070}
{"text":"We present new space velocities for the nearest late-M, L-, and T-dwarf stars based on high-precision radial velocity measurements obtained with the HARPS spectrograph at La Silla Observatory in Chile over an 8-year period (2003-2009). We find that all but one of these objects are members of young nearby open clusters or associations. The only exception is LP 944-20 which has been shown to be a member of the Hyades supercluster by its proper motion.  These results confirm previous suggestions that most brown dwarfs have ages less than 100 Myr. They also show that there may exist two populations among the very low-mass stars and brown dwarfs: those associated with open clusters\/associations and those apparently not gravitationally bound to any cluster. This suggests that either some brown dwarfs form outside open clusters and\/or they evaporate rapidly after formation. Finally we discuss possible explanations for the observed kinematics of the lowest mass stars and brown dwarfs.","label":1,"model":"bloomz","source":"arxiv","id":3071}
{"text":"In this work, we propose an algorithm for constructing the posterior distribution of deformable models using stochastic approximation (SA). The proposed method is based on the use of Metropolis-Hastings within Gibbs sampling and SA to approximate the intractable likelihood function in order to obtain samples from the posterior distribution. We show that our approach converges under certain conditions. In addition, we demonstrate its performance by applying it to synthetic data as well as real medical images. Finally, we compare our results with those obtained by other methods such as variational Bayes and expectation propagation algorithms. \nKeywords: Bayesian inference, deformation model, image segmentation, convergence study, stochastic approximation algorithm. 1 Introduction Image segmentation has been widely studied over the past decades due to its importance in many applications including computer vision, pattern recognition, medical imaging analysis, etc.. Among various approaches developed so far, statistical modeling techniques have attracted much attention because they can provide more accurate solutions than deterministic ones [1] . Statistical modeling techniques are usually divided into parametric and nonparametric categories depending on whether or not there exists any prior knowledge about the underlying distributions [2] .\nThe most popular parametric technique is probably the mixture model which assumes each pixel belongs to one of several classes [3] , where the number of components needs to be specified beforehand [4] . However, the choice of the optimal number of components remains difficult since different numbers may lead to very similar results [5] . To overcome these difficulties, some researchers have recently introduced new types of mixture models [6] - [8] . For example, Banerjee et al. [9] presented a hierarchical Dirichlet process mixture model (HDP-M) which allows the number of clusters to grow automatically during the clustering procedure; while Ishwaran & Zarepour [10] proposed a finite normal mixture model (FNMM), which uses a truncated Gaussian density instead of a Dirac delta function to represent each cluster's probability mass function. Although both HDP-M and FNMM can avoid specifying the number of clusters manually, their computational complexity increases dramatically when dealing with high-dimensional problems [11] .","label":1,"model":"bloomz","source":"arxiv","id":3072}
{"text":"We present new high-precision photometric observations of the red giant star nu Indi, obtained with the Kepler space telescope over a period of three months (Q0-Q3). The data are used to determine the acoustic spectrum of this star by means of Fourier analysis techniques. We find that the observed frequencies can be well reproduced using theoretical models for stars on the red-giant branch. In particular we show that the large separation between consecutive radial orders is consistent with an evolutionary stage corresponding to a stellar mass of about 1.5 Msun. Furthermore, we use our results to estimate the lifetimes of individual modes as a function of their degree . Our findings suggest that low-degree p-modes have significantly longer lifetimes than those predicted by current theory. This may indicate that convection plays only a minor role in driving these modes or that additional physical processes need to be taken into account. \n \n Keywords: Red giants","label":1,"model":"bloomz","source":"arxiv","id":3073}
{"text":"We report on the continuous loading of $1^{{\\rm S}}_{0}_0$ calcium atoms in a magneto-optical trap (MOT) by using a Zeeman slower and a MOT with counterpropagating beams. The number density is measured to be $10^{11}~{\\rm cm}^{-3}$, which corresponds to about 10 million trapped atoms at room temperature. We also measure the lifetime of the trapped atoms as a function of the detuning between the cooling laser frequency and the atomic resonance frequency. This measurement shows that the lifetime decreases when the detuning increases due to spontaneous emission. Finally we study the dependence of this lifetime on the intensity of the trapping lasers. These results are compared to theoretical predictions based on Monte Carlo simulations. \nThe experiment was performed in the Laboratoire Kastler Brossel at Ecole Normale Sup\u00e9rieure de Paris. \n \n In recent years there has been considerable interest in studying cold atom physics both for fundamental reasons and because it offers new possibilities for applications such as quantum information processing [1] or precision measurements [2] . A key requirement for these experiments is to have large numbers of cold atoms available for manipulation [3] .\nIn this letter we present experimental data obtained during the continuous loading of $1^{\\mathrm {s}}_{0}\\mathrm {0}$ Ca atoms into a magnetooptical trap (M OT). Our setup consists of two stages : first, a Zeeman slower [4] , then a standard M OT [5] . The slowing process allows us to load up to 1\u00d710 9 atoms per second [6] . Afterwards, the atoms are cooled down to temperatures below 100 \u00b5K before being loaded into the M OT. The number density inside the M OT is measured to be around $10^{11}{\\rm cm}^{-3}$ corresponding to about 10 6 trapped atoms at room temperature [7 ] . \nTo determine how many atoms remain trapped after some time t, we use absorption imaging [8] . Figure 1(a) shows typical images taken after different times spent in the M OT. As can be seen, the cloud size grows over time mainly due to collisions between trapped atoms [9] . To extract the","label":1,"model":"bloomz","source":"arxiv","id":3074}
{"text":"We report on the detection by HESS of an exceptional flaring activity in the very-high-energy (VHE) gamma-ray band for the blazar PKS 2155-304, which was observed between September and November 2007 with a flux doubling time scale as short as ~1 day. The source reached its highest recorded state ever detected at TeV energies during this period. We present results based on data taken simultaneously with Swift-XRT and UVOT instruments that show no significant change in X-ray or optical emission properties over the course of the outburst. This suggests that the high-energy emission is produced via inverse Compton scattering off relativistic electrons located close to the central black hole rather than synchrotron radiation emitted by particles accelerated up to high Lorentz factors within the jet itself. These findings are consistent with theoretical models where particle acceleration takes place in shocks formed when jets collide with surrounding clouds. Keywords: Blazars, Very High Energy Emission","label":1,"model":"bloomz","source":"arxiv","id":3075}
{"text":"We study the stringy e-functions of smooth hypersurfaces in weighted projective spaces, which are defined by the generating functions for Gromov-Witten invariants with insertions of arbitrary genus zero descendant classes. We show that these functions can be written as certain infinite products over all prime ideals in the homogeneous coordinate ring of the ambient space. As an application we give explicit formulas for the stringy e-function of any smooth hypersurface in $\\mathbb{P}^n(\\mathcal{O}_{\\mathbb{C}})$ or in $\\mathbb{P}^n(t_1^{a_1}, ..., t_r^{a_0})$ (with $a_i \\geq 1$). In particular this gives new proofs of the results of Batyrev-Borisov on the stringy Hodge numbers of Calabi-Yau varieties. \nThe second part is devoted to the stringy e-functons of Brieskorn-Pham singularities. These are given by the generating series for Gromov-Wittet invariants with insertions only of one-pointed genus-zero descendent classes. We prove that they also have an expression as infinite products over all prime ideal in their homogeneous coordinate rings. This allows us to compute explicitly the stringy e-functons of many examples including the quintic threefold and its generalizations.","label":1,"model":"bloomz","source":"arxiv","id":3076}
{"text":"We present the results of our investigation into how stellar evolution models are affected by individual element abundances in stars, focusing on the sensitivity to changes in helium abundance (Y). We use two different sets of evolutionary tracks with varying Y values for masses between 0.8 M\u2299 and 8 M\u2299 at solar metallicity. The first set is based on the Padova code while the second one uses the Geneva code. For each track we calculate synthetic spectra using the SPECTRUM code. These synthetic spectra are then used as input to determine the best-fit parameters of observed high-resolution optical spectra of Galactic open clusters. Our analysis shows that both codes produce similar results when fitting these cluster data. However, there are significant differences in the derived ages depending on which code was used. This discrepancy can be explained by the fact that the Padova tracks have been calculated without convective overshooting whereas the Geneva tracks include this effect.","label":1,"model":"bloomz","source":"arxiv","id":3077}
{"text":"We present new results on the evolution of the dust content in Lyman break galaxies (LBGs) using deep near-infrared data obtained by the UltraVISTA survey, which is part of the Sloan Digital Sky Survey III program. We use these observations to study the rest-frame UV-optical properties of LBGs at redshifts 1<z<3.5. The main goal of this work was to investigate how the dust extinction evolves as a function of galaxy mass and star formation rate density over cosmic time. Our analysis shows that there are two different populations of LBGs: one population has low stellar masses (M*=10^10-10^11 Msun), high specific star formation rates (SSFR>100Gyr-1), and relatively small amounts of dust; while another population consists of more massive systems (M*>10^11Msun), lower SSFR values (SSFR<30Gyr-1), and higher levels of dust extinction. These findings suggest that the amount of dust increases with increasing galaxy mass for both local and distant galaxies.","label":1,"model":"bloomz","source":"arxiv","id":3078}
{"text":"We report on high-resolution observations with Herschel PACS (Poglitsch et al., 2010) and SPIRE (Griffin et al., 2010 ) that reveal for the first time the presence of cold dust emission at temperatures as low as T = 20 K in two nearby ultraluminous infrared galaxies (ULIRGs), Arp220 and Mrk 231. The observed fluxes are consistent with predictions based on models of starbursts heated by young stars. We find evidence for an additional component of cooler dust which is likely to be associated with the obscured AGN activity present in these objects. In addition we have detected the [CII] 158 \u00b5m line in both sources using PACS spectroscopy. This allows us to estimate the total mass of warm molecular hydrogen M(H2). For Arp 220 this amounts to 1.5 x 10^9 solar masses within a radius of 100 pc around the nucleus.","label":1,"model":"bloomz","source":"arxiv","id":3079}
{"text":"We present an analysis of the nonlocal phenomenology in magnetohydrodynamic (MHD) turbulence with strong magnetic field anisotropy, which is relevant to solar wind and space plasmas. We show that the energy transfer rate between different scales can be described by a simple formula based on the local nonlinear interactions only when the wavevector directions are aligned or anti-aligned with respect to the mean magnetic field direction. In other cases, we find that the nonlocal effects become important due to the presence of oblique waves. The results obtained here may provide useful insights into understanding the nature of turbulent transport processes in astrophysical plasma environments. Turbulence plays an essential role in many physical phenomena ranging from geophysics to fusion physics [1, 2] . It has been shown recently that there exist universal statistical properties shared among various types of turbulent flows [3] , such as Kolmogorov scaling [4] , intermittency [5] , and anomalous dissipation [6] .\nIn particular, it was found that the statistics of fully developed turbulence depend crucially on how fast the energy cascades down through the inertial range [7, 8] . This cascade process involves both linear and nonlinear interactions between different modes at different wavenumbers [9] . For example, in hydrodynamics, the energy flux \u03a0(k) \u2261< |\u03b4u k \u00b7 \u03b4u * \u2212k | 2 > \/ < u 2 k > depends not only on the magnitude of the wavenumber k but also its orientation relative to the large-scale flow [10] . Here, u k denotes the Fourier transform of velocity fluctuations at scale k \u22121 . When the angle \u03b8 = arccos[(k\u00b7v 0 )\/|k||v 0 |] between the wavevector k and the large-scale flow v 0 is small, i.e., \u03b8 \u226a 1, the energy flux \u03a0 \u221d k \u22122\/3 sin 2\/3 \u03b8 [11] . On the contrary, if \u03b8 becomes large, then \u03a0 decreases rapidly because of the cancellation effect [12] . Similar behaviors have been observed in magnetohydrodynamics (MHD), where the energy flux \u03a0 \ufffd","label":1,"model":"bloomz","source":"arxiv","id":3080}
{"text":"We present the spectral energy distributions for a sample of PG quasars observed with Spitzer Space Telescope at 3.6, 4.5, 5.8, 8.0, 24, 70, 160, 250, 350 ,and 500 microns. We fit these data using an updated version of our model which includes dust emission heated by both AGN and star formation activity in addition to stellar photospheric emission. Our results show that most of the infrared luminosity is due to dust heating by young stars rather than the central black hole. This result suggests that the majority of quasar hosts are actively forming stars during their active phase. Infrared observations can be used as powerful tools to study the physical properties of quasar host galaxies. Keywords: infrared, quasar, SEDs, evolution, starburst, galaxy merger, AGN feedback, IRAC, MIPS, IRS","label":1,"model":"bloomz","source":"arxiv","id":3081}
{"text":"We report on the growth and characterization of epitaxial La2\/3Ca1\/3MnO3-\u03b4 (LCMO) thin films grown on SrTiO3(001) (STO). The LCMO film thickness was varied between 5 nm to 50 nm, while keeping the substrate temperature fixed at 700 \u00b0C during deposition. We find that for thinner films there is an increase in oxygen deficiency as measured by XPS and XAS. This leads to a decrease in Curie temperature TC = 240 K down to 180 K with decreasing film thickness. In addition we observe a reduction in magnetization M0 below 20 nm which can be explained by the presence of a ferromagnetic dead layer near the interface. Using X-ray magnetic circular dichroim (XMCD), we show that this ferromagnetic dead layer has a net moment along the out-of-plane direction but no in-plane component. Finally, using soft x-ray resonant reflectivity measurements we demonstrate that the Mn valence state changes across the interface due to charge transfer into STO.","label":1,"model":"bloomz","source":"arxiv","id":3082}
{"text":"We study the dynamics of hard-core bosons on an optical lattice with random disorder and nearest-neighbor hopping, using exact diagonalization techniques. We find that there is a crossover between two different regimes as we increase the strength of disorder. In one regime (weak disorder), the system shows Anderson localization behavior; while in another regime (strong disorder) it exhibits Bose glass behavior. The transition point depends strongly on the filling fraction of particles per site. For low fillings, this transition occurs at relatively small values of disorder strengths. However, for higher fillings, the transition to the Bose glass phase takes place only when the disorder becomes very strong. This suggests that the presence of interactions can significantly affect the nature of the ground state of the system even if they are weak compared to other energy scales such as the bandwidth or the disorder strength. \n \n Introduction \n \n Disorder plays an important role in determining many properties of condensed matter systems. It has been shown recently that disorder can lead to interesting phenomena like quantum Hall effect [1] , metal-insulator transitions [2] , and superconductivity [3] . One of the most studied models which incorporates both disorder and interaction effects is the so-called Anderson model [4] . In its simplest form, this model describes non-interacting electrons moving through a disordered medium. Although the original formulation was restricted to electronic degrees of freedom, it has also been extended to describe various physical situations involving interacting particles [5] - [8] .\n \nIn recent years, ultracold atoms have emerged as promising candidates for simulating complex quantum mechanical problems [9] - [11] . These experiments provide us with unprecedented control over all relevant parameters of the problem under consideration [12] - [14] . Moreover, these systems allow us to explore new physics beyond what is possible in conventional solid-state materials [15] - [17] . Ultracold atomic gases trapped in optical lattices offer unique opportunities to investigate the interplay between disorder and interactions [18] - [20] . Recently, several experimental groups [21] - [23] have observed signatures of Anderson localization [24] in cold atom systems by studying the transport properties of the gas across the lattice.","label":1,"model":"bloomz","source":"arxiv","id":3083}
{"text":"We study central limit theorems for estimators of parameters in linear regression models where errors are not necessarily normally distributed but have an elliptical distribution, and we allow some explanatory variables to be non-normal. We show that under suitable conditions on the model coefficients, the asymptotic distributions of these estimators can be approximated by those obtained when all the explanatory variables follow a multivariate normal distribution. The results are illustrated through simulation experiments. Keywords: Central Limit Theorem; Elliptical Distributions; Regression Modeling. 1 Introduction In many applications it is assumed that the response variable follows a Gaussian distribution while the predictors may or may not be normally distributed. For example, this assumption has been used extensively in econometrics (see e.g., Greene [2003] ). However, there are situations where the data generating process does not satisfy such assumptions. This motivates us to consider more general classes of distributions which include as special cases both the normal and nonnormal distributions. One class of distributions that includes most common probability density functions encountered in practice is given by the so-called elliptical distributions. These distributions were introduced independently by Kelker [1970] , H\u00fcsler and Reiss [1981] , and Fang et al. [1987] . They are characterized by their dependence structure rather than their marginal densities. A random vector X = (X1, ..., Xd)T \u2208 Rd belongs to the family of elliptical distributions if its characteristic function satisfies E[exp(itX)] = exp{\u2212V (t)},\nwhere V : R \u2192 [0, \u221e) is called the characteristic generator. If V \u2261 0 then X is said to belong to the family of spherical distributions. Examples of elliptical distributions include:","label":1,"model":"bloomz","source":"arxiv","id":3084}
{"text":"We study how mergers affect the kinetic SZ effect in galaxy clusters, and show that they can significantly enhance it by up to an order of magnitude at intermediate redshifts (0 < z < 1). We find that this enhancement is due to the fact that mergers increase both the gas temperature and density profiles within the cluster core region. The latter leads to higher values of the Comptonization parameter y, which describes the strength of the SZ effect. In addition we demonstrate that mergers also lead to significant changes in the shape of the pressure profile inside the cluster core. This results in a change of sign of the integrated SZ flux decrement, which becomes positive during merger events. Finally, we discuss possible observational signatures of these effects using current data sets as well as future surveys such as Planck. Galaxy clusters are known to be dynamically active systems with frequent mergers between sub-clusters or even individual galaxies. These mergers have been shown to produce observable effects on various physical properties of galaxy clusters including their X-ray luminosity, temperature distribution, optical morphology, and gravitational lensing mass measurements. However, little attention has so far been paid to the impact of mergers on the kinetic SZ effect produced by galaxy clusters. Here we present detailed numerical simulations of galaxy clusters undergoing major mergers, and investigate how mergers influence the kinetic SZ effect. Our main findings are:","label":1,"model":"bloomz","source":"arxiv","id":3085}
{"text":"We propose that the dim radio-quiet neutron star, 1E1207.4-5209 (hereafter E1207), is an evolved object with its surface temperature T = 2 \u00d7 10 6 K and radius R = 11 km. The observed X-ray luminosity Lx ~ 3\u00d71033 erg s-1 can be explained by the decay energy of radioactive nuclei produced in the core collapse supernova explosion at t ~ 1s after the birth of the progenitor star. We show that the total amount of 56Ni synthesized during the explosion is about 0.1M\u2299 which agrees well with the theoretical prediction for massive stars. In addition to the nuclear burning, we consider the effects of magnetic field decay due to ohmic dissipation as well as gravitational wave emission. It turns out that these processes are not important enough to explain the present state of E1207 within our lifetime. However, if the initial magnetic field strength was larger than 1013 G, it could have been reduced down to the current value of 5\u00d71012 G through ohmic dissipation before the age of the universe reached 13Gyrs.","label":1,"model":"bloomz","source":"arxiv","id":3086}
{"text":"We present results on INTEGRAL observations of the X-ray burster (XRB) KS 1741-293, which is located in the globular cluster NGC 6388 at a distance of about 12 kpc. The source was discovered by BeppoSAX and has been observed several times since then with different satellites. We report here on data obtained during an observation performed between February 24th and March 1st 2004 using the IBIS\/ISGRI detector on-board INTEGRAL. During this time we detected two bursts from the source. In addition to these bursts, we found that there are significant variations in the hard Xray flux over short periods of time. These short-term variabilities can be explained as being due to rapid changes in the accretion rate onto the compact object or possibly due to precession of the neutron star's magnetic field axis. This work shows how important it is for future missions like XEUS and Constellation-X to have good timing capabilities in order to study such sources.","label":1,"model":"bloomz","source":"arxiv","id":3087}
{"text":"We report on saturation effects observed in the sub-Dopplerbroadened absorption spectrum of cesium atoms confined to a cell with thickness L = 0.5 mm and length l = 1 cm, which is much thinner than the Doppler width (~30mm). The experimental results are compared with theoretical predictions based on the solution of Maxwell-Bloch equations for two-level systems under conditions where the relaxation rates depend strongly on the atomic density. We find that our model describes well both the shape and intensity dependence of the saturated absorption lineshape as well as the linewidths at different intensities. Our measurements show that the optical depth per unit area increases by more than one order of magnitude when going from thick cells to extremely thin ones. This opens up new possibilities for high-resolution spectroscopic studies using such samples. \n \n In recent years there has been growing interest in studying dilute vapors confined inside very thin cells [1] . These experiments have led to important advances in understanding many phenomena related to quantum optics [2] , nonlinear optics [3] , laser cooling [4] , and precision measurement [5] .\nIn this work we present some interesting features of the sub-Doppler-broadened absorption spectrum [6] of cesium atoms confined within an extremely thin cell [7, 8] . Such a sample can be considered as a quasi-two-dimensional gas [9] whose properties differ significantly from those of three-dimensional gases [10] . For example, it was shown recently [11] that the relaxation rate \u03931 of the excited state population depends strongly on the atomic density n0 due to dipole-dipole interactions between neighboring atoms [12] . As a result, the effective homogeneous broadening of the transition becomes dependent on the number N of atoms contained in the probing beam volume V [13] : \n \n \u0394\u03bdeff ~ N\/V \n\n\nwhere \u0394\u03bdD is the Doppler width associated with the thermal motion of the atoms along the direction perpendicular to the probe beam axis. It follows then that the optical depth per atom OD\/N also varies with the number of atoms in the probing region:","label":1,"model":"bloomz","source":"arxiv","id":3088}
{"text":"We present the results of our study of correlated bosonic systems in one dimension using dynamical mean field theory (DMFT). We consider both the case where all particles are condensed into a single state, as well as that with no condensation at all. In particular we focus on the effect of interactions between the bosons on their dynamics. \n \n The model is defined by the following Hamiltonian: \n \n \n\n H = \\sum_{i < j} \\Sigma_{ij} b_i^\\dagger b_j + V(b_i) - \\mu \\sum_{i} b_i^\\dagger + \\mu \\sum_{i} b_i,\n \nwhere $\\mu$ is an external chemical potential which controls the number of particles in the system. Here $b_i$ denotes the annihilation operator for a particle in site $i$, while $\\Sigma_{ij}$ describes nearest neighbor hopping. Finally $V(b_i)$ represents the interaction energy between two bosons occupying neighboring sites.","label":1,"model":"bloomz","source":"arxiv","id":3089}
{"text":"We present the results of our analysis of the photometric data obtained by the Advanced Camera for Surveys (ACS) on board HST in the F606W and F814W bands during Cycle 12 as part of program GO-10775. The survey consists of deep imaging observations of 16 globular clusters with metallicities ranging between [Fe\/H] = -2.2 to -0.7. We have used these data along with archival WFPC-2 images taken under programs GO-5269 and GO-6366 to study the properties of horizontal branch stars in each cluster. \n \n In this work we use theoretical stellar evolution tracks, isochrones, luminosity functions, and synthetic HB models to determine ages, reddenings, distances, helium abundances, and mass loss rates for all sixteen clusters studied here. Our main conclusions are summarized below: \n \n \n \n 1. Ages - We find that most of the clusters analyzed here appear younger than previously thought based upon their location relative to the fiducial ridge line defined by the Milky Way's old open clusters. This result suggests that either the age scale derived using open clusters may be systematically too young or that there has been significant dynamical evolution within many of the clusters since they formed. \n \n 2. Reddening - We find evidence for differential reddening across several of the clusters studied here. However, it appears that the majority of the clusters do not suffer from large amounts of differential reddening. For those clusters where we can measure individual reddenings for different populations of stars, we find no systematic differences between the values determined for blue stragglers versus normal giants. These results suggest that any differential reddening affecting these clusters must occur over scales smaller than the typical size of an open cluster. \n \n 3. Distances - Using the absolute magnitudes of RR Lyrae variables observed in each cluster, we derive distance moduli which agree well with previous estimates made using other methods such as main sequence fitting. We also compare the mean magnitude of the RGB bump in each cluster to predictions made using synthetic HB models. While some","label":1,"model":"bloomz","source":"arxiv","id":3090}
{"text":"We demonstrate that it is possible to generate all four Bell states in one single nonlinear crystal by using two pump beams with orthogonal polarizations and slightly different wavelengths, which are generated via second-harmonic generation (SHG) inside an optical parametric oscillator (OPO). The OPO consists of a periodically poled lithium niobate (PPLN) crystal as nonlinear medium and a concave mirror for cavity feedback. We show experimentally that this approach allows us to obtain high-visibility quantum interference between photons emitted at degenerate wavelength pairs across the entire PPLN acceptance bandwidth. This method can be used to simplify future experiments on continuous-variable entanglement distribution over large distances. \n \n Quantum information processing requires the ability to create and manipulate entangled states of light. In particular, the Bell state measurement plays a key role in many applications such as teleportation or quantum repeaters [1] . However, generating these highly nonclassical states is challenging because they require indistinguishable photon pairs [2] , which cannot be produced deterministically [3] .\nIn recent years, several approaches have been developed to overcome this problem [4] . One possibility is based on spontaneous parametric down-conversion (SPDC), where a pump beam creates correlated pairs of signal and idler photons [5] . By adjusting the relative phases of the pump fields [6] , it has become possible to produce any desired superposition of the four Bell states [7, 8] . Another option uses squeezed vacuum states [9] or displaced number states [10] instead of coherent laser pulses [11] . These methods allow for efficient generation of entangled states but usually suffer from low visibility due to imperfections [12] .","label":1,"model":"bloomz","source":"arxiv","id":3091}
{"text":"We present an analysis of gravitational lensing by large-scale structure in the universe, as traced by neutral hydrogen (HI) at high redshifts z > 6. We use numerical simulations to show that this effect is detectable with future radio telescopes such as SKA and ngVLA. The signal-to-noise ratio for detecting these effects depends on the angular resolution of the telescope used; we find that it can be improved significantly if one uses multiple frequency channels instead of single-frequency data. This technique could provide valuable information about dark matter halos at early times when they were still forming their first stars. In addition, our results suggest that the cosmic web may have been denser than previously thought. Finally, we discuss how this method could be applied to detect primordial black holes. Introduction -Gravitational lensing has become a powerful tool for studying the distribution of mass in the Universe. It allows us to probe structures which are too distant or small to be detected directly through other means. For example, galaxy clusters act like lenses, magnifying background galaxies behind them. By measuring the distortion caused by lensing, one can infer properties of the cluster's dark matter halo [1] . Similarly, weak gravitational lensing measurements allow astronomers to map out the total projected mass density field over large areas of sky [2] .\nIn recent years there has been growing interest in applying gravitational lensing techniques to study high-redshift objects [3] , including the epoch of reionization [4] . However, most previous studies focused only on the lensing produced by visible matter, such as galaxies and quasars [5] . Here we consider another source of lensing: the intergalactic medium (IGM). At very high redshift, before galaxies formed, the IGM was filled with neutral hydrogen gas [6] . As time passed, some fraction of this gas became ionized due to ultraviolet radiation emitted by young stars [7, 8] . But even today, much of the IGM remains neutral [9] . Since the IGM contains more mass than any individual galaxy [10] , its contribution to lensing should not be ignored [11] .\nThe goal of this","label":1,"model":"bloomz","source":"arxiv","id":3092}
{"text":"We study the concentration-mass relation for dark matter haloes in cosmological N-body simulations with different initial conditions and resolutions, focusing on the dependence on halo mass and redshift. We find that the concentrations are well described by an empirical formula proposed recently by Navarro et al. (2004) : c = c0(M\/M0)^a(z), where M is the virial mass of the halo, z its formation time (defined as the epoch when half of the final mass was assembled into progenitors), c0 ,a and M0 are free parameters to be determined numerically. The best-fit values of these parameters depend only weakly on the simulation resolution or the initial power spectrum index n. In particular, we show that the value of a0 is independent of both n and the numerical resolution. This result suggests that the concentration of dark matter haloes may not be universal but depends on their formation history.","label":1,"model":"bloomz","source":"arxiv","id":3093}
{"text":"We study the profinite complex associated to a family of smooth projective curves over a field k with fixed genus g>0. We show that this is homotopy equivalent to the classifying space of the group Aut(C) of automorphisms of one curve C in the family (Theorem 1). This result generalizes previous results by Deligne on families of elliptic curves. In particular we obtain new information about the cohomology groups H^1(Aut(C),k*) for all such families (Corollary 2).\nIn section 3 we use our results to prove some basic facts about the anabelian geometry of the moduli stack M_g of stable curves of genus g. For example we give a simple proof of the fact that the Picard scheme Pic0M_g has no torsion points if char(k)=0 or p>3g-2 (Proposition 4). The main tool used here are the results obtained in sections 1-2 combined with Grothendieck's comparison theorem between \u00e9tale fundamental groups and Galois groups.","label":1,"model":"bloomz","source":"arxiv","id":3094}
{"text":"The magnetic properties of carbon nanotubes and fullerenes have been studied extensively, but the effect on their electronic structure is still not well understood. In this work we use density functional theory to investigate how the presence of an external magnetic field affects the electronic structure of single-wall zigzag (n = 3) and armchair (n = 4) nanographenes with different sizes. We find that for small n=3 or n=4 nanographenes there are no significant changes in the band gap when applying a magnetic field up to 10 T along the tube axis. However, as the size increases, the energy gaps decrease significantly under applied fields due to the formation of Landau levels. The results show that the magnetic response depends strongly on the chirality of the nanographene. For example, while the zigzag tubes exhibit diamagnetic behavior at low fields, they become paramagnetic above 1 T. On the other hand, the armchair tubes always display paramagnetic behavior even at very weak fields.","label":1,"model":"bloomz","source":"arxiv","id":3095}
{"text":"We present an analysis of the Bondi accretion rate onto primordial black holes (PBHs) during the radiation-dominated era, taking into account the effects of pressure and viscosity on the gas flow. We find that for PBH masses M \u226b 10^(10) g, the Bondi radius is much larger than the Schwarzschild radius, so that the standard Bondi-Hoyle-Lyttleton formula can be used to calculate the accretion rate. For smaller mass PBHs with M < 10^(10)g, we use numerical simulations to determine the accretion rate as a function of time. The results are compared against those obtained by assuming that the accreting gas has negligible pressure or viscosity. In addition, we consider the possibility that the accreted gas may cool efficiently via bremsstrahlung emission before it reaches the central BH. Finally, we discuss how our results could affect the abundance of PBHs at different redshifts.","label":1,"model":"bloomz","source":"arxiv","id":3096}
{"text":"We present here an analysis of the X-ray afterglow data for GRB 970508, which is one of the most intense bursts ever observed in gamma rays and has been extensively studied at all wavelengths since its detection on May 8th 1997 by BeppoSAX WFC (Costa et al., 1997) . We have analyzed the temporal behavior of this source using both the PDS method developed by Norris & Bonnell (2006) and the standard power law decay model. The results obtained with these two methods are consistent within their errors. \n \n In particular we find that the light curve can be fitted well with a broken power law function with indices \u03b11 = 1.2 \u00b1 0.3 , \u03b12 = 2.0 \u00b1 0.4 and break time tbreak = 3.6 +1.7 \u22121.5 \u00d7 10 4 s . This result confirms previous findings based on optical observations made by GCN observers (Fox et al. , 2000 ; Harrison et al. , 2001 ) . \n \n \n \n We also report the first measurement of the spectral index \u03b2 of the X-ray afterglows of GRBs performed with Swift\/XRT. Using our best fit values for the temporal parameters we obtain \u03b2 = \u22120.9 +0.8 \u22120.7 . \n \n Finally we compare our results to those previously reported in literature and discuss possible interpretations of them.","label":1,"model":"bloomz","source":"arxiv","id":3097}
{"text":"We present new photometric and spectroscopic observations of the classical nova EX Hya made in October 2005, when it was still bright (V = 8 mag). The light curve shows that EX Hya is an intermediate polar with a period of P orb = 0.084 d. We find evidence for two accretion regions on the white dwarf surface which are responsible for the double-peaked emission lines observed during outburst. In addition to these features we also detect narrow absorption components at velocities up to -1500 km s-1 . These absorptions may be caused by material ejected during previous eruptions. Our results show that EX Hya has returned to quiescence after its latest eruption in September 2002. Classical novae have been known since antiquity but their underlying physics remains poorly understood. They are believed to result from thermonuclear runaways triggered by unstable nuclear burning on the surfaces of white dwarfs (WD) in close binary systems. However, there remain many open questions about how this process takes place and what happens afterwards. One such question concerns the nature of the WD magnetic field. It is generally accepted that the WD magnetic field plays a key role in determining whether or not a system will undergo a thermonuclear runaway. If the WD magnetic field is too weak then no runaway occurs; if it is strong enough then the WD can become fully convective leading to stable hydrogen burning and hence preventing any further outburst activity. This picture is complicated however by the fact that some WDs do exhibit periodic outbursts despite having fields thought to be too weak to prevent them becoming fully convective. Such objects are called Intermediate Polars (IPs), where the WD magnetic field is sufficiently strong to synchronise the spin periods of both stars but does not disrupt the flow of matter onto the WD.","label":1,"model":"bloomz","source":"arxiv","id":3098}
{"text":"Epsilon Aurigae is an F-type main sequence star with a mass of 1.8 M\u2609 and radius 2 R\u2609, located at about 40 light-years away in the constellation Auriga.  It has been known for many years to be surrounded by dusty material that obscures its visible spectrum.   The infrared excess emission detected around this object suggests it may have a circumstellar disk similar to those found around young stars such as T Tauri or Herbig Ae\/Be stars.   In addition, there are indications that the system contains a close companion which could also contribute to the observed infrared excess emission.    We present new photometric observations obtained using the United Kingdom Infrared Telescope (UKIRT) on Mauna Kea over the period 1997-2001 covering wavelengths between 0.9-2.5 microns.  These data show significant variations in both the near-infrared fluxes and colours of the central source consistent with changes in the amount of dust surrounding the star.  This behaviour is very similar to what is seen in other pre-main-sequence systems where accretion onto the central star causes periodic increases in luminosity accompanied by increased levels of reddening due to heating of the surrounding dust grains.   Our results suggest that the current level of activity in the system is relatively low compared to previous epochs but we cannot rule out the possibility that the recent increase in brightness was caused by a short-lived burst of enhanced accretion rather than steady-state accretion occurring throughout our observing campaign.","label":1,"model":"bloomz","source":"arxiv","id":3099}
{"text":"We present the results of an optical survey for high redshift galaxies in the field surrounding the radio galaxy PKS 1138-262 (z = 3.9). We have detected over 100 candidate Lyman-alpha emitting galaxies with redshifts between 2 and 5, including several new spectroscopically confirmed members of this cluster. The spatial distribution of these objects is consistent with that expected if they are located within a single dark matter halo centered on the radio source. This result suggests that clusters may be identified by their diffuse emission as well as individual member galaxies. In addition to confirming the existence of a massive cluster around PKS 1138-262 we find evidence for two other overdensities of Lyman-alpha emitting sources near the line-of-sight to the radio source. These structures could represent additional clusters or proto-clusters which will evolve into richer systems like those found today. Finally, our data suggest that there exists a large population of faint Lyman-alpha emitting objects whose properties are similar to those observed locally but whose number density increases rapidly towards higher redshifts.","label":1,"model":"bloomz","source":"arxiv","id":3100}
{"text":"We study the effect of radiative transfer (RT) on ultraviolet pumping of the 21 cm line at high redshifts, using cosmological hydrodynamic simulations with RT and without it. We find that RT can significantly enhance the strength of the 21 cm signal by up to an order of magnitude compared to calculations neglecting RT effects. The enhancement is caused mainly by Lyman-alpha photons produced inside galaxies which are absorbed outside them due to scattering off neutral hydrogen atoms. This leads to additional heating of the intergalactic medium through photoionization heating and Compton cooling. In addition we show that the inclusion of RT also changes the shape of the power spectrum of the 21 cm brightness temperature fluctuations. Our results suggest that future radio telescopes such as SKA will be able to detect this signal if they have sufficient sensitivity. Keywords: Hydrogen, Radiation transfer, Power Spectrum, Cosmic Dawn","label":1,"model":"bloomz","source":"arxiv","id":3101}
{"text":"We present new constraints on warm dark matter (WDM) models by combining the results of two recent surveys for gravitationally lensed quasars, SDSS and CFHTLS Wide. We find that the observed number density of lenses is consistent with predictions based on cold dark matter simulations but inconsistent at more than 3 sigma confidence level if we assume a standard thermal relic WDM model with mass mX = 1 keV. This result suggests either that the current WDM scenario needs to be modified or that there are other systematic effects which have not been taken into account in our analysis. The full text can be found at: http:\/\/arxiv.org\/abs\/astro-ph\/0604070v1.pdf . \nThe existence of dark matter has now been established beyond reasonable doubt through its gravitational influence on visible matter. However, despite decades of research, little else about this mysterious substance is known. In particular, it remains unclear whether dark matter consists of one particle species only - as assumed in most theoretical studies -or whether it comprises several different particles. One possibility is that dark matter consists of weakly interacting massive particles (WIMPs), such as neutralinos predicted within supersymmetric extensions of the Standard Model [1] .\nIn order to test these scenarios observationally, astronomers look for signatures of dark matter in astrophysical objects like galaxies [2] , clusters [3] and quasars [4] . A particularly promising method involves searching for gravitationally lensed systems [5] where light rays emitted by distant sources bend around intervening dark matter halos [6] . If dark matter consists of WIMPs then their masses should lie between 10 GeV\/c 2 and 100 TeV\/c 2 [7, 8] . For example, the recently discovered galaxy cluster Abell 2218 [9] may contain a halo made up entirely of WIMPs [10] .","label":1,"model":"bloomz","source":"arxiv","id":3102}
{"text":"We have used the Hubble Space Telescope (HST) to study the lensing effect on background quasars by foreground galaxies associated with Mg II absorption systems at 0.4 <z abs < 2.0 in order to determine their masses and sizes. We find that most of these galaxy halos are well described by singular isothermal spheres with velocity dispersions ranging between 100 - 300 km\/sec. The mean mass density within one effective radius for our sample is $\\rho_e = 1.5 \\times 10^{20} \\ M_{\\odot}\/\\text{Mpc}^{3}$ which corresponds to an average halo circular velocity of $220\\ \\mathrm{km\/sec}$. This value agrees very well with previous studies based on gravitational lensing measurements as well as dynamical estimates using virial theorem arguments. Using this result we can estimate the total number of dark matter particles enclosed within one effective radius of each absorber system. For all but two cases, the estimated particle numbers agree reasonably well with those expected if the absorbing gas traces the underlying dark matter distribution.","label":1,"model":"bloomz","source":"arxiv","id":3103}
{"text":"We present new observations with the High Sensitivity Array (HSA) at 1.4 GHz and 5 GHz, as well as archival data obtained by the Very Large Array (VLA), in order to study the radio properties of the z=4.4 quasar BRI1335-0417. We detect two compact components separated by 0.3 arcsec on both sides of the optical position of this source. The total flux density is S5GHz=(1.6 \u00b10.2) mJy for component A and S1.4Ghz=0.7\u00b10.1 mJy for component B. Both sources are unresolved within our beam size. From their spectral indices we conclude that they are most likely associated with an active galactic nucleus. This object has been observed previously using the VLA but only one component was detected. Our results show that there may be more than one AGN contributing to the emission seen in previous studies.","label":1,"model":"bloomz","source":"arxiv","id":3104}
{"text":"We report on an analysis of archival Chandra data for the central region of the nearby starburst galaxy M82 (NGC 3034). We find that there are two bright, point-like sources in this field which have been previously identified as ULXs (Ultra-Luminous X-Ray Sources) by Swartz et al. (2004) . The first source is located at RA = 12 h 54 m 55 s .6 and Dec = 69\u00b059'45'' with a count rate of 1.1 x 10^-3 counts sec-1. This source has a luminosity of 2 x 10^39 erg\/sec assuming it lies at 8 kpc distance. The second source is located at RA=12h54m55s.7 and Dec=69\u00b059'46'' with a count rate 0.9 x 10^-3 countssec-1. It also has a luminosity of about 2 x 10^39erg\/sec if it lies at 8kpc. Both these sources appear to be variable over timescales ranging between hours and days.  These results suggest that both sources may contain black holes accreting close to their Eddington limit.","label":1,"model":"bloomz","source":"arxiv","id":3105}
{"text":"We study the entropy of iterated function systems (IFS) with respect to their invariant measures, which are known as holonomically generated probability distributions.  We show that these entropies can be computed by solving certain variational problems involving convex functions on vector spaces over finite fields.   In particular we prove that if an IFS has positive topological entropy then its holonomically generated measure is unique up to scaling.    The results presented here generalize previous work in this area due to Rokhlin, Kolmogorov-Sinai, Abramovich-Karandikar, and others. 1 Introduction Iterated Function Systems (IFSs), introduced by Hutchinson [Hut81] , have been studied extensively since they were first proposed more than thirty years ago [Whi84] . They provide a powerful tool for modeling many phenomena arising in applied mathematics such as fractals, computer graphics, image processing, and computational biology [Kea98] .\nIn recent years there has been renewed interest in studying properties of IFSs because of their connections to quantum mechanics and statistical physics [BH02] .  For example, it was shown recently that the spectrum of the Laplacian operator associated with a self-similar set coincides with the spectrum of the transfer operator corresponding to the IFS generating the self-similar set [FJ03] . This connection between IFS theory and spectral geometry has led to new insights into the structure of fractal sets [FJ04] .\nThe most basic object associated with an IFS is the attractor, defined as the unique non-empty compact subset A of some metric space X satisfying  \\(\\displaystyle \\bigcap _{i}B_{i}^{(A)}=1\\) , where B^{(A)} denotes the open ball centered at A with radius r>0. If the map F:X\u2192X satisfies the contraction condition d(Fi(x), Fi(y))\u2264\u03bbd(x, y) (1)\nfor all i\u2208{1,...,N} and x, y \u2208 X, then the attractor A is called a \u03bb-contraction or simply","label":1,"model":"bloomz","source":"arxiv","id":3106}
{"text":"We report on four new quasars at redshifts z > 6, found in the Canada France Hawaii Telescope Legacy Survey (CFHTLS). The objects were selected as part of an ongoing survey for high-z quasars using photometric data obtained with CFHT and Spitzer Space Telescope. We present their optical to near-infrared SEDs, which are well fitted by composite quasar templates. Their luminosities range between 1.5 x 10^14 erg s-1 cm-2 and 2.1 x 10^15 erg s-1 cm-2 . These results show that there is still room for discovering very luminous quasars beyond redshift six. They also provide further evidence that supermassive black holes grew rapidly during this early phase of galaxy formation. Four quasars have been discovered at redshifts greater than 6 in the Canada France Hawaii telescope legacy survey (CFHTLS) by combining deep infrared observations taken with the Spitzer space telescope with optical data collected with the Canada France Hawaii telescope.","label":1,"model":"bloomz","source":"arxiv","id":3107}
{"text":"We present the results for the one-loop amplitudes in N=8 supergravity, which are obtained by using the spinor helicity formalism with dimensional reduction to four dimensions.  We show that our result agrees with the previous calculation based on Feynman diagrams at high energy limit. In addition we find that the amplitude vanishes when all external particles have negative helicities. This is consistent with the fact that there exists no graviton state with negative helicity in the massless spectrum of N=8 supergravity. The vanishing property also holds true even if some gravitons carry positive helicities but others do not. \nThe organization of this article is as follows. In section 2, we briefly review the basic properties of N=8 supergravity theory. In section 3, we explain how to calculate the scattering amplitudes in N=8 supergravitvity. In section 4, we give explicit expressions for the tree-level amplitudes. In sections 5-8, we present the results for the 1-loop amplitudes. Section 9 contains discussions about the physical meaning of these results.","label":1,"model":"bloomz","source":"arxiv","id":3108}
{"text":"The radial distribution of the inner magnetospheric (IM) plasma pressure is studied by analyzing the magnetic field and particle measurements made onboard two satellites at low altitude in the equatorial plane for an intense geomagnetic storm that occurred between March 1-5, 1982 . The results show that there are significant differences in the IM plasma pressure profiles obtained with different satellites. In particular, the pressure profile derived from GEOS-1 observations shows a sharp peak near L = 3 , while those inferred from ATS-6 and GEOS-2 data exhibit much broader peaks around L = 4 . These discrepancies may be due to the fact that these satellites have different orbits which sample different regions of space. It is also found that the pressure profiles inferred from the three satellites agree well when they are shifted outward along the L-shell coordinate system. This suggests that the observed difference in the pressure profiles can be attributed mainly to the spatial variation of the pressure rather than temporal changes.","label":1,"model":"bloomz","source":"arxiv","id":3109}
{"text":"We present the regularized version of two popular families of evolution codes for general relativity, namely those based on the BSSN formulation and those using the moving puncture approach. The new schemes are designed to be used with any gauge conditions that can be written as first-order differential equations. We show how these schemes can be implemented into existing codes by adding only one extra equation per time step. In addition we provide an explicit expression for the source term appearing in this additional equation which is valid for all gauges considered here. Finally, we demonstrate the performance of our scheme numerically by evolving several test problems including black hole spacetimes containing apparent horizons. Our results indicate that the new scheme performs well even when evolved over many dynamical timescales. This work was supported by NSF grant PHY-0456728 (J.M.) and NASA grants NNG06GH50G and NNX07AG77G (A.S.).","label":1,"model":"bloomz","source":"arxiv","id":3110}
{"text":"We propose that gamma-ray bursts (GRBs) are the result of collisions between primordial black holes and stars in globular clusters, which occur at rates predicted by current models for GRB production.  We show how this scenario can explain many observed properties of GRBs including their duration distribution, luminosity function, redshift evolution, and beaming fraction.   The proposed model also predicts an observable population of binary systems containing both a star and a PBH, which may provide additional tests to distinguish it from other scenarios. Gamma-ray bursts (GRBs; see Figure 1 ) are intense flashes of high-energy radiation lasting only milliseconds up to several minutes [1] . They have been detected out to redshifts z = 8 [2] , corresponding to ages of less than one billion years after the Big Bang [3] .\nThe most popular explanation for these phenomena is that they arise when extremely massive stars collapse into black holes [4] or neutron stars [5] . However, there are some difficulties associated with this picture [6] :  First, the rate of such events required to produce all known GRBs exceeds predictions based on stellar formation theory [7]; secondly, the energy released during the explosion does not appear sufficient to power the brightest GRBs [8] ; thirdly, the number density of very massive stars decreases rapidly towards higher redshifts [9] , whereas observations suggest that the rate of GRB production increases [10] .  Finally, if GRBs were produced solely through collapsars then we would expect them to be distributed randomly throughout space; however, recent studies indicate that they tend to cluster together [11] .\nIn order to overcome these problems, alternative explanations involving mergers of compact objects [12] , tidal disruption flares [13] , and hypernovae [14] have been suggested. In addition,...","label":1,"model":"bloomz","source":"arxiv","id":3111}
{"text":"We study the influence of an external magnetic field Bext on the dynamics of a central spin S0 interacting with N spins S1,...,SN-1 in an antiferromagnetically ordered environment, where each spin Si interacts only with its nearest neighbors and is described by the Heisenberg model. We show that for small values of Bext (compared to J), the relaxation rate \u0393(Bext) exhibits a non-monotonic behavior as function of Bext. In particular, we find that there exists a critical value Bc such that \u0393(Bext) decreases monotonically if |Bext| < Bc while it increases monotonically otherwise. The dependence of Bc on the number of spins N is studied numerically using exact diagonalization techniques. \nThe results are explained within a phenomenological approach based on the assumption that the effect of the external magnetic field can be modeled by introducing additional effective interactions between the spins which depend on their mutual distances.","label":1,"model":"bloomz","source":"arxiv","id":3112}
{"text":"We present the results of our analysis on how to measure dark energy using gamma-ray bursts (GRBs) as standard candles, together with other cosmological probes such as Type Ia supernovae (SNe), baryon acoustic oscillations (BAOs), cosmic microwave background radiation anisotropies (CMBAs), galaxy clusters, weak gravitational lensing, and Hubble constant measurements.  We find that GRBs can be used in combination with SNe data sets to determine the equation-of-state parameter w{\\displaystyle w} for dark energy at redshifts z>2.5. The combined constraints are consistent with those obtained by Planck satellite observations alone. In addition, we show that combining GRBs with BAO\/CMBA\/cluster\/weak-lensing\/Hubble-constant data sets leads to tighter constraints than any individual probe or combinations thereof. Finally, we discuss possible systematic effects associated with GRB luminosity functions and their host galaxies.","label":1,"model":"bloomz","source":"arxiv","id":3113}
{"text":"We have observed the infrared colors (J-H, H-K) for 16 Mira variables with high resolution spectroscopy in order to investigate their relation to intensity ratios of SiO maser lines at 43 GHz. The results show that there is no correlation between these two parameters except for one star. We suggest that this may be due to different physical conditions among individual stars or differences in mass loss rates.  Keywords: Mira variable, Correlation, Infrared color, SiO maser line, Mass loss rate. 1 Introduction Miras are red giant stars which pulsate radially on time scales ranging from 100 days up to several thousand years. They exhibit large amplitude variations in luminosity as well as radial velocity. Their light curves can be described by a simple sinusoidal function with periods longer than about 300 days [1] . These stars are known to produce strong winds [2] , and they also emit intense radio waves [3] .\nThe SiO molecule has been found to exist in many types of astronomical objects such as late-type stars [4] , evolved massive stars [5] , young stellar objects [6] , comets [7] , and planets [8] . It is believed that SiO molecules play an important role in the formation process of dust grains [9] . SiO masers were first detected toward AGB stars [10] . Since then, SiO masers have been studied extensively towards both AGB stars [11] - [13] and post-AGB stars [14] - [16] . Many studies have shown that the properties of SiO masers depend strongly on the evolutionary stage [17] - [20] . For example, it was reported that the peak flux density decreases rapidly during the transition phase from AGB to post-AGB [21] .","label":1,"model":"bloomz","source":"arxiv","id":3114}
{"text":"We present an analysis of archival Hubble Space Telescope (HST) data for the stellar cluster associated with the Galactic center's Sgr A* black hole, which we refer to as \"the arches\". We use these observations to determine the mass function and luminosity distribution of stars in this region. The observed number density profile is well-fit by a power law model that has been modified by a Gaussian core at small radii. This yields a best fit value of $\\gamma = 1.7 \\pm 0.3$ for the slope of the underlying mass function between $0.1 - 100 M_{\\odot}$. In addition, we find evidence for two distinct populations of stars within the central arcseconds: one population that follows a power-law mass function similar to that found outside of the central arcsec; another population whose mass function appears to be shallower than the first but steeper than Salpeter.","label":1,"model":"bloomz","source":"arxiv","id":3115}
{"text":"We present an extension of the Standard Model (SM) with three right-handed neutrinos that is based on the assumption of a spontaneously broken generalized $\\mu$-$\\tilde{\\mu}$-symmetry. The model predicts a complex Cabibbo-Kobayashi-Maskawa quark mixing matrix as well as Majorana masses for all three known neutrino species. We show how this scenario can be realized in terms of a renormalizable Yukawa Lagrangian. In addition to the SM Higgs doublet we introduce two scalar fields which are responsible for breaking the electroweak gauge group SU(2)L x U(1). One of these scalars acquires its vacuum expectation value at tree level while the other one develops a non-vanishing vev only after radiative corrections have been taken into account. This leads to a natural explanation why the mass scale of the lightest neutral Higgs boson is close to the Z-boson mass.","label":1,"model":"bloomz","source":"arxiv","id":3116}
{"text":"The present work is devoted to study the effect of pairing interaction on nuclear properties using the relativistic mean field theory (RMFT). The RMFT has been used for calculation of ground state properties like charge density, single particle energy levels, total binding energy etc., of even-even nuclei with mass number A = 16 - 40. In this approach we have considered two different forms of nucleon-nucleon interactions namely Gogny D1S force and NL3* force alongwith Coulomb interaction. We have also calculated the root-mean-square radius(rms) values by considering various combinations of these forces. It was found that the rms value increases as one goes away from the valley of stability towards the neutron-rich side. This increase can be understood due to the presence of extra neutrons which are loosely bound compared to protons. Also it was observed that the rms value decreases when pairing correlation is included.","label":1,"model":"bloomz","source":"arxiv","id":3117}
{"text":"The inter-hourly-variability index is proposed to measure geomagnetic activity on hourly basis, which can be used as an indicator for solar wind speed variation over time scales longer than one day.  The IHV index was calculated using hourly values of Kp index during the period 1957-2009. It shows that there are two peaks at about 1965-1970 and 1990-1995 . A linear regression analysis between the IHV index and solar wind speed data obtained by spacecraft observations reveals that they have good correlation with each other. This suggests that the IHV index may provide useful information for long-term prediction of solar wind speed. Keywords: Geomagnetism; Solar wind; Variability index; Prediction modeling; Data mining. 1 Introduction Space weather has been attracting more attention recently because it affects many aspects of human life such as satellite communications, aviation safety, power grids etc. (e.g., [1] ). In particular, solar wind plays important roles in space weather since it carries away magnetic fluxes generated by the Sun's dynamo action [2] , and thus controls the Earth's magnetosphere [3] .\nSolar wind is driven out from the Sun by coronal mass ejections [4] . Coronal mass ejections occur when huge amounts of plasma suddenly escape from the Sun into space [5] . They usually last several hours [6] . Therefore, solar wind speed varies significantly within 24 h [7, 8] . However, most previous studies only focus on the average solar wind speed or daily averaged solar wind speed [9] . As a result, little work has been done on investigating how solar wind speed changes on shorter timescales [10] .","label":1,"model":"bloomz","source":"arxiv","id":3118}
{"text":"We study the non-adiabatic current through an open quantum dot connected to two leads with different chemical potentials, which is driven by time-dependent gate voltages applied on both sides of the dot. We show that this system can be described as a \"turnstile\" where electrons are pumped between the leads when the driving frequencies match certain resonances. The effect is robust against disorder and dephasing. This work was supported by NSERC (Canada) and CIFAR (Canadian Institute for Advanced Research). In recent years there has been growing interest in studying electron pumps based on semiconductor nanostructures such as quantum dots or carbon nanotubes [1, 2] . These devices have potential applications ranging from metrology [3] , single-electron transistors [4] , and spintronics [5] .\nIn these systems, charge carriers are transported across the device via sequential tunneling processes [6] . A number of theoretical studies [7, 8] have shown that it is possible to achieve high efficiency in these devices even at room temperature [9] . However, most previous works focused only on adiabatic pumping [10] , i.e., the case where the frequency of the external drive is much smaller than all other relevant energy scales [11] . Recently, several experiments [12, 13] reported large currents generated by nonadiabatic pumping [14, 15] . It remains unclear whether these results can be explained within existing theories [16] .\nHere we consider a simple model of a quantum dot connected to two metallic leads [see Fig. 1(a) ] [17] . The dot level is modulated periodically by applying oscillating gate voltages V L\/R = \u00b1V 0 cos \u03c9t on each side of the dot [18] . When the modulation period T \u2261 2\u03c0\/\u03c9 matches one of the dwell times \u03c4 n = \u03c0 \/[2(E F \u2212 E n )] associated with the discrete levels E n of the isolated dot, electrons will be transferred coherently between the left and right leads [19] . Here E F denotes the Fermi energy of the leads [20] . As illustrated schematically in Figs. 1(b-c), depending on","label":1,"model":"bloomz","source":"arxiv","id":3119}
{"text":"We present an optimal algorithm for generating pointed trivalent diagrams, which are combinatorial objects that encode the topology of 3-manifolds. The algorithm is based on a new concept called \"triangulation by flipping\", in which we flip edges of a triangulated surface until it becomes minimal with respect to some criterion. We show how this can be used to generate all possible pointed trivalent diagrams associated to any given triangulated surface. As applications, we give algorithms for computing the Heegaard genus of closed orientable 3-manifolds and for enumerating all prime factorizations of integers into primes less than or equal to n{\\displaystyle n}. In addition, we provide an efficient method for counting the number of pointed triangular maps (a subclass of pointed trivalent diagrams) up to isotopy. This work was supported by NSF grant DMS-0805040. 1 Introduction Let M be a compact connected oriented 3-manifold whose boundary consists of tori. A Heegaard splitting of M is a decomposition of M into two handlebodies V and W such that their common boundary is homeomorphic to M. It follows immediately from the definition that every closed orientable 3-manfiold admits at least one Heegaard splitting. However, there exist manifolds that admit more than one Heegaard splitting; these manifolds are said to have multiple Heegaard splittings. For example, if S is a sphere with three holes then S\u00d7S has four different Heegaard splitttings [O] .","label":1,"model":"bloomz","source":"arxiv","id":3120}
{"text":"The author has responded in detail to the comments made by Prof. Guseinov, which are published as an extended commentary on his original article entitled \"\"One-range addition theorems for coulomb interaction potential and its derivatives\"\".  In this combined reply he shows that all the objections raised against him can be answered satisfactorily. He also presents some new results concerning the one-range addition theorem for the first derivative of the coulomb interaction potential. These results have been obtained using the same method used previously to derive the corresponding theorem for the second-order derivative. Finally, it is shown how these results may be applied to obtain accurate values of the energy levels of helium-like ions with nuclear charges Z = 1 - 6. This work was supported by the Australian Research Council under Grant No. DP0775560. The author thanks Dr. A. M. Aliev for useful discussions during the preparation of this manuscript.","label":1,"model":"bloomz","source":"arxiv","id":3121}
{"text":"We study the lightest and heaviest CP-even neutral Higgs boson masses in the Minimal Supersymmetric Standard Model (MSSM) with large values of tan(beta). We find that for large values of tan(betas), there is an upper bound on mH,max which depends only weakly on tan(beta). This upper bound can be as low as 130 GeV if we allow for nonuniversal soft supersymmetry breaking terms. The lower limit on mH,min increases rapidly with increasing tan(beta).  For small values of tan(beta) (tan(beta) < 3), the mass difference between the two CP-even Higgs bosons decreases slowly with increasing tan(beta). However, this decrease becomes more rapid when tan(beta) > 5.  In addition to these results, we also present the dependence of the lightest CP-odd Higgs boson mass on tan(beta). \nI. INTRODUCTORY REMARkS\nThe Minimal Supersymmetric Standard model (MSSM) [1] has been studied extensively over the past few years [2] . It contains many new parameters beyond those of the Standard Model (SM). These include the gaugino masses M1 , M2 , M3 , the higgsino mass parameter \u00b5, the trilinear scalar couplings A f , and the ratio of vacuum expectation values of the two Higgs doublets tan(beta).\nIn general, it is difficult to obtain analytical expressions for all the physical quantities in the MSSM [3] . Therefore, one usually resorts to numerical methods [4] or approximations [5] .\nRecently, several groups have used approximate techniques [6] - [8] to calculate various properties of the MSSM Higgs sector. In particular, Ref. [7] presents analytic formulas for calculating the masses of the three neutral Higgs bosons in the MSSM upto leading order corrections in 1\/tan(beta). They show that their results agree well with exact calculations [9] .","label":1,"model":"bloomz","source":"arxiv","id":3122}
{"text":"We consider the problem of time evolution for quantum fields on the Milne universe, which is an open FRW cosmology with negative spatial curvature and vanishing cosmological constant. We show that there are no normalizable solutions to the Klein-Gordon equation corresponding to massive particles at early times (t \u2192 \u2212\u221e). This result can be understood as follows. The Milne universe has a singularity at t = 0 where all physical quantities diverge. In particular, the energy density diverges like $1\/t^2$ near this point. As a consequence, any particle state localized around $t=0$ will have infinite energy. Therefore it cannot correspond to a physically meaningful solution of the field equations. On the other hand, we find that there exist normalizable states describing massless particles at early times. These states represent gravitons or photons propagating into the past along geodesics towards the big bang singularity.","label":1,"model":"bloomz","source":"arxiv","id":3123}
{"text":"The dielectric properties, phase transition behavior, and microstructure evolution were investigated for the (0.65[Pb(Ni-1\/3Nb-2\/3)O_3] -0.35PbTiO3) (PNT) ceramics with different sintering temperatures ranging from 850 to 1100 \u00b0C. The PNT samples exhibited high permittivity values up to ~10 4 , low loss tangent below 10 -2 , and large tunability over 30% under an electric field strength of 30 kV\/cm at room temperature. With decreasing temperature down to 77 K, the permittivity increased slightly while the loss tangent decreased significantly due to the freezing out of mobile ions. At cryogenic temperatures, two relaxation processes were observed in the frequency range between 1 Hz and 100 kHz. The first process was attributed to the grain boundary effect; it shifted towards higher frequencies as the temperature decreased. The second process was associated with ferroelectric domain wall motion; its relaxation time constant remained almost unchanged when the temperature changed.","label":1,"model":"bloomz","source":"arxiv","id":3124}
{"text":"The authors present the results of their study on the scattering between two protons and one neutron, which is known as the triton channel in nuclear physics.  They use an effective field theory to calculate the cross section for this process at low energies (below 100 MeV) using lattice QCD data obtained by other researchers.   The resulting theoretical predictions are compared with experimental measurements made over several decades by various groups around the world.    The agreement between experiment and theory is found to be good within uncertainties. This work was supported by the U.S. Department of Energy under Contract No. DE-AC02-05CH11231. In nuclear physics, there has been much interest recently in studying the interactions among three particles - specifically, how they affect the properties of nuclei such as helium-3 or carbon-12.  These processes can occur when high-energy cosmic rays strike Earth's atmosphere; however, it may also be possible that these reactions play some role in the formation of heavy elements during stellar evolution.  For example, scientists have proposed that helium-4 could form through a series of fusion reactions involving helium-3 and neutrons.  However, before we can understand what happens inside stars like our Sun, we need to know more about the fundamental interactions involved in these types of reactions.  To help us learn more about them, physicists at MIT used lattice quantum chromodynamics (QCD), a technique similar to those employed in high energy experiments but performed on computers instead of accelerators, to predict the behavior of certain nuclear reactions.  Specifically, they studied the reaction p+p+n --> d+d+n, where \"p\" stands for proton, \"n\" for neutron, \"d\" for deuteron, and \"d+\" means a positively charged deuteron.  Their calculations were based on...","label":1,"model":"bloomz","source":"arxiv","id":3125}
{"text":"We present new radial velocity measurements for the short-period binary system WZ Sge, which show that its orbital period is decreasing at an average rate of  _ P = -1.3 x 10^(-7) d\/s.  We use these data to derive dynamical mass estimates for both components and find M_1 = 0.85 \u00b1 0.05M_sun and M_2 = 0.65 \u00b1 0.04M_sun. The primary star has evolved off the main sequence but still retains some hydrogen in its atmosphere; it is therefore classified as a subdwarf B (sdB). Our results are consistent with previous determinations based on photometric observations. However, our analysis provides more accurate values because we have used higher quality spectroscopic data than were available previously. In addition, we have been able to determine the individual masses rather than just their ratio. This work was supported by NASA grant NAG5-13523.","label":1,"model":"bloomz","source":"arxiv","id":3126}
{"text":"We study the non-Markovian evolution of bipartite Gaussian states under local dephasing noise and global squeezing interaction with an environment at finite temperature. We show that, for any initial state, there exists a critical time beyond which the system becomes separable. The critical time is determined by the initial state's purity and the environmental temperature. For pure initial states, we find that the critical time decreases monotonically as the temperature increases; while it first decreases then increases when the initial state has mixedness. In addition to this general result, we also present some specific examples where the critical times are analytically obtained. Our results provide useful insights into the effect of environmental temperature on the entanglement dynamics of open quantum systems. Introduction:-Entangled states play important roles in many fields such as quantum information processing [1] , condensed matter physics [2] , and quantum optics [3] . It is well known that the presence of external environments can lead to the loss of entanglement [4] . Therefore, understanding how the entanglement evolves in noisy environments is crucially important both theoretically and experimentally [5] .\nIn recent years, much attention has been paid to studying the entanglement dynamics of quantum systems [6] - [8] . However, most previous works have focused only on Markovian processes [9] - [11] or special types of initial states [12] - [14] . Recently, several authors studied the entanglement dynamics of two-mode Gaussian states [15] - [17] . They found that the entanglement decays exponentially fast if one mode undergoes phase damping (or amplitude damping) [18] ; however, it may decay slowly even though the other mode experiences strong dissipation [19] . Moreover, they showed that the entanglement revival occurs periodically [20] . These studies were mainly based on numerical simulations [21] . Very recently, analytical solutions were presented [22] - [24] . Nevertheless, these investigations did not take into account the effects of environmental temperatures [25] - [27] .","label":1,"model":"bloomz","source":"arxiv","id":3127}
{"text":"The conference will be held on September 24-25 at the University of Milan-Bicocca (Italy). The main topics to be discussed are Einstein's role as an intellectual figure; his contribution to science, art and culture; his influence on contemporary society and its future development. \nThe event is organized by the Italian National Committee for UNESCO with the support of the International Union of Pure and Applied Physics (IUPAP), the European Physical Society (EPS) and the Institute of Advanced Studies of the University of Bologna. \n \n The conference aims to celebrate the centenary of Albert Einstein's birth and to promote the dissemination of knowledge about him among scientists, artists, students and the general public. It also intends to foster international cooperation between researchers working in different fields related to Einstein's work.  \n \n The program includes plenary lectures given by internationally renowned speakers, parallel sessions dedicated to specific themes, poster presentations and cultural events such as exhibitions, concerts and performances.","label":1,"model":"bloomz","source":"arxiv","id":3128}
{"text":"We present an ab initio study of the structure, energetics, and dynamics of water adsorbed on the MgO(001) surface in the submonolayer regime using density functional theory with van der Waals corrections. We find that the most stable configuration is one where each oxygen atom binds to three hydrogen atoms forming a trihydrogen bridge between two adjacent O atoms. The binding energy per molecule for this configuration is 1.6 eV. This value agrees well with previous theoretical results obtained within the generalized gradient approximation but disagrees significantly with experimental values which are typically larger by about 0.5-0.7 eV. Our calculations show that the discrepancy can be attributed mainly to the neglect of dispersion interactions in earlier studies. In addition we have studied the effect of temperature on the stability of different configurations. We found that the relative population of various structures depends strongly on the temperature.","label":1,"model":"bloomz","source":"arxiv","id":3129}
{"text":"We report the observation of measurement-induced dephasing in a Bose-Einstein condensate (BEC) using Ramsey interferometry and atom counting.  The experiment is performed by splitting a single trapped BEC into two spatially separated clouds, which are allowed to evolve for different times before being recombined on a beam splitter. We observe that the visibility of interference fringes decreases as we increase the number of atoms counted at one output port of the beam splitter. This effect can be explained by considering how repeated measurements affect the phase evolution of the system. Our results demonstrate that it is possible to use cold-atom experiments to study fundamental questions about quantum mechanics. Quantum mechanics predicts that any attempt to measure a physical quantity will disturb its value. In this work, we experimentally investigate such effects in a Bose-Einsteint Condensate (BEC). To do so, we perform Ramsey interferometry between two spatially separated regions of our sample. By varying the time spent evolving freely after splitting off part of the initial cloud, we control the relative phase accumulated during free evolution. After recombination, we count the number of atoms arriving at each output port of the beam-splitter and record their arrival-time distribution. As expected, we find that the visibility of the resulting interference pattern decreases when increasing the number of detected particles.","label":1,"model":"bloomz","source":"arxiv","id":3130}
{"text":"In this article, we prove sharp Lp estimates for the maximal operator associated with the surface measure on a closed convex surface in R3 when p>2.  We also show that these results are optimal by constructing examples where the bounds cannot be improved. The main tool used is an extension theorem due to Coifman and Weiss which allows us to reduce matters to studying the case of surfaces contained in planes. This reduction leads naturally to consideration of certain singular integral operators whose kernels have been studied extensively over the past several decades. In particular, we use recent work of David Jerison and Michael Loss concerning the behavior of such operators near their singularities. Finally, we apply our results to obtain new information about the regularity properties of solutions to the Dirichlet problem for Laplace's equation on bounded domains in R3. Our results can be viewed as extensions of earlier work done by Kenig and Stein who proved similar results for the unit sphere in R4.","label":1,"model":"bloomz","source":"arxiv","id":3131}
{"text":"We have calculated the ab initio melting curves for Mo and W using the phase-coexistence method with the generalized gradient approximation (GGA) to density functional theory (DFT). The results are compared with those obtained in previous studies, as well as experimental data on the melting points. We find that our GGA calculations give good agreement with experiment at high temperatures but underestimate the melting temperature significantly below 2000 K. This is probably due to anharmonic effects which we do not take into account here. In addition, we show how the electronic structure changes across the melting transition. \n \n Keywords: Molten metal, Phase diagram, Melting point, Ab initio calculation \n \n \n \n 1 Introduction \n \n It has been known since the early days of quantum mechanics that the properties of matter can be described accurately within this framework only if electron-electron interactions are taken into account explicitly [1] . However, it was soon realized that even simple approximations such as Hartree-Fock or DFT yield useful information about many physical phenomena [2] , including solid-state physics [3] .\n \nIn recent years there has been considerable interest in applying first-principles methods to calculate the thermodynamic properties of materials [4] . These include free energies [5] , phonon frequencies [6] , elastic constants [7] , surface tensions [8] , and other quantities [9] . One important application of these techniques is the prediction of the melting behaviour of solids [10] - [12] . For example, the melting temperature T m of metals can be determined directly from the Gibbs energy difference between the liquid and solid phases [13] :","label":1,"model":"bloomz","source":"arxiv","id":3132}
{"text":"We study gravitational interactions between two braneworlds, each with its own brane tension, by using an effective field theory approach that incorporates both Randall-Sundrum (RS) gravity and scalar-tensor theories. We show how this model can be used to describe the dynamics of binary systems such as double neutron stars or black holes. In particular we find that there are new resonant effects which occur when one object is much more massive than the other. These effects lead to large deviations from standard general relativity predictions for the orbital evolution of binaries containing compact objects. The results presented here may have important implications on our understanding of strong-field gravity phenomena like gravitational waves produced during mergers of supermassive black holes at galactic centers. Introduction: Gravitational wave observations will provide us with unprecedented information about the nature of gravity in the strongfield regime [1] . This has led to renewed interest in alternative models of gravity beyond Einstein's general relativity [2] , especially those inspired by string\/M-theory [3] .\nIn recent years it was shown [4] - [8] that many interesting features of these models could be captured within the context of effective field theories where higher-dimensional fields propagate in extra dimensions [9] . One particularly successful class of models consists of so-called braneworld scenarios [10] , where Standard Model particles are confined to live on a four dimensional brane embedded in a five dimensional bulk space-time [11] . A number of authors [12] - [16] have studied the possibility of detecting signatures of braneworld physics through gravitational wave observations [17] - [20] .","label":1,"model":"bloomz","source":"arxiv","id":3133}
{"text":"We study the ground state properties of the spin-1\/2 square lattice with nearest-neighbor antiferromagnetic interactions and vacancies, using exact diagonalization (ED) on finite clusters up to 12x12 sites. We find that for low concentrations of vacancies there is no significant change in the magnetic order parameter or the energy gap between singlet and triplet excitations as compared to the pure system. However, we observe an increase in the density of states at zero energy when increasing the concentration of vacancies. This effect can be explained by considering the formation of bound pairs of vacancies which are localized around each other due to their mutual interaction. The binding energies of these pairs depend strongly on the distance between them but only weakly on the size of the cluster considered. In addition, we show how this behavior changes if one considers next-nearest neighbor interactions instead of nearest-neighbor ones. Finally, we discuss possible experimental realizations of our results.","label":1,"model":"bloomz","source":"arxiv","id":3134}
{"text":"We consider the problem of scheduling jobs on parallel machines with probabilistic anonymity constraints, where each job has to be scheduled in such a way that its execution time is hidden among other jobs. We show how this problem can be solved by using admissible schedulers for the underlying deterministic problem. In particular we present two algorithms based on list-scheduling and randomization respectively. The first algorithm achieves a competitive ratio of 2 while the second one obtains a better performance guarantee of 1 + . Both results are tight up to constant factors. Finally, we provide some experimental evidence showing that our randomized algorithm performs well also in practice. Probabilistic anonymity arises naturally when considering privacy issues related to data mining or computational social choice problems. For example, suppose that there exists a set of n users who have to submit their preferences over m alternatives (e.g., candidates) before a certain deadline T. Each user i submits her preference profile Pi = {(ai1 , bi1 ), . . . , (aim , bim )}, where aij denotes the utility she derives from alternative j and bij indicates whether she likes it or not. Then, if all profiles are submitted simultaneously at time t, they should be processed so as to hide which user prefers what alternative. This means that no information about individual preferences should be revealed until after the deadline T.","label":1,"model":"bloomz","source":"arxiv","id":3135}
{"text":"The fitting problem is one of the most important problems in cosmology, which has been studied for more than 50 years.  The main goal of this work was to study the effect of different types of initial conditions on the evolution of density perturbations in an expanding universe with a positive cosmological constant (dark energy).  We have used two methods to solve numerically the Einstein equations coupled to the matter fields:  the standard method based on the expansion of the metric tensor into spherical harmonics; and the new method developed by us that uses the expansion of the metric into Chebyshev polynomials.  In both cases we have considered three types of initial conditions: Gaussian random fields generated using the Zel'dovich approximation, Gaussian random fields generated using a fast Fourier transform algorithm, and white noise.  Our results show that all these models give similar predictions at late times when they are evolved up to z = 0.5.  However, there are some differences between them at early times.  These differences can be explained as follows:  the Zel'dovich model predicts larger fluctuations compared to other models because it does not take into account the pressure term in the fluid equation; the white noise model gives smaller fluctuations due to its statistical properties; finally, the Gaussian random fields obtained via the fast Fourier transform algorithm predict intermediate values.  This result shows that the choice of the initial conditions may affect significantly the final value of the power spectrum of primordial density fluctuations predicted by inflationary theories.","label":1,"model":"bloomz","source":"arxiv","id":3136}
{"text":"We consider the limit behavior of discrete-time Markov chains with state space $\\{0,1\\}^{\\mathbb{N}}$ and transition probabilities given by a sequence $(p_n)_{n\\geq 1}$ such that $p_n$ is supported on all subsets of size $n$ of $\\mathbb{N}$ for each $n\\in \\mathbb{N}$.  We show that under suitable conditions these processes converge to an infinite-dimensional diffusion process in the sense of finite dimensional distributions. The limiting diffusion can be described as follows: Let $W$ denote standard Brownian motion on some probability space $(\\Omega, \\mathcal{F}, (\\mathcal{F}_t), P)$ equipped with a filtration $\\mathcal{F}_{t}$ satisfying the usual hypotheses. For every $t>0$ let $$X_{t,x} = \\sup_{s \\leq t} W_s + \\int_0^x (W_{t}-W_s) \\, ds$$ where $x \\in [0,\\infty)$ denotes the initial value at time zero. Then we prove convergence of the finite dimensional distributions of the discrete-time Markov chain towards those of $X_{t,x}$.","label":1,"model":"bloomz","source":"arxiv","id":3137}
{"text":"We present new observations of the H$_2$D$^+(1s)$ line at $230.538 nm$ toward two young protoplanetary disks, TW Hydrae and HD 163296.  The data were obtained with the Vacuum Tower Telescope (VTT), located on Roque de los Muchachos Observatory, La Palma, Canaries Islands. We have detected this line for both sources using an echelle grating centered around 230nm. For TW Hydrae we find that the line is blueshifted by about -40 km\/s compared to the systemic velocity of the source. This result suggests that there are outflows or winds associated with the disk. In addition, we detect several other lines such as He I $\\lambda = 587.6 nm$, O I $\\lambda = 630.0 nm$, C II $\\lambda = 723.3 nm$, Si III $\\lambda = 455.4 nm$, Fe II $\\lambda = 468.7 nm$, and Mg II $\\lambda = 448.2 nm$. These lines can be used to determine physical conditions within the disk atmosphere.","label":1,"model":"bloomz","source":"arxiv","id":3138}
{"text":"We report on the observation and analysis of molecular bound states in ultracold cesium atoms trapped by an optical lattice potential. The molecules are created using magnetic field induced Feshbach resonances at temperatures below 1 microkelvin. We measure their binding energies as functions of both magnetic field strength and laser intensity. These measurements allow us to determine the scattering length between two fermionic atoms with high precision. In addition we observe that the molecule formation rate is strongly enhanced when the trapping lasers are detuned into resonance with excited vibrational levels of the atomic ground state. This effect can be explained by stimulated emission processes which lead to rapid relaxation towards deeply bound molecular states. Finally we demonstrate how these results can be used for precise determination of the s-wave scattering lengths between different spin species. Our work opens up new possibilities for studying quantum many-body phenomena such as superfluidity or supersolidity in systems of interacting fermions. \n \n We present experimental data obtained during our study of ultracold cesium (Cs) atoms confined within an optical lattice trap. Using magnetic field induced Feshback resonances we create weakly bound Cs2 dimer molecules out of pairs of fermionic atoms. By measuring the binding energy of the molecules as function of magnetic field strength and laser power density we obtain accurate values for the scattering length between two Cs atoms. Furthermore we find that the molecule formation process is strongly enhanced if the trapping lasers have a frequency close to one of the atomic transitions. This effect can be understood by considering stimulated emission processes leading to fast relaxation towards deeply bound molecular levels.","label":1,"model":"bloomz","source":"arxiv","id":3139}
{"text":"We study lattice chiral gauge theories with Wilson fermions in four dimensions, focusing on their phase structure at finite temperature T . We show that there is no spontaneous breaking of parity (P) or time-reversal symmetry (T ) for any value of the bare quark mass m0 if the number Nf of flavors satisfies Nf > 2. This result implies that the theory does not have an order parameter associated to P and\/or T , which are spontaneously broken by the standard model. In particular, we find that the spectrum contains two degenerate Dirac fermion species corresponding to left-handed and right-handed quarks, respectively. These fermions can be identified as mirror fermions because they transform into each other under reflection about one spatial axis. The existence of these mirror fermions leads to interesting consequences such as the absence of flavor changing neutral currents mediated by gluons. \n \n Introduction \n \n Chiral gauge theories play important roles both theoretically and phenomenologically. They provide a natural framework for describing low-energy phenomena involving hadrons [1] . On the other hand, it has been suggested recently that some extensions of the Standard Model may contain extra space-time symmetries beyond Poincar\u00e9 invariance [2] . It would then be very useful to develop techniques to analyze the possible effects of such new symmetries on physical observables [3] .\n \nIn this Letter, we consider a class of chiral gauge theories defined on a Euclidean spacetime lattice [4] . Our main interest lies in studying how the presence of additional discrete symmetries affects the phase diagram of the system. For simplicity, let us first focus on the case where only parity (P), charge conjugation (C), and time reversal (T ) transformations act nontrivially on fields [5] . Then, the action S = d4 x L(U; \u03c8,\u03c8) should satisfy the following conditions [6] :","label":1,"model":"bloomz","source":"arxiv","id":3140}
{"text":"The dynamics-based approach is an emerging method for studying terrestrial exoplanets, which are planets with masses similar to that of Earth orbiting other stars in the solar system.  The main goal of this research is to study how these planets form and evolve over time.   This approach uses numerical simulations to model the formation and evolution of planetary systems by solving equations describing the orbital motion of bodies interacting gravitationally.   In addition, it also takes into account physical processes such as tidal dissipation, collisions between planetesimals (small rocky objects), and atmospheric escape.    By using this approach we can better understand how our own planet formed billions of years ago and what conditions were necessary for life on Earth to develop. Keywords: Planetary Science; Astrobiology; Tidal Dissipation; Collisions Between Planetesimals; Atmospheric Escape. Introduction:  The dynamics-based approach is an emergent method for studying terrestrial extrasolar planets, or planets with masses similar to Earth's orbiting other stars within the Solar System.  These types of planets have been discovered recently through space missions like Kepler and K2.  The main goal of the dynamics-based approach is to study how these worlds form and evolve over time.  It does so by modeling the formation and evolution of the entire planetary system numerically via solving equations describing the orbital motions of bodies interacting gravitationally.  Additionally, it incorporates physical processes including tidal dissipation, collisions among planetesimals (smaller rocky objects) and atmospheric escape.  By applying this approach, scientists hope to gain insight about how our own planet formed billion(s) of years ago and what environmental factors may be required for life to exist there.","label":1,"model":"bloomz","source":"arxiv","id":3141}
{"text":"We present the results of 2D hydrodynamic simulations of supernova explosions inside wind bubbles blown by massive stars with different initial masses and metallicities. We find that for all considered cases, the shock wave is decelerated at distances between 0.1 pc to 1 pc due to interaction with dense shells formed during previous stellar winds. The density structure of these shells depends on metallicity and mass loss rate of the progenitor star. For low-mass progenitors (M < 20 Msun), we observe formation of thin shell which leads to efficient cooling behind it. In this case, the explosion energy is efficiently thermalized into hot plasma confined within the bubble interior. On the other hand, for high-mass progenitors (20 Msun < M < 40 Msun) we see formation of thick shell which does not cool down significantly after the passage of the blast wave. This leads to higher temperatures in the shocked gas and more energetic X-ray emission. \n \n Keywords: Supernova remnants","label":1,"model":"bloomz","source":"arxiv","id":3142}
{"text":"We report on infrared spectroscopic observations with the Infrared Spectrograph (IRS) aboard the Spitzer Space Telescope toward the nearby late-type galaxy NGC 3621, which is known to harbor a supermassive black hole at its center. The IRS spectrum shows prominent emission lines such as [Ne II]12.81 and [S III]18.71 \u00b5m that are commonly seen in active galactic nuclei (AGNs). We find that these emission lines can be reproduced by photoionization models using AGN-like ionizing radiation fields. From the observed line ratios we estimate the electron density n e = 10 3 cm \u22123 , temperature T e = 1000 K, and ionization parameter U H = 1 \u00d7 10 \u22122 . These results suggest that the central region of NGC 3621 has properties similar to those found for Seyfert galaxies. This work was supported by NASA through grant number GO-08460.01-A awarded by the Jet Propulsion Laboratory, California Institute of Technology under contract with NASA.","label":1,"model":"bloomz","source":"arxiv","id":3143}
{"text":"We present new results on the evolution of galaxy clustering in the range 0 < z < 5, based on an analysis of data obtained with the VIMOS spectrograph at the Very Large Telescope (VLT). The sample consists of about 2000 galaxies selected by their Lyman-alpha emission line fluxes and covers a wide range of redshifts between 2<z<5. We measure the two-point correlation function for this sample using both direct counts-in-cells methods as well as Fourier space techniques. Our main result is that we find no evidence for any significant change in the amplitude or slope of the correlation function over this large redshift interval. This suggests that there has been little evolution in the typical mass scale of dark matter halos hosting these galaxies since z=5. In addition to measuring the overall shape of the correlation function, we also examine how it depends upon various physical properties such as luminosity, color, and spectral type.","label":1,"model":"bloomz","source":"arxiv","id":3144}
{"text":"We report on new observations made with Chandra, XMM-Newton, and VLA that provide evidence for an association between the compact X-ray source 1E 1547. 0-5408  and the radio shell G327. 24-0.13 . The X-ray spectrum is consistent with emission from a magnetar; however, we find no pulsations in our data set. We also present optical spectroscopy of two stars near the center of the remnant which show strong Balmer absorption lines characteristic of Wolf-Rayet (WR) stars; these are likely to be associated with the supernova event that created the remnant. \n \n Keywords: Supernova remnants, Pulsar wind nebulae, Wolf Rayets, Chandra, XMM-NEWTON, VLA, Optical Spectroscopy \n \n Introduction \n \n In this Letter, we report on new observational results concerning the possible association between the candidate magnetar 1E 1547.0+ 5408 , located at the center of the supernova remnant G327.24 -0.13 , and its surrounding environment. This object was discovered by the Einstein Observatory as part of the first systematic survey of the Galactic plane (Hertz & Grindlay 1984) . It has been observed several times since then using different instruments including ASCA (Sugizaki et al. , 1997) , BeppoSAX (Giacani et al. , 2001 ) , RXTE (Israel et al. , 2002 ) and Chandra (Pavlov et al. , 2004 ) . Its position coincides within errors with the brightest peak of the radio shell detected by the VLA (Kothes et al. , 2006 ; Gaensler et al. , 2008 ) . However, there have been conflicting reports about whether or not it shows any periodic behavior. While Israel et al. (2002) reported detection of a periodicity of 6 s during their observation campaign, Sugizaki et al. (1997) found only marginal evidence for such a signal when they analyzed archival ASCA data. More recently, Pavlov et al. (2004) did not detect any significant pulsation down to a","label":1,"model":"bloomz","source":"arxiv","id":3145}
{"text":"The electrochemical properties of materials are often strongly influenced by their phase transitions, which can be difficult to predict using conventional methods. In this work we present an approach that uses cyclic voltammetry (CV) data collected at different temperatures as input into a machine learning model in order to examine the thermodynamic stability of phases within a material system. We demonstrate our method on two prototypical systems - Fe3O4 and LiFePO4 - where it is able to successfully identify metastable phases and accurately predict equilibrium compositions across wide temperature ranges. The results showcase how CV-based approaches could provide new insights into complex chemical phenomena such as solid state reactions or battery degradation mechanisms. \n \n Introduction \n \n Many important physical processes occur through changes in crystal structure driven by thermal fluctuations [1] . These include transformations between polymorphs [2] , amorphous states [3] , and even liquid crystalline structures [4] . Such structural rearrangements have been shown to significantly affect the electrical [5] , optical [6] , magnetic [7] , mechanical [8] , and catalytic [9] properties of materials. As a result, understanding these transformations has become increasingly important for applications ranging from energy storage [10] to catalysis [11] .\n \nIn many cases, however, predicting the outcome of a transformation requires knowledge about its underlying free energies [12] . This information cannot always be obtained directly via experimentation due to kinetic barriers [13] , making computational techniques [14] particularly useful [15] . Unfortunately, most current theoretical models [16] require extensive parameterization [17] and\/or detailed experimental characterization [18] before they can be applied effectively [19] . Moreover, while some recent studies [20] have demonstrated successes with deep neural networks [21] , there remains significant uncertainty regarding whether these approaches will generalize well [22] . \n \n Herein, we propose a novel approach based on cyclic voltammetry [23] that allows us to probe the thermodynamics of phase transformations without requiring any prior assumptions about the nature of the transition [24] . Our technique relies on collecting CV data over a range of temperatures [25] and then training a supervised [26] machine learning algorithm [27] to learn relationships between the measured currents [28] and the corresponding Gibbs free energies [29] . Once trained,...","label":1,"model":"bloomz","source":"arxiv","id":3146}
{"text":"The power law is an important concept for understanding the statistical properties of many natural phenomena, including earthquakes and financial markets.  In this article we discuss how to use maximum likelihood estimation (MLE) to find parameters that best fit a given set of data with a power law distribution.   We also present several examples using simulated data sets as well as real-world data sets taken from the Internet Archive's Wayback Machine. The power law is an important model for describing the statistical behavior of many natural phenomena such as earthquakes [1] , solar flares [2] , human mobility [3] , and even stock market prices [4] . It has been shown that the probability density function of these systems can be described by a simple mathematical formula known as a Pareto distribution [5] :  $$P(x) = \\frac{\\alpha}{x^{\\beta}}$$ where $\\alpha$ is a normalization constant and $\\beta$ determines the shape of the distribution [6] .\nIn this article we will show you how to estimate the values of $\\alpha$ and $\\beta$ that best describe your data using Maximum Likelihood Estimation (MLE). MLE is one of the most common methods used to determine the parameters of a statistical model [7][8][9] .  This method works by finding the values of the parameters that maximize the likelihood of observing some particular data set [10] .","label":1,"model":"bloomz","source":"arxiv","id":3147}
{"text":"The stability, equilibrium states and the dynamic behavior of solar coronal loops are investigated by applying thermodynamic irreversible energy principles to an idealized model loop system with constant cross section. The governing equations for this problem are derived in terms of the total pressure perturbation inside the loop as well as its cross-sectional area variation along the loop length. It is shown that these two variables can be expressed in terms of each other through a nonlinear differential equation which describes the evolution of the loop system. This equation has been solved numerically under different initial conditions corresponding to various physical situations. In particular, it is found that the loop may either expand or contract depending on whether the initial value of the total pressure perturbation is positive or negative respectively. Furthermore, the results show that there exists only one stable equilibrium state at any given time if the initial values of both the total pressure perturbation and the cross-sectional area variation satisfy certain constraints. Finally, the temporal evolution of the loop system is studied when the initial condition corresponds to a static equilibrium solution.","label":1,"model":"bloomz","source":"arxiv","id":3148}
{"text":"We study the phase diagram and electronic structure of bilayer ruthenate Sr3Ru2O7 using density functional theory (DFT) calculations, which show that this material is close to an insulator-metal transition driven by charge transfer between layers. We find that the Fermi surface topology changes dramatically across the metal-insulator boundary, with the appearance of new hole pockets at the Brillouin zone center. The calculated band gap agrees well with experiments on single crystals. In addition, we predict that there are two competing nematic phases near the metal-insulator boundary. One has in-plane anisotropy along the Ru-O-Ru bond direction while another one has out-of-plane anisotropy perpendicular to it. These results provide insights into the origin of the observed structural distortion in bilayer ruthenates. Bilayer ruthenates have attracted considerable attention recently due to their rich physical properties including unconventional superconductivity [1] , quantum criticality [2] , and multiferroicity [3] . Among these materials, Sr3Ru2O7 shows particularly interesting behavior because its ground state can be tuned continuously from metallic to insulating states through chemical doping or applying pressure [4] .\nIn recent years, several experimental studies have been performed to investigate the nature of the metal-insulator transition (MIT). For example, angle resolved photoemission spectroscopy measurements [5] found that the Fermi surface topology changed significantly when crossing the MIT line. X-ray scattering [6] showed that the crystal symmetry was lowered from tetragonal to orthorhombic below TMI = 160 K. Neutron scattering [7] revealed that the lattice parameters were different for the ab plane and c axis below TMIT ~ 150 K. However, despite extensive investigations, the microscopic mechanism behind the MIT remains unclear [8] .","label":1,"model":"bloomz","source":"arxiv","id":3149}
{"text":"We have carried out an extensive spectroscopic survey for high redshift (z > 2) galaxies using the VLT\/VIMOS instrument on the ESO Very Large Telescope, targeting sources selected by their strong rest-frame UV emission lines and photometric redshifts. We present here our results obtained with this sample at wavelengths ranging from radio to X-ray. The main goal is to investigate how star formation proceeds in these distant objects through detailed studies of their physical properties such as stellar masses, ages, metallicities or dust content. In particular we focus on two samples of Lyman-alpha emitting galaxies which are known to be very young systems undergoing intense bursts of star formation. Our analysis shows that they exhibit large amounts of cold gas but also significant quantities of dust. This suggests that the bulk of the observed infrared luminosity may not come directly from newly formed stars but rather from reprocessed light emitted by hot dust heated by older populations and\/or AGN activity.","label":1,"model":"bloomz","source":"arxiv","id":3150}
{"text":"We present an analytic model for anisotropy in galaxy correlation functions due to magnification distortion, which is caused by gravitational lensing effect on galaxies and their images. We show that this effect can be described as a convolution between the real space galaxy correlation function and the magnification kernel. The latter depends only on cosmological parameters such as matter density parameter $\\Omega_m$ and Hubble constant $H_0$ through angular diameter distance $D_A(z)$ at redshift $z$. Using our model we calculate the expected signal-to-noise ratio (S\/N) of the anisotropic galaxy correlation function with future surveys like Euclid or LSST. Our results suggest that it will be possible to detect the anisotropy up to redshifts around 1.5-2 if these surveys are deep enough. \n \n\n Keywords: Anisotropy, Magnification, Gravitational lensing, Galaxy clustering, Cosmic shear","label":1,"model":"bloomz","source":"arxiv","id":3151}
{"text":"We present new Chandra X-ray Observatory observations and optical spectroscopy for the galaxy cluster Abell 576, which is known to have two merging components separated by about 1 arcmin (about 700 kpc). The northern component has been previously studied as an example of a \"line-of-sight bullet cluster\"; it shows no evidence of significant substructure or shock heating along its line of sight but does show signs of recent merger activity on smaller scales. In contrast, we find that the southern component appears relaxed with little sign of disturbance; however, this may be due to projection effects since there are several galaxies at large projected distances from the center of the cluster whose redshifts indicate they lie behind the cluster core. We also detect diffuse emission extending beyond the virial radius of both clusters, possibly indicating ongoing accretion onto these systems. These results suggest that Abell 576 will evolve into a single massive system within a few Gyrs.","label":1,"model":"bloomz","source":"arxiv","id":3152}
{"text":"We present new abundance determinations for the light neutron-capture elements Sr, Y, Zr, Ba, La, Ce, Nd, Sm, Eu, Gd, Dy, Yb, and Lu in eight planetary nebulae (PNe). The PNe are selected to have high signal-to-noise ratio optical spectra obtained with HST\/STIS or FUSE that allow accurate measurements of their elemental abundances. We find that all but one PN show evidence for enrichment by the slow-neutron capture process (s-process) relative to solar values. In most cases we can identify specific s-process contributions from individual isotopes such as 92Zr, 138Ba, 144Sm, 146Eu, 151Gd, 157Dy, 162Yb, 174Lu, 176Hf, 182W, and 205Pb. These results provide important constraints on theoretical models of nucleosynthesis in low-mass asymptotic giant branch stars. \n \n Keywords: Elemental abundances, Planetary nebula","label":1,"model":"bloomz","source":"arxiv","id":3153}
{"text":"We present the clustering properties of star forming galaxies at z ~ 1, 2 & 3 in the GALEX Deep Imaging Survey (DIS) field using photometric redshifts derived by combining deep optical data from the Canada-France-Hawaii Telescope Legacy Survey (CFHTLS). We use two different methods to select our galaxy samples; one based on their observed NUV fluxes and another based on their intrinsic SFRs estimated from their UV luminosities. The results show that both these selection criteria yield similar clustering strengths for all three redshift bins considered here. However, we find evidence for evolution in the bias parameter between each redshift bin which is consistent with previous studies. In addition, we also study how this bias evolves as a function of stellar mass and UV luminosity. Our analysis shows that there are no significant differences in the bias values obtained when considering only those galaxies above or below a given threshold value of either stellar mass or UV luminosity.","label":1,"model":"bloomz","source":"arxiv","id":3154}
{"text":"We study the mechanical response to uniaxial compression and extension of an elastic-plastic continuum model with a nonlinear kinematic hardening rule, which is motivated by recent experiments on metallic glasses. The plastic deformation mechanism consists of localized shear-transformation zones (STZs) that are randomly distributed throughout the sample volume at zero stress. We show that this simple model can reproduce many features observed experimentally such as serrated yielding, strain softening after yield, and pronounced anisotropy during flow. In particular we find that the STZ density increases rapidly near the onset of plasticity but then decreases slowly until it reaches its steady-state value. This behavior leads to a characteristic peak in the macroscopic stress-strain curve just before the final collapse into a densely packed state. Finally, we discuss how our results relate to previous theoretical work based on mean-field approximations. DOI: 10.1103\/PhysRevE.77.020101","label":1,"model":"bloomz","source":"arxiv","id":3155}
{"text":"We study the orbital evolution and stability properties of oligarchic co-orbitals in the Solar System, i.e., bodies with masses comparable to that of Jupiter which are trapped on orbits close to those of Neptune or Uranus for billions of years.  We show how these objects can be identified by their long-term dynamical behavior as well as by their current positions relative to Neptune's orbit. The existence of such bodies is confirmed by numerical integrations over timescales up to 10 billion years using the symplectic N-body code SyMBA. In addition we find that there exist at least two other stable regions where oligarchs may reside. These results suggest that the Solar System contains several dozen oligarchic co-orbitals:  - At least four known trans-Neptunian objects (Pluto, Charon, Haumea, Makemake) have been found to exhibit this type of dynamics; - There exists another region around 30 AU containing three additional bodies (Sedna, 2000 CR 105 , 2003 SQ 317 ); - Finally, our simulations indicate that there might also be an additional group of oligarchs located between 50-60 AU.","label":1,"model":"bloomz","source":"arxiv","id":3156}
{"text":"We present an analytic expression for the one-loop mass-less triangle Feynman integral in terms of generalized hypergeometric functions. The results are obtained by using Mellin-Barnes representation and contour integration techniques. We also provide numerical values for some special cases which can be used to check our analytical expressions. This work is motivated by recent interest on the study of higher order corrections to various physical processes, such as Higgs decay into two photons or gluons at next-to-leading-order (NLO) accuracy. \nI. INTRODUCTORY REMARK\nThe calculation of loop diagrams plays an important role in theoretical physics. In particular, it has been shown that the inclusion of radiative corrections leads to significant changes in the predictions of many observables [1] . For example, the NLO QCD correction to the decay widths of heavy quarks [2] , top quark pair production [3] , Higgs boson decays [4] etc., have been calculated recently with great success. However, there still remain several open problems related to the evaluation of multi-loop integrals [5] .\nIn this letter we consider the following one-loop mass-less triangle Feyman integral [6] : \nwhere m 1 = m 2 \u2261 m 3 \u2261 m 4 \u2261 0 and s 12 = q 2 . It should be noted here that I(q 2 ) vanishes when any three masses become equal i.e. m 1 = m 2 = m 3 = m 4 .","label":1,"model":"bloomz","source":"arxiv","id":3157}
{"text":"We have performed an analysis of archival Chandra data for the elliptical galaxy NGC 4593, which is known to harbor a supermassive black hole (SMBH) with mass M BH = 1.7 x 10^8 M sun . We find that there are two distinct components in its nuclear spectrum; one component has a power-law shape and dominates at energies above 2 keV while another component shows strong emission lines below 2 keV. The former can be explained by Comptonization of soft photons emitted from the accretion disk around SMBHs, whereas the latter may originate from photoionized plasma surrounding the central engine or from relativistic jets. In addition, we found evidence for absorption features due to ionized iron K-shell transitions near 6.4-6.9 keV. These results suggest that this object harbors both a standard thin accretion disk as well as a hot corona.","label":1,"model":"bloomz","source":"arxiv","id":3158}
{"text":"We study the G-convergence for nonlinear viscoelastic flows in bounded domains with smooth boundaries, where the constitutive relation is given by an integral-type equation. We prove that if the sequence of solutions to the corresponding problems converges weakly* in $L^\\infty(0,T;L^2)$ then it also converges strongly in $L^1(0,T;H^{-1})$. As applications we consider two examples of such equations. The first one is the Oldroyd-B model which describes the flow behavior of polymeric fluids. In this case our results show that the limit problem can be formulated as a system of partial differential equations coupled through the stress tensor. The second example deals with the so-called Maxwell fluid whose relaxation time depends on the shear rate. Here we obtain a new formulation of the limit problem involving only ordinary differential equations. Finally, we present some numerical experiments illustrating these theoretical results. Keywords: Viscoelasticity, Homogenization, Limiting process","label":1,"model":"bloomz","source":"arxiv","id":3159}
{"text":"The geochemical behavior of uranium (U) and thorium (Th), which are radioactive elements, is discussed in relation to their influence on the origin and evolution of the crust of earth as well as biological evolution.  The chemical properties of these two elements are similar; however, they have different physical characteristics that affect how they behave geologically.  Uranium has an atomic number of 92 with a half-life of 4.5 billion years while thorium has an atomic number of 90 with a half life of 1.4 billion years.   Both elements occur naturally throughout the Earth's crust but at varying concentrations depending upon the rock type.  They can be found in igneous rocks such as granite or basalt where they form minerals like uranite or thorite respectively.  These minerals may also contain other trace metals including lead, silver, gold, copper, zinc, arsenic, selenium, molybdenum, cadmium, mercury, bismuth, antimony, tellurium, cobalt, nickel, manganese, iron, vanadium, chromium, tungsten, titanium, zirconium, niobium, tantalum, hafnium, rhenium, osmium, iridium, platinum, palladium, rhodium, ruthenium, and iridium.  Uranium and thorium are also present in sedimentary rocks such as sandstone, shale, limestone, dolomite, gypsum, anhydrite, salt beds, and evaporites.  Sedimentary rocks are formed by weathering processes when water erodes...","label":1,"model":"bloomz","source":"arxiv","id":3160}
{"text":"We present results on flame evolution during type Ia supernova (SN) explosions, based on two-dimensional hydrodynamic simulations with detailed nuclear reaction networks for both deflagrations and detonations. We find that the transition to detonation is triggered by shock-induced turbulent mixing at densities around 10$^{9}$ g\/cm$^3$. The resulting detonation wave propagates through the entire white dwarf star within about 1 s after ignition. In this scenario, the observed light curve can be reproduced if we assume an initial central density of 2 x $10^8$ g\/cm$^3$ or higher. For lower values of the central density, the explosion fails to produce enough 56Ni to explain observations. This work was supported by NASA grant NNX10AD03G. Keywords: Flame propagation, Hydrodynamics, Nuclear burning, Supernovae, White dwarfs. Subject headings: Nucleosynthesis","label":1,"model":"bloomz","source":"arxiv","id":3161}
{"text":"The effect of an external magnetic field on the structure and dynamics of vortices in type-II superconductors is studied by numerical simulations based on the Ginzburg-Landau model with periodic boundary conditions. The results show that, for certain values of the applied current density Jc , there are two different types of vortex structures which can be observed depending on the magnitude of the external magnetic field H . For small fields (H < 0.5Jc ), we observe a triangular vortex lattice similar to Abrikosov's solution. However, when increasing the field strength further (0.5Jc < H < 1.2Jc ) this regular pattern becomes unstable and transforms into a disordered state consisting of randomly distributed dislocations. Finally, at higher fields (1.2Jc < H ) , the system undergoes another phase transition where it forms a square vortex lattice. This behavior is explained by considering the stability properties of the vortex solutions as well as their interaction energies.","label":1,"model":"bloomz","source":"arxiv","id":3162}
{"text":"We study the computational complexity of games with imperfect information, where players have no knowledge about each other's moves and actions are not necessarily alternating between them. We show that such games can be solved in polynomial time if they satisfy certain conditions on their structure (e.g., being bipartite). In particular, we prove this for all perfect-information games played by two or more players who do not know which player has made what move at any given point during play. Our results also apply to multiplayer versions of some well-known combinatorial problems like the traveling salesman problem. Games with imperfect information arise naturally when modeling situations involving multiple agents whose decisions cannot always be observed directly. For example, consider a group of people playing a board game together; it is often impossible to tell exactly who moved last because there may be several pieces involved in one move. Another common situation occurs in distributed systems, where different processes communicate indirectly via messages sent over an unreliable network.","label":1,"model":"bloomz","source":"arxiv","id":3163}
{"text":"The author presents the controversy surrounding the derivation and use of a dispersion relation for magnetohydrodynamic (MHD) waves in plasmas, which is based upon an assumption that the plasma frequency is much greater than the wave frequency.  The author shows how this assumption leads to incorrect results when applied to Alfv\u00e9n waves propagating parallel or antiparallel to magnetic field lines; he also discusses other problems with using such relations. Finally, he proposes a new method for deriving a dispersion relation for these waves. This article was originally published as part of the series \"Perspectives in Plasma Physics\" by the American Institute of Physics. It has been reproduced here with permission. For more information about reproducing AIP content visit http:\/\/journals.aip.org\/authors\/rightsandpermissions. The author thanks Dr. J. D. Heymann for his help in preparing this version of the manuscript.","label":1,"model":"bloomz","source":"arxiv","id":3164}
{"text":"We present the results on morphology and luminosity function for the most luminous galaxy clusters in the Universe, selected by their X-ray emission (the RCS2 sample). We find that these objects are characterized by an elliptical shape with axial ratio q = 0.7 \u00b1 0.1 and by a steep luminosity function dN\/dL \u221d L\u22122.5\u00b10.3 . The observed properties suggest that they may be identified as fossil groups or proto-clusters at z > 1.0 .\nThe data used here were obtained during our observing runs performed at ESO telescopes under programs IDs: 073.A-0505(B), 078.A-0518(C) and 079.A-0739(D) . In this work we study the morphological and photometric properties of the brightest galaxy clusters in the universe. These systems have been detected through their X-ray emission using the ROSAT All Sky Survey (RASS; Voges et al., 1999) , and then followed up spectroscopically to confirm their redshifts and measure their velocity dispersions (see e.g. Rosati et al. , 1998 , Gladders & Yee 2005 , Eisenhardt et al. , 2008 . They represent some of the most massive structures known so far in the universe, being able to host several thousands of galaxies each one. Their high mass makes them ideal targets to investigate how such large scale structures form and evolve over time.","label":1,"model":"bloomz","source":"arxiv","id":3165}
{"text":"We present the results on galaxy population in the most massive supercluster, SCl 126 (Abell 1689), based on spectroscopic data obtained with VLT\/VIMOS and Keck\/DEIMOS telescopes. We find that galaxies are distributed along filaments which connect clusters at different redshifts. The fraction of blue galaxies increases towards lower redshift, while the fraction of early-type galaxies decreases. This trend is more pronounced for bright galaxies than faint ones. In addition to this general picture we also detect some interesting features such as an excess of late-type galaxies around Abell 1689A cluster or a lack of bright galaxies between Abell 1689B and C clusters. These findings suggest that there may be significant differences among galaxy properties within individual clusters depending on their location relative to other structures. Our analysis shows that the observed trends can not be explained by simple passive evolution of stellar populations but require additional mechanisms like mergers and\/or interactions.","label":1,"model":"bloomz","source":"arxiv","id":3166}
{"text":"The purpose of this study is to examine the role that evaluation plays in communication between teachers and students during computer-based learning environments (CBLE). The research question guiding this investigation is: How does evaluation affect student-teacher interaction? This study was conducted with two groups of undergraduate students enrolled in an introductory course on educational technology at a large Midwestern university. Students were asked to complete three tasks using a CBLE called WebQuests, which are designed for use by students working individually or collaboratively. Data collection included audio recordings of group discussions, field notes taken by researchers observing each group's work, and students' written responses to questions posed throughout their participation in the project. Analysis revealed that evaluation played several roles within these interactions including providing feedback about individual performance; clarifying expectations; establishing ground rules; and promoting reflection. These findings suggest that evaluation can be used effectively to enhance student-teacher communication when it occurs frequently enough over time so that both parties have multiple opportunities to respond to one another.","label":1,"model":"bloomz","source":"arxiv","id":3167}
{"text":"In this work, we propose an adaptive service provisioning scheme to provide quality-of-service (QoS) guarantees and maximize the network utility by jointly optimizing resource allocation at both base stations (BSs) and mobile users (MUs). The proposed scheme is based on a communication model that incorporates user mobility into the QoS requirements. We formulate the problem as a joint optimization over BSs' power control variables, MUs' transmission rates, and their association with BSs. To solve it efficiently, we first decompose the original problem into two subproblems: one for each BS and another for all MUs. Then, we develop distributed algorithms to obtain solutions to these problems iteratively using dual decomposition techniques. Finally, simulation results show that our proposed algorithm can achieve better performance than existing schemes under various system settings. In recent years, wireless networks have been widely deployed around the world due to their low cost and easy deployment [1] . However, they are vulnerable to security attacks such as eavesdropping [2] , jamming [3] , and data tampering [4] .\nTo enhance the security level of wireless communications, physical layer security has attracted much attention recently [5] - [8] . Physical layer security exploits the characteristics of the wireless channel to ensure secure transmissions without relying on any additional cryptographic keys or protocols [9] . It was shown in [10] that if the legitimate transmitter-receiver pair shares no common information about the statistical properties of the channels between them and other potential eavesdroppers, then perfect secrecy cannot be achieved even when there exists infinite number of antennas at the transmitter side. Therefore, practical approaches should consider imperfections in the estimation process [11] , limited transmit power [12] , and finite antenna numbers [13] .","label":1,"model":"bloomz","source":"arxiv","id":3168}
{"text":"We update the analysis of Ref.  by including recent measurements and theoretical developments, with particular emphasis on the impact of lattice QCD results for form factors at large recoil momentum transfer. We find that the current experimental data are consistent with Standard Model predictions within uncertainties. The most stringent bounds on possible contributions to these processes from beyond-the-Standard-Model particles can be obtained using the measured branching fractions together with the corresponding decay amplitudes predicted by specific models. In this work we consider several scenarios motivated by supersymmetric extensions of the Standard Model as well as generic effective field theories containing additional heavy degrees of freedom. \nI. INTRODUCTORY REMARkS\nThe study of rare charm meson decays is an important tool to search for new phenomena beyond the Standard Model (SM). These processes occur through loop diagrams involving virtual W bosons or top quarks; hence they provide sensitive probes of new heavy states which may exist near the TeV scale [1] . Moreover, since the SM contribution to such processes is suppressed due to helicity conservation, any observed deviations could potentially signal the presence of new interactions [2] .\nIn this Letter we present updated limits on new physics effects in two classes of rare charm decays: D 0 \u2192 K * \u00b5 + \u00b5 \u2212 , where K * denotes either a vector or tensor resonance [3] ; and D 0 \u2192 \u03c0 0 \u2113 + \u2113 \u2212 [4] . For both cases we use the latest available experimental information [5] - [8] along with state-of-the-art calculations of hadronic matrix elements [9] - [11] . Our main focus will be on the implications of recent lattice QCD computations [12] - [14] for the case of D 0 \u2192 K * \u2113 + \u2113\u2212 .","label":1,"model":"bloomz","source":"arxiv","id":3169}
{"text":"In this note we present some comments on the examples given in [1] and [2] . We show that these examples are not counterexamples to the generalized Jacobian conjecture, as stated by M. Laurent (see [3] ). In fact they do not even contradict the weaker statement made by J.-P. Serre [4] , which is equivalent to the Jacobian conjecture for curves over finite fields. Finally we give an example showing how one can construct counterexamples to the generalized Jacobi conjecture using our method. Let k be any field with char(k) = p > 0. For every integer n \u2265 1 let Xn denote the smooth projective curve defined over k by y n + a1yn\u22121 + \u00b7 \u00b7 \u00b7 + anny0 = xn+1,\nwhere ai \u2208 k * . It was shown by A. N. Parshin [5] that if char(k) = 2 then there exists a positive integer m such that the jacobian variety JacXm has complex multiplication. This implies that the jacobian varieties JacXn have complex multiplication for all integers n \u2261 \u00b11 mod m. If char(k) = 3 it follows from [6] that JacX3 does not have complex multiplication. However, it still remains open whether or not JacX4 has complex multiplication.","label":1,"model":"bloomz","source":"arxiv","id":3170}
{"text":"The asteroid 144898 was discovered on September 24, 2004 by the Catalina Sky Survey at an apparent magnitude of 18.7 and is classified as potentially hazardous due to its large size.  The orbit has been determined using astrometric observations made with the US Naval Observatory's 1-meter telescope in Flagstaff Arizona between October 2005 and March 2007.   These data show that this object will not impact Earth during the next 100 years but may be a good candidate for future space mission targets. This work was supported by NASA under grant NNX07AG70G issued through the Planetary Defense Coordination Office. We report here our results of physical investigations carried out on the surface of the asteroid 144898 (2004VD17). Our analysis shows that it is a S-type asteroid with a diameter D = 2.5 \u00b1 0.2 km. Its rotation period P = 3.6 \u00b1 0.1 hours and pole position are also derived.","label":1,"model":"bloomz","source":"arxiv","id":3171}
{"text":"In this work, we propose an efficient multimedia content distribution scheme for hybrid wireless networks (HWNs). The proposed scheme is based on weighted clustering and can be used to distribute the same or different types of contents simultaneously over HWNs with multiple base stations (BSs) and mobile users (MUs), where each BS has limited storage capacity. In order to maximize the total number of MUs that receive their requested contents within their deadlines, our objective is to minimize the maximum delay among all MUs by optimizing the cluster sizes at each BS. We formulate the problem as a mixed integer linear programming model and solve it through Lagrangian relaxation-based heuristics. Extensive simulation results show that compared with existing schemes, the proposed one achieves better performance in terms of average user satisfaction ratio and average transmission time per MU. Keywords: Multimedia content distribution; Hybrid wireless networks; Delay minimization","label":1,"model":"bloomz","source":"arxiv","id":3172}
{"text":"We propose an algorithm to select the injection point in hybrid networks, which is based on local information and can be implemented with low overheads. The proposed algorithm has two phases. First, it elects one node as the injection point by using only local information. Second, if there are multiple nodes that have been elected as the injection points, then these nodes will negotiate among themselves to determine exactly one injection point. We prove that our algorithm always selects at least one node as the injection points when all nodes are reachable from each other. Furthermore, we show through simulations that our algorithm performs well even under various network conditions such as dynamic topologies or limited communication ranges. Keywords: Distributed system, Localization, Self-organization, Wireless sensor network (WSN), Data dissemination, Energy efficiency, Routing protocol, Location-based routing, Clustering, Load balancing, Traffic engineering, Internet service provider","label":1,"model":"bloomz","source":"arxiv","id":3173}
{"text":"We present the results of an optical spectroscopic survey aimed at finding tidal dwarf galaxies (TDGs) candidates among a sample of ultraluminous infrared galaxies (ULIRG). We have obtained spectra with FORS2@VLT and GMOS-N@Gemini telescopes, covering the wavelength range between 3600 \u00c5 and 9200 \u00c5 , for a total of 16 ULIRGs selected on the basis of their high far-infrared luminosity (L FIR > 10 12 L \u2299 ) and low redshift (z < 0.1).\nThe main goal is to find TDGs that are expected to be found around interacting systems such as ULIRGs. The presence of young stars in these objects would make them detectable through strong emission lines like H\u03b1 or [OII] . However, we do not detect any significant excess of emission line flux over what can be explained by star formation occurring within the host galaxy itself. This result suggests either that there are no TDGs associated with our sample of ULIRGs or that they are too faint and\/or too red to be detected using current facilities.","label":1,"model":"bloomz","source":"arxiv","id":3174}
{"text":"We present an overview of recent theoretical progress in understanding B decays to light vector mesons, including both $\\rho$ and $\\pi^0$ states.  We discuss how these processes can be used as probes for new physics effects at low energies (e.g., via flavor-changing neutral currents) or high scales (via virtual heavy particles).  In particular we focus on the possibility that some puzzling experimental results observed recently by BaBar may signal new physics contributions to the decay amplitudes. Finally, we briefly comment on other interesting topics related to this subject such as CP violation in B decays into two light vectors and rare radiative transitions between vector meson pairs. The study of semileptonic and nonleptonic weak interactions involving hadrons is one of the most important tools available to particle physicists today. These reactions are sensitive to many fundamental parameters of our theory, like quark masses, CKM matrix elements, and couplings of gauge bosons to quarks and leptons. They also provide crucial information about the structure of hadronic matter through their dependence on form factors describing the distribution of charge and current inside composite systems. Moreover, they play a key role in testing the consistency of the Standard Model (SM), since any deviation from SM predictions would indicate the presence of New Physics (NP).","label":1,"model":"bloomz","source":"arxiv","id":3175}
{"text":"The global potential energy minima for the water clusters H$_{2}$O, H$_{3}$O+ and H$_{4}$O++ have been determined by ab initio molecular dynamics simulations using density functional theory with B3LYP\/6-311G(d,p). The results show that these three clusters can be stabilized in different ways on graphite surfaces depending upon their size and charge state. For example, the neutral cluster H$_{2}$O is found to adsorb flatly onto the surface via hydrogen bonding between its oxygen atoms and carbon atoms at the edges of the graphite sheets; while the protonated cluster H$_{3}$O+$ is observed to form an angle of $70^{\\circ}$ with respect to the surface normal due to electrostatic attraction between the positively charged cluster core and negatively charged carbon atoms; finally, the doubly-charged cluster H$_{4}$ O++ is seen to lie almost parallel to the surface plane because it has two positive charges which are balanced by one negative charge located near each end of the cluster.","label":1,"model":"bloomz","source":"arxiv","id":3176}
{"text":"We present an analysis of the evolution of interstellar dust grains, based on their size distribution inferred by infrared observations with ISO (Infrared Space Observatory). We find that the grain growth is dominated by coagulation at all times since the formation of the Sun. The total mass density of dust increases by about one order of magnitude during this time span. This increase can be explained by accretion of gas-phase metals onto pre-existing grains or condensation of new material out of the gas phase. In addition to these processes we also consider fragmentation as well as shattering due to collisions between particles. Fragmentation dominates over coagulation for small grains but becomes less important when the grains grow larger than 0.1 micrometres. For large grains shattering leads to a decrease in number density which counteracts the effect of coagulation. Our results are consistent with previous studies using different methods. \n \n Keywords: Interstellar medium","label":1,"model":"bloomz","source":"arxiv","id":3177}
{"text":"The main goal of this research is the study and analysis of how different users can contribute in an open source software project, by sharing their knowledge through comments on issues or pull requests (PRs). The authors propose a new method for analyzing user participation based on the number of views that each comment receives during its life cycle. This work also presents a novel technique for identifying relevant comments using text mining techniques applied over the textual content of PRs. Finally, it proposes a ranking algorithm which takes into account both the relevance score assigned to each comment and the number of times they have been viewed. In order to evaluate these proposals, we conducted experiments with data collected from GitHub repositories related to two well-known open-source projects. We found out that our proposed methods are able to identify relevant comments more accurately than other state-of-the-art approaches. Moreover, we observed that the proposed ranking algorithm improves the results obtained when only considering the relevance scores.","label":1,"model":"bloomz","source":"arxiv","id":3178}
{"text":"We study the effects on the entropy and Hawking temperature of extremal black holes due to higher derivative terms in the action, namely those coming from supersymmetric (SUSY) or quadratic curvature invariants such as the Gauss-Bonnet term. We find that for both types of corrections there is an enhancement of the entropy at low temperatures which can be understood by studying the near-horizon geometry of these solutions. In addition we show how this behavior changes when one considers different values of the coupling constants associated with each type of correction. Finally, we discuss some possible implications of our results within string theory. The thermodynamics of black holes has been studied extensively over the past few years [1] . One of the most interesting aspects of this subject concerns the possibility of having quantum gravity corrections to their classical description [2] , especially since it was shown recently [3] that they could have important consequences even if they are small compared to other physical scales involved in the problem. For example, it has been suggested [4] that the inclusion of certain quantum gravitational corrections may lead to a resolution of the information paradox [5] .\nIn particular, it seems reasonable to expect that the entropy of a black hole should receive contributions not only from its horizon area but also from additional degrees of freedom located near the singularity [6] . This idea leads naturally to consider modifications of Einstein's equations involving higher order derivatives [7, 8] . However, although many authors have considered various forms of higher-order corrections [9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59 ], very little attention has been paid so far [60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71,","label":1,"model":"bloomz","source":"arxiv","id":3179}
{"text":"We report on Suzaku observations for four active galactic nuclei (AGNs) detected by Swift\/BAT survey, which are classified as \"obscured AGNs\" with column density larger than 10 24 cm-2 . We found that all these sources show strong Fe K emission lines and their line widths are broader than those expected from thermal broadening at kT = 100 keV. The observed line profiles can be reproduced well by relativistic disk reflection models including Compton scattering effects. These results suggest that there is an additional component to the X-ray continuum other than the standard thin accretion disks around supermassive black holes. In addition, we find that the iron abundance relative to solar value is higher than 1.5 times in three out of four objects. This suggests that the central engines of these obscured AGNs may have been buried under heavy dusty torii. Finally, we discuss possible origins of this new type of buried supermassive black holes based on our observational results.","label":1,"model":"bloomz","source":"arxiv","id":3180}
{"text":"We study the Green function of a relativistic $\\alpha$-ssble process in a half-space with reflecting boundary condition at the boundary, and obtain two-sided estimates on its tail behavior by using the method of reflection principle. The results are applied to the case when the underlying space is $\\mathbb{R}^d$ or $\\mathbb{H}^1(\\mathbb{R})$. We also give an example showing that our upper bound can be strictly smaller than the lower one. Finally we prove some properties of the Green functions of relativistic $\\alpha$-stables processes killed upon leaving a domain. Let $(X_t)_{t \\geq 0}$ be a relativistic $\\alpha$-self-decomposable L\u00e9vy process (see Definition 2 below), which has been introduced independently by Bertoin [1] , Sato [2] and Watanabe [3] . It is known that this process possesses many interesting properties such as self-similarity, infinite divisibility etc. (see e.g., [4] ). In particular, it follows from [5] that there exists a constant $c > 0$ depending only on $\\alpha$ such that \n$$P[X_1 = 0] \\leq c \\exp \\left( - \\frac{1}{2} |x|^{2 + \\alpha} \\right). $$","label":1,"model":"bloomz","source":"arxiv","id":3181}
{"text":"We study the Laplacian spectrum of complex networks, focusing in particular on its relation to the dynamics of random walks on them. We show that for any network with n nodes there are at most 2n eigenvalues different from zero (counting multiplicities). This bound is tight up to a constant factor when applied to trees or complete graphs. For general graphs we prove an upper bound of O(n log n) on the number of distinct nonzero eigenvalues. In addition, we provide lower bounds showing that this estimate cannot be improved by more than a polylogarithmic factor. Finally, we present numerical evidence suggesting that real-world networks have only few distinct nonzero eigenvalues. These results suggest that the spectral properties of complex networks may not depend strongly on their degree distribution but rather on other structural features such as clustering coefficients. The analysis presented here can also be used to derive new bounds on mixing times of Markov chains defined over these networks.","label":1,"model":"bloomz","source":"arxiv","id":3182}
{"text":"The Stochastic Loewner Evolution (SLE) is an important tool in the study of random curves and fractals, especially those arising as scaling limits of discrete models such as lattice paths or self-avoiding walks.  The SLE trace can be thought of as a continuous version of Brownian motion with drift; it has been shown to have connections to many other fields including quantum gravity, string theory, statistical mechanics, probability theory, mathematical physics, number theory, and computer science.   In this article we will give a brief introduction to the basic concepts behind the SLE process, along with some examples that illustrate its use. We also provide references for further reading on the subject. For more information about the SLE process see the following articles:  http:\/\/arxiv.org\/abs\/math\/9906028 http:\/\/arxiv.org\/sabs\/0909.0366 http:\/\/arxiv.org\/10.1103\/PhysRevE.71.026110","label":1,"model":"bloomz","source":"arxiv","id":3183}
{"text":"We consider the generalized Korteweg-de Vries (gKdV) equation with periodic boundary conditions and initial data in Sobolev spaces, which are close to solitary waves. We prove that solutions exist globally in time and converge exponentially fast towards the corresponding solitary wave as t goes to infinity. \nThe main idea is to use an appropriate Lyapunov functional together with suitable energy estimates. The proof relies on refined asymptotic expansions near solitary waves developed by Martel & Merle [J. Math. Pures Appl. (9), vol. 84, no. 1, pp. 1-33, 2004] . \nIntroduction\n\nIn this work we study the following nonlinear evolution equation: \n \n \n \n u_t + 6uu_x - u_{xxx} = 0 , x \\in [0,1] , t > 0 ,\n u(x,0) = u_0(x) , x \\in [0,1] .\n \nHere u : [0,1] \\times [0,\\infty) \\to \\mathbb{R}$ denotes the unknown function. This problem arises naturally when studying water waves over a flat bottom under the influence of gravity. In fact, if one considers the flow of shallow water at rest above a horizontal bottom, then it can be shown that the free surface elevation satisfies the so-called Benney-Luke system, see e.g. \n\n\n[1] or [2] . If one neglects higher order terms arising from the dispersion relation, then one obtains the classical Korteweg-de Vires (KdV) equation. For more details about the derivation of the model considered here, we refer to [3] .","label":1,"model":"bloomz","source":"arxiv","id":3184}
{"text":"We report on the generation of an unprecedentedly broad spectrum (>10 THz) by using a novel type of fiber, which is based on a microstructured optical fiber with a suspended core surrounded by air holes filled with liquid. The suspended-core fiber supports two guided modes that are confined to different regions along its length. One mode propagates through the central region of the fiber where it experiences strong confinement due to the high index contrast between silica glass and air; this results in large effective nonlinearity. The other mode propagates outside the central region of the suspended core, where it sees only weak refractive-index guiding. This leads to low loss for both modes over a wide wavelength range. We demonstrate experimentally that these two modes can be coupled together efficiently via stimulated Raman scattering when pumped at 1.55 \u00b5m. As a result, we obtain a broadband continuum spanning more than 10 THz centered around 2.7 \u00b5m.","label":1,"model":"bloomz","source":"arxiv","id":3185}
{"text":"We present new optical\/NIR data for GRB 060206, which show that its X-ray to radio afterglow is well described by an achromatic break at tbreak = 1.3 days followed by a power-law decay with index -1.2 (F\u03bd \u221d t-1.2). The lack of any spectral evolution across this break suggests it was caused by energy injection into the blast wave. We find no evidence for dust extinction along our line-of-sight; however we cannot rule out significant reddening due to host galaxy dust. Our results are consistent with previous claims that achromatic breaks observed in many other bursts may be explained as being due to late-time energy injections rather than jet-break effects. \n \n Keywords: Gamma-ray burst, Afterglow emission, Energy injection, Jet break, Redshift measurement \n \n INTRODUCTION \n \n In recent years there has been growing interest in understanding how gamma ray bursts (GRBs) produce their broadband electromagnetic radiation. This effort has led to several successful models describing the prompt phase of GRB emission (see e.g., Piran 2005; Zhang 2007), but less progress on explaining the origin of the afterglow component. A key feature of most afterglows is the presence of a steepening or 'jet break' in the light curve around one day postburst (Rhoads 1999) . Such breaks have traditionally been interpreted as marking the time when the relativistic ejecta becomes optically thin to synchrotron self-absorption, causing the flux density to drop rapidly. However, some authors argue that such breaks can also arise if the ejecta undergoes continued energy input following the initial explosion (e.g., Kumar & Panaitescu 2000; Granot et al. 2001; Chevalier & Li 2000) , while others suggest that they could instead result from changes in the geometry of the emitting region (e.g., Racusin et al. 2008 ). An alternative explanation for these breaks invokes interstellar scintillation (Goodman 1997; Goodman & Narayan 2006 ) - a phenomenon","label":1,"model":"bloomz","source":"arxiv","id":3186}
{"text":"We present the first measurement of single-transverse-spin asymmetries (SSA) for hadronic dijets produced at midrapidity in p+p collisions at sqrt(sNN) = 5.02 TeV using data collected by the CMS experiment during 2012 corresponding to an integrated luminosity of 2.3 fb-1 . The SSAs are extracted as functions of jet transverse momentum and rapidity, azimuthal angle between jets, and event centrality. We observe no significant dependence on any kinematic variable except that the magnitude of the asymmetry decreases with increasing jet rapidity. Our results are compared to theoretical predictions based on perturbative QCD calculations including higher-order corrections and parton distribution function uncertainties. \nThe measured values agree well within experimental and theoretical uncertainties. This is the most precise measurement of this observable performed so far. \n \n Introduction \n \n Single transverse-spin asymmetries have been observed in several processes involving polarized protons or neutrons [1] , such as inclusive pion production [2] , semi-inclusive deep-inelastic scattering [3] , Drell-Yan lepton pair production [4] , prompt photon production [5] , and direct photons [6] . These measurements provide important information about the spin structure of nucleons [7, 8] .\n \nIn particular, they can be used to test the validity of factorization theorems [9] which relate hard-scattering cross sections to partonic distributions inside the proton [10] . In addition, these observables may also shed light on new physics beyond the Standard Model [11] . \n \n For example, it has recently been suggested [12] that large single-spin asymmetries could arise due to the interference of two amplitudes describing different helicities of quarks emitted from longitudinally polarized gluons in high-energy pp collisions. Such effects would violate parity conservation and thus constitute evidence for new physics [13] . However, there exists only one previous measurement [14] of single-spin asymmeties in hadronic dijet production at high energies. That study was carried out at RHIC [15] where the center-of-mass energy per nucleon-nucleon collision \u221asNN=200 GeV is much lower","label":1,"model":"bloomz","source":"arxiv","id":3187}
{"text":"We present new age determinations for star clusters in the Small Magellanic: Cloud (SMC). We use photometry obtained with HST\/WFPC2 and ground-based telescopes to determine ages, metallicities, reddenings, and distances for these objects using isochrone fitting techniques. The resulting sample consists of 39 open clusters and associations ranging in age between 1 Myr and 10 Gyr. Our results are compared with previous studies as well as theoretical predictions based on chemical evolution models. In particular we find that there appears to be an excess number of young open clusters relative to older ones which cannot be explained by current evolutionary synthesis models or simple closed-box chemical evolution scenarios. This suggests either that the SMC has experienced recent bursts of star formation or that it may have been more gas-rich in its past than currently observed. These findings will provide important constraints on future generations of chemical evolution models. \n \n Keywords: Open cluster","label":1,"model":"bloomz","source":"arxiv","id":3188}
{"text":"We present the results of an extensive survey for absorption lines in high-resolution spectra of quasars at redshifts z < 0.3, obtained with HST\/STIS (the Hubble Space Telescope Spectrograph). The sample consists of 12 QSOs observed over a total exposure time of about 1 Ms. We detect more than 1000 Lyman alpha forest absorbers along each line-of-sight to these QSOs; we also find that most sight-lines show strong absorption by high-ionization species such as C IV and Si IV. In addition, we identify several hundred systems showing absorption due to highly ionized oxygen, O VI . These are among the strongest known O VI absorbers anywhere in the universe. By combining our data set with previous surveys, we obtain new constraints on the physical conditions of this gas phase. Our main conclusions are:  -The number density of O VI absorbers per unit redshift is n(O VI) = 2.6 +\/- 0.4 x 10^-3 cm^-3 , which corresponds to a mean separation between absorber pairs of r = 4.1 +\/- 0.5 h-1 Gpc.  -Most O VI absorbers have temperatures T ~ 10 5 K and densities log ne\/cm -3 > 13.8. About half of them appear to be associated with galaxies within 50 kpc projected distance. -There appears to be no correlation between the properties of O VI absorbers and those of their host galaxy population.","label":1,"model":"bloomz","source":"arxiv","id":3189}
{"text":"We present new optical integral field spectroscopy data for the galaxy cluster Abell 3627 (z=0.25) obtained with SINFONI at VLT\/UT4. We detect an extended emission line region around one member galaxy, ESO 137-001, which is associated to star formation activity. The total extent of this region is about 1 Mpc along the line-of-sight. This galaxy shows evidence for recent interaction\/merger events as well as ongoing nuclear starbursts. In addition we find several other galaxies showing signs of on-going or past interactions within the central part of the cluster. These results are discussed in terms of the effects of environment on galaxy evolution. Keywords: Galaxy clusters; Star formation rate; Environment; Integral Field Spectroscopy. Subject headings: Galaxies: clusters: general -Galaxy clusters -Star formation rates -Environmental effects -Integral field spectroscopy","label":1,"model":"bloomz","source":"arxiv","id":3190}
{"text":"We present new results on the chemical composition of interstellar matter (ISM) based on high-resolution spectroscopy of two bright Galactic planetary nebulae, NGC 6153 and IC 4593. We find that these objects are characterized by an overabundance of oxygen relative to iron compared with solar values. This is consistent with previous studies of other PNe but our data show for the first time that this effect persists even when we consider only those ions which are not affected by photoionization effects.  The observed O\/Fe ratio can be explained if most of the Fe atoms were locked up into dust grains prior to their destruction during the PN phase. In addition, we detect significant amounts of sulfur in both targets indicating that at least some fraction of S was also incorporated into dust grains before being released back into the gas phase after grain destruction. Finally, we argue that the presence of such large quantities of metals in the ISM may lead to systematic errors in determinations of elemental abundances using emission-line ratios measured in H II regions.","label":1,"model":"bloomz","source":"arxiv","id":3191}
{"text":"We report on kinetic-ion simulations addressing whether ion trapping inflates stimulated Brillouin backscattering (SBS) reflectivities in the presence of an electron beam and plasma waves. We find that, for typical parameters relevant to high-power laser-plasma experiments, SBS is dominated by electrostatic Langmuir wave instabilities rather than ion-acoustic modes. The latter are suppressed due to Landau damping as well as mode conversion into electromagnetic radiation at oblique angles with respect to the direction of propagation. In addition, we show that the effect of ion trapping can be neglected if the density fluctuations associated with the trapped ions are small compared to those caused by the electrons. Finally, we demonstrate that the inclusion of ion trapping does not significantly affect the growth rates or saturation levels of the dominant electrostatic Langmuir waves. This finding suggests that the observed discrepancies between theory predictions and experimental results may originate from other effects such as nonlocality and\/or nonlinear coupling among different types of waves.","label":1,"model":"bloomz","source":"arxiv","id":3192}
{"text":"We report on observations made with Herschel Space Observatory (Pilbratt et al., 2010) of water vapour emission lines at 557 GHz, 1669 GHz and 1720 GHz towards two young stars surrounded by circumstellar disks: HD 100546 and TW Hya. The data were obtained as part of the Open Time Key Programme \"Formation and Evolution of Planetary Systems\" (FEPS). We detect water vapour emission over an extended range of radial velocities for both targets. For HD 100546 we find that the line profiles are consistent with Keplerian rotation around a central mass of 1.8 M . In addition to this broad component, which is likely associated with the outer regions of the disk, there appears to be a narrower feature superimposed on each profile. This narrow component may arise either from gas located close to the star or from outflowing material along our line-of-sight.","label":1,"model":"bloomz","source":"arxiv","id":3193}
{"text":"We study the phenomenological consequences of general speed of sound in brane inflationary models, where the inflaton is identified as the distance between two parallel branes moving on an extra dimension. We find that for small values of the speed of sound (cs < 0.1), there are no significant changes to the predictions made by standard slow-roll inflation. However, when cs > 0.1 we find that the tensor-to-scalar ratio r and the running of the spectral index dns\/d ln k can be significantly enhanced compared to their usual values predicted within the context of single field slow roll inflation. In particular, if cs = 1 then r = 16(nT)2\/5 and dns\/d ln k = \u22128(nT)1\/5, which may provide a possible explanation for recent observations of high value of nT reported by WMAP7 data combined with other CMB experiments.","label":1,"model":"bloomz","source":"arxiv","id":3194}
{"text":"We present an analytical model for the magneto-rotational instability (MRI) in protoplanetary disks, which is based on the assumption that the disk can be divided into two regions with different physical properties and dynamics. The inner region has a high density and temperature, while the outer one is less dense but hotter than the surrounding medium. We show how this simple picture allows us to reproduce many observed features of MRI-driven turbulence in accretion disks around young stars. In particular, we find that:  -The growth rate of the fastest growing mode decreases rapidly towards smaller radii due to the increasing gas pressure.  -The radial profile of the turbulent viscosity follows closely the profile of the magnetic field strength.  -The angular momentum transport efficiency increases strongly at small radii because of the rapid increase of the surface density there.  -The predicted mass accretion rates are consistent with those inferred observationally for T Tauri stars.","label":1,"model":"bloomz","source":"arxiv","id":3195}
{"text":"We present an analytical model that describes how quasars are triggered by galaxy mergers in elliptical galaxies (EEs). The merger rate is determined by the number density evolution of EEs as well as their merging timescale. We show that our model can reproduce observed quasar luminosity functions at different redshifts with reasonable parameters. In addition to triggering quasars, galaxy mergers also trigger starbursts which may be responsible for producing dusty obscured quasars. Our results suggest that most quasars reside in massive halos with masses greater than 10^12M_sun\/hour. This implies that quasars play important roles in regulating the growth of supermassive black holes and the formation of large-scale structures through energy feedback processes. Keywords: Quasars, Galaxy Evolution, Energy Feedback, Large-Scale Structure Formation. 1 Introduction   A large fraction of the baryons in the universe exist in the form of hot plasma known as intergalactic medium (IGM), but only about 5% of them have been detected so far. It has been suggested that this missing baryon component could be associated with dark matter halos via gravitational interactions (White et al., 1993; Fukugita et al., 1998; Cen & Ostriker 1999) . However, it remains unclear what physical mechanisms drive gas into these halos.  One possible mechanism is that cold streams penetrate deep inside dark matter halos and heat up due to shocks or other heating sources such as cosmic rays (Kere\u0161 et al., 2005; Dekel et al., 2009 ). Another possibility is that galactic winds driven by supernovae and\/or active galactic nuclei (AGNs) blow out some of the gas from the host galaxies (Springel & Hernquist 2003; Scannapieco et al., 2006) . These two scenarios predict very different properties of the accreted gas onto central supermassive black holes (SMBHs).  For example, if AGN-driven winds dominate over cold streams, then SMBHs should grow faster during early epochs when there were more powerful winds. On the contrary","label":1,"model":"bloomz","source":"arxiv","id":3196}
{"text":"We study the supersymmetry breaking in type II string theory on Calabi-Yau threefolds with fluxes turned on. We show that there is an infinite number of solutions to the equations for the supersymmetry conditions which are related by duality transformations. The dualities relate different types of branes wrapping cycles inside the internal space. In particular we find new solutions where D-brane instantons wrap non-trivial 3-cycles. These solutions can be interpreted as arising due to gaugino condensation on wrapped D7-branes. We also discuss how these results fit into the context of mirror symmetry. Finally, we comment on possible generalizations to other compactification manifolds such as orbifold singularities or G 2 -manifolds. Superstring theories provide us with a unique opportunity to explore quantum gravity at energies well below the Planck scale. However, it has been known since the early days of string theory [1] , that perturbative calculations break down when one tries to describe phenomena like black holes or cosmological evolution near the big bang\/crunch. This problem was solved by Sen [2] who showed that nonperturbative effects could stabilize certain moduli fields (like the dilaton) even if they were not protected by any gauge symmetries. Since then many authors have studied this phenomenon using various techniques [3] . One particularly interesting approach involves turning on background fluxes [4] .\nIn recent years much progress has been made towards understanding the dynamics of supersymmetric vacua in string theory [5] . It turns out that the low-energy effective action describing the physics around a given vacuum takes the form of N = 1 supergravity coupled to matter multiplets [6] . For example, in the case of type IIB strings on a Calabi Yau manifold M CY , the low energy description is given by the following action: \nwhere F p+1 denotes the RR field strength associated with the p + 1-form potential C p+1 . Here K(M CY ) is the K\u00e4hler potential encoding information about the complex structure deformations of M CY while W (M CY ) is its holomorphic","label":1,"model":"bloomz","source":"arxiv","id":3197}
{"text":"We present an analytical model to study the formation of red elliptical galaxies in hierarchical cosmologies. We assume that supermassive black holes (SMBHs) grow by accretion at high redshift z > 6 when gas is abundant but cooling times are short. The SMBH growth rate depends on its mass M\u2022(z), which we calculate using semi-analytical models based on merger trees extracted from N-body simulations. In our scenario, quasars shine brightly during their active phase with luminosities LQSO ~ 1045 erg s-1. They heat up surrounding cold gas clouds via photoionization heating and Compton scattering, preventing them from forming stars efficiently until they fade away after about 1 Gyr. This leads to a bimodal color distribution of early-type galaxies as observed today. Our results show good agreement with observations such as the quasar luminosity function, galaxy clustering statistics, and the fraction of red ellipticals among all massive galaxies.","label":1,"model":"bloomz","source":"arxiv","id":3198}
{"text":"We report on an experimental study of the spin-dependent properties of a two-dimensional electron system in GaAs\/AlGaAs quantum wells by means of polarized light scattering (PL) and magneto-optical spectroscopy (Raman). The PL experiments are performed at low temperatures, where we observe circularly polarized emission with opposite helicity for right- and left-circularly polarized excitation. We show that this effect is due to the presence of a strong exchange interaction which leads to a splitting of the excitonic states into two branches corresponding to different spins of the carriers. In addition, we find that the degree of circular polarization depends strongly on temperature and magnetic field strength. By comparing our results with those obtained by polarized Raman scattering under similar conditions, we demonstrate that both techniques provide complementary information about the electronic structure of the investigated samples. Our findings can be used as input parameters for theoretical models describing the transport phenomena in semiconductor nanostructures.","label":1,"model":"bloomz","source":"arxiv","id":3199}
{"text":"We report on the detection by Swift\/BAT and Konus-Wind of gamma-ray burst (GRB) 060927, which is one of only two events detected with redshifts greater than five to date.  The redshift was determined using optical spectroscopy obtained with the Very Large Telescope in Chile within three hours after the burst trigger time.  We find that this event has an extremely hard spectrum, similar to other high-redshift bursts observed previously.  Using our current understanding of the physics behind these phenomena we can estimate the total energy released during the prompt emission phase of the burst to be ~10^53 erg.  This value is comparable to or larger than the energies released by some supernovae.  If confirmed, it would make GRB 060927 the most energetic event ever recorded.  In addition, if the host galaxy mass function evolves similarly to what is seen locally, then the progenitor system responsible for producing such an energetic explosion must have been hosted by a very massive galaxy.  These results are consistent with theoretical predictions made about the progenitors of high-z GRBs.  Finally, we discuss how future observations of this object may help us understand the end of cosmic reionization.","label":1,"model":"bloomz","source":"arxiv","id":3200}
{"text":"The purpose of this article is to present an overview of some recent results in quantum field theory (QFT) on curved spaces with noncommutative coordinates. The main motivation for studying QFTs on such spaces comes from string theories which are formulated as open strings attached to D-branes whose positions can be described by noncommuting matrices. In particular we will focus our attention on the so-called Groenewold-Moyal plane which is defined as the space generated by two non-commuting coordinates satisfying the commutation relations [q\u03bc(x), q\u03bd(y)] = i\u03b8\u03bc\u03bd\u03c1q\u03c1(xy). We will show that it is possible to define a covariant derivative operator acting on fields living on the GroenewoldMoyal plane. This allows us to introduce a notion of spinor fields on the Groenewold- Moyal plane. Moreover we will discuss how one can construct gauge invariant actions for these fields. Finally we will study the action of the discrete symmetries C,P,T and CP T .","label":1,"model":"bloomz","source":"arxiv","id":3201}
{"text":"We report the detection of warm dust emission around an old star, EF Cha (HD 162826), with age ~10Myrs and mass M = 0.8M\u2299 at a distance of ~30pc using Spitzer Space Telescope Infrared Array Camera (IRAC) observations in four bands centered on 3.6, 4.5, 5.8 & 8 microns. The observed fluxes are consistent with those expected for a blackbody temperature T~150K orbiting within 1 AU of its host star. We also detect excess infrared emission beyond that predicted by our model atmosphere fits to optical photometry which we attribute to circumstellar material surrounding this young star. This is one of only two known examples of such warm dust detected around nearby stars. Our results suggest that EF Cha may be undergoing planet formation similar to younger systems like Beta Pictoris or HR 8799. Keywords: Planet Formation","label":1,"model":"bloomz","source":"arxiv","id":3202}
{"text":"The cosmic code comparison project is an effort to compare the results produced by different cosmological codes, and thereby test their accuracy.  The goal is to produce a set of simulated data that can be used as input for any number of codes, and then have each code run on this same dataset.   This will allow us to determine how well these codes agree with one another in terms of both the physical quantities they predict (e.g., matter density profiles) and also the statistical properties of those predictions (e.g., power spectra).   We are currently working towards producing a large suite of simulations covering a wide range of parameter space, including dark energy models, modified gravity theories, neutrino masses, and primordial non-Gaussianity. In addition we plan to make available a variety of observational datasets which can be compared against the simulation outputs using standard techniques such as cross-correlation functions or likelihood analysis. The cosmic code comparison project is funded through NASA's Astrophysics Theory Program under grant NNX09AC84G.","label":1,"model":"bloomz","source":"arxiv","id":3203}
{"text":"En este trabajo se estudia la existencia y estabilidad de agujeros de gusano en el espacio tiempo descrito por una soluci\u00f3n exacta del campo gravitacional generado por un cuerpo esf\u00e9rico con simetr\u00eda axial, que corresponde al caso m\u00e1s simple de agujero negro no rotante. Se muestra que los agujeros de gusano son estables bajo ciertas condiciones sobre las constantes cosmol\u00f3gicas involucradas. Adem\u00e1s, se presenta una nueva clase de soluciones exactas para el problema de Einstein-Klein-Gordon en espacios homog\u00e9neos e is\u00f3tropos. Estas soluciones corresponden a ondas escalares estacionarias localizadas alrededor de un punto singular donde existe una densidad infinita de energ\u00eda escalar. Finalmente, se discute brevemente c\u00f3mo estas soluciones pueden ser utilizadas como fuentes de radiaci\u00f3n gravitatoria coherente. En este trabajo estudiamos la existencia y estabilidad de los llamados agujeros de gusano  en el espacio-tiempo descrito por una soluci\u00f3n exacta correspondiente al campo gravitacional generado por una esfera con simetr\u00eda axial. Esta soluci\u00f3n corresponde al caso m\u00e1s sencillo posible de agujero negro sin rotaci\u00f3n. Mostramos que estos agujeros de gusano resultan ser estables bajo determinadas condiciones sobre las constantes cosmol\u00f3gica involucradas.  Tambi\u00e9n presentamos una nueva clase de soluciones ex\u00e1ctas para el problema de Klein-Gordon-Einstein en espacios homog\u00e9neos isotr\u00f3picos. Estas soluciones corresponden a olas escalares estacionarias localizdas alrededor de un punto singular en donde hay una densidad infinita de energ\u00eda escalara. Por \u00faltimo discutimos brevemente c\u00f3mo estas soluciones pueden utilizarse como fuentes de radiaci\u00f3n gravitacional coherente.","label":1,"model":"bloomz","source":"arxiv","id":3204}
{"text":"We perform global three-dimensional magnetohydrodynamic simulations to study grain growth, dust settling, and planetesimal formation around protoplanets embedded within turbulent disks with dead zone boundaries. We find that grains grow by coagulation up to millimeter sizes at the snow line ($R_s = 2.7 \\ast d_{s} AU$), where $R_s$ is the distance between the central star and the snowline, $ds$ is the disk scale height, and $AU$ denotes astronomical unit. Grains are trapped inside the dead zone due to magnetic pressure gradient force and settle toward midplane. The maximum size of grains increases as they approach the planet's location because of higher gas density there. Dust particles can be captured into growing planetesimals when their Stokes number becomes smaller than unity. Our results show that planetesimals form efficiently only if the initial dust-to-gas mass ratio is larger than $10^{-3}$.","label":1,"model":"bloomz","source":"arxiv","id":3205}
{"text":"We report on multiwavelength observations of the X-ray, optical and radio afterglows of the short-hard gamma-ray burst (GRB) 050724 detected by Swift satellite at 07:24:06 UT on 24 July 2005. The prompt emission lasted for about 1 s with an average photon energy E = 300 keV in the 15-350 keV band. We find that the temporal decay index is ~1.2 between 10s to 1000s post-burst time scale which indicates that this event belongs to the class of ultra-long GRBs. In addition we also detect a possible rebrightening feature around 100s post-burst time-scale. Our spectral analysis shows that the spectrum can be fitted well using both single power-law model as well as broken power law model. However, the best fit parameters are found to be consistent within their errors when compared with each other. Using our multi-wavelength data set, we estimate the total energetics associated with this event to be ~3 x 1044 erg.","label":1,"model":"bloomz","source":"arxiv","id":3206}
{"text":"We present the results of our analysis of the effects of cosmic rays on galaxy clusters, using cosmological hydrodynamic simulations with and without CRs. We find that the presence of CRs leads to an increase in gas temperature at large radii (r > 0.5Rvir) due to adiabatic compression of the ICM during cluster formation. The effect is more pronounced for higher values of the initial magnetic field strength. In addition we find that there are significant differences between the radial profiles of X-ray surface brightness obtained with and without CRs which can be used as observational signatures of their presence. Finally, we show that the inclusion of CRs has little impact on the global properties such as total mass or luminosity but does affect the distribution of metals within the cluster. This work was supported by NASA grant NAG5-9998. Cosmological hydrodynamical simulations were performed using Enzo developed by the Laboratory for Computational Astrophysics at the University of California, San Diego.","label":1,"model":"bloomz","source":"arxiv","id":3207}
{"text":"We present the first complete catalog of high-energy spectral parameters (photon index, low-energy cutoff) and durations observed by the Burst Alert Telescope on board NASA's Swift satellite. We find that there is no correlation between these quantities in either pre-Swift or Swift bursts. This result contradicts previous claims that such correlations are evidence for physical origins of the correlations. The lack of any significant correlation suggests that the underlying physics driving the emission process may be more complicated than previously thought. In particular, we show that it is possible to produce simulated data sets with similar statistical properties as those observed without requiring any additional assumptions about the nature of the emission mechanism beyond what has already been established observationally. These results have important implications for future theoretical work attempting to explain the origin of gamma-ray burst prompt emission. Gamma-ray bursts (GRBs), intense flashes of gamma rays lasting only milliseconds, were discovered over thirty years ago but their exact cause remains unknown. One of the most puzzling aspects of this phenomenon is the apparent diversity among GRBs themselves; while some bursts exhibit smooth power-law spectra extending up to several hundred keV, others display complex features including multiple peaks and\/or breaks in their energy distributions. Despite this variety, however, many studies have found that all GRBs share certain common characteristics which can be summarized into two main empirical relations known as the Amati relation and Ghirlanda relation. \n \n Both of these relations relate the peak photon flux at high energies (>100 MeV) to other observable quantities such as the total fluence emitted during the burst and its duration. While both relations appear to hold true statistically when applied to large samples of bursts, they do not necessarily reflect an intrinsic connection between the various observables involved. Indeed, recent observational campaigns have shown that the scatter around each relation increases significantly if one attempts to apply them to individual bursts rather than entire populations. Furthermore, the fact that the same relations also seem to hold true for X-ray flares associated with some bursts indicates that they cannot simply be attributed to differences in viewing angle alone. Instead, these observations","label":1,"model":"bloomz","source":"arxiv","id":3208}
{"text":"We study the possibility that electroweak symmetry breaking is triggered by strong dynamics at an ultraviolet (UV) fixed point, as suggested by gauge-Higgs unification models with warped extra dimensions. We show how this scenario can be realized within the framework of composite Higgs models based on strongly-coupled gauge theories. In particular we consider two different realizations of such scenarios: one where the Higgs arises as a pseudo-Nambu-Goldstone boson associated to spontaneous breaking of approximate global symmetries; another where it emerges as a bound state of new fermions charged under the Standard Model gauge group. The latter case leads to novel signatures for Higgs production through gluon fusion which are potentially observable at future colliders. Finally, we discuss possible implications of these results for cosmology. Gauge-Higgs unification provides a compelling explanation for why the weak scale is so much smaller than any other mass scale in nature [1] . It also offers a natural solution to the hierarchy problem between the Planck and TeV scales [2] , since quantum corrections to the Higgs potential are cut off at the UV scale [3] .\nIn order to realize this idea in practice, however, several challenges must be overcome [4] : i) the Higgs should arise naturally out of some strongly coupled sector; ii) the Higgs couplings to SM particles should agree with experiment; iii) there should exist a mechanism to generate masses for all SM fields without introducing large hierarchies among them. These issues have been addressed recently using the Randall-Sundrum model [5] , where the Higgs field lives on the IR brane while gravity propagates into the bulk [6] - [8] . This setup allows for a calculable description of the Higgs physics [9] , but introduces additional complications due to the presence of Kaluza-Klein gravitons [10] .","label":1,"model":"bloomz","source":"arxiv","id":3209}
{"text":"The isobaric analog state (IAS) in nuclei has been studied by using the folding model with microscopic nucleon-nucleon interactions based on chiral effective field theory. The IAS was found to be sensitive to both the isoscalar and isovector parts of the nuclear matter density distribution as well as the strength of the spin-orbit interaction. In particular, it was shown that the IAS can provide useful information about the density dependence of the nuclear symmetry energy at subsaturation densities. It was also demonstrated that the effect of the tensor force on the IAS depends strongly on the choice of the nuclear mean-field potentials used for describing the ground-state properties of nuclei. Finally, we have discussed how one could extract the information about the nuclear symmetry energy from experimental data on the IAS in heavy nuclei. PACS numbers: 25.20.Lj, 27.60.+j, 29.40.+s","label":1,"model":"bloomz","source":"arxiv","id":3210}
{"text":"We present the catalog of host galaxies used by the Laser Interferometer Gravitational-Wave Observatory (LIGO) to search for gravitational waves associated with compact binary coalescences, including black hole-neutron star and neutron star-black hole binaries.  The catalog contains information on all known host galaxies within the sensitivity volume of the first-generation Advanced LIGO detectors during their initial science runs between September 2005 and January 2010. We provide positions, redshifts, distances, and references for each galaxy. This is an update to our previous publication [1] . \nThe data are available at http:\/\/arxiv.org\/abs\/1306.5481. They can also be downloaded as a text file or via the Zenodo repository [2] .\nAuthor Information\n\nCorresponding Author:\nE-mail: jennifer.marrone@ligo.gov; Jennifer Marrone, National Science Foundation","label":1,"model":"bloomz","source":"arxiv","id":3211}
{"text":"We present new observations with the Atacama Large Millimeter\/submillimeter Array (ALMA) and Karl G. Jansky Very Large Array (VLA), which reveal an extended, diffuse source in the vicinity of the radio galaxy PKS 1302-102 at z = 0.084. The ALMA data show that this emission is dominated by dust continuum emission at 870 \u00b5m, while the VLA data detect both synchrotron and free-free emission between 1.4 GHz and 5 GHz. We use these data to characterize the physical properties of the intergalactic medium surrounding the host galaxy of PKS 1302-102. Our results indicate that there are two distinct components contributing to the observed emission: one component associated with the host galaxy itself; and another component located along the line-of-sight toward the host galaxy. This second component has a temperature T ~ 10 K and a density n ~ 100 cm-3 .","label":1,"model":"bloomz","source":"arxiv","id":3212}
{"text":"We introduce the notion of framed bicategory, which is an internalization of the concept of 2-category with duals in the context of enriched categories.  We show that any strict monoidal category gives rise to such a structure, and we prove that this construction has a left adjoint.  This allows us to define a model for the homotopy theory of (strict) monoidal categories as a simplicial set whose n-simplices are given by framed bicategories with n-objects.   Finally, we give some examples of how our results can be applied to study the homotopical properties of various classes of monoidal categories. The main result of this article is Theorem 4.3.1 below, which states that there exists a Quillen equivalence between the model category of small strict monoidal categories and the model category of small framed bicategories. In particular, it follows that the classifying space of the former is equivalent to the nerve of the latter.","label":1,"model":"bloomz","source":"arxiv","id":3213}
{"text":"We report on the polarization dependent photoluminescence (PL) and Raman scattering spectra of CdSe\/CdS core\/shell quantum dots in solution at room temperature. The PL spectrum shows that the emission is polarized along the direction perpendicular to the excitation light, which can be explained by the selection rules for dipole transitions between electronic states with different angular momenta. In addition we observe an anisotropic broadening of the Stokes linewidths as well as a splitting into two components when exciting circularly polarized light. These effects are attributed to the presence of exciton fine structure due to spin-orbit coupling. We also find evidence for a strong electron-phonon interaction leading to phonon sidebands in both the Stokes and anti-Stokes parts of the Raman spectrum. Finally, we show how these results can be used to determine the orientation of individual QDs embedded in a polymer matrix. Polarized luminescence measurements have been performed on single QD emitters using confocal microscopy.","label":1,"model":"bloomz","source":"arxiv","id":3214}
{"text":"We present an all-sky survey for neutral hydrogen (HI) clouds associated with the Large Magellanic cloud (LMC). The LMC is known to have many small, isolated HI clouds that are not gravitationally bound and may be tidally stripped material or remnants of dwarf galaxies destroyed by tidal forces during close encounters between the Milky Way Galaxy and the LMC.  We use data obtained at Arecibo Observatory as part of the ALFALFA survey to search for new HI clouds near the LMC. In addition we combine our results with previous surveys conducted using Parkes telescope observations and single dish telescopes located on Mauna Kea, Hawaii. Our analysis reveals a total of 16 previously uncatalogued HI clouds within 10 degrees of the LMC center. These clouds range in size from 1 kpc to 15 kpc and contain up to 3 x 1012 M\u2299 of HI gas.","label":1,"model":"bloomz","source":"arxiv","id":3215}
{"text":"Anhedonia is an important symptom of depression and schizophrenia, but its underlying neurobiological mechanisms are still poorly understood. In this study we investigated whether exposure to chronic social defeat (CSD) can induce anhedonic-like behavior in male C57BL\/6J mice. We found that CSD-exposed animals exhibited decreased preference for sucrose solution compared with control group. Moreover, CSD-exposed mice showed increased immobility time during forced swimming test as well as reduced locomotor activity in open field test. Finally, we observed significant decrease in dopamine release in nucleus accumbens shell after acute administration of amphetamine in defeated mice. These results suggest that CSD may be used as animal model of anhedonia. Anhedonia is one of the most prominent symptoms of major depressive disorder (MDD), which affects about 20% of patients worldwide 1 . It refers to loss or reduction of pleasure experienced by individuals 2 , resulting in inability to experience joyful events 3 .\nIn addition to MDD, anhedonia has been also described in other psychiatric disorders such as schizophrenia 4 , bipolar disorder 5 , obsessive-compulsive disorder 6 , eating disorders 7 , substance abuse 8 , and borderline personality disorder 9 . However, despite being considered a core feature of several mental illnesses 10 , there is no consensus regarding how it should be assessed 11 . The lack of standardized assessment methods makes it difficult to compare findings across studies 12 . Therefore, new approaches have emerged aiming at improving the diagnosis and treatment of anhedonia 13 .\nThe main challenge associated with studying anhedonia lies in the fact that it is not possible to measure directly 14 . Instead, researchers use indirect measures based on behavioral tests 15 . For example, the sucrose consumption test 16 , the forced swim test 17 , and the open field test 18 are commonly employed to assess hedonia 19 . Although these tests provide valuable information related to anhedonia 20 , they do not allow us to understand the neural circuits involved 21 . Thus, further investigations using more sophisticated techniques are needed 22 .","label":1,"model":"bloomz","source":"arxiv","id":3216}
{"text":"We report on results obtained by INTEGRAL observations during the 2005 outburst of the black hole candidate GRO J1655\u00e2\u20ac\u201c40 (Nova Muscae 1991). The source was observed in the 20-100 keV range for about 100 days, starting at MJD 53000 and ending at MJD 53300. We have analyzed these data using both ISGRI and SPI instruments aboard INTEGRAL satellite. In addition to the main spectral component which is well described by a power law model modified by an exponential cutoff, we find that there are two additional components present in the spectrum. One of them has been previously reported by other authors but its origin remains unclear. Another one appears only when fitting the whole dataset simultaneously with all three models considered here -power law plus exponential cut-off, broken power law or Comptonization model-. This new feature can be interpreted either as a reflection hump produced by cold material surrounding the central X-ray source or as a broad iron line around 6.4 keV.","label":1,"model":"bloomz","source":"arxiv","id":3217}
{"text":"We present the first results for gravitational waves generated by binary black holes in full general relativity, using both post-post-Newtonian (PPN) theory and numerical relativity. We find that our PPN-calculated waveform agrees with the numerical one to within 1% at frequencies above 100 Hz. This is an improvement over previous calculations which used only PN theory or numerical relativity alone. \n \n The agreement between these two methods provides us with confidence that we can use this hybrid approach as part of future searches for gravitational waves produced during mergers of compact objects such as neutron stars and\/or black holes. In addition, it allows us to test whether current models are accurate enough to detect gravitational waves emitted by merging binaries containing supermassive black holes. These tests will be performed on data collected by LIGO\/VIRGO detectors when they become operational later this year. Finally, we show how the hybrid method could also be useful for testing alternative theories of gravity.","label":1,"model":"bloomz","source":"arxiv","id":3218}
{"text":"The authors study the effect that random fluctuations in synaptic conductances have on neuronal firing.  They use an integrate-and-fire model neuron with fluctuating excitatory and inhibitory inputs, which are modeled as Ornstein-Uhlenbeck processes (OU). The OU process is characterized by two parameters:  $\\mu$ - mean value of the process; $\\sigma$ - standard deviation of the process. In this work they vary these parameters to examine how they affect the spiking activity of the simulated neuron. \nThey find that when the input has low variance (low $\\sigma$), the neuron fires regularly at its intrinsic frequency.  When the input has high variance (high $\\sigma$) it causes irregular firing patterns.  This occurs because the membrane potential of the neuron can be driven above threshold for longer periods than normal causing bursts of action potentials.  These results agree well with experimental data obtained from recordings made from real neurons.","label":1,"model":"bloomz","source":"arxiv","id":3219}
{"text":"We study the statistical properties of surface fluctuations for different values of the lateral size L and height H of the system, using numerical simulations on square lattices with periodic boundary conditions. We find that the probability density function (PDF) of the local slope angle \u03b8 is well described by an exponential decay at large angles, while it displays power-law tails at small slopes. The crossover between these two regimes occurs around \u03b8 = 0.5\u03c0. In addition to this behavior, we observe that the PDFs are strongly dependent upon both L and H. This dependence can be explained as finite-size effects: when L or H becomes smaller than some characteristic length scale, the PDFs become broader due to the presence of more rare events. Finally, we show how our results compare favorably with experimental data obtained from scanning tunneling microscopy measurements performed on Si(111). PACS numbers: 68.35.Bs","label":1,"model":"bloomz","source":"arxiv","id":3220}
{"text":"The geometric frustration in the spin-1\/2 triangular lattice is studied by means of neutron powder diffraction, magnetization measurements, specific heat data, and first-principles calculations for two new compounds Sr3NiRhO 6 and Sr 3 Ni Pt O 6 . The results show that both compounds are antiferromagnetic insulators with N\u00e9el temperatures TN = 5 K (Sr3NiRhO6) and T N = 7 K (Sr3NiPtO6). In addition to the expected collinear antiferromagnetism, we find evidence for noncollinear ordering in Sr3NiRhO6: First, there is an additional weak reflection at Q = 1.5 \u00c5 -1 , which can be explained as superlattice peak due to a small rhombohedral distortion; secondly, the temperature dependence of the ordered moment shows a kink around 2 K indicating a change of the order parameter below this temperature.","label":1,"model":"bloomz","source":"arxiv","id":3221}
{"text":"Cet article est une traduction d'un texte en anglais, paru dans le Journal of Algebraic Geometry (vol. 1 n{\\'e} 2, p. 157-170). Il s'agit d'une version revue et corrig{\\'ee} par l'auteur du travail original. Le but principal de cet article est d'introduire la notion d'entrelacement d'alg\\`ebre de Lie. Cette construction permet d'obtenir des alg\\`ebres de dimension infinie qui sont isomorphes aux produits tensoriels d'alg\\`ebbres de dimensions finies. Nous donnons quelques exemples simples pour illustrer cette construction. En particulier nous montrons que l'alg\\`ebre enveloppante universelle U(L) d'une alg\\`ebre de Lie L peut s'identifier au produit tensoriel de toutes les repr\\'esentations irr\\'eductibles de L. Nous d\\'efinissons ensuite un morphisme naturel entre deux entralg\\`ebres associatives A et B comme un morphisme d'alg\\`ebriques de Hopf f : A \\longrightarrow B tel qu'il existe un morphisme g : B \\longrightarrow A tel que f o g = idB .","label":1,"model":"bloomz","source":"arxiv","id":3222}
{"text":"We propose an algorithm that learns how to segment time series data into meaningful segments by using a combination of Recurrent Neural Network (RNN) Experts and Adaptive Variance Estimation (AVE). The proposed method is based on the idea that each expert in our ensemble has its own internal representation of the input sequence which can be used as prior knowledge when estimating the uncertainty associated with the prediction made by this particular expert. We show that such approach leads to better results than state-of-the-art methods both in terms of accuracy and computational efficiency. \nThe main contributions of this work are summarized below: \n1. A novel framework for modeling uncertain predictions produced by multiple RNN Experts.\n2. An efficient online training procedure for updating parameters of all Experts simultaneously.\n3. Extensive experiments conducted on several real-world datasets demonstrate superior performance of the proposed method over existing approaches. Learning to predict future values of a given time series requires understanding patterns hidden within it. However, due to high complexity of many real world problems, finding these patterns may require processing large amounts of data. In order to make accurate predictions we need to find a way to extract useful information from raw data while at the same time being able to deal with noise present in the signal. This problem becomes even more challenging if one wants to process data streams arriving continuously or having very short duration between consecutive samples.","label":1,"model":"bloomz","source":"arxiv","id":3223}
{"text":"We present new results on the nature and geometry of the compact object in the gamma-ray binary system LS I +61 303, based on observations with the INTEGRAL satellite. We find that the source is variable at all wavelengths studied here (radio to hard X-rays), but shows no evidence for orbital modulation or eclipses. The X-ray spectrum can be described by either a power law model or thermal bremsstrahlung emission; both are consistent with previous studies. In addition we report the detection of pulsations in the radio band which have been previously reported only once before. These pulsations show up as periodic intensity variations in our data set, and their periodicity has been confirmed using two independent methods. Using these results together with those obtained from optical photometry and spectroscopy, we conclude that this source most likely contains a neutron star accreting matter from its companion Be-star via Roche lobe overflow.","label":1,"model":"bloomz","source":"arxiv","id":3224}
{"text":"We present new observations at 1.4 GHz and 8.5 GHz with the Australia Telescope Compact Array (ATCA) of the radio afterglow associated to the gamma-ray burst GRB030329, which occurred on March 29th 2003. The data were taken between April 2nd and May 8th 2004, i.e., more than one year after the burst event. We find that the source is still brightening in flux density by about 0.5% per day at both frequencies. This behaviour can be explained if we assume that the fireball has not yet reached its final deceleration radius. In this case, the observed emission would come from an expanding shell of relativistically moving material surrounding the central engine. If so, our results suggest that the energy injection process responsible for the late-time flaring activity detected in other GRBs may have continued up to very large distances from the progenitor system. \n \n Keywords: Gamma-Ray Bursts","label":1,"model":"bloomz","source":"arxiv","id":3225}
{"text":"We present an exact expression for the pressure tensor of a fluid confined by any external potential, which is valid at all temperatures. The result can be obtained as a special case of the virial expansion for the grand canonical partition function. We show that this expression reduces to known results when applied to specific potentials such as harmonic traps or periodic lattices. Finally we apply our general formula to calculate the equation of state of a gas of fermions with attractive interactions in two dimensions. In particular, we find that the system undergoes a phase transition into a superfluid state below some critical temperature Tc. This work was supported by NSF grant PHY-0456747 (M.A.) . \nI. INTRODUCTORY REMARK\nThe thermodynamic properties of many-body systems are often studied using statistical mechanics methods [1] , where one considers ensembles of particles interacting via a given potential energy V(r). For example, if the particles interact through short-range forces only, then it is possible to derive expressions for various physical quantities like density profiles [2] , compressibility [3] , heat capacity [4] , etc., starting from the microscopic definition of entropy S = -k B ln Z, where k B is Boltzmann's constant and Z is the partition function defined as:","label":1,"model":"bloomz","source":"arxiv","id":3226}
{"text":"We study the solidification process in a model system consisting of particles interacting via repulsive soft-core potentials and subject to an external driving force. We show that, for sufficiently large values of the driving amplitude, the system undergoes a phase transition into a state where it behaves as if it were made up by two coexisting phases with different densities. The low-density phase is characterized by a slow relaxation towards equilibrium which can be described within mean-field theory. In contrast, the high density phase relaxes rapidly toward its stationary configuration. \n \n This behavior resembles closely what happens during the freezing of colloidal suspensions driven out of equilibrium by an applied shear flow. Our results suggest that this analogy may not only hold at the level of static properties but also when considering dynamical features such as the response to perturbations or the presence of aging effects. Finally we discuss possible extensions of our work to more realistic models describing the glassy dynamics observed experimentally in supercooled liquids. \nI. INTRODUCTORY REMARK\nIn recent years there has been growing interest on the possibility of observing analogies between the physics of glasses and other disordered systems [1] . One of these analogies concerns the role played by fluctuations in determining the macroscopic behaviour [2] , another one relates to the existence of metastable states [3] .\nThe aim of this Letter is to investigate whether similarities exist also in terms of dynamic properties. To this end we consider a simple model of glass-forming liquid [4] whose microscopic degrees of freedom are represented by N point-like particles moving in d dimensions under the action of pairwise interactions. These particles interact through a potential energy function U(r) = 4\u03b5[1 \u2212 exp{\u2212\u03b1(r\/\u03c3)}] 2 \/\u03c0\u03c3d, where r denotes their separation distance, \u03b5 sets the overall scale of energies, \u03b1 controls the range of interaction (we take here \u03b1 = 1), while \u03c3 fixes the length unit. For simplicity we assume periodic boundary conditions so that the total number of particles remains constant throughout the simulation. As usual, we define the reduced temperature T * \u2261 kT\/","label":1,"model":"bloomz","source":"arxiv","id":3227}
{"text":"We present the results of an investigation into the properties and evolution of transients that arise when initial conditions are generated using Lagrangian perturbation theory (LPT) for cosmological N-body simulations. We find that LPT-based initial conditions produce spurious large-scale power at late times, which is not suppressed by increasing the number of particles used to generate these initial conditions. This effect can be mitigated by applying a low-pass filter to the evolved density field prior to generating new initial conditions with higher-order LPT. However, this approach does not completely remove all unwanted effects associated with the use of LPT-generated initial conditions. In addition, we show how the choice of time step used to evolve the initial conditions affects their accuracy. Finally, we demonstrate that it is possible to construct accurate initial conditions for large-volume cosmological simulations without having to resort to expensive high-resolution hydrodynamic simulations. The generation of realistic initial conditions for cosmological N-body","label":1,"model":"bloomz","source":"arxiv","id":3228}
{"text":"We study the effect on cluster abundance and Sunyaev-Zeldovich (SZ) power spectrum due to primordial non-Gaussianity in the context of inflationary models with an additional scalar field, which is responsible for driving cosmic acceleration at late times. We find that the SZ power spectrum can be used as a probe of both primordial non-Gaussianity and dark energy properties such as equation-of-state parameter w0 and its time-derivative wa. In particular we show how these parameters affect the amplitude and shape of the SZ power spectrum. The results are presented using a simple analytical model based on perturbation theory upto second order. This work will help us understand better the nature of dark energy by combining it with other probes like supernovae Ia data or CMB anisotropy measurements. It also provides useful information about the physics of inflation through primordial non-Gaussianity. Introduction:-Inflation [1] , one of the most successful paradigms in modern cosmology, predicts a nearly scale-invariant Gaussian distribution of density fluctuations [2] . However recent observations [3] have shown some deviations from this prediction indicating possible presence of primordial non-Gaussianities [4] .\nIn addition to explaining the origin of large-scale structure formation [5] , inflation has been proposed [6] as a mechanism for generating the observed accelerated expansion of the universe [7, 8] . Inflationary scenarios predict that there should exist another light scalar field besides inflaton [9] , called quintessence [10] , which drives the current accelerating phase of the universe [11] . Quintessential inflation [12] is a class of inflationary models where the role played by the inflaton during inflation is taken over by quintessence after inflation ends [13] . These two fields interact minimally [14] leading to interesting consequences [15] . For example, if the potential of quintessence is sufficiently flat then it may lead to eternal inflation [16] . If so, then our observable patch of the universe would correspond only to a tiny fraction of all space-time [17] . Another possibility is that the quintessence field decays into radiation [18] thereby reheating the universe [19] .","label":1,"model":"bloomz","source":"arxiv","id":3229}
{"text":"The mass spectrum and decay properties of the scalar mesons are studied by using the QCD sum rule method with the light-cone distribution amplitudes (LCDAs). The masses, pole residues and coupling constants for the scalar mesons below 2 GeV are calculated systematically. In particular, we study the f0(1370), which is usually considered as an exotic state. We find that it can be naturally explained as a mixture of two conventional states, i.e., the lowest lying scalar glueball and the scalar quarkonium. Our results show that its mixing angle \u03b8 = \u221220\u00b0 \u00b1 5\u00b0 , where the first error comes from the uncertainty of the LQCD data used to determine the parameters of LCDAs, while the second one arises from the uncertainties of the input parameters such as Borel parameter M2B and threshold s0B . \nI. INTRODUCTIO N\nIn recent years, there has been great interest in studying the low energy hadronic physics due to both theoretical and experimental reasons [1] - [4] . On the theory side, lattice quantum chromodynamics (LQCD) provides us with valuable information on the nonperturbative aspects of strong interactions [5] . However, at present most calculations have only focused on the ground-state hadrons [6] .\nOn the other hand, the experimental observations of many new excited states beyond the naive quark model predictions [7] - [9] provide further motivation to explore their underlying structures [10] - [12] . For example, the newly observed scalars around 1.4-1.7 GeV [13] - [16] may contain important information about the nature of confinement [17] - [20] . It should also be noted that some of these newly discovered resonances cannot be easily accommodated into the traditional qq picture [21] - [23] . Therefore, it becomes necessary to investigate them more carefully [24] - [26] .\nIn this work, we will use the QCD sum rules [27] - [29] to calculate the masses, pole residues and couplings of various scalar mesons below 2GeV systematically [30] . In particular, we focus our attention on the f 0 (1370), whose existence","label":1,"model":"bloomz","source":"arxiv","id":3230}
{"text":"We have studied the low temperature (T < 1 K) electron spin relaxation rate, T1e-1\/T1e = 1\/(1 + T2e), for gold-palladium alloys with different compositions by using pulsed muon-spin rotation and relaxation measurements. The results show that the observed relaxation rates are consistent with those expected from Elliott-Yafet scattering mechanism at high temperatures but deviate significantly below 0.5 K. We find that this deviation can be explained if we assume an additional contribution to the relaxation rate due to spin-orbit interaction between conduction electrons and localized magnetic moments associated with Pd atoms. This is supported by our theoretical calculations based on density functional theory which predict a large enhancement of the spin-orbit coupling strength as one moves away from the center of the Brillouin zone towards the Fermi surface. Our findings suggest that the presence of localised magnetic moment may play an important role in determining the transport properties of these materials even though they do not order magnetically down to lowest measured temperatures.","label":1,"model":"bloomz","source":"arxiv","id":3231}
{"text":"We report on optical spectroscopy and multicolor photometry of GRB 040924, an intermediate-duration (T90 = 5 s) event detected by Swift\/BAT at 07:55 UT on 24 September 2004. The prompt emission was followed by a bright X-ray flare peaking about 1 hour later than the main pulse. We find that the spectrum is well fitted with a power law plus blackbody model in the range 3000-9000 \u00c5 . The best-fit parameters are  _  = -1.1 \u00b1 0.2 , TBB = 6200 +1800 -900 K, and EBB = 2.5 +1.0 -0.7 keV for the power-law index,  temperature, and normalization of the blackbody component respectively.  These values are consistent with those observed in other short-hard GRBs. In addition to this thermal component, we detect strong Fe II absorption lines blueshifted by ~10,000 km\/s relative to their rest wavelengths. This suggests that the progenitor system may be similar to that inferred for short-hard GRBs such as GRB 050509b.","label":1,"model":"bloomz","source":"arxiv","id":3232}
{"text":"We present the results of our survey for southern M-dwarf binaries with separations between 0.1 AU and 10 AU, using data obtained by the Wide Field Camera 3 (WFC3) on board HST as part of program GO-12775. We find that about half of all systems are unresolved or only marginally resolved at these distances. The fraction of close binaries is higher among lower mass objects than it is among more massive ones; we estimate that this difference may be due to observational biases rather than intrinsic differences in formation mechanisms. \n \n In addition, we have identified several new candidate companions around known nearby ultracool dwarfs: GJ 436Bb, GJ 581Cc, GJ 674Ab, and GJ 758Aa. These candidates will require further observations before their status can be confirmed. Finally, we report the discovery of two previously unknown substellar objects: 2MASS J03552385+1133413AB and 2MASS J16252618-2434439AB.","label":1,"model":"bloomz","source":"arxiv","id":3233}
{"text":"We present the results of an analysis of the anisotropy in the distribution of satellite galaxies around isolated field galaxies, using data obtained by the Sloan Digital Sky Survey (SDSS). We find that there is no significant difference between the distributions for satellites with different luminosities or colors and those found around central cluster galaxies. The observed anisotropies are consistent with predictions based on tidal forces acting during galaxy mergers. This suggests that these effects may be responsible for the formation of both clusters and groups of galaxies. \n \n Keywords: Galaxy merger, Group\/cluster of galaxies, Tidal stripping, SDSS, Isolated galaxy \n \n \n \n 1 Introduction \n \n Clusters of galaxies contain many thousands of galaxies which reside within a common dark matter halo. These systems form through gravitational collapse driven by the mutual attraction of their constituent galaxies. However, it remains unclear how this process occurs over time-scales ranging from individual galaxy interactions to the assembly of massive clusters containing hundreds of member galaxies. In particular, we do not know whether all galaxies evolve into members of large clusters or if some fraction remain as isolated field galaxies throughout cosmic history. \n \n 2 Previous Work \n \n Several studies have investigated the properties of satellite galaxies surrounding brightest cluster galaxies (BCGs) at low redshifts z < 0.1. For example, Carlberg et al. (1997), Lin & Mohr (2004a), and Hansen et al. (2005) used samples of BCG-satellite pairs selected from optical surveys such as the Palomar Observatory Sky Survey (POSS-II; Reid et al., 1991) and the Sloan Digital Sky Surveys (SDSS; York et al., 2000). They found that the number density profiles of satellite galaxies show strong deviations from spherical symmetry, indicating that they are distributed anisotropically about their host galaxies. Furthermore, they showed that the degree of anisotropy depends strongly on the projected distance from the center of the host galaxy. At small distances, the radial profile shows a steep decline towards the center of the host while the tangential component increases rapidly beyond a characteristic radius R","label":1,"model":"bloomz","source":"arxiv","id":3234}
{"text":"We have developed and tested an advanced radio telescope system, which is composed of a new sideband-separating superheterodyne receiver (SIS) operating at 200GHz band and a newly designed 60cm diameter Cassegrain antenna. The performance of this system was evaluated by observing Jupiter's atmosphere in the millimeter wave region using the Nobeyama 45-m telescope as a backend instrument. We found that our system has sufficient sensitivity to detect atmospheric emission lines such as H_2O(1 10--0 00), CO(2 11--1 01), and CH_3OH(5 1--4 0). This result shows that we can use our system as a powerful tool to study physical conditions of planetary atmospheres. Keywords: Millimeter-wave astronomy, Superconductor-insulator-superconductor mixer, High-resolution spectroscopy, Planetary science. Millimeter-wavelength observations are important tools for studying physical conditions of planetary atmosphers. However, it is difficult to perform high resolution spectroscopic studies on planets because of their small angular sizes compared with those of astronomical objects observed in other wavelength regions. In order to overcome these difficulties, we have developed a new radio telescope system consisting of a new sidebandseparating superheterodyne\/receiver (SIS) operating around 200GHz and a newly designed 60 cm diameter Cassegrain antenna. \n \n Figure 1: A schematic diagram of the new radio telescope system.","label":1,"model":"bloomz","source":"arxiv","id":3235}
{"text":"The hydrogen bond is the strongest noncovalent interaction in water, and it plays an important role in determining its physical properties.  The strength of this bond can be measured by infrared spectroscopy or nuclear magnetic resonance (NMR) techniques.   Infrared spectroscopy measures how much energy is absorbed when vibrating molecules are exposed to infrared light.   NMR uses radio waves instead of light to measure the amount of energy needed to change the spin state of atoms within a molecule.   ... Figure 1 . Water has two different types of hydrogen bonds that form between neighboring molecules:  O-H\u22efO hydrogen bonds occur along the edges of tetrahedral clusters; these bonds have relatively short distances but weak strengths.  H-O\u22efH hydrogen bonds connect adjacent tetrahedra into larger structures called ice crystals; these bonds have longer distances than those found on cluster edges but stronger strengths.  These images were created using VMD software.","label":1,"model":"bloomz","source":"arxiv","id":3236}
{"text":"We study the problem of finding an optimal set of points on the unit circle that are equidistant to each other and have minimum angular separation between them, known as the Costas array or Costas configuration.  We show how this problem can be formulated as a convex optimization problem with linear constraints over the space of probability measures supported by the unit circle. This formulation allows us to use tools from convex analysis for solving it efficiently. In particular we provide a polynomial time algorithm which computes an approximate solution within any desired accuracy. The proposed method is based on the Frank-Wolfe (FW) algorithm combined with the Sinkhorn-Knopp scaling technique. Finally, numerical experiments demonstrate the efficiency of our approach compared to existing methods. Keywords: Convex Optimization; Probability Measures; Frank Wolfe Algorithm; Scaling Technique; Unit Circle; Costas Array; Costas Configuration.","label":1,"model":"bloomz","source":"arxiv","id":3237}
{"text":"We study the advantages and disadvantages of composite Higgs models in four dimensions (4D) versus five dimensions (5D). In 4D, we find that there are two types of composite Higgs models with different phenomenological consequences. The first type is based on an underlying global symmetry group SU(2)L \u00d7SU(2)R \u00d7U(1)B\u2212L which leads to three Goldstone bosons after spontaneous breaking of this symmetry down to U(1)EM . This model has been studied extensively by many authors including ourselves [1\u20133] .\nThe second type is based on an extended gauge symmetry group SU(3)C \u00d7SU(2)L \u00d7U(1)Y \u00d7Z\u2032 where Z\u2032 is a new abelian gauge factor associated with extra spatial dimension [4\u20136] . We show that both these models can be embedded into 5D theories compactified on orbifolds [7\u20139] , but they have very different properties when considered as effective 4D theories.","label":1,"model":"bloomz","source":"arxiv","id":3238}
{"text":"We study thermodynamics and transport properties of the Kondo necklacemodel (KNM) in the vicinity of its quantum phase transition to an ordered state, which is driven by spin fluctuations. We show that this transition can be described within the framework of Landau-Ginzburg-Wilson theory with a nontrivial scaling dimension of the order parameter field. The latter determines the universality class of the transition as well as the temperature dependence of various physical quantities such as specific heat or resistivity. In particular we find that at low temperatures the system exhibits non-Fermi liquid behavior characterized by power-law dependences of these quantities on T . \nI. INTRODUCTORY REMARkS\nThe Kondo necklace model 1 ,2 describes a chain of magnetic impurities coupled via antiferromagnetic exchange interactions J. It has been introduced originally for describing the physics of heavy fermion compounds 3 but it also appears naturally in other contexts 4-7 .\nIn the present work we consider the case when the coupling constant J exceeds some critical value Jc = 2t \/ U where t denotes hopping amplitude between neighboring sites and U stands for local Coulomb repulsion energy 8-10 . At zero temperature the ground state of the system corresponds then to a ferromagnetically ordered state 11-13 while at finite temperatures one expects a continuous quantum phase transition into a paramagnetic state 14-17 . This transition occurs due to strong spin fluctuations 18-20 and belongs therefore to the same universality class as the Heisenberg chain 21-23 . However there are important differences between both models 24-26 : First, the Kondo necklace contains two types of excitations -spinons and holons 27-29 -while the Heisenberg chain only consists of spin-1\/2 particles. Second, the Kondo necklaces contain additional degrees of freedom associated with charge carriers 30-32 . These features lead to new phenomena not observed in the Heisenberg chain 33-36 . For example, the Kondo necklacelike systems may exhibit unconventional superconducting states 37-39 .","label":1,"model":"bloomz","source":"arxiv","id":3239}
{"text":"We present results on interface widths in liquid-gas systems, obtained by molecular dynamics simulations with different potentials. We find that the potential should be able to reproduce both bulk properties (e.g., density) as well as surface tension at coexistence conditions simultaneously. The latter requirement is not fulfilled by standard Lennard-Jones or hard-sphere models. In order to overcome this problem we propose an alternative model which reproduces all relevant physical quantities within their experimental uncertainty. This new model can also be used to study other phenomena like capillary waves. \n \n Introduction \n \n Interfaces are ubiquitous in nature and play important roles in many processes ranging from phase separation [1] over wetting [2] to crystal growth [3] . They have been studied extensively using computer simulations [4] , but it has proven difficult to obtain reliable data due to finite size effects [5] . These arise because interfaces are typically only one particle thick [6] so that they cannot be simulated directly. Instead, periodic boundary conditions must be applied [7, 8] leading to artificial interactions between images of the same interface [9] . As a result, the measured interfacial width depends strongly on system size [10] .\n \nIn recent years there has been considerable progress towards understanding these finite-size effects [11] . It was shown that the dependence of the interfacial width w on the linear dimension L of the system follows a power law [12] :","label":1,"model":"bloomz","source":"arxiv","id":3240}
{"text":"We propose to realize strong atom-field coupling between a single-mode microwave field and ultracold atoms trapped inside a microchip-based Fabry-Perot resonator with high finesse mirrors. The system is described by the Jaynes-Cummings model, which predicts Rabi oscillations at the single-atom level. We show that this scheme can be implemented using current technology. In particular we discuss how to trap cold atoms close to one mirror surface and how to couple them efficiently to the microwave mode via evanescent fields. Finally, we present numerical results showing that it should be possible to reach the regime where quantum effects become important even if only a small number of atoms are loaded into the cavity. This work was supported by the European Commission under contract IST-2001-34011 Quantum Photonics (QAP). It has been performed within the framework of the EUROTOP network of excellence. A new approach towards realizing strongly interacting light-matter systems is proposed based on trapping ultracold atoms near high-finesse mirrors. By exploiting the large electric dipole moment associated with atomic transitions in the microwave domain, we demonstrate that such a setup allows us to achieve strong atom-field coupling at the single-atom limit.","label":1,"model":"bloomz","source":"arxiv","id":3241}
{"text":"We present an elastic scattering theory for the electronic transport properties of suspended single-layer graphene, which is valid at all temperatures T . The theory takes into account both electron-electron (e-e) interactions as well as disorder effects due to charged impurities or ripples. We show that e-e interactions lead to a temperature-dependent resistivity $\\rho$ with a minimum around $T \\sim 100 K$. In contrast, we find that disorder leads to a monotonic increase of $\\rho$ with decreasing temperature. Our results are consistent with recent experiments on suspended samples. \n \n Introduction \n \n Graphene has attracted considerable attention since its experimental discovery [1] , mainly because it exhibits unique physical phenomena such as massless Dirac fermions [2] , Klein tunneling [3] , anomalous quantum Hall effect [4] , and high mobility [5] . These fascinating features have been extensively studied by various theoretical methods [6] - [8] . However, most previous works focused only on the low-temperature regime where phonon scattering dominates [9] - [11] . Recently, several groups [12] - [14] reported measurements of the electrical resistance of suspended graphene over wide ranges of temperature $T_{\\text{F}} = 5-300 K$ and carrier density $n$. They found that the resistivity $\\rho(n,T)$ shows non-monotonic behavior as a function of temperature, i.e., there exists a minimum value of $\\rho$ near $100 K$. This observation cannot be explained within the framework of conventional theories based on acoustic phonons [15] - [17] . It was suggested [18] that this unusual feature may originate from strong electronelectron (e-e) interactions between electrons in different valleys. Indeed, some authors [19] predicted theoretically that e-e interactions can give rise to a minimum in the resistivity at finite temperatures. On the other hand, others [20] argued that disorder plays a more important role than e-e interactions in determining the resistivity of suspended graphene. Therefore, it remains unclear whether e-e interactions play any significant role in the observed resistivity minimum [21] .\n \nIn this Letter, we develop a microscopic theory for the electronic transport in suspended","label":1,"model":"bloomz","source":"arxiv","id":3242}
{"text":"We present the results of numerical simulations of two-dimensional N = (2, 2)\nsuper-Yang-Mills theory with gauge group SU(N). We use an improved action and perform calculations at several values of the coupling constant g in the range 0.1 < g < 1.0. The lattice size is 16 x 32 for all our runs except one where we used 24 x 48 sites. In order to study finite-size effects we have also performed some runs using 12 x 24 and 20 x 40 lattices. \nThe main goal of this work was to check whether the phase transition between confinement and deconfinement phases observed previously by other authors persists when the continuum limit is approached. \n \n Our data show that there are no significant differences between the results obtained on different sizes of lattices within statistical errors. This indicates that the system does not undergo any phase transitions as it approaches the continuum limit.","label":1,"model":"bloomz","source":"arxiv","id":3243}
{"text":"We propose an extension of the linear mixed model that allows researchers to control for unobserved individual heterogeneity when estimating treatment effects using panel data. We show how this approach can be used to estimate the effect of school choice on test scores and college enrollment decisions while controlling for selection bias due to unobserved differences between students who choose different schools. Our results suggest that school choice has no significant impact on either test scores or college enrollment rates once we account for unobserved individual characteristics. \n \n Keywords: Panel Data, Mixed Models, Treatment Effects, School Choice \n \n \n \n 1 Introduction \n \n In recent years there have been numerous studies examining the relationship between school choice policies and student outcomes (see Hanushek et al., 2007 , for a review). Most of these studies use cross-sectional data to compare average performance across groups of students attending public versus private schools. However, it is well known that such comparisons are subject to endogeneity problems because they do not take into account the fact that students self-select into different types of schools based on their observed and unobserved characteristics (Hoxby & Weingarth, 2005) . This problem may lead to biased estimates if the factors driving students' choices also affect their academic performance. \n \n 2 The Model \nIn order to address this issue, several authors have proposed alternative estimation strategies aimed at correcting for sample selectivity by explicitly modeling the process through which individuals make educational choices (e.g., Heckman, 1979; Lee, 1997; Wooldridge, 2002 ) . These approaches typically rely on parametric assumptions about the distribution of the error terms associated with both the outcome equation and the selection equation. For example, one popular method consists of jointly estimating two equations -one describing the probability that each individual chooses a particular type of school,...","label":1,"model":"bloomz","source":"arxiv","id":3244}
{"text":"In this work, we present an analysis of design process and experiments conducted in developing a generic tutoring system for intelligent tutoring applications (ITAs). The main goal is to provide insights into how such systems can be developed with minimal effort by reusing existing components. We describe our experiences in building the AnITA system that provides adaptive feedbacks based on student's actions during problem solving activities. In addition, we discuss some issues related to evaluation of ITAs using different types of students as subjects. Finally, we conclude with lessons learned throughout the development cycle. This article is available from: http:\/\/arxiv.org\/abs\/1207.5995. Keywords: Intelligent tutoring application; Adaptive feedbacks; Problem-solving activity; Student modeling; Evaluation experimentation. 1 Introduction Intelligent tutoring applications (ITAS) are computer-based learning environments designed to help students learn specific skills or concepts through interactive problem-solving activities [1] . They have been shown to improve students' performance when compared to traditional teaching methods [2] .\nThe development of ITA requires significant efforts due to their complexity [3] , which includes several components including domain knowledge representation [4] , student modeling [5] , pedagogical strategies [6] , user interface [7] , and adaptation [8] . To reduce these efforts, researchers have proposed frameworks [9] - [11] and tools [12] - [14] that allow developers to build new ITS without having to start from scratch. However, most of them focus only on one aspect of ITS development [15] , e.g., authoring tool [16] , content management [17] , or student modeling [18] .","label":1,"model":"bloomz","source":"arxiv","id":3245}
{"text":"We present an evolutionary algorithm for the problem of mesh numbering, which is to assign integers in the range [0, ..., n-1] to all vertices of a given graph G = (V, E) such that adjacent vertices have consecutive numbers and no two edges share the same pair of endpoints with the same order.  The proposed method uses a population-based approach where each individual represents a valid solution by means of a permutation vector. We show how this representation can be used to efficiently compute fitness values using only local information. In addition we propose several genetic operators to explore the search space. Finally, we report on preliminary results obtained by applying our method to some well-known benchmark instances. Keywords: Graphs, Optimization problems, Evolutionary algorithms, Genetic algorithms. 1 Introduction A common task when working with graphs is to label their nodes or edges with unique identifiers. This process is known as node or edge numbering respectively. For example, it may be required to number the nodes of a road network so that every path between any two points has a unique sequence of labels. Another application arises in circuit design, where one needs to assign unique addresses to components of electronic circuits.","label":1,"model":"bloomz","source":"arxiv","id":3246}
{"text":"We present an analysis of the alignments between galaxy spins and tidal fields in real space, using data from the Two Mass Redshfit Survey (TMRS). We find that galaxies are preferentially aligned perpendicular to their local tidal field on scales larger than 1 Mpc\/h. This alignment is stronger for more massive galaxies at higher redshifts. The observed spin-tide correlation can be explained by the effect of gravitational torques exerted by large-scale structures during the formation process of these galaxies. Our results suggest that this mechanism may play an important role in shaping galactic angular momenta. These findings have implications for understanding how dark matter halos acquire their angular momentum as well as for interpreting observations of cosmic shear statistics. Introduction: Galaxies form within overdense regions of the universe where they experience strong gravitational interactions with other objects such as neighboring galaxies or clusters of galaxies. During the formation process, these interactions induce gravitational torques which affect the orientation of the galactic angular momentum vector. In turn, the orientations of galactic angular momenta determine the shapes of galaxies through dynamical friction processes. Therefore, it has been suggested that the shape distribution of galaxies could provide information about the origin of galactic angular momentums (e.g., Catelan & Theuns 1996; Lee et al. 2008) . However, observational studies show conflicting results regarding whether there exists any preferred direction of galaxy spin axes relative to their neighbors' positions (see e.g., Faltenbacher et al. 2002; Bailin et al. 2005; Paz et al. 2008; Codis et al. 2012 , for recent works).\nIn order to understand the physical mechanisms responsible for determining the directions of galactic angular momentas, we need to study the statistical properties of galaxy spin distributions over large volumes of the universe. Recent surveys like Sloan Digital Sky Survey (SDSS) allow us to measure galaxy orientations accurately enough to perform such analyses. For example, Lee et al. (2008) used SDSS DR4 data to investigate the alignments between galaxy spin vectors and their nearest neighbor's position angles. They found no","label":1,"model":"bloomz","source":"arxiv","id":3247}
{"text":"We present an anatomical study on the structure and evolution of malicious singularities in complex networks, which are defined as nodes that have a disproportionately large number of connections to other nodes. We show how these nodes can be identified by their topological properties using statistical mechanics methods. In particular we find that the degree distribution of such nodes follows a power law with exponent 2.5 \u00b1 0.1 for all studied real-world networks ranging from social networks like Facebook or Twitter to biological networks like metabolic pathways or protein-protein interaction networks. This finding is consistent across different network sizes and types. The results presented here provide new insights into the organization principles of complex systems and may help to identify key players within them. Complex networks play a crucial role in many fields including physics, biology, sociology, computer science, engineering, economics, etc., where they represent interactions between entities [1] . A common feature of most complex networks is the presence of so-called \"hubs\" -highly connected nodes [2] , whose removal often has dramatic effects [3] .\nIn this work we focus on identifying hubs in complex networks based solely on their topological features. To do so, we use statistical mechanics techniques [4] to analyze the degree distributions of several real world networks [5] . Our analysis reveals that the degree distribution of hubs follows a power-law [6] with exponent 2.5\u00b10.1 independently of the size and type of the considered network (see Fig.  1 ). Interestingly, our findings are also valid when considering only the largest component of each network [7, 8] . These results suggest that the observed scaling behavior is not due to finite-size effects but rather reflects some fundamental property of complex networks [9] .","label":1,"model":"bloomz","source":"arxiv","id":3248}
{"text":"We study the regularity properties of functions in Sobolev spaces with respect to the free additive convolution product. We prove that if two functions belong to some Sobolev space then their free convolutions also do so (Theorem 1). This result is used for proving that the set of all functions belonging to some Sobolev spaces forms an algebra under the free convolution operation (Corollary 2).\nIn particular we show that the set of all infinitely differentiable functions on ${\\mathbb R}^d$ or ${\\mathbb Z}^d$ which are bounded together with all their derivatives belongs to this class of algebras (Proposition 3); it follows that these sets form Banach algebras as well. Finally, we give examples showing that our results cannot be extended to higher order Sobolev spaces. The main tool in our proofs is the Fourier transform. In fact, we use the following theorem proved recently by J. Bourgain, A. Brezis and G. Mironescu. \nLet us recall briefly what the free convolution means. Let $f\\in L^1({\\mathbb R}^d;\\mathbb C)$ and $g\\in L^1({\\mathbf{Z}}^d;\\mathbb C)\\cap L^2({\\mathbf{Z}^d})$. Then there exists a unique function $h\\in L^1({\\mathscr H};\\mathbb C)$ such that \n$$h(\\xi)=\\int_{\\mathscr H}\\chi_{|\\xi|<1}f(x)g(x+y)e^{-2\\pi i x \\xi y}dxdy,$$\nwhere $\\mathscr H=(0,+\\infty)\\times {\\mathbf{R}}\\setminus \\{0\\}$ denotes the upper half-plane equipped with its natural measure.","label":1,"model":"bloomz","source":"arxiv","id":3249}
{"text":"We present new observations of the molecular line emission in the vicinity of the massive young open cluster Westerlund 1 (Wd-1). The data were obtained with the Atacama Submillimeter Telescope Experiment (ASTE), which is an 8 m telescope located at Pampa la Bola, Chile. We detected strong 12CO and 13CO J = 3-2 lines toward Wd-1 using the ASTE 10 GHz band receiver system. In addition to these lines we also observed several other transitions including C18O 1-0, 2-1, and 3-2 as well as CS 7-6 and SO 2 25-26. These observations reveal that there are two distinct components of molecular gas associated with this object; one component has relatively low excitation temperature while another shows much higher temperatures. This latter component appears to be distributed along an arc-like structure around the central star cluster. Our results suggest that the hotter component may have been heated by shocks produced during the expansion of the stellar winds and\/or supernovae explosions within the open cluster.","label":1,"model":"bloomz","source":"arxiv","id":3250}
{"text":"We report the discovery by INTEGRAL\/IBIS and Swift\/BAT of a new X-ray binary, IGR J00234+6141 (l = 111 deg., b = +6 deg.), located in the Norma arm at a distance of about 8 kpc. The system is composed of a neutron star orbiting around its companion with P orb ~ 3 days. We have performed optical spectroscopy on this object using VLT\/FORS2 instrument which shows that it has strong emission lines typical for accreting binaries. From these observations we conclude that the system hosts a white dwarf primary component and a magnetic neutron star. This makes IGR J00234+61 one more example of intermediate polar class of objects. In addition to the main spectral features there are also some absorption lines present in our spectrum. These could be due either to the secondary star or to interstellar medium along the line-of-sight towards the source.","label":1,"model":"bloomz","source":"arxiv","id":3251}
{"text":"We study the nonlinear dynamics of infectious diseases transfer in a population, where individuals are divided into three classes: susceptible (S), infected (I) and recovered\/removed (R). We consider two different models: SIR model and SEIR model. In both cases we assume that there is no birth or death in the population. The main goal of this work is to investigate how the disease spreads through the population depending on its parameters. For example, if the infection rate is too high then it may lead to an epidemic outbreak. On the other hand, if the recovery rate is very large compared to the infection rate then the number of infectives will decrease rapidly. Finally, we show some numerical simulations which illustrate our results. \n \n Keywords: Nonlinear dynamics, infectious diseases, tuberculosis, SIR model, SEIR model. 1 Introduction \n \n Many mathematical models have been developed over time to describe the spread of infectious diseases within populations [1\u20133] . These models can be used as tools to understand the transmission mechanisms of these diseases and help public health authorities make decisions about prevention strategies [4] .\n \nIn particular, many researchers have studied the effects of vaccination programs [5\u20137] , quarantine [8, 9] and isolation [10, 11] on the evolution of epidemics. Other studies focus on the impact of environmental factors such as temperature [12, 13] , humidity [14, 15] and rainfall [16] on the propagation of pathogens. \nThe majority of existing works use deterministic models based on ordinary differential equations [17] . However, stochastic models [18, 19] and agent-based models [20, 21] also exist. Agent-based models allow us to take into account individual behaviors [22] while stochastic models provide more realistic descriptions of random events [23] . \n \nIn this article, we propose new mathematical models describing the spread of infectious diseases in a closed population. Our aim is to analyze the influence of various parameters on the behavior of the system. More specifically, we want to determine whether the disease will die out naturally or cause an epidemic outbreak. To do so, we first introduce the basic reproduction number R0 [24] , which represents the average number","label":1,"model":"bloomz","source":"arxiv","id":3252}
{"text":"We consider the possibility that our universe underwent two stages of accelerated expansion, first hybrid inflation and then modular inflation.  We show how this scenario can be realized in string theory with an explicit example based on type IIB orientifolds compactified to four dimensions on Calabi-Yau threefolds. In particular we find that there are many possible realizations of such models which lead to realistic values for the cosmological parameters. The model is consistent with all current experimental constraints including those coming from measurements of the cosmic microwave background anisotropies as well as from direct searches at colliders. Finally we discuss some phenomenological aspects of these scenarios. Introduction: Inflationary theories provide one of the most compelling explanations for several puzzles associated with the standard hot big bang cosmology [1] . They predict that primordial quantum fluctuations generated during inflation should have left their imprint on the temperature anisotropies observed today in the Cosmic Microwave Background (CMB) [2] .\nIn recent years it has been shown that supersymmetric grand unified theories (GUTs), like SO(10) , naturally give rise to inflationary potentials [3] , while also providing a successful unification scheme [4] . However, GUT scale inflation suffers from the so-called \u03b7-problem [5] : the predicted value of the tensor-to-scalar ratio r = 16\u01eb H \/\u03b7 2 [6] leads to too large CMB quadrupole anisotropies [7, 8] unless \u01eb H \u226a 1 [9] or \u03b7 \u226b 10 \u22129 [10] . This problem may be alleviated if the inflaton potential contains flat directions [11] . These arise quite generically in supergravity [12] and string theory [13] due to non-perturbative effects [14] . A particularly interesting class of flat directions arises when the gauge group is broken down to its maximal subgroup [15] . Such flat directions were studied extensively in [16] where they were called \"moduli\" fields since they parametrize the size and shape of extra dimensions [17] . Moduli fields play an important role in string theory [18] because they determine the vacuum expectation values of various moduli fields appearing in the low energy effective action [19]","label":1,"model":"bloomz","source":"arxiv","id":3253}
{"text":"The future neutrino factory experiments will be able to search for new physics beyond the Standard Model (SM) with unprecedented precision, and are expected to provide important information on the origin of matter-antimatter asymmetry as well as dark matter candidates.  In this talk I will present an overview of our recent studies on how to probe various types of new physics using these facilities. The results presented here were obtained by combining the analyses performed at the T2K experiment and its off-axis near detector ND280. These include searches for sterile neutrinos, lepton flavor violating processes such as neutrinoless double beta decay, CP violation effects in leptonic sector, and exotic Higgs bosons that can couple to both quarks and leptons. We also discuss possible improvements in sensitivity which may be achieved if we combine the data taken at T2K and NOvA experiments. Finally, prospects for probing new physics at future accelerator-based neutrino factories are discussed.","label":1,"model":"bloomz","source":"arxiv","id":3254}
{"text":"We study the representations of tame quivers with relations, which are finite-dimensional algebras over an algebraically closed field k. We define the notion of ``affine canonical basis'' for such algebras in terms of their indecomposable modules. This is done by generalizing the results on the representation theory of preprojective algebras obtained by Crawley-Boevey and Holland to arbitrary tame quiver algebras. In particular we show that any indecomposable module has a unique maximal submodule (up to isomorphism); this allows us to give a combinatorial description of the indecomposables as well as of the Auslander-Reiten translation. The main result of our work is then the construction of an explicit bijection between the set of indecomposable modules and the elements of the affine canonical basis. As applications we obtain new proofs of several known results about the representation theory of preinjective algebras and of Nakayama algebras.","label":1,"model":"bloomz","source":"arxiv","id":3255}
{"text":"We report on the experimental demonstration of an all-optical method for reconstructing molecular vibrational wavefunctions by means of nonlinear optical spectroscopy and quantum state tomography (QST). The technique is based on the measurement of the third-order polarization induced in a sample of gas-phase acetylene molecules by two pump pulses, followed by a probe pulse at variable time delay. By scanning this delay we obtain a series of signals that are used to determine the density matrix elements associated with each vibrational level populated during the interaction between the three pulses. We show how these measurements can be combined into a single QST experiment which allows us to retrieve information about both the population distribution among different levels as well as their relative phases. This approach provides direct access to the full vibrational wavefunction without any need for prior knowledge or assumptions regarding its form. \n \n In particular, it enables one to measure directly the phase difference between adjacent energy eigenstates, which has been shown to play a crucial role in determining many important physical properties such as chemical reactivity and transport coefficients.","label":1,"model":"bloomz","source":"arxiv","id":3256}
{"text":"We develop an agent-based model to study the evolution of social complexity in human societies, focusing on how heterogeneity among agents can lead to increasing returns that drive economic growth and technological innovation. We find that heterogeneous populations are more likely than homogeneous ones to evolve complex economies with specialized production units, markets for goods and services, and division of labor between producers. The presence of these features leads to higher levels of productivity and population density, which further increase the benefits associated with specialization and trade. Our results suggest that the emergence of such socioeconomic structures may have been one important factor driving the evolution of human society over the past few million years. In this work we present a computational model aimed at studying the evolutionary dynamics underlying the development of socio-economic systems. More specifically, our goal is to understand what factors might be responsible for the appearance of increasingly complex forms of organization observed across many different cultures throughout history. To address this question, we focus on the role played by two key ingredients commonly found in real-world systems: (i) diversity within the population; and (ii) positive feedbacks leading to increasing returns.","label":1,"model":"bloomz","source":"arxiv","id":3257}
{"text":"We present results on the MiniBooNE experiment, which has recently reported an excess in electron neutrino appearance events at low energies ($E < 100 MeV$). We show that this excess can be explained by adding two light sterile neutrinos to the Standard Model. The new states are required to have masses below $2\\times 10^{-3}$ eV\/c2 and mix strongly into the active neutrino flavors. In addition we find that there is no need for additional CP violation beyond what is already predicted within the Standard Model. This work was supported by the U.S. \n \n Department of Energy under Contract No. DE-AC02-05CH11231. We thank J. A. Aguilar-Arevalo, S. Babu, D. Blum, C. Boehm, R. Brandt, P. Cardarelli, E. Cholis, T. Creminelli, G. Danilov, K. Enberg, H. Eriksson, L. F. Ferreira, W. Galsterer, V. Grigoriev, N. Gupta, Y. Hanagaki, A. Iwamoto, A. Khodjamirian, A. Kravchuk, A. Lehtinen, A. Marchesini, A. Melnikov, A. Mironov, A. Murayama, A. Oganessian, A. Ostapchenko, A. Petrovich, A. Polukhin, A. Popov, A. Pruneda, A. Reece, A. Ryabtsev, A. Shabalina, A. Smirnov, A. Stepanenko, A. Strumia, A. Tanaka, A. Trubnikov, A. Tsang, A. Voloshin, A. Zatsepin, A. Zhavoronkov, A. Zhitnitsky, A. Zvezdaev, A. Zwickl, A. Zyuzikov, A. Zyuzaev, A.","label":1,"model":"bloomz","source":"arxiv","id":3258}
{"text":"We prove that for any surface S of general type, the action of its absolute Galois group G(S) on the set of connected components of the modulis space M_g(S) is faithful.  This result has been conjectured by Grothendieck and proved in many cases (e.g., when g = 0 or 1).  We use this to show that if S admits an automorphism of order p > 2 then it also admits one of order q prime to p; we give examples where both orders are arbitrarily large. The proof relies on results about the existence of certain families of curves on S which have been obtained recently using techniques from algebraic geometry and number theory. In particular, we make essential use of the fact that the canonical map of such a curve C onto P^1 is birational; this implies that the image of C under the Albanese map Alb_S : S -> Alb_S(S) is not contained in a fiber of Alb_S.","label":1,"model":"bloomz","source":"arxiv","id":3259}
{"text":"We study the encoding problem for iterated function systems (IFS) on solenoids by unitary representations of wavelet groups, which are infinite-dimensional Lie groups generated by affine transformations of the real line. We show that if an IFS satisfies certain conditions then it can be encoded into a unitary representation of its associated wavelet group. This result is applied to prove that every self-similar fractal set with finite local complexity has a unique invariant measure up to scaling factors. The proof relies on the fact that any such fractal set can be approximated by a sequence of compact sets whose boundaries have zero Lebesgue measures. In particular, we obtain a new characterization of self-affine carpets as those fractals satisfying this condition. Finally, we give examples showing how our results apply to some well-known fractals including Sierpinski gasket, Koch snowflake, von Koch curve, and Menger sponge.","label":1,"model":"bloomz","source":"arxiv","id":3260}
{"text":"The effect of three different sugars (trehalose, maltase and sucrose) on the structure and dynamics of lysozyme has been investigated by molecular dynamics simulation at 300 K for 100 ns in each case. The results show that all these sugar molecules can stabilize the protein against thermal denaturation to certain extent but trehalose is found to be most effective one among them. Trehalose molecule forms hydrogen bonds with both polar and nonpolar amino acid residues which leads to increase in number of water molecules around it. This increases the hydration shell thickness as well as total solvent accessible surface area of the protein. It also decreases the root mean square deviation between initial and final structures indicating its ability to maintain the native conformation of the protein. Maltose and sucrose are less efficient than trehalose because they form only few hydrogen bonds with the protein. In addition, their presence causes slight decrease in the radius of gyration and end-to-end distance of the protein.","label":1,"model":"bloomz","source":"arxiv","id":3261}
{"text":"We study fluctuations of finite-time Lyapunov exponent (FTLE) for chaotic orbits on the standard map with periodic boundary conditions. We show that FTLE fluctuates around its mean value, which is determined by the largest unstable periodic orbit embedded into the chaotic attractor. The amplitude of these fluctuations decreases exponentially as time increases. In addition to this exponential decay we observe an algebraic tail at large times. This algebraic tail can be explained by the presence of small islands inside the chaotic sea. These results are confirmed numerically using different methods. \nI. INTRODUCTORY REMARK\nThe concept of finite-time Lyapunovexponent(FTLE), introduced by Wolf et al [1] , has been widely used recently [2] - [4] . It describes how fast nearby trajectories diverge or converge during some fixed period of time T . For example, if one considers two neighboring points x 0 = x(t 0 ) and y 0 = x(t 1 ), where t 0 < t 1 , then their separation after time T will be given by: \nwhere \u03bb max > 0 is the maximum Lyapunov exponent characterizing the rate of divergence between nearby trajectories [5] .\nIn order to calculate the FTLE it is necessary to solve the following variational equation:\nwhere J is the Jacobian matrix corresponding to the flow generated by Eq. (1). If the initial condition z 0 = x t0 + \u03b5y t0 is close enough to the reference trajectory x t0 , i.e., |\u03b5| \u226a 1, then the solution of Eq.(2) can be written as:\nwhere P n (T ; t 0 , t 1 ) is the nth-order Taylor polynomial expansion of the evolution operator U (T ; t 0 ). Then the FTLE can be calculated from:","label":1,"model":"bloomz","source":"arxiv","id":3262}
{"text":"We study the dynamics of an overdamped Brownian particle in a periodic potential with time-delayed feedback control, where the delay is introduced by a finite response time of the controller. We show that this system can be described as a flashing ratchet and derive analytical expressions for its current-voltage characteristics. The results are compared to numerical simulations. \nI. INTRODUCTIO N\nThe motion of particles driven out of equilibrium has been studied extensively over recent years [1] . In particular, it was shown that under certain conditions such systems may exhibit directed transport even without any net bias [2] , which is known as stochastic resonance [3] or coherence resonance [4] .\nIn many cases, however, these effects occur only if the noise level exceeds some threshold value [5] . This problem can be overcome using delayed feedback control [6] , i.e., controlling the system not instantaneously but after a fixed amount of time t d . It turns out that this method allows one to obtain directed transport at arbitrarily small noise levels [7, 8] . Moreover, it also works when the external driving force vanishes [9] . However, all previous studies were restricted to unidirectional potentials (i.e., potentials with reflection symmetry). Here we consider a more general case of a bidirectional potential [10] , which exhibits two local minima separated by a barrier.","label":1,"model":"bloomz","source":"arxiv","id":3263}
{"text":"We report on the detection of X-ray flares in low mass stars (0.5-0.8 M\u2299) with ages between 1 and 10 Myr, located within the Orion Nebula Cluster. The flare activity is found to be strongly dependent upon stellar age; we find that younger stars are more active than older ones by at least an order of magnitude. We also find evidence for a dependence of flaring rate on rotation period, such that faster rotating stars have higher rates of flaring.  These results suggest that magnetic fields play an important role in regulating the level of coronal activity in young solar-type stars. This work was supported under NASA contract NAS8-39073. We thank J. Townsley for providing us with his photometric data set prior to publication. We acknowledge useful discussions with A. Feigelson, D. Getman, C. Hillenbrand, R. Herbst, S. Preibisch, B. Reipurth, T. Stassun, E. Wolff, and W. Zuckermann.","label":1,"model":"bloomz","source":"arxiv","id":3264}
{"text":"The free energy of activation (\u0394G*) is calculated for the comorosan effect, which describes the formation of an intermediate state in the reaction between carbon dioxide and water to form carbonate ions.  The \u0394G* value obtained by this method is compared with that determined by other methods such as calorimetry or electrochemistry. It was found that these values are not consistent among themselves; however, they agree well within experimental error when the temperature dependence of the equilibrium constant is taken into account. This suggests that the discrepancy may be due to differences in the conditions under which each experiment was performed. In addition, it has been shown that the \u0394G* value depends on the nature of the solvent used in the experiments. Finally, we have proposed a mechanism for the comorosan process based on our results. The free energy of activation (\u2206G*) is calculated using the Arrhenius equation for the comorosan reaction, which describes the formation","label":1,"model":"bloomz","source":"arxiv","id":3265}
{"text":"The Very Energetic Radiation Imaging Telescope Array System (VERITAS) is an array of four 12m diameter imaging atmospheric Cherenkov telescopes located at the Fred Lawrence Whipple Observatory in southern Arizona, USA. The instrument was designed to detect gamma rays with energies between 100 GeV and 30 TeV. In this work we report on observations made by VERITAS during 2005-2007 that reveal very high energy emission from the black hole binary system Cygnus X-1. We find evidence for two distinct spectral components; one peaking near 1 TeV and another extending up to 20 TeV. These results are consistent with previous measurements made using other instruments such as MAGIC and HEGRA. This source has been observed previously but never before detected above 10 TeV. Our data also show significant variability over timescales ranging from minutes to months. Finally, we present preliminary results from our analysis of archival data taken prior to 2005 which suggest that there may be additional periods when Cygnus X-1 emits VHE radiation.","label":1,"model":"bloomz","source":"arxiv","id":3266}
{"text":"We present scaling laws that govern the performance and design trade-offs in all-optical soliton pulse compressors based on cascading quadratic nonlinearities, such as self-phase modulation (SPM) followed by cross-phase modulation (XPM). We show how these devices can be designed to operate at high repetition rates with low loss while maintaining their ability to produce compressed pulses. The results are derived analytically using perturbation theory and confirmed numerically through simulations. Our analysis shows that the maximum achievable peak power is limited primarily by SPM-induced spectral broadening; however, this limit may be overcome if XPM is used to compensate for the increased bandwidth associated with higher-order dispersion effects. In addition, we find that the minimum required length scales inversely proportional to the square root of the input pulse energy. Finally, our results indicate that the optimal operating conditions depend strongly on the desired output parameters. \n \n \u00a9 2009 Optical Society","label":1,"model":"bloomz","source":"arxiv","id":3267}
{"text":"We report on observations made with the Spitzer Space Telescope and Hubble Space Telescope (HST) that reveal aromatic infrared bands in the ionized gas surrounding the brightest star in the Horsehead Nebula, NGC 2264. The observed PAH features are consistent with those seen toward other H II regions excited by hot stars. We find no evidence for an enhanced abundance or unusual excitation conditions of these molecules compared to Galactic H II regions. This suggests that the physical processes responsible for the formation of PAHs may be similar in both environments. These results provide new insights into how PAHs form under extreme radiation fields. \n \n Keywords: Infrared, PAH, Horsehead Nebula, Starburst galaxies \n \n The Horsehead Nebula is one of several spectacular bipolar reflection nebulae associated with young open clusters in Orion's molecular cloud complex. It contains a central O-type star surrounded by a compact cluster of B- and A-type stars embedded within a dense shell of dusty material. Previous studies have shown that this region harbors large amounts of warm dust and cold molecular gas, as well as numerous Herbig-Haro objects indicative of ongoing mass ejection events. However, little has been known about its chemical composition until now because it lies behind thick clouds of interstellar extinction at optical wavelengths. Here we present mid-infrared spectroscopic data obtained using the Infrared Spectrograph aboard the Spitzer Space Telescope which show strong absorption features due to polycyclic aromatic hydrocarbons (PAHs). These features arise from vibrational modes of carbonaceous materials containing 50-200 atoms arranged in planar rings and chains. They are commonly found in photodissociation regions (PDRs), where ultraviolet photons emitted by nearby massive stars break apart hydrogen molecules, creating a layer of partially ionized gas between the illuminated surface of the molecular cloud and the neutral interior. Our analysis shows that the PAH features detected here are remarkably similar to those seen towards Galactic H II regions such as M17 SWex, suggesting that they originate from the same type of photoionization process.","label":1,"model":"bloomz","source":"arxiv","id":3268}
{"text":"Gravastars are hypothetical objects that may be the final state of stellar evolution, and which have been proposed as alternatives to black holes.  Gravastars can be distinguished from black holes by their gravitational lensing effects on background stars.   In this work we show how these differences in lensing behavior could be used to distinguish between gravastars and black holes using observations made with current or future space-based telescopes such as LSST (Large Synoptic Survey Telescope) and WFIRST-AFTA (Wide Field Infrared Space Telescope - Astro-Physics). Gravitational lenses are powerful tools for studying distant galaxies because they magnify images of background sources.  The most famous example is the Einstein ring produced when light passes close to the Sun's mass.  However, there are many other types of gravitational lenses including those formed by galaxy clusters, individual galaxies, and even dark matter halos around single galaxies.  Gravitational lenses also provide information about the masses of both foreground and background objects.  For instance, if one knows the distance to an object producing a gravitational lens effect then its mass can be determined directly from the observed angular diameter of the lensed image.  This technique has been used successfully to measure the masses of supermassive black holes at the centers of nearby galaxies.  Gravitational lensing occurs whenever light travels through regions where gravity varies spatially.  Thus it should occur wherever gravity deviates significantly from Newtonian predictions.  Black holes violate general relativity near their event horizons so they produce strong deviations from Newtonian gravity.  On the other hand, gravastars do not violate general relativity outside their surfaces...","label":1,"model":"bloomz","source":"arxiv","id":3269}
{"text":"We present the results on GRB 051022, one of the most extinguished gamma-ray bursts ever detected (A V = 1.6 mag). The optical afterglow was discovered by Swift UVOT at T + 5 s with an initial magnitude of 18.7 \u00b1 0.1 in white light. We performed photometric observations using several telescopes to determine its temporal evolution over more than two decades in time. Our data are well fitted by a single power law decay with index \u03b1 = \u22120.9 \u00b1 0.2 between 10 3 < t < 2 \u00d7 10 4 sec. This value is consistent within errors with that expected for the forward shock emission produced during the interaction of relativistic ejecta with their surrounding medium. Using our best fit spectral model we find that this burst has a total energy release E iso = (3.4 \u00b1 0.5) \u00d7 10 53 erg, which makes it similar to other bright bursts such as GRBs 990123 or 060111b.","label":1,"model":"bloomz","source":"arxiv","id":3270}
{"text":"The present work is devoted to the study of the properties of the system at the point of its first-order phase transition, using the concept of the \"critical nucleus\" introduced by Langer (Langer, 1963) . The main idea behind this approach consists in considering that the formation of the new phase occurs when the size of the critical nucleus becomes equal or larger than some characteristic length scale of the system. \n \n In particular we have studied the behavior of an Ising-like model on a square lattice as a function of temperature T and magnetic field H. We have found that our results are consistent with those obtained within mean-field theory. Moreover, it has been shown that the dependence of the free energy barrier on the external parameters can be used to determine both the critical temperature Tc and the surface tension \u03c3 between two phases. Finally, we have also investigated how these quantities depend on the number N of spins composing the system. \nI. INTRODUCTORY REMARK\nIn recent years there has been considerable interest in studying the phenomenon of phase transitions occurring in many-body systems [1] . This problem attracted attention mainly due to its relevance in various fields such as statistical mechanics [2] , condensed matter physics [3] , biology [4] , chemistry [5] , geophysics [6] etc.. It turns out that one of the most important aspects related to the occurrence of phase transitions concerns the determination of the so-called \"order parameter\" which characterizes each different thermodynamic state [7, 8] .\nIt should be noted however that the description of phase transitions based solely on the knowledge of the order parameter may not always provide satisfactory information about all relevant physical phenomena taking place during the process of transformation [9] . For example, if the considered system undergoes a first-order phase transition then the discontinuity observed in the value of the order parameter does not necessarily imply the existence of a well-defined interface separating the coexisting phases [10] . Indeed, in this case the presence of a large amount of metastable states makes the identification of the actual equilibrium configuration extremely difficult [11] . As a consequence, the analysis of the structure of the system near the point of the","label":1,"model":"bloomz","source":"arxiv","id":3271}
{"text":"We study the dynamics of a model system that undergoes both a jamming and rigidity transition, by means of extensive numerical simulations. We find that the relaxation time scales as $\\tau \\sim t^{1\/z}$ with $z = 1.5$ in the vicinity of the jamming point, while it follows an exponential law at higher densities. The latter behavior is consistent with previous results for hard spheres near the rigidity threshold. In addition to this slowing down we observe a crossover between two different regimes of diffusion. At low density particles diffuse heterogeneously on top of a static background, whereas they move homogeneously above the jamming point. This crossover can be understood within a simple mean-field theory which predicts a power-law decay of the mobility correlation function. Finally, we show how our findings are related to recent experiments on vibrated grains. Granular materials exhibit fascinating phenomena such as avalanches or force chains [1] . These properties have been studied extensively over many years [2] , but only recently has attention shifted towards their dynamical response [3] .\nIn particular, there has been growing interest in understanding the collective motion of dense packings [4] . It was found that even though individual particles may not move much, the entire packing can still rearrange significantly [5] . Moreover, these systems often display anomalous transport [6] : Particles diffuse slowly compared to Brownian motion [7, 8] , and the diffusion constant depends strongly on the particle size [9] . Recent work suggests that these features arise due to the presence of soft modes [10] , i.e., low-energy excitations that do not cost energy [11] . However, despite considerable progress [12, 13] , the origin of these modes remains unclear [14] .\nThe most prominent example of a disordered material displaying slow dynamics is amorphous solids [15] . Here, the relaxation times diverge close to the so-called jamming point [16] where the pressure vanishes [17] . Interestingly, the same type of slow dynamics also appears in other types of glasses [18] . For instance, colloidal suspensions [19] and foams [20] behave similarly [21] .","label":1,"model":"bloomz","source":"arxiv","id":3272}
{"text":"We present new observations of the kinematic properties of the dense gas associated with the young stellar cluster forming within the Ophiuchus molecular cloud (d = 140 pc). We used the Submillimeter Array to observe two fields, one centered on the infrared source IRAS 04368+2557 and another located about 1 arcmin away at the position of the Spitzer c2d survey target L1641N. The data reveal that both regions are characterized by complex velocity structures which we interpret as being due to multiple overlapping protostellar cores. In particular, our results show that:  -The region surrounding IRAS 04368+25570 is dominated by three distinct components separated by less than 0.1 pc along the line-of-sight.  -The region around L1641N contains several compact sources embedded in an extended envelope whose emission peaks at velocities between 5 and 10 km s-1 relative to the local standard of rest.   -Both regions contain numerous outflows driven by protostars still deeply embedded in their natal envelopes.","label":1,"model":"bloomz","source":"arxiv","id":3273}
{"text":"The Large Area Telescope (LAT) is one of two instruments aboard NASA's Fermi Gamma-ray Space Telescope, launched in June 2008. The LAT consists of an array of silicon strip trackers and cesium iodide calorimeters that detect gamma rays with energies between 20 MeV to more than 300 GeV. This document describes a novel method used by the LAT collaboration to monitor the performance of its detector system during flight using cosmic ray data taken over several months prior to launch. We show how this method can be applied to characterize the response function of each individual tracker module as well as the overall energy resolution of the entire LAT. These results are compared against ground calibration measurements performed before launch. Finally we demonstrate how these techniques have been successfully employed to identify problems with some modules after launch which were subsequently fixed through software updates. The Large Area Telescope (L AT ) is one of two instruments flown on NASA's Fermi Gamma-Ray Space Telescope [1] . Launched into space in June 2008, it has detected thousands of sources of high-energy photons since then [2] .\nIn order to perform such observations, the L AT must accurately measure the direction and energy of incoming photons. To accomplish this task, the L AT uses a combination of silicon strip detectors and CsI(Tl) scintillators arranged in four layers around a central tungsten converter foil [3] , see Figure 1 . Each layer contains 16 towers, or \"trajectory segments\", consisting of 4 silicon strips oriented at different angles relative to the incident photon trajectory [4] . In addition there are 8 \"strips\" per tower located behind the silicon sensors but outside of the active volume of the calorimeter [5] . Together they form a total of 56 independent tracking channels [6] .","label":1,"model":"bloomz","source":"arxiv","id":3274}
{"text":"We present the two-loop corrections to nuclear matter within an effective field theory approach based on chiral perturbation theory and relativistic mean-field theory. The calculation is performed using dimensional regularization, minimal subtraction at one loop order and the modified minimal subtraction scheme at higher orders. We find that the results are consistent with previous calculations obtained by other methods such as Feynman diagrams or dispersion relations. In addition we show how our method can be used to calculate the energy density functional up to next-to-next-to-leading order. This work was supported by the U.S. \n\n\nDepartment of Energy under Contract No. DE-AC02-05CH11231. PACS: 11.10.Kk, 12.38.Gc, 14.20.Dh, 25 .75.-q, 26.60.+z, 27.40.-n, 27.70.+f, 28.30.-j","label":1,"model":"bloomz","source":"arxiv","id":3275}
{"text":"We study the possibility that black holes can be retained in binary systems by gravitational radiation reaction even if their initial velocities are very large, and we find that this is possible for certain ranges of parameters.  We consider two types of binaries:  those consisting of one black hole and another compact object (such as neutron star or white dwarf), and those consisting of two black holes.   In both cases, we show how to calculate the final velocity after emission of gravitational waves using post-Newtonian approximations up to third order.  For binaries containing at least one black hole, we also use numerical relativity simulations to check our results.  Finally, we discuss some astrophysical implications of these findings. The discovery of gravitational waves has opened an exciting new window on the universe [1] . One of its most surprising features was the detection of merging black holes [2] , which were found to have masses ranging between about 4M\u2609 and 36M\u2609 [3] . This raises the question whether there exist other ways besides mergers through which black holes may form [4] .\nIn particular, it would be interesting to know what happens when a black hole moves into a binary system composed of either another black hole or a nonblack-hole companion [5] . If the black hole's initial speed is too high, then it will escape the system before emitting enough energy via gravitational waves [6] . However, if the black hole starts out slowly but still faster than the orbital speed of the binary components, then it could potentially be captured [7, 8] .  Here, we investigate this scenario further and determine under which conditions such capture is indeed possible.","label":1,"model":"bloomz","source":"arxiv","id":3276}
{"text":"We present an analytical model to study decoherence and relaxation processes in a few coupled superconducting quantum interference devices (SQUID) immersed into a phonon bath at low temperatures. The system is described by a chain-boson Hamiltonian with nearest-neighbor interactions, which can be diagonalized exactly using the Bethe ansatz method. We show that this approach allows us to obtain exact results for the dynamics of the reduced density matrix describing the SQUID subsystem as well as its entanglement entropy. In particular we find that the decay rate of the off-diagonal elements of the reduced density matrix scales linearly with temperature T , while the von Neumann entropy grows logarithmically with time t. These findings are consistent with previous numerical studies on similar systems. \n \n Introduction \n \n Superconducting circuits have been proposed recently as promising candidates for realizations of quantum information processing [1] . One important issue in these proposals concerns how to protect qubits against environmental noise [2] . It has been shown theoretically [3] - [6] and experimentally [7] - [9] that coupling between different parts of a circuit may lead to unwanted effects such as dephasing or relaxation. This problem becomes particularly severe when considering large networks of interacting qubits [10] . \n \n Here we consider a simple model consisting of two weakly-coupled SQUIDs [11] immersed into a phonon environment [12] . Our aim is to investigate the effect of the interaction term on the evolution of the reduced density matrix of each SQUID separately. To do so, we use the Bethe ansatz [13] to solve analytically the Schr\u00f6dinger equation corresponding to our model. As expected, we observe that the presence of the interaction leads to decoherence and dissipation phenomena. Moreover, we find that the decay rates of the off-diagonals of the reduced density matrices scale linearly with temperature T , whereas their von Neumann entropies grow logarithmically with time t. \nModel\n\nThe total Hamiltonian H = H0 + V describes the system composed of N = 2 SQUIDs coupled via a weak tunneling amplitude J immersed into a phonon reservoir at zero temperature.","label":1,"model":"bloomz","source":"arxiv","id":3277}
{"text":"We study how galactic discs warp in response to tidal forces exerted by dark matter haloes and intergalactic gas filaments, using high-resolution cosmological simulations with radiative cooling and star formation. We find that the majority (>80%) of simulated galaxies have significant warping at z = 0. The amplitude of the warp increases with decreasing galaxy mass, but is independent of redshift for massive galaxies. Warp amplitudes are typically less than 10 kpc, which agrees well with observations. Our results suggest that most observed warps can be explained as being caused by external gravitational torques on galactic discs. This implies that there may not exist any intrinsic mechanism within galactic discs themselves that causes warps. In addition, we show that the presence of an AGN does not significantly affect the shape or strength of the warp. Finally, we demonstrate that our model predicts a correlation between the direction of the warp and the angular momentum vector of the host halo.","label":1,"model":"bloomz","source":"arxiv","id":3278}
{"text":"We present the first simultaneous broadband (0.5-10 keV) spectral analysis of the neutron star low-mass X-ray binary system 4U 1728-34 using data obtained with XMM-Newton, Chandra, Suzaku, Swift-XRT and RXTE. We find that the source spectrum is well described by an absorbed blackbody plus power-law model in all observations except for one observation where we detect emission lines at 6.7 and 7.1 keV which are consistent with being produced by highly ionized iron. The temperature of the blackbody component varies between 0.6-0.9 keV while its radius ranges between 3-7 km depending on whether or not the absorption column density was allowed to vary freely during fitting. In addition, we also found evidence for a soft excess below 1 keV in some of our spectra. Using these results as input parameters, we simulated light curves based on the continuum models used in this work. Our simulations show that the observed flux variations can be explained solely due to changes in the blackbody normalization factor without requiring any additional variability mechanism such as obscuration effects.","label":1,"model":"bloomz","source":"arxiv","id":3279}
{"text":"We present an explicit characterization of the set of separable states for two qubits or three qutrits, respectively, as well as the corresponding sets of maximally entangled states. The results are obtained using only one-qubit and one-qutrit unitaries acting on pure product states. We also show that these characterizations can be used to construct optimal quantum circuits for state preparation and measurement. \nPACS numbers: 03.67.Mn, 03.65.Ud, 05.45.Mt \nI. INTRODUCTORY REMARkS\nThe problem of determining whether a given mixed state is separable (i.e., it can be written as a convex combination of product states), has been studied extensively over the past decade [1] . In particular, this question was shown to be NP-hard [2] , which implies that no efficient algorithm exists for solving it exactly [3] .\nIn spite of its apparent difficulty, several useful sufficient conditions have been derived [4] - [8] . These include the positive partial transpose criterion [9] , the realignment criterion [10] , and the Peres-Horodecki criterion [11] . However, none of them is necessary [12] . Moreover, they do not provide any information about how many product terms should appear in such decomposition [13] . This makes their practical use limited [14] .\nRecently, there were some attempts at finding exact solutions for special classes of problems [15] - [17] . For example, it was shown that all 2 \u00d7 D dimensional density matrices with rank \u2264 1 are separable [18] . Also, it was proved that all 3 \u00d7 D dimensional density matrices whose spectrum consists of nonnegative integers are separable [19] . Finally, it was proven [20] that if a 4 \u00d7 D matrix satisfies certain additional constraints then it must be separable.","label":1,"model":"bloomz","source":"arxiv","id":3280}
{"text":"We present the first systematic survey of X-ray flares observed in gamma-ray burst (GRB) afterglows with the Swift satellite, which has been operating since 2004. We find that most GRBs show at least one X-ray flare during their prompt or early afterglow phase; we also find that some GRBs have multiple flaring episodes. The majority of these flares are soft, but there is evidence for both hard and very-hard flares as well. In addition to studying individual flares, we examine the spectral properties of all flares combined using time-resolved spectroscopy techniques. Our results indicate that the average spectrum can be described by an absorbed power law model with photon index \u0393 = 1.6 \u00b1 0.1 and hydrogen column density NH = 2 \u00d7 10^22 cm^{-2}. \n \n Using this best-fit model, we calculate the total energy emitted in each flare over its duration. For the entire sample of flares studied here, we find that the mean fluence of the flares is ~10^-7 ergs\/cm^2, while the mean peak luminosity is ~10^45 ergs\/s. These values correspond to typical energies released in solar flares.","label":1,"model":"bloomz","source":"arxiv","id":3281}
{"text":"We present an agent-based computational model for simulating the growth and invasion of non-small cell lung cancers (NSCLCs). The model is based on a hybrid cellular automaton that incorporates both discrete and continuous variables, which are updated simultaneously at each time step using a set of coupled ordinary differential equations. We use this framework to simulate tumor growth in three dimensions under different conditions including varying levels of oxygenation and nutrient availability as well as treatment strategies such as chemotherapy or radiation therapy. Our results show that our model can reproduce several key features observed experimentally during NSCLC development and progression. In particular, we find that hypoxic regions develop within tumors due to rapid proliferation of cells leading to insufficient blood supply. These hypoxic regions promote angiogenesis by secreting pro-angiogenic factors into the surrounding tissue thereby creating new vessels through sprouting and branching processes. Finally, we demonstrate how our model could be used to study drug efficacy against NSCLC.","label":1,"model":"bloomz","source":"arxiv","id":3282}
{"text":"The composition of cosmic rays is studied by measuring their energy spectrum and mass distribution at Earth. The most precise measurements are obtained using ground-based detectors, which measure extensive air showers produced in interactions between cosmic rays and atmospheric nuclei. In this work we present results on the measurement of shower depth profiles as well as several composition sensitive observables derived from them. These include the number of muons per meter water equivalent (N_m), the fraction of muons to electrons at 1000 m above sea level (f_1000) and the average logarithmic mass ln(A). We compare these results for different zenith angles and energies. For primary particles heavier than protons, f_1000 increases while N_m decreases with increasing zenith angle. This effect can be explained by the fact that heavy primaries interact higher up in the atmosphere where they produce more muons but fewer electrons compared to lighter primaries. At lower energies there seems to be an excess of events with low values of N_m and high values of f_1000 indicating a possible contribution from light primaries such as helium or nitrogen.","label":1,"model":"bloomz","source":"arxiv","id":3283}
{"text":"The Standard Model (SM) is an extremely successful theory, but it leaves many questions unanswered about physics at very high energies. In particular, there are no known fundamental principles that can explain why the SM has three generations of quarks and leptons with such different masses or how gravity fits into this picture. Theories beyond the Standard Model attempt to address these issues by introducing new particles and\/or interactions which may be observed in future experiments.  Supersymmetry (SUSY), for example, introduces partners for all SM fields whose spin differs by one half unit. These partner states have identical gauge quantum numbers as their SM counterparts, so they could mix with them if SUSY were broken at low energy scales. This mixing would lead to deviations from SM predictions for observables like cross sections and decay rates. Many extensions of the Standard Model also predict new phenomena associated with extra dimensions of space-time. For instance, theories based on string\/M-theory often contain additional spatial dimensions compactified down to tiny sizes. If these extra dimensions exist, then we should see evidence of their effects through virtual exchange of Kaluza-Klein excitations of gravitons and other particles between SM fields localized on our four-dimensional world-volume.","label":1,"model":"bloomz","source":"arxiv","id":3284}
{"text":"We present an alternative thermal derivation of the Coleman-DeLuccia tunneling prescription for black hole formation in de Sitter space-time, which is based on the Euclidean path integral approach to quantum gravity and uses only standard techniques of statistical mechanics. We show that this method reproduces exactly the same results as those obtained by Hawking's original calculation using semi-classical methods. The advantage of our new approach lies in its conceptual simplicity and generality - it can be applied not only to Schwarzschild-deSitter but also to Kerr-deSitter solutions with arbitrary angular momentum. In addition we discuss possible extensions of these ideas beyond the semiclassical approximation. PACS: 04.20.-q; 98.80.Cq Keywords: Black holes, Quantum gravity, Tunneling, Thermalization, Path integrals, Statistical mechanics, De Sitter space time. 1 Introduction Black holes are one of the most fascinating predictions of Einstein's theory of gravitation [1] . They have been studied extensively over many years both theoretically [2] , numerically [3] and experimentally [4] .\nThe main difficulty in studying black holes comes from their singular nature at the horizon. This problem has been overcome recently by Hawking [5] who showed how black holes evaporate due to quantum effects. He found that black holes emit particles according to a Planck spectrum [6] \nwhere T H =h\/8\u03c0M P l c 2 is the temperature of the black hole (with M being its mass) and \u03ba = 1\/4G N is the surface gravity of the event horizon. It was shown later [7] that the emission rate \u0393 \u221d exp(\u22128\u03c0M\/M P l ) agrees very well with numerical calculations [8] . However, despite all efforts made so far there still remains some controversy about whether or not black holes really exist [9] .","label":1,"model":"bloomz","source":"arxiv","id":3285}
{"text":"We study the statistical properties of Barkhausen noise generated by an Ising spin system with random fields and competing interactions at its surface, using wavelets to analyze the time series produced by this model.  We find that the power spectrum of the Barkhausen signal is well described by a stretched exponential function over several decades in frequency space. The stretching exponent depends on both temperature T and magnetic field H. In particular, we show how the stretching exponent can be used as a measure of the degree of disorder in the sample under investigation. Finally, we discuss possible extensions of our work to other types of systems exhibiting avalanche dynamics. Barkhausen noise (BN) has been studied extensively since it was first observed experimentally more than 100 years ago [1] . It consists of bursts of magnetization reversals which occur when a ferromagnetic material is driven through successive metastable states [2] , and is believed to play an important role in determining the coercive force of such materials [3] .\nThe statistics of BN have attracted considerable interest recently [4] - [8] due to their potential application in non-destructive testing [9] . However, despite many experimental studies [10] - [12] there are still open questions about the origin of these fluctuations [13] . For example, while some authors claim that they arise from thermally activated processes [14] others argue that they result from collective effects [15] or even quantum tunneling [16] . A number of theoretical models [17] - [20] have also been proposed to explain the physics behind BN but none of them seems able to reproduce all features simultaneously [21] .","label":1,"model":"bloomz","source":"arxiv","id":3286}
{"text":"We study the evolution of cosmic strings in an expanding universe, focusing on their formation mechanism and subsequent growth. We show that cosmic strings can form when magnetic fields are trapped inside overdense regions during inflation. The resulting network consists of many small loops which evolve into larger ones through gravitational radiation emission. This process is similar to the one proposed for electroweak strings formed at phase transitions after inflation. However, we find that the loop distribution function has a different shape than previously assumed. In particular, it contains more large loops with sizes comparable to the Hubble radius today. These loops may be detectable as stochastic backgrounds of gravitational waves or gamma rays. Cosmic strings have been predicted to exist since the early 1980s [1, 2] . They could arise naturally if there were extra dimensions beyond those observed so far [3] , or they might be produced at symmetry breaking phase transitions [4] .\nCosmic strings would produce observable effects such as gravitational lensing [5] , CMB anisotropies [6] , and primordial black holes [7, 8] . Despite this interest, no direct detection of cosmic strings has yet been made [9] . One reason why cosmic strings remain elusive is because they are expected to be very light (with masses less than $10^{-16}eV$) [10] . Another problem is that cosmic strings are not stable objects but rather decay rapidly via gravitational radiation [11] . Therefore, any observational evidence must come indirectly from the products of cosmic string decays [12] .\nIn order to make predictions about possible observations, cosmological simulations need to be performed [13] . A number of groups have studied cosmic string networks using N-body codes [14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64","label":1,"model":"bloomz","source":"arxiv","id":3287}
{"text":"We present an analysis of the kinematics, metallicity distribution function (MDF), and chemical abundances in the outer halo of our Galaxy using data obtained with the Subaru Telescope for two fields along the leading arm of the Magellanic stream. We find that the MDFs are well represented by three Gaussian components centered at [Fe\/H] = -1.7, -0.9, and +0.2 dex, respectively. The metal-poor component is found to be associated with the Galactic thick disk\/halo population, while both intermediate- and high-metallicity populations show significant differences between the two fields. In particular, we detect a large fraction of high-alpha stars in one field but not in another located farther away from the center of the LMC. These results suggest that the origin of these streams may have been triggered by tidal interactions between the Milky Way and its satellite galaxies such as the Sgr dwarf galaxy and\/or the LMC.","label":1,"model":"bloomz","source":"arxiv","id":3288}
{"text":"The aim of this talk is to present some recent results on the relation between integrability in quantum field theory, statistical mechanics or mathematical physics and the existence of special geometric structures (complex algebraic curves) which are associated with these models. \nWe will discuss how such geometries can be used for solving exactly certain physical problems by using techniques coming from algebraic geometry like Riemann surfaces, theta functions etc.. We will also explain why it seems that many interesting integrable models have an underlying structure of a Riemann surface. Finally we will give examples where this connection has been made explicit. The talk will include several new results obtained recently by the author together with his collaborators. This work was partially supported by the DFG under grant SFB\/TR9. Integrable systems play an important role in various branches of mathematics as well as theoretical physics. In particular they appear naturally when studying solvable lattice models in statistical mechanics or quantum field theories. It turns out that there exists a deep relationship between integrability and the presence of special geometric structures called algebraic curves. These curves provide powerful tools for solving exactly certain physical questions via methods from algebraic geometry like Riemann Surfaces, Theta Functions,...","label":1,"model":"bloomz","source":"arxiv","id":3289}
{"text":"We present an empirical photometric redshift technique for luminous red galaxies (LRGs) and clusters, using the Sloan Digital Sky Survey Data Release 5 (SDSS DR5). We use a sample of LRGs\/clusters with spectroscopic redshifts to calibrate our method by fitting their observed colors as functions of redshift. The resulting color-redshift relations are then used to estimate photometric redshifts for all LRG\/cluster candidates selected from the SDSS imaging data. Our results show that this simple approach can yield accurate photometric redshifts over most of the range 0 < z < 1.2 covered by the survey. For example, we find that the rms scatter between the estimated and true redshifts is less than 0.05(1+z), which corresponds to about 60 km\/s at z = 0.6. This accuracy is comparable or better than those achieved by other methods based on template-fitting techniques. \n \n Keywords: Empirical photometric redshifts; Luminous Red Galaxy; Cluster","label":1,"model":"bloomz","source":"arxiv","id":3290}
{"text":"We present new radio continuum and H I absorption data for the dwarf irregular galaxy VIRGOHI 21, which is located at the edge of the virgo cluster. The galaxy has an optical diameter of about 1 kpc (0.3 arcmin) and shows no signs of star formation activity. We find that its neutral hydrogen mass amounts to M_HI = 2 x 10^{10} M_sol. Its total luminosity L_TOT = 3.5 x 10^{8} L_SOL corresponds to a B-band absolute magnitude MB = -18.7 mag. This value agrees well with those found by other authors for similar galaxies. From our analysis we conclude that this object may be considered as a candidate for a \"dark galaxy\". It contains only little or even no stars but still possesses a large amount of cold gas. If confirmed, it would provide further evidence for the existence of such objects.","label":1,"model":"bloomz","source":"arxiv","id":3291}
{"text":"The aim of this study was to develop and evaluate an algorithm that can be used to assess uncertainties associated with DVH computations using Monte Carlo (MC) simulations. The proposed approach is based on the concept of \"uncertainty bands\", which are defined as regions within which all possible values of a given quantity lie at some specified probability level. In order to demonstrate its feasibility, we applied our methodology to two clinical cases involving different treatment techniques. For each case, we performed MC calculations using various numbers of histories ranging between 1x10 5 and 2x10 6 . We then compared results obtained by applying our new technique against those produced by conventional methods such as the standard deviation or confidence intervals. Our findings indicate that the proposed method provides more accurate estimates than other approaches commonly employed in practice. This work has been presented at the International Conference on Medical Physics and Biomedical Engineering held in Singapore during August 2009. \n \n Keywords: Uncertainty bands, Dose Volume Histogram, Monte Carlo","label":1,"model":"bloomz","source":"arxiv","id":3292}
{"text":"We present an algorithm to find galaxy groups using photometric redshifts, which is based on the Voronoi tessellation technique (VT). The VT method has been widely used for identifying clusters of galaxies with spectroscopic redshifts but it had not yet been applied to identify galaxy groups with photometric redshifts. We use the Sloan Digital Sky Survey data release five (SDSS DR5) as our input sample. Our results show that the VT method can be successfully applied to identify galaxy groups even when only photometric redshifts are available. In this work we have identified more than 12000 galaxy groups at 0 < z < 0.3. These groups contain about 30000 member galaxies. We also provide a catalog containing basic information such as positions, magnitudes, colors, and photometric redshifts for all these groups. \n \n Keywords: Galaxy Group, Photometric Redshift","label":1,"model":"bloomz","source":"arxiv","id":3293}
{"text":"We have analyzed the spectral and timing properties of Cygnus X-2 using data obtained with the Rossi X-ray Timing Explorer (RXTE). The source was observed for about 100 ks during 1996-1997, when it showed strong aperiodic variability on time scales ranging from milliseconds to hours. We find that the power density spectrum can be described by two components; one is flat below 10 Hz and another has an index of -1 above this frequency. In addition we detect quasi-periodic oscillations at frequencies around 300 Hz which are coherent over several hundred seconds. Using these results as input parameters into our model fitting procedure, we obtain good fits to both the energy spectra and the light curves simultaneously. Our best-fit models show that there exists a correlation between the temperature of the Comptonizing plasma and its optical depth. This result suggests that the hot plasma responsible for the hard X-ray emission may also produce soft photons through thermal bremsstrahlung or synchrotron radiation.","label":1,"model":"bloomz","source":"arxiv","id":3294}
{"text":"We present the first results on the large-scale structure (LSS) of short-lived Ly\u03b1 emitters at z = 3.1, using deep narrow-band imaging data taken with Subaru\/Suprime-Cam and follow-up spectroscopy obtained with Keck\/DEIMOS. We find that these galaxies are distributed in filaments up to several Mpc across, which is consistent with previous studies for more massive galaxies. The LSS shows no significant difference between star-forming galaxies selected by their rest-frame UV luminosity or equivalent widths of Ly\u03b1 emission lines. However, we do not detect any galaxy clusters associated with our sample down to an overdensity limit of $\\sim$10. This suggests that the clustering strength of Ly\u03b1 emitting galaxies may be weaker than those of more massive galaxies. Our result also implies that the typical halo mass of Ly\u03b1 emitting galaxies could be smaller than $10^{13}$M\u2299.","label":1,"model":"bloomz","source":"arxiv","id":3295}
{"text":"The spin transistor is an important device for future quantum information processing and communication technologies, but its realization in practice has been challenging due to the lack of suitable materials with large spin-orbit coupling (SOC). Here we propose that graphene can be used as such material by exploiting its unique electronic structure. We show how this leads to a novel type of spin transistor which operates at room temperature without external magnetic fields or applied gate voltages. The proposed device consists of two ferromagnetic contacts connected via a single layer of graphene. By applying a voltage between these contacts one can control the SOC strength in the graphene channel leading to a change in the transmission probability through it. This results in a switching behavior similar to conventional transistors. In addition, our analysis shows that the proposed device exhibits high on\/off ratios even when operating under realistic conditions. Finally, we discuss possible experimental realizations of the proposed device. Graphene is a promising candidate for applications in spintronics because of its unique electronic properties [1] . It offers the possibility to realize devices based on pure spin currents [2] , which are not limited by Joule heating effects [3] .\nIn particular, the spin Hall effect [4] allows for efficient generation [5] and detection [6] of spin currents using only electric fields [7, 8] . However, despite many theoretical proposals [9] , there have so far been very few successful attempts to experimentally demonstrate spintronic devices based on graphene [10] . One reason might be the difficulty to find appropriate materials with sufficiently strong spin-orbit interaction [11] . Another problem is related to the fact that most experiments were performed at low temperatures [12] where thermal fluctuations limit the performance of spintronic devices [13] .","label":1,"model":"bloomz","source":"arxiv","id":3296}
{"text":"The measurement was performed at the Cyclotrons and Accelerators Laboratory (CYCLONE) in JINR, Dubna using the proton beam with energy E = 1 GeV. The experiment was carried out to study the pion production in nuclear reactions induced by relativistic protons on nuclei Ta(p, \u03c0+). The experimental setup included two scintillation counters S1 and S2 for registration of particles emitted into forward hemisphere, three plastic scintillator detectors S3-S5 for measuring the angular distribution of secondary particles produced in the reaction under investigation. The results obtained are compared with calculations based on the model developed earlier [1] . \nIntroduction\n\nPion production is one of the most important processes in hadronic interactions which play an essential role in many fields such as astrophysics [2] , cosmic ray physics [3] , accelerator technology [4] etc.. In this work we present new data on the pion production in nuclear collisions induced by relativistic protons interacting with nuclei Ta(p,\u03c0 + ). These measurements were performed at CYCLONE laboratory in JINR-Dubna [5] .\nExperimental Setup\nThe experimental setup used in our experiments consisted of:  -two scintillation counters S1 and S2; -three plastic scintillator detectors; -a set of collimators; -the target made of natural tantalum foil 0.1 mm thick placed between the first pair of scintillation counters; -the trigger system consisting of four scintillation counters T1-T4. \nThe layout of the experimental setup is shown schematically in Fig. 1 . The main parameters of the detector system are listed in Table I . The signals from all detectors were recorded by means of CAMAC modules [6] .","label":1,"model":"bloomz","source":"arxiv","id":3297}
{"text":"We report on the discovery and characterization of eleven new very low mass (VLM) objects discovered by the Two Micron All Sky Survey (2MASS). The sample includes ten previously undiscovered late-type M dwarfs with spectral types ranging between M8 and M9.5, as well as one object that is likely to be an early type brown dwarf or possibly even a planetary-mass object nearing its end stage of evolution toward becoming a white dwarf. We also present evidence for possible binarity among two of our targets based upon their infrared colors and optical spectra. These results are discussed within the context of current models of VLM star formation and substellar evolutionary theory. Keywords: Brown dwarf, Infrared, Low Mass Star, Spectroscopy, Substellar Object, White Dwarf, Very Low Mass Star, 2MASS, Nearby Stars, Planetary Nebula","label":1,"model":"bloomz","source":"arxiv","id":3298}
{"text":"We study the phase diagram and critical behavior of an anisotropic system consisting of Nx \u00d7 Ny spins on a square lattice, where each spin is described by the two-dimensional XY model. We find that there are three phases depending on the values of J1 \/ J2 (J2 > 0); ferromagnetic state for small J1 \/ J2 , spiral state for intermediate J1 \/ J2 , and paramagnetic state for large J1 \/ J2 . The transition between these states belongs to the universality class of the Ising model. In particular we show that the spiral state has a nontrivial structure which can be regarded as a superposition of ferromagnetically ordered domains with different orientations. This result suggests that the spiral state may have some relevance to the physics of high-Tc cuprates. \n \n Introduction \n \n It was shown recently [1] that the ground-state properties of the twodimensional Heisenberg antiferromagnet with nearest-neighbor interactions depend strongly on whether or not the exchange interaction along one direction vanishes identically. For example, if the exchange interaction along the y-direction vanishes completely, then the ground state becomes ferromagnetic even though it consists only of S = 1\/2 spins. On the other hand, when the exchange interaction along both directions does not vanish simultaneously, the ground state is always antiferromagnetic [2] .\n \nIn this work, we consider another type of anisotropy in the two-dimensional XY model: namely, we assume that the coupling constant along the x-direction is larger than that along the y-direction. As will become clear later, such an anisotropy plays an important role in determining the nature of the ground state.","label":1,"model":"bloomz","source":"arxiv","id":3299}
{"text":"Reverberation is an important feature of brain function, but its role remains unclear. We show that reverberation can be generated by the interplay between calcium signaling and short-term plasticity (STP) at excitatory synapses. In our model, STP leads to bursts of spikes which are followed by periods of low firing rate due to depletion of neurotransmitter vesicles. The resulting slow recovery of transmitter release causes a build-up of residual calcium concentration [Ca] res , leading to facilitation of glutamate release during subsequent bursts. This positive feedback loop generates sustained reverberatory activity with multiple time scales. Our results suggest that reverberation may play a key role in information processing within neural circuits. Reverberation is one of the most prominent features of brain function [1] . It has been observed across different species [2] - [4] as well as in various brain areas including hippocampus [5] , neocortex [6] , thalamus [7] , striatum [8] , cerebellar cortex [9] , olfactory bulb [10] , and retina [11] .\nDespite its ubiquity, however, it still remains unknown what functional roles reverberation plays in the brain [12] . One possibility is that reverberation serves as a mechanism for memory storage [13] or retrieval [14] . Another hypothesis suggests that reverberation could serve as a substrate for working memory [15] . Yet another idea is that reverberation might provide a means for temporal coding [16] . Finally, some studies have suggested that reverberation may simply reflect ongoing spontaneous activity [17] .","label":1,"model":"bloomz","source":"arxiv","id":3300}
{"text":"We consider the problem of scheduling multiple bag-of-work applications on parallel machines with non-cooperative tasks, where each task has its own deadline and budget constraints. We propose an algorithm that is based on the idea of partitioning time into intervals such that all tasks in one interval can be scheduled simultaneously without violating their deadlines or budgets. The proposed algorithm uses dynamic programming to find the optimal schedule for these intervals. Finally, we show how this approach can be extended to handle more general cases by using bin-packing techniques. Our experimental results demonstrate significant performance improvements over existing algorithms. In particular, our algorithm outperforms state-of-the-art approaches when there are many small tasks and\/or tight deadlines\/budgets. Keywords: Parallel computing, Computational complexity theory, Computational geometry, Data mining, Bioinformatics, High-performance computing, Grid computing, Cloud computing, Big data analysis, Distributed systems","label":1,"model":"bloomz","source":"arxiv","id":3301}
{"text":"We present an analysis of helioseismic data for the angular velocity profile of the Sun, which is based on the assumption that the total angular momentum of the Sun is conserved during its evolution. We find that this hypothesis leads to a good agreement between theoretical predictions and observations at all latitudes except near the poles where we observe significant deviations. The discrepancies are explained by taking into account the effects of torsional oscillations in the convective envelope of the Sun. Our results show that these oscillations can be excited by internal magnetic fields generated by differential rotation in the radiative interior of the Sun. This mechanism may also operate in other solar-type stars with strong surface magnetic activity. In our study we use high-precision measurements of the frequency splitting of acoustic modes obtained using space-based instruments (GOLF and MDI) aboard the Solar and Heliospheric Observatory satellite. \n \n Keywords: Angular momentum, Torsional oscillations","label":1,"model":"bloomz","source":"arxiv","id":3302}
{"text":"We study the phase behavior and structure of binary mixtures composed of soft repulsive spheres with attractive depletants, which are modeled as hard-spheres that interact only via excluded volume interactions. We find that these systems exhibit rich phase diagrams including gas-liquid coexistence at low temperatures for all compositions studied here (0.25 < f < 0.75), where f is the fraction of particles made up by the smaller species. The liquid-gas binodal lines shift to higher pressures upon increasing the size ratio between the two components. For large size ratios we observe an additional fluid-fluid transition line along which both fluids have similar densities but different structures. This new fluid state has been observed experimentally in colloidal suspensions containing nonadsorbing polymer chains. Our results show good agreement with experimental data on colloid-polymer mixtures over wide ranges of temperature, pressure, and composition. \nI. INTRODUCTIO N\nThe presence of small particles can dramatically affect the properties of larger ones through depletion forces [1] . These effects play important roles in many physical phenomena such as protein crystallization [2] , gelation [3] , and sedimentation [4] .\nDepending on their sizes relative to each other, the mixture may be either miscible or immiscible [5] . In addition, there exist regions of metastability [6] and even multiple phases [7, 8] . A number of theoretical studies [9] - [11] have investigated the effect of depletion attractions on the phase diagram of simple model systems. However, most of them focused on idealized models neglecting hydrodynamic interactions [12] , finite-size effects [13] , polydispersity [14] , and particle shape [15] . Only recently did some authors [16] take into account more realistic features like Brownian motion [17] , electrostatic repulsion [18] , and van der Waals attraction [19] . Despite this progress, it remains difficult to predict the exact location of the critical point [20] due to strong correlations [21] among the particles [22] . Moreover, the influence of depletion forces on the structural [23] and dynamical [24] properties of complex fluids still needs further investigation [25] .\nIn recent years, experiments [26]","label":1,"model":"bloomz","source":"arxiv","id":3303}
{"text":"We consider the problem of computing all eigenvalues of large sparse polynomials with coefficients in \\{\\pm 1\\}, which are known to be NP-hard problems. We present an algorithm that computes all roots of such polynomials by solving polynomial eigenvalue problems (PEPs) for certain matrices whose entries depend on the coefficients of the original polynomial. The PEPs we solve have special structure that allows us to use efficient numerical algorithms based on Krylov subspaces. Our approach is inspired by recent work on delay differential equations, where similar ideas were used to compute the spectrum of time-varying systems. In particular, our method can also be applied to find the stability region of linear switched systems. As a first step towards this goal, we show how to efficiently compute the largest root of a given polynomial using only O(n log n) arithmetic operations over the binary field. Finally, we demonstrate the practicality of our approach by applying it to several examples arising from control theory.","label":1,"model":"bloomz","source":"arxiv","id":3304}
{"text":"We study the Josephson effect across an S\/F\/S junction where F is a thin film made up of magnetic domains oriented along different directions, and show that this leads to a rich variety of behaviors depending on the relative orientations between the magnetization vectors of neighboring domains.  We find that for certain domain configurations there are regions within which no supercurrent flows at all (the so-called \"dead zones\"). In other cases we observe multiple Andreev bound states inside each dead zone. Finally, we also consider junctions containing two or more parallel layers of ferromagnets separated by nonmagnetic spacers. The results obtained here can be used as guidelines for designing novel devices based on the Josephson effect. Introduction: Superconducting spin valves have been proposed recently as possible candidates for applications such as high-frequency oscillators [1] , microwave detectors [2] , and quantum information processing [3] . These devices consist of a normal metal layer sandwiched between two superconductors, one of them being ferromagnetic [4] .\nIn order to understand how these systems work it is important to know what happens when they are connected together via a weak link. This problem has attracted considerable attention over recent years [5] - [8] . It was shown theoretically [9] and experimentally [10] that if both superconductors are identical then the system behaves like a single superconductor with modified parameters [11] . However, if the two superconductors differ significantly [12] , new phenomena may occur [13] . For example, if one of the superconductors is strongly type-II while the other remains weakly type-I [14] , then the phase difference between the two superconductors will not change continuously but rather jump discontinuously [15] . If the thicknesses of the two superconductors are comparable [16] , then the system becomes similar to a double-barrier structure [17] .","label":1,"model":"bloomz","source":"arxiv","id":3305}
{"text":"We present new observations and analysis of the infrared emission lines in the spectra of two yellow hypergiants, IRC+10420 and AFGL2136. We find that these objects have very high mass-loss rates (10^-6 to 10^-5 Msun\/yr) with outflow velocities ranging between 100-200 km\/sec. The observed line profiles are consistent with an expanding shell model for the wind. In addition we detect several forbidden transitions which indicate the presence of highly ionized species such as Fe + , Si ++ , S ++ . These ions may be formed by photoionization or collisional ionization processes within the stellar winds. \n \n Keywords: Yellow Hypergiants; Circumstellar Envelopes; Mass loss rate; Outflows; Emission Lines; IRAS 08544-4431. Astronomy & Astrophysics manuscript no. aa20031118 May 31, 2003 \nThe chemical composition of the circumstellar-envelope","label":1,"model":"bloomz","source":"arxiv","id":3306}
{"text":"We study the dynamics of three agent games with two strategies each, where agents are connected by an underlying network and play pairwise interactions according to their strategy choices. We show that for any initial state there is always at least one absorbing state in which all agents have the same strategy choice. In addition we find that if the number of nodes with either strategy exceeds 1 then this state can be reached within finite time. Finally, we provide bounds on how fast such convergence occurs as well as conditions under which it will occur exponentially quickly. The results presented here generalize previous work done on two-agent evolutionary games to multi-agent systems. Evolutionary game theory has been used extensively over the past decade to model competition between different species or individuals competing for limited resources [1] . A common approach taken when modeling these types of problems is to consider a population consisting of many interacting agents who choose among several possible strategies [2] , and then use mathematical tools developed in statistical physics [3] to analyze the resulting system behavior [4] .\nIn recent years researchers have begun studying more complex models involving multiple populations [5] , spatial structure [6] , and heterogeneous environments [7, 8] . However, most existing research focuses only on two-player games [9] , while less attention has been paid to multi-agent systems [10] . Here we present new results showing that even simple multi-agent systems exhibit richer dynamical behaviors than previously thought [11] .","label":1,"model":"bloomz","source":"arxiv","id":3307}
{"text":"We present the results of our analysis on stellar surface structure effects in high precision space-based optical astrometric observations, which are expected to be achieved by future space missions such as Gaia and SIM. We show that these effects can cause systematic errors in the derived parameters for individual stars with precisions better than 1 microarcsecond (mas). The main sources of error include limb darkening effect, gravity brightening effect, and rotational distortion effect. These effects depend strongly on the spectral type of observed stars. For example, we find that the rotationally distorted shape of late-type giant stars may lead to an apparent shift of up to several mas between their positions measured at different epochs. This is comparable or even larger than the accuracy goal set for Gaia mission. Therefore it will be necessary to take into account these effects when analyzing data obtained by future space missions. Keywords: Astrometry, Gravity","label":1,"model":"bloomz","source":"arxiv","id":3308}
{"text":"We present the discovery and timing analysis of two millisecond radio pulsars (MSPs) orbiting an intermediate mass black hole (IMBH). The MSPs are located within the central region of the globular cluster NGC 6397, which is known to contain an IMBH with a mass of about \\(3\\times 10^{3}M_\\odot\\), as inferred by its high velocity dispersion. \n \n We detected both pulsars using the Arecibo Observatory at frequencies between 1.4 GHz and 3 GHz over a span of more than three years. Both pulsars have spin periods less than 10 msec and orbital periods ranging from 0.5 days up to 2 weeks. Their masses range from 1.1 to 1.6 solar masses. These properties place them among the most massive and fastest spinning MSPs ever discovered. They also provide strong evidence that these systems were formed through direct capture of neutron stars by the IMBH rather than via exchange interactions or tidal captures during close encounters with other stars.","label":1,"model":"bloomz","source":"arxiv","id":3309}
{"text":"We introduce the concept of brane tilings, which are generalizations of tiling groups and can be used to describe many different types of physical systems in terms of their symmetries.  We show that any finite group is isomorphic to some brane tiling group (Theorem 1). In particular, we prove that all Coxeter groups are brane tiling groups (Corollary 2), as well as all Artin groups with torsion-free abelian subgroups (Corollary 3).  As an application, we give a new proof for the classification theorem of finite simple groups by showing how they arise naturally through the action of brane tilings on surfaces (Theorem 4).\nIntroduction\n\nTilings have been studied extensively over the past few decades due to their connections to various branches of mathematics such as number theory, combinatorics, geometry, and physics.   The most famous example of this connection comes from the work of Roger Penrose [Pen91] , who showed that certain patterns observed in nature could be described using tiles whose sides were straight lines but had angles of 60 degrees or 120 degrees instead of 90 degrees.   These so-called \"non-standard\" tilings are now known as Penrose tilings.    Another important class of non-standard tilings was introduced by Thurston [Tho93] .    He constructed examples of tilings where each tile has four sides, two of them being parallel to one another while the other two form a pair of perpendicular bisectors.   This type of tiling is called a quadrilateral tiling.   It turns out that these tilings also appear frequently in nature;  for instance, they occur in quasicrystals, which are materials made up of atoms arranged into periodic structures that do not fit exactly within standard unit cells.   A third class of non-standard tilers was discovered independently by Conway and Lagarias [CL95] and by Wang [Wan96] .\nIn recent years there has been growing interest in studying mathematical models of quantum gravity, string theory, and M-theory.   One approach towards understanding these theories involves constructing spaces...","label":1,"model":"bloomz","source":"arxiv","id":3310}
{"text":"We present deep imaging data for the nearby dwarf spheroidal galaxy, Hercules (dSph), obtained with the Large Binocular Telescope (LBT). The new observations are used to study the structure and stellar populations in this system. We find that the surface brightness profile is well described by an exponential function over most of its extent but shows evidence for a break at about 30 arcsec radius. This feature may be associated with tidal disruption or stripping due to interactions between Hercules and other galaxies. Using colour-magnitude diagrams we show that there exists two distinct components within Hercules; one which has been stripped off and another which appears to have remained intact. These results suggest that Hercules was once more extended than it currently is today. Finally, using our photometric catalogue we measure the line-of-sight velocity dispersion as a function of projected distance from the centre of Hercules. Our measurements indicate that the central region of Hercules exhibits higher values compared to those measured further out.","label":1,"model":"bloomz","source":"arxiv","id":3311}
{"text":"The nonequilibrium steady states (NESS) of matrix product form are the focus of this work, which is intended to be useful for researchers in computational physics and chemistry who wish to solve problems with such NESSs using numerical methods.  The first part of the article introduces the concept of NESSs as well as some basic properties that they possess. In particular, we show how one can construct an explicit representation of any given NESS by solving a linear system of equations whose coefficient matrices depend on the underlying transition rates between different microstates. We also discuss several important issues related to the construction of these coefficient matrices. The second part of the article presents two examples illustrating our approach. Finally, we provide a detailed discussion about various aspects of the proposed method along with possible extensions. Nonequilibrium steady states (NESs), i.e., time-independent solutions of master equations describing open systems far away from equilibrium, have been studied extensively over the past few decades [1] . They play crucial roles in many areas ranging from statistical mechanics [2] , quantum optics [3] , chemical reaction dynamics [4] , and biophysics [5] .\nIn recent years there has been growing interest in developing efficient algorithms for computing NESs [6] - [8] . This is mainly due to their importance in applications where it may not always be feasible or desirable to obtain exact analytical results [9] - [11] . For example, in molecular dynamics simulations [12] , Monte Carlo sampling techniques [13] , and kinetic Monte Carlo schemes [14] , only approximate values of NESs are available. Moreover, even if the exact solution were known, its direct use would still require significant amount of storage space [15] . Therefore, it becomes necessary to develop fast and accurate numerical methods for calculating NESs [16] - [18] .\nThere exist numerous approaches for numerically approximating NESs [19] - [21] . Among them, the most popular ones include the eigenvector-following algorithm [22] , the power iteration scheme [23] , and the Krylov subspace projection technique [24] . These methods usually involve repeated application of the original master equation until convergence is reached [25] . However, since the number of...","label":1,"model":"bloomz","source":"arxiv","id":3312}
{"text":"We present new results on the asteroseismic analysis of the primary component in the binary system Alpha Centari, based on data obtained with the HARPS spectrograph at La Silla Observatory (Chile). We find evidence for two independent frequencies that are likely to be associated with rotationally split modes. The observed frequency pattern is consistent with theoretical predictions and suggests an inclination angle between 40\u00b0and 60\u00b0for this star. \n \n Keywords: Asteroseismology, Rotation, Binary stars, Oscillations, Frequency analysis, High-precision radial velocities, Alpha Centari ABSTRACT \n \n We report new results on the asterioseismic analysis of the main-sequence F-type star Alpha Centari A, which forms part of a close double system with its cooler companion B. Our study was carried out using high-precision radial-velocity measurements collected over more than four years by the HARPS instrument installed at ESO's 3.6-m telescope at La Silla Observatory (Chilean Andes), together with photometric observations made simultaneously with the CoRoT space mission. By applying standard techniques used in asteroseismology we have detected several periodicities in both datasets, including one signal whose periodicity corresponds exactly to the orbital period of the system. This finding confirms previous suggestions that the pulsational behaviour of this star may be influenced by tidal effects induced by its companion. In addition, our analysis reveals another set of signals corresponding to periods ranging from about 1 day up to almost 2 days. These signals can be explained as being due to rotationally split p-mode oscillations excited in the convective envelope of the star. Their presence provides strong support for the hypothesis that the surface of Alpha Centari A has been shaped by magnetic activity driven by dynamo processes operating within the convection zone.","label":1,"model":"bloomz","source":"arxiv","id":3313}
{"text":"We present the results of our survey for giant planets around stars that have debris disks and central clearing, using data obtained by the Spitzer Space Telescope's Infrared Array Camera (IRAC). We find no evidence for any new planet candidates beyond those previously reported in the literature.  The lack of detections is consistent with previous studies showing that most debris disk systems are not accompanied by massive planets on wide orbits. Our non-detection suggests that there may be an upper limit to the number of giant planets orbiting within 100 AU of their host star. This work was supported by NASA under grant NNX10AD65G issued through the Astrophysics Data Analysis Program. Keywords: Planet formation; Exoplanet; Debris disk; IRAC; Spitzer Space Telescope; Circumstellar disk; Stellar mass loss. 1 Introduction   Debris disks surround many main-sequence A-type stars at distances ranging from tens to thousands of astronomical units (AU) from their host stars (e.g., Wyatt 2008) . These dusty disks contain large amounts of small grains produced by collisions between larger bodies such as planetesimals or asteroids.   Many debris disks also show clearings near their centers where the amount of dust decreases significantly compared to the surrounding regions (e.g., Bryden et al. 2006 , Currie et al. 2012 . Such central holes can be explained if they were cleared out by one or more planetary companions located close to the central star (e.g., Quillen & Thorndike 2002 , Kenyon & Bromley 2005 .   Recent observations suggest that some debris disks harbor massive planets on wide orbits (\u223c100-1000 AU) (e.g., Greaves et al. 2007 , Lagrange et al. 2009 ). However, it remains unclear whether these planets are common among debris disk hosts because only a few dozen debris disk systems have been searched for planets so far (see Table 2 below).   For example, Bryden et al. (2006) found two debris disk systems with central holes that could be caused by planets on wide orbits but did not report","label":1,"model":"bloomz","source":"arxiv","id":3314}
{"text":"In this work, we introduce the higher order Schwarzian derivative (HOSD) to study chaotic behavior in dynamical systems. The HOSD is defined as the second-order differential operator with respect to time variable t acting on the first-order derivatives of the state variables x(t). We show that the HOSD can be used to construct new invariant sufficient conditions of chaos by using its properties such as non-negativity and monotonicity under some suitable assumptions. In addition, it also provides an alternative way to investigate the existence of periodic orbits in nonlinear autonomous systems. Finally, numerical examples are given to illustrate our results. Keywords: Dynamical systems; Chaos; Periodic orbit; Nonlinearity; Higher order Schwarzian derivative. 1 Introduction Let us consider the following nonautonomous ordinary differential equations (ODEs)\nx = f(t; x; u), where f: R \u00d7 Rn \u00d7 Rm \u2192 Rn, (1.1) which describes many physical phenomena arising in engineering fields [1] . Here, t \u2208 [0, T] denotes time; x \u2208 Rn represents the state vector; and u \u2208 Rm stands for control input or parameter vector. It should be noted that the function f may depend explicitly on both time t and control parameters u. For example, if one considers the motion of a particle moving along a straight line at constant speed v, then the position of the particle at any instant of time t is described by the equation x = vt + x0, where x0 is the initial position of the particle [2] .\nThe main goal of this article is to present a novel approach based on the higher order Schwarzian","label":1,"model":"bloomz","source":"arxiv","id":3315}
{"text":"We present an analysis of the dynamics of the four giant planets of our Solar System during their formation, using numerical simulations with N-body codes that include hydrodynamical effects (gas drag). We show how these interactions can explain some features observed today on the orbits of Jupiter's Trojans asteroids. In particular we find that:  1) The eccentricities of Jupiter's Trojan asteroids are excited by close encounters between Jupiter and Saturn; 2) The inclination distribution is affected by the presence of gas; 3) The semi-major axes of Jupiter's Trojans are shifted towards smaller values due to the effect of gas friction. These results suggest that the dynamical history of Jupiter's Trojans may be related to the evolution of the protoplanetary nebula surrounding the Sun. This work was supported by CONACyT grant No. 164713. We thank J. Laskar for providing us his code used to calculate the secular frequencies of the planetary systems. \n \n Keywords: Giant planet migration, Gas drag","label":1,"model":"bloomz","source":"arxiv","id":3316}
{"text":"We present an explanation for the geometric origin of physical constants based on a Kaluza-Klein tetrad model, where the extra dimension is compactified to a circle with radius R = 1\/mPlanck and the four-dimensional space-time metric has signature (+ ---). The five-dimensional action contains two terms corresponding to Einstein gravity and Yang-Mills gauge fields respectively. We show that the gravitational constant G and the fine structure constant \u03b1 are related by  G\u03b12 = 8\u03c02R4\/3m5Planck3. In this way we obtain values for these constants which agree well with experimental data. Furthermore, we find that the mass scale mPlanck can be identified as the fundamental Planck mass M0. Finally, we discuss how our results could be tested experimentally. Introduction: It is generally believed that all known interactions including gravitation should be described within one consistent framework at very high energies [1] . This idea leads naturally to the concept of unification [2] , i.e., the description of different forces in terms of a single force carrier or field [3] .\nThe most promising approach towards such a unified theory seems to be string theory [4] . However, despite many efforts over more than twenty years there still exists no convincing evidence supporting its validity [5] . On the other hand, it was shown recently [6] that the standard model of particle physics together with general relativity may emerge from a higher dimensional spacetime picture [7, 8] . In particular, if the additional dimensions are small enough they might not have been detected so far [9] .","label":1,"model":"bloomz","source":"arxiv","id":3317}
{"text":"We present new observations of the outflow driven by the young star cluster NGC 1333 IRAS 4A, located at the center of the Perseus molecular cloud (d = 235 pc). The data were obtained with the Submillimeter Array and include continuum emission at 1.3 mm as well as CO(2-1) line emission. We find that the outflow is highly collimated along an axis oriented NNE-SSW, which coincides with the direction to the nearby Herbig-Haro objects HH 7-11. The total mass of the outflowing gas is estimated to be ~0.1 Msun, while its kinetic energy amounts to ~10^50 ergs. These values are comparable to those found for other low-mass protostellar systems. However, we also detect significant differences between this system and others previously studied. In particular, our results suggest that the outflow may have been triggered recently due to the interaction of the central source with another object or structure within the dense core surrounding it.","label":1,"model":"bloomz","source":"arxiv","id":3318}
{"text":"We present new spectroscopic observations for more than 1000 Galactic OB supergiants, obtained with FLAMES\/GIRAFFE at the Very Large Telescope (VLT). The sample includes all known O-type dwarfs and giants as well as B-type supergiants brighter than about Mbol = -4 mag within 25 pc distance to Earth. We derive atmospheric parameters T eff , log g, microturbulence velocity vmic, and chemical composition including nitrogen abundance [N\/Fe] . For comparison we also analyse a large number of Galactic red supergiants observed by GOSSS project using similar methods. Our results show that there is no significant difference between the mean values of these quantities derived for both samples. However, our analysis reveals systematic differences between different studies based on smaller samples published so far. In particular, we find that the majority of previous investigations overestimated the temperatures of hotter objects due to neglecting non-LTE effects or underestimating gravities because they did not take into account stellar winds.","label":1,"model":"bloomz","source":"arxiv","id":3319}
{"text":"We consider the linearization problem for the Painleve' equations PIII, PV and PVI in terms of their Lax pairs. We show that these problems are equivalent to certain reductions of the three-wave resonant system. The results obtained here can be regarded as an extension of those by Zakharov et al., who studied the reduction of the two-wave resonant system corresponding to the Painleve' II equation. In particular we prove that there is no non-trivial solution of the linearized PIII or PV equation which decays exponentially at infinity if one imposes some additional conditions on its asymptotic behavior near the origin. \nIntroduction\n\nThe Painleve' equations play important roles both in mathematics and physics (see e.g.  [1] , [4] ). They have been extensively investigated during last decades mainly due to their rich structures such as soliton solutions and B\u00e4cklund transformations. Recently it has been shown that they also appear naturally in various physical models including nonlinear optics [2] . For example, the so-called NLS equation with cubic-quintic nonlinearity arises from the propagation of intense laser beams through Kerr media [3] .\nIn this article we study the linearization problem for several types of the Painleve's equations. More precisely let us consider the following systems of partial differential equations: \nwhere u = u(t, x) \u2208 C n+1 , v = v(t, x) and w = w(t, x) are complex-valued functions of t > 0 and x \u2208 R 1 . Hereafter subscripts denote differentiation with respect to variables indicated by them. It should be noted that all the above systems possess infinitely many conservation laws given by","label":1,"model":"bloomz","source":"arxiv","id":3320}
{"text":"We present an analysis of the basis-set dependence of post-Hartree-Fock (HF) corrections to atomization energies for small molecules, using explicitly correlated Gaussian functions and extrapolation techniques.  We show that the correlation energy contribution is more sensitive than the HF energy to the choice of basis sets used in calculations. The results are compared with those obtained by other authors who have studied this problem previously. Finally we discuss how these findings can be applied to improve the accuracy of thermochemical data calculated at the CCSD(T) level. In recent years there has been considerable interest in improving the accuracy of theoretical predictions of thermochemical properties such as enthalpies of formation or heats of reaction. These quantities are often determined experimentally but it would clearly be useful if they could also be predicted theoretically. One approach which has proved successful involves calculating the total electronic energy E tot [n] of a molecule within some chosen approximation n to quantum mechanics, where n = 1 corresponds to Hartree-Fock theory and n = 2 to second-order M\u00f8ller-Plesset perturbation theory (MP2). Corrections beyond MP2 may then be estimated either by performing higher-level ab initio calculations on smaller subsets of atoms [1] , or alternatively by fitting empirical parameters to experimental data [2] .","label":1,"model":"bloomz","source":"arxiv","id":3321}
{"text":"We present an explicit expression for the effective potential in terms of renormalization group functions, which is valid to all orders in perturbation theory and can be used as input into numerical calculations.  We show that this result agrees with previous results obtained by other methods at one-loop order. The new formula has several advantages over existing expressions:  It does not require any additional approximations beyond those inherent in the use of perturbation theory; it allows us to calculate the effective potential directly without having to solve complicated integral equations first; finally, we are able to obtain the full dependence on temperature and chemical potentials simultaneously. This work was supported by the U.S. Department of Energy under Contract No. DE-AC02-76SF00515. In recent years there have been many attempts to determine the effective potential Veff(T) = ln Z(T), where T denotes the temperature, using various techniques such as the imaginary time formalism [1] , the real time formalism [2] or the functional renormalization group (FRG) [3] . These approaches provide useful information about the phase structure of quantum field theories but they usually involve some kind of approximation scheme. For example, in the FRG approach one often uses truncations of the exact flow equation [4] .\nIn this letter we will derive an explicit expression for Veff(T) in terms of renormalisation group functions [5] . Our method is based on the observation [6] that the effective action \u0393k(\u03c6) (where k denotes the momentum scale) satisfies a differential equation known as the Wetterich equation [7, 8] \nHere Rk(\u0393k; \u03c6) is called the regulator function and describes how the infrared modes are suppressed when integrating out high energy degrees of freedom. By solving Eq. (1) numerically [9] one obtains the running coupling constants gk(\u03c6). Using these quantities together with the corresponding \u03b2-functions one can then compute Veff(T) according to","label":1,"model":"bloomz","source":"arxiv","id":3322}
{"text":"The classical interatomic potentials are used to study the martensitic transformation in Ti-based alloys. The results show that the classical potentials can reproduce the experimental lattice parameters for all three phases (alpha, beta, omega) as well as their elastic constants with reasonable accuracy. It is found that the energy barrier for the martensitic transformation decreases rapidly when temperature increases. This indicates that the martensitic transformation may be suppressed at high temperatures due to thermal fluctuations. \n \n Keywords: Classical potentials; Martensitic transformation; Titanium alloy. 1 Introduction Titanium has been widely applied in aerospace industry because it combines low density, good corrosion resistance and excellent mechanical properties [1] . However, its poor ductility limits its application [2] , especially under extreme conditions such as high pressure or high temperature [3] .\nIn order to improve the ductility of titanium materials, many efforts have been made by researchers [4] - [8] . Among these methods, the most promising one seems to be the introduction of second-phase particles into pure titanium matrix [9] - [11] . In fact, some commercialized titanium alloys contain small amounts of Al [12] , V [13] , Mo [14] , Fe [15] , Cr [16] , Ni [17] , Cu [18] , Sn [19] , Si [20] , Zr [21] , Nb [22] , Ta [23] , W [24] , Co [25] , Mn [26] , Ag [27] , Au [28] , Pd [29] , Pt [30] , Rh [31] , Ir [32] , Ru [33] , Os [34] , Re [35] , Sb [36] , Bi [37] , P [38] , As [39] , Ge [40] , B [41] , C [42] , N [43] , O [44] , F [45] , S [46] , Se [47] , Te [48] , I [49] , Xe [50] , Hg [51] , Gd [52] , Dy [53] , Er [54] , Yb [55] , Lu [56] , Y [57] , Sc [58] , La [59] , Ce [60] , Pr [61] , Nd [62] , Sm [63] , Eu [64] , Gd [65]","label":1,"model":"bloomz","source":"arxiv","id":3323}
{"text":"We present the results of our study on the electronic structure and magnetic properties of La0.7Sr1.3MnO3 using density functional theory (DFT) calculations within the local spin-density approximation (LSDA). We find that LSDA underestimates the band gap by about 0.5 eV, which is corrected to 1.2 eV when we use the self-interaction correction scheme proposed by Perdew et al.. Our calculated value agrees well with experimental data obtained from optical measurements. In addition, we have studied the effect of electron correlation on the ground state energy as a function of Ueff = U - J where U and J are respectively Coulomb repulsion and exchange parameters between two localized spins. It has been found that the total energy decreases monotonically upto Ueff ~ 3 eV beyond which it increases rapidly indicating strong electron correlations among Mn3+ ions. Finally, we show that the inclusion of spin-orbit coupling leads to an increase in the band gap by about 30%.","label":1,"model":"bloomz","source":"arxiv","id":3324}
{"text":"In this article, we study the confluence of the second order linear differential equations with variable coefficients and their solutions by applying the method of successive approximations to find the solution of the confluent hypergeometric equation (CHGE). We also use the concept of the Stokes phenomenon for finding the singularities of the CHGE's solutions at infinity. The results are illustrated numerically through examples. Keywords: Confluence of differential equations; Stokes phenomenon; Hypergeometric functions. 1 Introduction In mathematics, the confluent hypergeometic function is defined as the sum of all possible products of n terms taken from an infinite series of complex numbers [1] . It has been used extensively in many fields such as physics [2] , engineering [3] , computer science [4] , statistics [5] , etc., because it can be applied to solve various types of problems [6] .\nThe confluent hypergeometric function is denoted by ${}_1F_1$ or ${_2F_1}$ depending on whether the first argument is equal to one or not [7, 8] . For example, if the first argument equals one then the function is called the Kummer confluent hypergeometric function:","label":1,"model":"bloomz","source":"arxiv","id":3325}
{"text":"We study the generalized Dicke model with an arbitrary number N of two-level atoms interacting with one-mode radiation field, and show that it can be mapped to a spin-1\/2 system by using the Holstein-Primakoff transformation. We then use the exact diagonalization method to calculate its ground state energy spectrum for different values of the coupling constant g and the number N . The results are compared with those obtained by other methods such as perturbation theory and numerical integration. It is found that our results agree well with previous ones when the coupling strength is small but deviate significantly from them if the coupling becomes strong. Finally we discuss some possible applications of this work. PACS: 03.65.Ud, 05.45.Mt, 11.10.Gh, 12.20.Dc, 13.25.Gv \nI. INTRODUCTIO N\nThe Dicke model [1] describes how many identical two-level atoms interact collectively with a single mode of electromagnetic field. In recent years there has been renewed interest in studying this model because of its potential application in quantum information processing [2] , quantum optics [3] , condensed matter physics [4] , etc.. For example, the collective spontaneous emission rate of the atomic ensemble depends on the total angular momentum J = N \/2 (N being the number of atoms) [5] .\nIn fact, the Dicke model was originally proposed more than half century ago [6] . Since then various theoretical approaches have been developed to solve it [7 -10] . Among these approaches, the most successful one is probably the so-called HolsteinPrimakoff transformation [11] which maps the original problem into a spin-1\/2 system [12] . This approach works very well at weak-coupling regime where the interaction between atom-field is relatively small. However, it fails completely at large-coupling limit since the mapping procedure breaks down due to the appearance of unphysical states [13] . Recently, several authors [14 -19] have tried to overcome this difficulty by introducing new transformations or approximations. Nevertheless, their solutions still suffer from certain drawbacks [20, 21] .","label":1,"model":"bloomz","source":"arxiv","id":3326}
{"text":"We present an extension of the concept of coherence between two univariate signals to pairs of multivariate signals. The proposed measure is based on the generalized cross-spectrum (GCC) which can be estimated by means of Welch's averaged periodogram method or using the multitaper approach. We show that this new measure has several desirable properties such as being invariant under linear transformations in both components of the pair of signals considered. In addition we propose a procedure for removing the contribution at lag 0 due to common sources. This procedure consists of subtracting the average value over all lags of the GCC obtained with one component delayed relative to the other. Finally, we illustrate our results through numerical simulations and real data examples. Coherence measures are widely used in neuroscience to assess functional connectivity among different brain areas. However, most existing methods only consider bivariate relationships between pairs of single-unit recordings. Here we introduce a novel measure of coherence applicable to any number of simultaneously recorded signals. Our measure is based on the Generalized Cross-Spectrum (GCC), which can be estimated either via Welch's averaged periodogramm method or using the multi-taper approach. We demonstrate that it possesses many useful properties including invariance under linear transformations within each signal set. Furthermore, we provide a simple way to remove the contribution at lag 0 arising from common sources. Finally, we illustrate these results with simulated and experimental data.","label":1,"model":"bloomz","source":"arxiv","id":3327}
{"text":"We have identified a sample of candidate hidden Seyfert galaxies by searching for X-ray sources with hard spectra (\u0393 < 1) and high luminosities (Lx > 1043 erg s-1). We find that these objects are preferentially located at redshifts z ~ 0.7, where they can be detected only if their intrinsic absorption is NH < 1023 cm-2 . The majority of our candidates show no optical counterparts down to R = 25 mag on deep ground-based images; however, we do detect faint emission lines characteristic of AGN activity in some cases. Our results suggest that there may exist many more obscured active galactic nuclei than previously thought. This work was supported by NASA grant NAG5-7262. \n \n Keywords: Active Galactic Nuclei, Galaxy Evolution, X-Ray Astronomy \n \n Introduction \n \n In recent years it has become clear that most bright quasars reside in massive elliptical galaxies or bulges of spiral galaxies (e.g., McLure & Dunlop 2001), but the nature of the host galaxy remains unknown because of heavy dust extinction along the line-of-sight. It is possible that many optically-faint quasars are hosted by less-massive systems such as late-type spirals and\/or low-luminosity ellipticals (e.g., Hao et al. 2005). \n \n To understand how supermassive black holes grow over cosmic time, it is important to study both unobscured and obscured active galactic nucleus (AGNs) across a wide range of environments. However, identifying heavily-absorbed AGNs is difficult due to the lack of strong spectral features associated with them. One way to identify absorbed AGNs is through their X-ray properties. For example, Compton-thick AGNs are characterized by very flat X-ray continua and large equivalent widths of iron K\u03b1 fluorescence lines (EW>500 eV) (see e.g., Risaliti 2002). Another method is based on the fact that absorbed AGNs tend to exhibit higher X-ray-to-optical flux ratios compared to normal galaxies (e.g..","label":1,"model":"bloomz","source":"arxiv","id":3328}
{"text":"We report on the first simultaneous infrared (IR) and X-ray observations of Sgr A*, made with the Chandra X-Ray Observatory and the Spitzer Space Telescope in 2007-2008. We find that the IR emission is consistent with being produced by dust heated to temperatures between 100 K and 1000 K; this temperature range corresponds to an observed flux density at 8 microns ranging from 0.1 mJy to 1 Jy. The spectral index of the IR emission does not change significantly during these variations. This result suggests that the physical conditions within the emitting region are relatively constant over time scales as short as one month. These results also suggest that the IR emission may be dominated by optically thin thermal bremsstrahlung rather than synchrotron radiation. \n \n Keywords: black hole physics, infrared astronomy, radio source variability, space telescopes, X-ray astronomy \n \n \n \n Black holes have been predicted to produce intense electromagnetic fields near their event horizons. However, direct observational evidence has remained elusive because of the extreme environment surrounding such objects. One possible way to detect such fields would be through the detection of polarized light emitted close to the horizon. Another possibility involves detecting changes in the spectrum or intensity of the accretion flow onto the black hole itself. Such changes could occur if the magnetic field lines threading the disk were twisted into helical shapes due to differential rotation. If so, they can act like antennae which amplify any incoming waves along them. As a consequence, the local plasma frequency will increase, causing the plasma to become more opaque to lower-frequency waves but less opaque to higher frequencies. Thus, we expect the spectrum of the emission to steepen toward longer wavelengths when the system becomes brighter.","label":1,"model":"bloomz","source":"arxiv","id":3329}
{"text":"We present an analysis of the baryon acoustic oscillations (BAO) in the Sloan Digital Sky Survey Data Release 7 galaxy sample, using two different estimators to measure the BAO: the power spectrum and correlation function. We use these measurements to constrain cosmological parameters including the total matter density $\\Omega_m$, dark energy equation-of-state parameter $w$, Hubble constant $H_0$, and neutrino mass $m_{\\nu}$. The results are consistent with those obtained by other recent studies based on similar data sets. \n \n Keywords: Cosmology, Galaxy clustering, Neutrinos, Dark Energy, Baryonic Acoustic Oscillations, SDSS DR7, WiggleZ survey, Power Spectrum, Correlation Function, Cosmic Microwave Background Radiation, Planck satellite, CMB anisotropy, Inflation, Large Scale Structure, Weak Lensing, Matter Anisotropies","label":1,"model":"bloomz","source":"arxiv","id":3330}
{"text":"We present X-ray luminosity functions (XLFs) for normal galaxies at z=0.5-2, based on deep Chandra observations of the Great Observatories Origins Deep Survey-North field. We use photometric redshifts to select galaxy samples with different stellar masses and star formation rates. The XLFs are derived by fitting the observed number counts using a Schechter function plus an exponential cutoff below Lx=1041 erg s-1. Our results show that the faint-end slope is steeper than -1.0 for all mass bins considered here. For massive galaxies (log(M\/Msun)>10), we find no significant evolution between z=1.6 and 0.7. However, there appears to be some evidence for evolution towards lower characteristic luminosities as well as flatter slopes at higher redshift. These results suggest that the most massive galaxies may have evolved into less luminous systems over time.  We also compare our results with previous studies which used shallower data or smaller sample sizes.","label":1,"model":"bloomz","source":"arxiv","id":3331}
{"text":"We present the results for the beta functions of Yang-Mills and Yukawa theories to second order in perturbation theory using the method of differential renormalization.  We show that our results are consistent with those obtained by other methods, including dimensional regularization.   In addition we discuss how one can use this approach to obtain higher-order corrections to the beta functions. The beta function is an important quantity characterizing quantum field theories (QFTs). It describes the running coupling constant as a function of energy scale. For example, it determines whether or not QFTs have fixed points where the coupling becomes independent of the energy scale. This property has been used extensively to study non-abelian gauge theories such as Quantum Chromodynamics (QCD) which describe strong interactions between quarks and gluons. However, calculating the beta function beyond leading order requires knowledge about Feynman integrals containing multiple scales. These integrals cannot be evaluated analytically but only numerically. Therefore, there exist several different approaches to calculate them. One possibility is to apply the technique of dimensional regularization [1] . Another option is to use the method of differential renomalization [2] , which was developed recently [3, 4] .\nIn this work we will focus on the latter approach. Differential renormalization allows us to express all relevant quantities in terms of so-called master integrals. These are defined as vacuum expectation values of products of free fields. They can be calculated perturbatively in powers of the coupling constant. To determine the beta function up to next-to-leading order, i.e., O(g4), we need to know the master integrals to third order in g.","label":1,"model":"bloomz","source":"arxiv","id":3332}
{"text":"We present the results of our analysis on the mass-to-light ratio (M\/L) of Lyman alpha emitters (LAEs), based on data obtained by Subaru\/Suprime-Cam in the fields surrounding four quasars with redshifts 5.7 < zqso < 6.6. We use photometric redshift techniques to select LAE candidates within a narrow window around each quasar's emission line redshift. The M\/L values are derived using SED fitting for individual galaxies as well as stacked samples. Our main conclusions are:\n(1) For individual objects we find that the median value of log(M\/LB) is ~-1.8 dex over the range of luminosities probed here.  This result suggests that most LAEs have low stellar masses compared to typical LBGs.\n(2) Stacking analyses show that the average M\/LB increases towards higher luminosity bins. In particular, the highest luminosity bin has an average log(M\/LB ) = -1.3 \u00b1 0.4 dex which corresponds to a factor of 3 lower than what would be expected if these systems were dominated by old stars alone.","label":1,"model":"bloomz","source":"arxiv","id":3333}
{"text":"We present the results of observations made by the Multiband Imaging Photometer for Spitzer (MIPS) in 24 and 70 micron bands toward the Lupus molecular clouds. The data were obtained as part of the Spitzer Space Telescope's \"Cores to Disks\" Legacy program. We have identified more than 1000 infrared point sources associated with these clouds using our source extraction technique. These include protostars, young stellar objects, and background galaxies. In addition we find that there are many extended emission features which may be related to outflows or other phenomena associated with star formation. A comparison between the observed number counts at 24 microns and those predicted based on models of interstellar dust suggests that most of the detected sources are likely to be low mass stars surrounded by disks. This is consistent with previous studies of this region. However, it appears that some fraction of the brightest sources could also be high-mass protostars.","label":1,"model":"bloomz","source":"arxiv","id":3334}
{"text":"We present here an analysis of the kinematics of stars in the solar neighborhood based on Hipparcos data (ESA 1997). We use the method developed by Dehnen & Binney 1998a to determine directly the epicyclic frequency $\\Omega_e$ for each star with known distance and proper motion. The results are compared with those obtained using the Jeans equation applied to the observed distribution function. In addition we derive the circular rotation speed at the Sun's position $V_0$ as well as its uncertainty $\\sigma(V_0)$. Our main conclusions are: \n1. For most of our sample stars, the values of $\\Omega_e$ determined directly agree within their errors with those inferred indirectly from the Jeans equation. \n2. There is no significant dependence of $\\Omega_e$ or $V_0$ on metallicity [Fe\/H] over the range -0.5 <[Fe\/H]< +0.3 dex. \n\n\n3. The value of $V_0$ that we find agrees very well with previous determinations but has smaller error bars than any other measurement so far. \n\n4. The dispersion in the radial velocities of nearby F-type dwarfs is found to be $11 \\pm 1 km\/sec$ which corresponds to a random orbital eccentricity of $0.09 \\pm 0.01$.","label":1,"model":"bloomz","source":"arxiv","id":3335}
{"text":"The shell-model calculations are performed for the ground states and low-lying excited states in 16,17O nuclei using the effective interaction derived by Brown et al.  The results show that the calculated excitation energies agree well with experimental data except for some cases where the observed levels have large mixing between different configurations. In addition to the energy level scheme, we also calculate electromagnetic transition probabilities among these states. It is found that our calculation reproduces fairly well the measured B(E2) values but underestimates the B(M1) values significantly. This may be due to the fact that the M1 operator used here does not include the spin-orbit term which plays an important role in this case. Finally, we discuss briefly about the possible existence of new collective excitations in 17O nucleus. PACS: 21.10.Ks, 22.60.-n Shell Model Study of Neutron Rich Oxygen Isotopes","label":1,"model":"bloomz","source":"arxiv","id":3336}
{"text":"Francium is an extremely radioactive element that has never been observed in nature.  It can be produced by bombarding lighter elements with neutrons or alpha particles.   The most stable isotope of francium (Fr) has a half-life of about one second;  all other isotopes have even shorter half-lives.    This article presents theoretical results for excitation energies, polarizabilities,  multipole transition rates, hyperfine structure constants, and lifetimes   of ions along the franconium isoelectronic sequence up to Z = 92. These calculations are performed using relativistic many-body perturbation theory within the framework of the Dirac-Coulomb-Breit Hamiltonian. We find good agreement between our calculated values and available experimental data. In addition we predict several new energy levels as well as radiative properties which will hopefully stimulate further experiments on these exotic systems. Francium is an extremely radioactive atom that has never been observed naturally.  It can only be created artificially by bombarding lighter atoms such as calcium with neutrons or alpha particles.  The most stable isotope, Fr, has a half life of around 1 second while all others have much shorter half lives.  This article presents theoretical results obtained for excitation energies, polarizations, multipole transition rates,   hyperfine structure constants, lifetimes, and oscillator strengths of ions along the franconia isoelectronic series up to Z=92. These calculations were carried out using relativistic many body perturbation theory within the framework...","label":1,"model":"bloomz","source":"arxiv","id":3337}
{"text":"We present an analysis of the relativistic Riemann problem for ideal fluids in two space dimensions, with emphasis on the role played by vortex sheets. We show that the solution to this problem can be constructed as a sequence of self-similar solutions which are determined uniquely up to translations along the x-axis (the direction of propagation). The first step is to construct a family of exact solutions describing the interaction between a planar shock wave and a vortex sheet. These solutions have been obtained previously using different methods but we provide here a new derivation based on the method of characteristics. In particular, we obtain explicit expressions for the density and pressure profiles across the shock front. Next, we consider the case where the initial data consists of a single vortex sheet separating regions of constant density and pressure. This situation corresponds physically to a fluid initially at rest being accelerated impulsively by a piston moving at speed c = 1. We prove that there exists exactly one such solution corresponding to each value of the total mass M . Finally, we study numerically the evolution of more general initial data consisting of several vortex sheets separated by shocks.","label":1,"model":"bloomz","source":"arxiv","id":3338}
{"text":"We study the production of magnetic fields and their subsequent evolution during the relativistic blast wave phase of gamma-ray burst (GRB) afterglow emission, using numerical simulations with high spatial resolution. We find that the magnetic field is amplified to equipartition strength within several hundred milliseconds at most for typical parameters. The amplification occurs mainly through turbulent dynamo action driven by the kinetic energy of the shocked fluid. After reaching its peak value, the magnetic field decays gradually due to adiabatic expansion losses as well as Ohmic dissipation. Our results suggest that the observed X-ray flares are likely produced by internal shocks between shells ejected from different regions inside the progenitor star. \n \n Keywords: Gamma-Ray Bursts, Magnetic Fields, Dynamo Action, Relativistic Blast Wave, Turbulence \n \n 1. Introduction \n \n In recent years there has been growing evidence suggesting that gamma-ray burst (GRBs) may be associated with massive stars (e.g., Woosley & Bloom 2006) . If this is true, then it would imply that some fraction of these stars explode into space while still surrounded by dense stellar winds or envelopes. These environments can significantly affect the dynamics of the explosion and the properties of the emitted radiation. For example, Chevalier et al. (2004) showed that if the density profile of the surrounding medium follows an r-2 power law, then the resulting light curve will exhibit a plateau followed by a steep decay phase. This behavior was later confirmed observationally (e.g., Panaitescu 2005; Kumar & Panaitescu 2008) , which led to the suggestion that many GRBs might originate from such progenitors (e.g., Zhang 2007). However, other authors have argued against this scenario on theoretical grounds (e.g., Ramirez-Ruiz et al. 2005 ) and observational ones (e.g., Lazzati et al. 2009 ). It should also be noted that even though the majority of GRBs seem to follow this general trend, there exist cases where no clear signature of interaction with a wind-like environment could","label":1,"model":"bloomz","source":"arxiv","id":3339}
{"text":"We study the distribution of low-lying states on random surfaces with boundaries, which are obtained by gluing together closed Riemann surfaces along their boundary components. We show that this problem is equivalent to counting the number of distinct ways one can tile a given surface using tiles of different shapes and sizes. This leads us to consider the statistical mechanics of an ensemble of interacting particles living on such surfaces. The partition function for these systems turns out to be related to the generating functions of certain classes of Feynman diagrams. In particular we find exact expressions for the free energy density as well as the entropy density at large volume (large genus) limit. These results provide new insights into the structure of the so-called \"string landscape\". Introduction:-In recent years there has been considerable interest in studying the properties of quantum gravity theories defined over spaces with non-trivial topology [1, 2] . One of the most interesting questions in this context concerns the existence of stable ground state configurations corresponding to classical geometries [3, 4] .\nThe simplest example of such a theory corresponds to the case where space-time is taken to have the topology of a torus T 2 = S 1 \u00d7S 1 , or more generally any orientable two-dimensional manifold M g of genus g > 0 [5] . It was shown recently [6] that in this case it is possible to define a consistent set of physical observables associated with the vacuum expectation values of local operators O(x), x \u2208 M g . Moreover, it follows from general arguments [7, 8] that all correlation functions of these operators factorize into products of two-point correlators < O i (x)O j (y) >= C ij \u03b4(x \u2212 y). Here C ij denotes some constant depending only on the positions of points x and y within the underlying geometry [9] . For instance, if we take M g to be a sphere then C ij will depend only on the geodesic distance between x and y [10] .","label":1,"model":"bloomz","source":"arxiv","id":3340}
{"text":"The article presents results of analysis and interpretation of remote sensing data on the territory of Armenia. The study was carried out using Landsat TM\/ETM+ imagery with spatial resolution 30 m (for 1988-1993) and 15 m (for 2003-2005) . It is shown that the use of high-resolution satellite images allows to detect small-scale deformations of the surface associated with earthquake activity. In particular, it has been found that after the earthquake of December 7, 1988 , there were significant changes in the relief of the Ararat region . These changes are manifested as an increase in the number of linear elements of the landscape - ridges, scarps, faults .\nIt should be noted that the most pronounced changes have occurred along the southern border of the Ararat caldera . This fact indicates that this area may be one of the zones of increased seismic activity . After analyzing the obtained results we can conclude that the application of remote sensing methods makes it possible to obtain new information about the state of the environment affected by natural disasters such as earthquakes .","label":1,"model":"bloomz","source":"arxiv","id":3341}
{"text":"We present Chandra observations of supernova (SN) 2004et, which is one of only two type IIp SNe ever observed in X-rays. The data were obtained on 2005 February 24-26 with the Advanced CCD Imaging Spectrometer (ACIS-S). We detect no significant emission above background at energies below 1 keV or above 8 keV; we therefore restrict our analysis to the range 1-8 keV. In this energy band, we find that the spectrum can be fit by an absorbed blackbody model with kT = 0.7 \u00b1 0.1 keV and N H = 2.5 +1.0 \u22120.8 \u00d7 10 22 cm \u22122 . These values are consistent with those found for other type IIp SNe. Using these parameters as well as the distance inferred from optical photometry, we calculate the luminosity of SN 2004et during its first 100 days after explosion. This value agrees very well with theoretical predictions based upon models of stellar evolution.","label":1,"model":"bloomz","source":"arxiv","id":3342}
{"text":"We present exact solutions to the field equations of the Einstein-Gauss-Bonet (EGB) gravity with negative cosmological constant in 5D space-time. We find that there are three classes of black hole solutions depending on whether the Gauss-Bonnet coupling constant is positive or negative. The first class contains two types of static spherically symmetric black holes which have no horizons but possess naked singularities at their centers. In addition we also obtain another type of solution describing an asymptotically anti-de Sitter wormhole whose throat connects two asymptotic regions. Finally, by using the method developed recently by one of us, we construct a new type of solution representing a time-dependent spacetime horn. This work was supported by NSFC under Grant No. 10875030. PACS numbers: 04.20.-q, 11.10.-z, 98.80.Cq . \nI. INTRODUCTORY REMARK\nThe discovery of gravitational waves has opened up a new window into our understanding of gravitation [1] , especially when it comes to testing general relativity [2] . However, despite its successes, general relativity still fails to explain some phenomena such as dark energy [3] and quantum gravity [4] . Therefore, many alternative theories of gravity were proposed over the years [5] .\nOne of these alternatives is the so-called Einstein-Gauss-Bonnet (EGB) gravity [6] - [8] . It can be viewed as a natural generalization of general relativity since it includes higher-order curvature corrections [9] . Moreover, this theory admits various interesting solutions including black holes [10]- [12] , wormholes [13] - [15] and even time dependent spacetimes [16] - [18] . Recently, EGB gravity attracted much attention due to its possible role in explaining the accelerated expansion of the universe [19] - [21] .","label":1,"model":"bloomz","source":"arxiv","id":3343}
{"text":"We study the adiabatic evolution of an open-boundary spin-1\/2 chain with nearest-neighbor interactions, which is driven by slowly varying external magnetic field and transverse fields. We show that this system undergoes a second-order phase transition at zero temperature when the longitudinal field changes sign. The ground state evolves smoothly through the critical point in the thermodynamic limit but exhibits singular behavior for finite systems. In particular, we find that the fidelity susceptibility diverges as $1\/L$ near the critical point where $L$ denotes the number of spins. This result implies that the adiabatic process fails to be efficient if one tries to drive the system across the critical point using slow driving rates. Finally, we discuss how our results can be generalized to other models exhibiting similar features. Introduction:-Adiabatic quantum computation (AQC) [1] has been proposed as a promising approach towards solving hard computational problems [2] . It relies on the fact that it may be possible to solve certain optimization problems efficiently by evolving the initial ground state of some simple Hamiltonian into the ground state of another complicated problem Hamiltonian via a series of intermediate Hamiltonians [3] .\nIn AQC, the time-evolution operator corresponding to each step of the algorithm is obtained by applying a sequence of local unitary transformations to the identity matrix [4] , i.e., U = exp(\u2212iHt\/h), where H is the instantaneous Hamiltonian describing the physical system under consideration and t is the total duration of the algorithm. If the rate of change of the parameters characterizing the instantaneous Hamiltonians is sufficiently small compared to their characteristic energy scales then the final state will be close to the ground state of the target Hamiltonian [5] . However, there are several issues associated with implementing such algorithms experimentally [6] - [8] . For example, even though the adiabatic theorem guarantees that the final state will be very close to the ground state provided the evolution occurs over many orders of magnitude slower than the inverse gap between the ground and first excited states [9] , it does not provide any information about the speed required to achieve a given accuracy [10] . Moreover, since","label":1,"model":"bloomz","source":"arxiv","id":3344}
{"text":"We present near-infrared (NIR) observations of the central region of the Carina Nebulae obtained with ISAAC at the VLT in Chile, and compare them to previous optical studies by Smith et al. (2000) . The NIR data reveal new details on the structure of the nebular shell surrounding the open cluster Trumpler 16. We find that the brightest part of the shell is located between two dark lanes which are probably caused by dust extinction. In addition we detect several small knots embedded within the shell. These knots may be remnants of massive stars or protostars formed during an earlier phase of star formation activity in this region. Finally, we identify a number of Herbig-Haro objects associated with the open clusters Trumpler 14 and 15. Our results show that the Carina Nebulae is still actively forming stars today. This work was supported by the European Southern Observatory under programme ID 072.C-0488(B).","label":1,"model":"bloomz","source":"arxiv","id":3345}
{"text":"The book Nurturing Breakthroughs by James Watson and Peter Winkler is about how to create breakthroughs in science, technology, engineering or mathematics (STEM). The authors argue that the best way for scientists to make new discoveries is not through individual genius but rather by working together as teams on problems they are passionate about.  They also claim that it helps if you have an idea of what your problem looks like before you start solving it because this will help guide your research efforts. This article describes some of their ideas and provides examples of how these concepts can be applied to physics research. In his book Nurturing Breakdowns, James Watson argues that the most successful people who solve complex problems do so by working with others instead of trying to work alone. He says that when we work individually our brains tend to focus only on one aspect of the problem at hand which may lead us down dead ends while working collaboratively allows us to see all aspects of the problem simultaneously.","label":1,"model":"bloomz","source":"arxiv","id":3346}
{"text":"We present results on the role played by the rho meson in describing pion electroproduction data obtained with the CLAS detector at Jefferson Lab (JLab). The analysis is performed within an effective field theory approach, where we use chiral perturbation theory to describe the interaction between pions and nucleons up to next-to-leading order. We then introduce vector-meson degrees of freedom through the hidden gauge formalism. In particular, we consider contributions coming from one-loop diagrams involving rho mesons as well as tree-level processes mediated by rho mesons. Our theoretical framework allows us to study both neutral current reactions such as elastic ep scattering or charged current reactions like single-pion production off protons. Using this model, we are able to reproduce experimental data for all these observables simultaneously. Finally, we discuss how our findings can be used to extract information about the properties of the rho meson. This work was supported by the U.S. \n\n\nDepartment of Energy under Contract No. DE-SC0012704.\n\nPACS numbers: 12.38.Mh","label":1,"model":"bloomz","source":"arxiv","id":3347}
{"text":"We study the nonlinear evolution of large-scale magnetic fields (LSMFs) in two dimensions by using direct numerical simulations of incompressible magnetohydrodynamics with periodic boundary conditions. We find that LSMFs are generated spontaneously through inverse cascade processes, which is similar to three-dimensional MHD turbulence. However, we also observe some differences between 2D and 3D cases. In particular, the energy spectrum of LSMF has an exponential tail at large wave numbers instead of power-law behavior as observed for 3D case. The origin of this difference can be understood by considering the effect of magnetic helicity conservation on the dynamics of LSMF. Furthermore, we show that the effective drift velocity of LSMF depends strongly on its initial configuration. Finally, we discuss possible applications of our results to solar physics. PACS number(s): 47.27.Gs, 47.27.Gk, 52.35.Jm","label":1,"model":"bloomz","source":"arxiv","id":3348}
{"text":"We present results on the gravitational wave emission during the final stages of black hole binary coalescence, when the holes are surrounded by an accretion disk and their orbital evolution is driven by radiation reaction. We use numerical relativity simulations to study how the mass loss rate due to gravitational waves depends on the spin magnitudes and orientations of the two black holes. Our main result is that for equal-mass binaries with aligned spins (both parallel or anti-parallel to the orbital angular momentum), the total radiated energy increases monotonically as the system loses orbital energy through gravitational waves. For unequal masses and\/or misaligned spins, however, we find that there can be significant fluctuations in the emitted power over time scales comparable to the orbital period.  These fluctuations occur because the amount of gravitational-wave luminosity generated at each instant varies strongly depending on whether the black holes' orbits are circularized or not. The resulting variability may have important consequences for observations of merging galaxies using gravitational-wave detectors such as LIGO\/VIRGO.","label":1,"model":"bloomz","source":"arxiv","id":3349}
{"text":"The human genome contains thousands of enhancers, which are non-coding DNA sequences that can activate transcription at distant sites. Enhancer activity is often cell-type specific and regulated by the binding of sequence-specific transcription factors (TFs). However, how TFs cooperate to regulate enhancer function remains poorly understood. Here we show that cooperative interactions between two distinct classes of TFs play an important role in regulating enhancer activity during lytic infection with herpes simplex virus 1 (HSV-1), a large double-stranded DNA virus. We find that HSV-1 encodes three proteins - VP16, ICP4 and ICP27 - each containing multiple domains capable of interacting directly or indirectly with other cellular proteins. Using biophysical methods, we demonstrate that these three viral proteins interact cooperatively on a single promoter element to drive expression of genes required for efficient viral replication. Our results suggest that cooperative protein-protein interactions may be a general mechanism used by viruses to control host gene expression.","label":1,"model":"bloomz","source":"arxiv","id":3350}
{"text":"We present the first results on transition region (TR) lines obtained by the Extreme Ultraviolet Imaging Spectrometer (EIS), aboard the Solar-B satellite, in active regions and quiet Sun areas. The EIS data are compared to those taken simultaneously by the EUV Imaging Telescope (EIT). We find that TR line intensities show significant enhancements over their photospheric values at temperatures between 10 5 K and 2 \u00c2 10 6 K. In addition, we observe strong spatial variations in the intensity ratios among different TR lines. These observations suggest that there is an abundance variation across the solar surface for some elements such as Fe xii, Fe xiii, Si vii, and Mg ix. This work was supported by NASA grants NNG05GE73G and NNX07AQ85G. Hinode is a Japanese mission developed and launched by ISAS\/JAXA, collaborating with NAOJ as domestic partner, NASA and STFC (UK) as international partners. Scientific operation of the Hinode mission is conducted by the Hinode science team organized at ISAS\/JAXA.","label":1,"model":"bloomz","source":"arxiv","id":3351}
{"text":"We consider the problem of finding an optimal first-order reduced basis space in the context of model reduction using Proper Orthogonal Decomposition (POD). We show that this can be achieved by solving a sequence of generalized eigenvalue problems associated with certain polynomials, which are derived from the original POD Galerkin formulation and its adjoint. The proposed approach is illustrated on two numerical examples. In particular we demonstrate how it allows to find a better approximation than standard greedy algorithms. This work was supported by EPSRC grant EP\/K034494\/1. Model reduction techniques have been widely used over recent years as efficient tools for reducing computational complexity when simulating complex physical systems governed by partial differential equations (PDEs) [1] . One popular class of methods relies on Proper Orthogonal Decompostion (POD), where one seeks to approximate the solution u(x, t) \u2208 R N of a PDE system by projecting onto a low-dimensional subspace spanned by a set of orthonormal functions {\u03c6i(x)}Ni=1 [2] , i.e.,","label":1,"model":"bloomz","source":"arxiv","id":3352}
{"text":"The formation and evolution of young massive star clusters (YMSCs) is an important topic for understanding galaxy formation, especially at high redshifts.  In this review we summarize recent progress on YMSC studies using data obtained with the Hubble Space Telescope (HST). We focus primarily on results based on HST observations of distant galaxies that are undergoing intense episodes of star-formation activity. The main goal of these investigations has been to study how individual stars form within dense stellar environments such as those found in YMSCs. This information can then be used to understand how entire galaxies evolve over time. A number of different techniques have been developed to investigate the properties of YMSCs including photometry, spectroscopy, and direct imaging. These methods provide complementary views into the physical processes involved in cluster formation and evolution. For example, photometric surveys allow us to determine the ages and masses of large samples of YMSCs while spectroscopic studies yield detailed measurements of chemical abundances and kinematics. Direct imaging provides spatially resolved information about the internal structure of YMSCs which allows us to probe their dynamical state.","label":1,"model":"bloomz","source":"arxiv","id":3353}
{"text":"Issai Schur (1886-1951) was one of the most influential mathematicians in twentieth century mathematics, and his work has had an enormous impact on many areas including number theory, functional analysis, operator theory, harmonic analysis, group representation theory, ergodic theory, probability theory, mathematical physics, computer science, statistics, numerical analysis, combinatorics, algebraic geometry, and dynamical systems.  In this talk we will survey some of these contributions with emphasis on their historical context.   We will also discuss how Schur's ideas have been developed by other mathematicians over the past seventy years or so. The lecture is intended for graduate students who are interested in learning about the history of modern mathematics as well as its current state-of-the-art. It should be accessible to undergraduates with a background in real variable calculus and linear algebra. This course fulfills the requirements of both MATH 3010 and MATH 3310 .","label":1,"model":"bloomz","source":"arxiv","id":3354}
{"text":"We present results on the statistical properties of earthquake sequences generated by the 2D Burridge-Knopff (BK) model with random initial conditions and periodic boundary conditions, using numerical simulations. We find that the BK model produces power-law distributions for both the inter-event time distribution and magnitude-frequency relation in agreement with observations. The exponent values are found to be dependent upon the system size N . In particular we show that the exponents decrease as 1\/N , which is consistent with previous studies. Finally, we discuss possible reasons behind this dependence. Keywords: Earthquake statistics; Power laws; Random initial conditions; Periodic boundary conditions; Statistical mechanics; Numerical simulation; Burridge-Knopf model. 1 Introduction It has been known since Gutenberg's work [1] that there exists an empirical relationship between the frequency f of occurrence of earthquakes and their magnitudes M : log10(f ) = \u03b1 \u2212 \u03b2M .\n(\nThe constants \u03b1 and \u03b2 depend on the region under consideration [2] . This relationship can also be expressed in terms of the number n of events per unit area A within some range [Mmin, Mmax] of magnitudes as: dn\/dA \u221d 10 \u03b3\u2212\u03b4M min where dn\/da \u221d 10 \u03b3\u2212\u03b1M max [3] . For example, if one considers all earthquakes occurring over a period of time T in a given geographical area then it follows that: \nwhere Ntot denotes the total number of earthquakes during the observation period T . If one instead counts only those earthquakes whose magnitude lies in the interval [Mmin, Mmax]:","label":1,"model":"bloomz","source":"arxiv","id":3355}
{"text":"We present new observations made with the Cosmosoma experiment, which were designed to search for evidence of an excess in cosmic microwave background (CMB) temperature fluctuations above those predicted by standard cosmological models. The data are consistent with predictions based on current theoretical understanding but show some unexpected features that may be related to previously unidentified foreground sources or systematic effects associated with our analysis techniques. \n \n We have used these results to place limits on possible contributions from primordial gravitational waves and other exotic phenomena such as topological defects. These limits are comparable to previous measurements obtained using different experimental approaches. In addition we report the detection of a significant signal at frequencies below 10GHz, which is not expected within conventional cosmological models. This could represent either a new source of foreground contamination or a novel physical effect. Further investigation will require additional experiments to confirm this result and determine its origin. If confirmed it would provide important constraints on theories attempting to explain the observed anisotropy in the CMB spectrum.","label":1,"model":"bloomz","source":"arxiv","id":3356}
{"text":"The present work is concerned with the development and analysis of a mathematical model describing the dynamics of a copolymerization reaction occurring inside droplets dispersed into water, which are stabilized by surfactants.  The system consists of three partial differential equations (PDEs) coupled through nonlinear boundary conditions at the interface between two phases. In addition to the usual convection-diffusion-reaction terms appearing in such models, we also include diffusion due to Marangoni stresses induced by surface tension gradients across the interface. We prove existence of global weak solutions using Galerkin approximations combined with compactness arguments. Finally, we perform numerical simulations that illustrate our theoretical results. Keywords: Copolymerization; Emulsions; Surface tension gradients; Mathematical modelling. 1 Introduction Polymeric materials have been widely used as coatings on solid surfaces or as additives in many industrial processes including paints, cosmetics, pharmaceuticals, food processing etc., see e.g. [21, 22] . A common way to produce these materials involves polymerizing monomers within small droplets suspended in water, called emulsions. This process can be achieved either chemically or physically depending upon whether the droplets contain chemical species necessary for initiating the polymerization reactions or not. For example, if the droplets do not contain any chemicals then they must first be prepared separately before being added to the main mixture containing all other ingredients. Once this has been done, the droplets will begin to grow until their size becomes comparable to the wavelength of light passing through them. At this point, the droplets become optically opaque and the growth continues unabated leading eventually to the formation of large particles known as micelles. These particles may then be separated out from the rest of the solution by centrifugation or filtration techniques. Alternatively, one could use a different type of emulsifier so that the droplets remain transparent throughout the entire process. However, it should be noted that this approach requires additional control over the amount of energy supplied during the process since too much heat generated by the exothermic nature of","label":1,"model":"bloomz","source":"arxiv","id":3357}
{"text":"We study the slave-boson mean-field theory for the extended Falicov--Kimball model in two dimensions, which is known to exhibit an interesting phase diagram with various ordered phases and quantum critical points. We show that the ground state energy per site can be expressed as a function of the order parameters by using the saddle-point method. The resulting self-consistent equations are solved numerically on finite clusters up to 16 sites. It turns out that there exist three different solutions depending on the electron filling fraction n = N\/Nc (N: number of electrons; Nc: number of lattice sites). One solution corresponds to the metallic phase where both charge density wave and spin density wave orders vanish. Another one describes the insulating phase characterized by nonvanishing CDW or SDW orders. Finally we find another solution corresponding to the coexistence region between these two phases. In this case, the system exhibits either commensurate or incommensurate charge-density waves accompanied by spiral magnetic ordering.","label":1,"model":"bloomz","source":"arxiv","id":3358}
{"text":"We present an open-source code, called XSCORT (X-ray Spectral Code for Reprocessed Torus), which computes the reprocessing of radiation by optically-thick matter in the form of a torus and its associated wind. The code is written as a set of IDL routines that can be easily modified to study different geometries or physical conditions. We describe how we implemented our model using Monte Carlo techniques and discuss some tests performed with simulated data. Finally, we show examples of applications of this new tool to two well-studied objects, NGC 4151 and Mrk 509. \nThe main goal of XSCORT is to provide a flexible framework where one can explore the effects of various parameters such as: geometry, density distribution, chemical composition, ionization state, etc., on the observed spectral properties of active galactic nuclei (AGN). This will allow us to better understand their nature and evolution. \n \n In particular, we are interested in studying the effect of the presence of an outflowing component on the shape of the reflection hump produced by the innermost regions of the accretion disc around supermassive black holes. These winds may play an important role in shaping the broad-band continuum emission of these sources through absorption and\/or scattering processes. They also affect the amount of material available to produce the reflected emission at larger distances from the central source.","label":1,"model":"bloomz","source":"arxiv","id":3359}
{"text":"We present the results on scalar propagator in AdS\/CFT correspondence obtained by using configuration space methods, which are based on Feynman path integral approach to quantum field theory. We show that this method is equivalent with the standard one used in momentum space but it has some advantages over it:  It allows us to obtain exact expressions for the Green functions at all orders in perturbation theory.  The calculations can be done without any reference to the boundary conditions imposed on fields or their derivatives. This makes our approach especially useful when dealing with problems where such conditions cannot be specified unambiguously. In particular we consider two-point correlation function of stress-energy tensor in N=4 SYM theory and calculate its imaginary part up to order $\\mathcal{O}(g^{10})$. We also discuss how to use these techniques to study time-ordered correlators in strongly coupled theories. Finally, we apply them to calculation of retarded correlator of vector current in QED.","label":1,"model":"bloomz","source":"arxiv","id":3360}
{"text":"We present the results of our study on super star clusters (SSCs) in which we have found that SSCs can be divided into two categories, namely, those having a single mode and those having a double-mode solution for their density profiles. We show how these solutions are related to each other by using approximate analytic methods. The main aim is to understand why some SSCs appear as point sources while others do not. In this work, we also discuss the possibility of formation of such objects through mergers between smaller clusters or stars. Super massive star clusters (SMCs), known as young globular clusters (YGCs), open clusters (OCs), compact elliptical galaxies (CEGs), etc., are observed in many galactic systems ranging from dwarf irregular galaxies to giant ellipticals. These objects are believed to form during violent events like galaxy mergers, tidal interactions, and\/or gas-rich major mergers. However, it has been shown recently that there exists another class of SMCs whose luminosity function shows a peak at intermediate masses (10^6-10^7 Msun). This type of cluster is referred to as \"Intermediate Massive Clusters\"(IMCs; Portegies Zwart et al. (2010)). It appears that IMCs may represent a transition phase between open clusters and YGCs.","label":1,"model":"bloomz","source":"arxiv","id":3361}
{"text":"We present an algorithm for searching in databases that are stored as qubits, which is the basic unit of information in quantum computers. The search problem can be formulated by using Grover's algorithm and its variants to find one or more solutions among many possibilities. We show how this approach can be used to solve problems such as finding a particular molecule within a large chemical compound library. Our results demonstrate that it may be possible to use quantum algorithms to accelerate searches on future quantum computer hardware. Quantum computing has been proposed as a new paradigm for solving computational problems with applications ranging from chemistry to optimization theory [1-3]. In contrast to classical computers, where data is represented by bits (0s or 1s), quantum computers store information in qubits, which can take any superposition of 0s and 1s [4] . This feature allows quantum computers to perform certain computations exponentially faster than their classical counterparts [5] .\nIn order to make practical use of these advantages, however, we need efficient ways to implement quantum algorithms [6] , including those based on Grover's algorithm [7-9]. Here, we propose a method for identifying unambiguously a single solution out of multiple possibilities [10] . As an example application, our technique could be used to identify a specific molecule within a larger chemical compound library [11] .","label":1,"model":"bloomz","source":"arxiv","id":3362}
{"text":"We present the results obtained in digitizing and measuring astrometrically about 1000 photographic plates taken by the Carte du Ciel project at the Observatorio Astron\u00f3mico Nacional (OAN), San Pedro M\u00e1rtir Observatory (SPMO) between 1950 and 1960. The OAN\/SPMO is located on Mount Wilson, California, USA. We used a high resolution digital camera to scan these plates and then measured their positions relative to reference stars observed simultaneously on each plate. This allowed us to obtain accurate absolute proper motions for more than 100 000 stars down to V = 16 mag over an area of about 1 square degree centered around RA = 10 h 30 m , Dec = +60 deg . These data are available online through our web site http:\/\/obswww.unige.ch\/~fuchs\/CDCCatastro\/Movies\/movie1.html .\nThe accuracy achieved in this work is comparable or better than that reported previously for similar projects based on scanning film material. \n\n\nThis research was supported by NSF grant AST-0307481.","label":1,"model":"bloomz","source":"arxiv","id":3363}
{"text":"We report on experimental measurements of spatial correlation functions for the dynamics of supercooled liquids at different temperatures, using confocal microscopy and single particle tracking techniques to probe the motion of colloidal particles suspended in glycerol. We find that these correlation functions can be well described by an exponential decay with a characteristic length scale which increases as we lower the temperature towards the glass transition point. This increase is consistent with theoretical predictions based on mode-coupling theory (MCT), but our results show deviations from MCT near the glass transition temperature Tg. These deviations are likely due to dynamic heterogeneities present close to Tg. The data presented here provide new insights into the nature of spatio-temporal fluctuations in glassy systems. Glass-forming liquids exhibit slow relaxation processes over many decades in time scales [1] . In particular, they often display non-exponential relaxations [2] , aging [3] , and other phenomena associated with glassy behavior [4] .\nTheories such as Mode-Coupling Theory (MCT) [5] have been developed to describe this complex phenomenology [6] . However, despite its successes [7, 8] , there remain open questions about how MCT describes real physical systems [9] . One important issue concerns the role played by spatial correlations between local regions where particles move more or less rapidly than average [10] . Such correlations may arise because of cooperative rearrangements [11] and\/or dynamical heterogeneity [12] . It has recently been shown theoretically [13] that spatial correlations play an essential role in determining the shape of the intermediate scattering function Fs(q,t). Here q denotes the wavevector corresponding to the probed lengthscale, while t represents the lag-time used to calculate Fs(q, t).\nIn order to test whether theories like MCT capture all relevant physics, it is necessary to measure experimentally the spatial correlations predicted by those theories. Previous experiments [14, 15] have focused primarily on measuring temporal correlations [16] . Recently, however, several groups [17] - [20] have begun to study spatial correlations directly [21] .","label":1,"model":"bloomz","source":"arxiv","id":3364}
{"text":"We present deep optical photometry in B, V , R c I c bands for the dwarf irregular galaxy IC 1613 obtained with the Wide Field Imager (WFI) at the MPG\/ESO 2.2 m telescope on La Silla Observatory. The data were reduced using standard IRAF routines. We derived total magnitudes within an aperture radius of 5 arcsec by applying aperture corrections to the PSF-fitted magnitudes. Our results are compared with previous studies based on shallower observations. In addition we derive new estimates for the distance modulus DM = 27.9 \u00b1 0.1 mag and foreground extinction A V = 0.10 \u00b1 0.02 mag towards this galaxy. Using these values together with our photometric measurements we determined absolute magnitudes M B = \u221215.6 \u00b1 0.3 mag, M V = \u221214.7 \u00b1 0.4 mag, M Rc = \u221212.8 \u00b1 0.5 mag, M Ic = \u221211.0 \u00b1 0.6 mag and colour indices U\u2212B = 1.45\u00b10.25 mag, B\u2212V =0.70\u00b10.06 mag, V \u2212Rc=0.55\u00b10.05 mag, V \u2212Ic=1.00\u00b10.07 mag. These parameters allow us to estimate the mean metallicity Z = 0.008 \u00b1 0.001 dex and age t = 3 Gyrs for the stellar population of IC 1613.","label":1,"model":"bloomz","source":"arxiv","id":3365}
{"text":"We present multi-instrument observations of solar coronal plasma flows, which are obtained by combining data from the Extreme Ultraviolet Imager (EUVI) onboard STEREO-A and -B with those from the Helioseismic and Magnetic Imager (HMI), Atmospheric Imaging Assembly (AIA), and Interface Region Imaging Spectrograph (IRIS). The EUV images show that there is an apparent counterclockwise rotation of the bright loop-like structures at the limb between 2011 February 24 and March 1. We find that this rotation can be explained as a result of the differential motion between the two spacecrafts along their orbits around the Sun. By applying a cross-correlation technique to the EUVI 171 \u00c5 intensity profiles observed simultaneously by both satellites, we obtain the velocity field across the solar disk for each time step during the period under study. In addition, we use HMI magnetograms to calculate the magnetic flux density distribution over the solar surface. Our results reveal that the plasma flow patterns are closely related to the photospheric magnetic fields.","label":1,"model":"bloomz","source":"arxiv","id":3366}
{"text":"Nous proposons une approche variationnelle de la m{\\'e}thode du maximum d'entropie (MaxEnt) pour r{\\'e}soudre des probl\\`emes inverses; nous montrons que cette approche permet d'obtenir un algorithme num{\\'e}rique efficace et robuste, qui peut s'appliquer aux mod\\`eles non-lin{\\'e}aires avec incertitude sur les donn{\\'e}es d'entr{\\'e}e ou sur les param\\`etres inconnus. Nous appliquons notre m{\\'e}thodologie au cas d'un probl\\`eme inverse magn{\\'e}tique, o\u9709 l'on cherche {\\`a} estimer l'aimantation d'une source inconnue plong{\\'e}e dans un champ magn{\\'e}tique connu. Les exp{\\'e}riences num{\\'e}riques montrent que l'algorithme MaxEnt est capable d'estimer correctement la distribution de probabilit{\\'e} recherch{\\'ee}, tout en restant stable face \\u00e0 l'incertitude sur les donn{\\'e","label":1,"model":"bloomz","source":"arxiv","id":3367}
{"text":"We present results on the stability properties of magnetized relativistic jets in GRMHD (general-relativistic magnetohydrodynamics) simulations, focusing on their dependence on jet parameters such as Lorentz factor $\\gamma$ , magnetic field strength $B_0$ , and density contrast between jet and external medium ${\\Delta} = \\rho_j\/\\rho_e$ . We find that for sufficiently high values of these quantities, the jets are stable against kink instabilities up to distances comparable with those observed in radio galaxies.  In addition we show that the presence of an ordered helical magnetic field component can significantly affect the growth rate of the instability by suppressing it or even reversing its direction. Finally, we discuss how our findings may be used to explain some observational features of AGN jets. Subject headings: General Relativity - Black Holes - Jet Physics - Magnetic Fields - Instabilities","label":1,"model":"bloomz","source":"arxiv","id":3368}
{"text":"We study the propagation of traveling waves (TWs) in excitable media with spatially distributed parameters, which are subject to both external forcing and internal fluctuations. We show that TWs can be generated spontaneously even if there is no deterministic source for them. The mechanism responsible for this phenomenon is related to the presence of an unstable stationary state between two stable ones. In particular, we demonstrate how spontaneous generation of TWs occurs due to stochastic resonance induced by additive white Gaussian noise. Finally, we present numerical results illustrating the effect of multiplicative colored noise on the dynamics of TWs. Propagation of traveling waves (TW) in excitable media has been studied extensively over recent years [1] . It was shown that TWs may appear as a result of various mechanisms such as: i) intrinsic instabilities [2] , ii) coupling-induced instabilities [3] or iii) forced oscillations [4] .\nIn many cases it is assumed that the medium under consideration is homogeneous so that all its properties do not depend explicitly on space coordinates. However, real physical systems usually have spatial variations of their characteristics [5] . For example, one-dimensional models describing cardiac tissue [6] include heterogeneity in the form of local changes in refractory periods [7, 8] . Another important factor influencing wave propagation is noise [9] . Noise plays different roles depending on whether it acts additively [10] or multiplicatively [11] . Moreover, noise may also affect the shape of the propagating front [12] .","label":1,"model":"bloomz","source":"arxiv","id":3369}
{"text":"We study the phase structure of the following 1+1 dimensional quantum field theory with two coupling constants, $g,h$,\n$$ \\mathcal{L} = \\frac{1}{2} (\\partial \\phi_1)^2 - \\frac{1}{\\lambda_1^2} \\phi_1^2 + g \\phi_1^4 + h \\phi_1^6 $$ \nwhere $\\lambda_1$ is an energy scale parameter and $\\phi_i$ are real scalar fields.  We find that there exists a novel phase where both the massless and massive excitations exist simultaneously for some range of values of $g$ and $h$. This new phase can be regarded as a generalization of the Higgs phase to higher order interactions. \nThe existence of this novel phase has been confirmed by numerical simulations using the lattice regularization method. The results obtained here may have important implications on our understanding of strongly interacting systems such as QCD at finite temperature or density. In particular, we expect that the present work will shed light on the nature of chiral symmetry breaking in QCD.","label":1,"model":"bloomz","source":"arxiv","id":3370}
{"text":"The adiabatic motion of relativistic charged particles is studied in the framework of the Hamilton-Jacobi theory and its generalization to arbitrary curved space-times, which are described by general relativity (GR) or modified gravity theories such as scalar-tensor theories.  The main goal of this work is to derive an explicit expression for the energy-momentum tensor associated with the adiabatic motion of these particles on backgrounds that can be either static or time-dependent. This problem has been addressed previously only within the context of GR, where it was shown that the energymomentum tensor satisfies the covariant conservation equation if one assumes that the particle's four-velocity is parallel transported along geodesics. Here we show how this result follows naturally from our formalism without any additional assumptions about the geometry of spacetime. We also discuss some applications of our results to cosmology and astrophysics.","label":1,"model":"bloomz","source":"arxiv","id":3371}
{"text":"We show that the observed value of the cosmological constant is systematically affected by the presence of matter around us, and we calculate this effect for different models of dark energy.  We find that the correction can be significant (up to 30%) if one assumes that the universe has an average density equal to the critical density today. The correction becomes smaller when the universe is less dense than the critical density or more negative pressure is assumed. This systematic error may explain why the current measurements are not consistent with each other within their errors. In addition, it could also affect future experiments aimed at measuring the time evolution of the cosmological constant. Introduction -The recent discovery of accelerated expansion of our Universe [1] , which implies existence of some form of \"dark energy\" [2] , leads naturally to the question about its nature [3] . One possibility is that the dark energy is simply vacuum energy [4] . Another option is that the dark energy consists of new particles such as axions [5] .\nIn order to test these ideas observationally, precise measurement of the properties of dark energy is needed [6] . However, there are several difficulties associated with this task [7, 8] . For example, even though the present-day acceleration of the universe is very small compared to the Hubble parameter H 0 = 100h km\/s\/Mpc [9] , where h \u2248 0.7 \u00b1 0.1 [10] , the corresponding change in the scale factor over the age of the universe is large enough so that the effects on the distance-redshift relation cannot be neglected [11] . Moreover, since the dark energy affects both space and time [12] , it changes the rate of clocks [13] and thus introduces additional uncertainty into the determination of distances [14] . Finally, the fact that the dark energy evolves with time [15] makes the problem much harder [16] .\nOne way to overcome these problems is to measure directly the equation-of-state w(z), defined as [17] \nwhere p de and \u03c1 de are respectively the pressure and density of the dark energy. If the dark energy behaves like a perfect fluid then w(z) \u2261 \u22121 [18] . It was shown recently [19] that the most","label":1,"model":"bloomz","source":"arxiv","id":3372}
{"text":"We present new measurements of 3 He+ and 3 He++ column densities toward eight nearby stars, using data obtained with the Far Ultraviolet Spectroscopic Explorer (FUSE). The results are compared to previous observations made by Copernicus and IUE satellites as well as FUSE. We find that our values for N(3 He+)\/N(H+), which range between 0.0015-0.0125, agree within uncertainties with those measured previously at high latitudes but disagree significantly with lower latitude measurements. Our results suggest that there is an additional source of ionization near the Galactic plane not accounted for by cosmic rays or X-rays. This could be due to shocks driven into the interstellar medium by supernovae remnants and\/or winds associated with massive OB associations. \n \n Keywords: Helium abundance, Interstellar medium, Shocks, Supernova remnant, Winds, Cosmic ray","label":1,"model":"bloomz","source":"arxiv","id":3373}
{"text":"We present an efficient numerical scheme to detect the existence of periodic orbits in chaotically behaving dynamical systems, such as chaotic maps or chaotic flows. The proposed algorithm is based on the concept of shadowing trajectories which are close approximations of unstable periodic orbits embedded within the attractor. We show that our approach can be used to efficiently compute the topological entropy of chaotic maps with non-integer slopes. Finally we demonstrate how this new technique can be applied to study the dynamics of a model system describing the interaction between two coupled semiconductor lasers. Periodic orbits play an important role in understanding the behavior of many nonlinear dynamical systems. In particular they provide valuable information about the underlying structure of the attractors associated with these systems. However, it has been shown that finding all periodic orbits of a given periodicity may not always be possible due to their complicated nature [1] . This problem becomes even more challenging when dealing with chaotic systems where the number of periodic orbits increases exponentially with increasing period [2] .\nIn recent years there have been several attempts to develop techniques to find periodic orbits numerically [3, 4, 5, 6, 7, 8] , but most of them suffer from one or both of the following drawbacks: (i) They require very high computational resources. (ii) They do not guarantee convergence towards the desired orbit. Here we propose a novel numerical scheme to overcome these difficulties by using the concept of shadowing [9] . Shadowing refers to the property of some trajectories being close approximations of unstable orbits embedded inside the attractor. It was first introduced by Anosov [10] who showed that every trajectory starting sufficiently close to any unstable periodic orbit will remain close to it for at least a certain amount of time. Since then various authors [11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44","label":1,"model":"bloomz","source":"arxiv","id":3374}
{"text":"We present new HST\/ACS observations of four nearby Fanaroff-Riley type-I (FR I) radio galaxies, 3C 31, 3C 264, 4C+55.17 and PKS 0745-191 . We use these data to study their nuclear properties by means of aperture photometry and image decomposition techniques. The results show that all sources have an unresolved core component with typical luminosities between 1040-1050 erg s-1 Hz-1 at rest-frame 5100 \u00c5 , which is consistent with previous studies based on ground-based near-infrared spectroscopy. In addition we find evidence for extended emission around the nucleus in two out of four objects studied here. This extended emission has been previously detected only in one other FR I galaxy, namely M 87. Our analysis shows that this extended emission can be explained as scattered light from the central source. Finally, our results suggest that the observed polarization fraction increases towards shorter wavelengths, indicating that dust extinction plays an important role in determining the intrinsic polarization properties of these systems.","label":1,"model":"bloomz","source":"arxiv","id":3375}
{"text":"We present new near-infrared integral field spectroscopy data for the central regions of the nearby spiral galaxy NGC 4321, obtained with SINFONI at VLT\/UT4. The observations cover an area of 1.5 x 2 arcsec2 around the nucleus and include two pointings along the major axis of the galaxy to study possible non-circular motions. We detect several emission lines including H$\\alpha$ , Pa$\\beta$ , Pa$\\alpha$ , Br$\\alpha$ , [Fe II] $\\lambda1.644 \\mu m$ , He I $\\lambda1.083 \\mu m$ . Using these line ratios we derive electron densities between 100 - 1000 cm-3 and temperatures between 10000-20000 K. In addition, we find that the ionized gas is outflowing perpendicularly to the galactic disk by velocities up to 300 km\/s. These results are consistent with previous studies based on optical spectroscopic data.","label":1,"model":"bloomz","source":"arxiv","id":3376}
{"text":"We report on the observation of electronic spin transport across macroscopic distances (several millimeters) in suspended single-layer graphene devices at room temperature.  The experiments are performed using nonlocal spin-valve measurements with ferromagnetic contacts made by sputtering Co\/Ni multilayers onto exfoliated flakes of graphite, followed by annealing to form cobalt silicide contacts. We observe that the resistance-area product RA is strongly dependent upon the contact geometry; for example, we find that it decreases significantly when the length of one of the contacts increases beyond 1 micron. This behavior can be explained by considering the effect of disorder scattering near the interface between the metal and the carbon atoms. In addition, we show that the magnitude of the spin signal depends sensitively on the relative orientation of the magnetization directions of the two ferromagnets. These results demonstrate that the observed spin signals arise primarily due to spin injection into the graphene layer rather than being dominated by proximity effects or other mechanisms associated with the magnetic contacts themselves.","label":1,"model":"bloomz","source":"arxiv","id":3377}
{"text":"We present here the rigorous mathematical formulation and complete proof of our previous results on the equivalence between Ising spin glasses (ISGs) and Ising models (IMs). We show that, in the thermodynamic limit N \u2192 \u221e with fixed ratio J \/ T , ISG partition functions can be mapped to IM ones by means of a suitable transformation which preserves all relevant physical quantities such as free energy density f = \u2212k B T ln Z\/N . The main idea is to introduce a new set of variables {s} \u2261 {s1, ..., sN } representing the local magnetizations of each site i \u2208 {1, .., N }, and then to map the original problem onto one where spins are coupled only through their mutual interactions. This allows us to prove rigorously that the two systems have exactly the same statistical properties when the number of sites goes to infinity. \nThe manuscript is organized as follows. In Sec. II we define the model under investigation and state some basic definitions and notations used throughout this work. In Sec. III we provide the exact definition of the transformation introduced above and derive its explicit form for any finite system size N . Then, in Sec. IV, we discuss how it can be extended to infinite-size lattices. Finally, in Sec. V we give the full proof of the theorem stating the equivalence between ISGs and IMs.","label":1,"model":"bloomz","source":"arxiv","id":3378}
{"text":"We study the problem of finding generators for II_1 factors, that is, subgroups $H$ of ${\\mathbb Z}^+$ such that there exists an isomorphism between $(H,+)$ and $({\\mathbb R}\/Z,+)$.  We show how to find generators in time polynomial in the size of the input (the number of elements in the subgroup).  This improves on previous results by showing that this can be done without any assumption about the structure of the group or its presentation.   The algorithm uses only elementary number-theoretic techniques and does not require factorization into primes.   \n\nThe main idea behind our approach is to use the fact that every element $x$ of $(H,+), H \\subseteq {\\mathbb Z}^+$ has a unique representation as $x = \\sum_{i=0}^{r}a_i p_i$ with $a_i \\in {\\mathbb Z}$ and $p_i$ prime numbers.  Using this representation we are able to compute the order of all elements efficiently using fast integer multiplication algorithms.  In particular, we do not need to know the prime factorizations of the elements beforehand.\n\nOur result implies that it suffices to consider presentations of groups which contain at most one generator per prime power order.  As a consequence, we obtain improved bounds on the complexity of computing the rank of finite groups over fields of characteristic zero. \n\nKeywords: Group theory","label":1,"model":"bloomz","source":"arxiv","id":3379}
{"text":"We propose to test radiative neutrino mass generation in the context of supersymmetric grand unified theories (SUSY GUTs) by measuring the branching ratio for the decay B_s^+ -> K_L^+ + e^- + \\bar \\nu_e, which is sensitive to the product of the masses of the lightest neutralinos and charginos.  We show that this measurement can be performed with high precision using data collected during Run II of the Fermilab Tevatron Collider. The proposed analysis will provide an important cross check on the results obtained from searches for neutrinoless double beta decays. In addition, it may help resolve some of the current tensions between measurements of the atmospheric mixing angle $\\theta_{23}$ and the solar mixing angles $\\theta_{1\/2}^{\\ast}$. Finally, we discuss how our proposal could be extended to other rare flavor-changing processes such as $B_s \\to \\mu^+\\mu^-$ or $J\/\\psi \\to \\mu^+\\mu^{-\\ast}$.","label":1,"model":"bloomz","source":"arxiv","id":3380}
{"text":"We report the discovery and characterization of an unresolved binary system consisting of two late-type dwarf stars, SDSS J080531.8+481233.0AB (hereafter referred to as J0805+4812). The primary component is classified as a T6p dwarf star with a mass of ~70 MJup while its companion has been identified as a cool brown dwarf candidate with a temperature between 1000-2000 K. We have used high-resolution near-infrared spectroscopy obtained at Gemini Observatory in order to confirm that both components are gravitationally bound. Our analysis shows that this object is one of the most massive known binaries composed by two low-mass objects. This work was supported by NASA's Astrophysics Data Analysis Program under award NNX10AD20G issued through the Space Telescope Science Institute, which is operated by AURA for NASA under contract NAS 5-26555. In addition we acknowledge support from NSF grant AST-0908816. \n \n We present the discovery and characterization of a new unresolved binary system made up of two late type dwarfs, SDSSJ080531.8 + 481233.0 AB (hereafter J0805+4812; see Figure 1 ). The primary component is classified spectroscopically as a T6p star with a mass of about 70 M Jup , while its companion has been tentatively identified as a cool brown-dwarf candidate with temperatures ranging from 1000 - 2000K . Using high resolution infrared spectroscopy taken at Gemini Observatory , we show that these two objects are gravitationally bound . This makes it one of the most massive systems ever found containing two low-mass objects .","label":1,"model":"bloomz","source":"arxiv","id":3381}
{"text":"We present chemical abundances for a sample of 16 Planetary Nebulae (PNe) in the Small Magellanic cloud, obtained with high resolution spectroscopy at the VLT and Keck telescopes. The PNe are located along an age sequence ranging between 1 Myr to 13 Gyrs. We find that the abundance patterns of CNO elements show no significant differences among these objects. This is consistent with previous studies which found little or no evidence for enrichment by AGB stars during this period.  However we do detect some variations in the Ne\/O ratio as a function of time. These results suggest that the SMC experienced two episodes of star formation separated by about 10 billion years. In addition, our data indicate that there was only one episode of massive star formation within the last 2 billion years. Finally, we compare our results with those derived from H II regions and young open clusters in order to investigate possible systematic effects due to different evolutionary stages and\/or stellar populations.","label":1,"model":"bloomz","source":"arxiv","id":3382}
{"text":"We study energy density-flux correlations for a free scalar field in 1+1 dimensions, both at finite temperature T and in vacuum (T=0). We show that these correlation functions are nontrivial even when evaluated on the light cone x+x=t+t0=2ct. In particular we find that they exhibit power law behavior with exponents which depend continuously on c. This is in contrast to what happens in ordinary quantum mechanics where such correlators vanish identically outside the light cone. The results presented here can be obtained by using standard techniques developed within the framework of thermofield dynamics. They provide further evidence that this formalism provides a useful description of thermal states also beyond equilibrium situations. Energy density-flux correlations play an important role in various physical phenomena ranging from hydrodynamics to particle production processes in heavy ion collisions. However their calculation has been hampered so far by the fact that it requires knowledge about off-diagonal elements of the two-point function of the corresponding operator. Here we present explicit expressions for these quantities for a free massless scalar field theory in one spatial dimension.","label":1,"model":"bloomz","source":"arxiv","id":3383}
{"text":"We present an ab initio study on the electronic structure and magnetic properties of Fe3O4, CoFe2O4, NiFe2O4, MnFe2O4, CrFe2O4, and VFe2O4 compounds with different spin configurations. We show that these materials are characterized by large orbital moment contributions to their total magnetization which can be explained within the framework of density functional theory (DFT) using generalized gradient approximation (GGA). The calculated values for the orbital-to-spin ratio agree well with experimental data obtained via neutron scattering experiments. In addition we find that all studied systems exhibit non-collinear magnetic structures due to competing exchange interactions between neighboring atoms. Finally, we discuss how our results could be used as input parameters into existing theories describing macroscopic phenomena such as spin-orbit torques or anomalous Hall effects. This is an open access article under the terms of the Creative Commons Attribution License, which permits use, distribution and reproduction in any medium, provided the original work is properly cited. \n \n Authors: \n\n\nKai Hwang , \nJens K\u00fchn , \nSusanne Schreiber , \nAlexander Sokolov , \nAndreas Wurmehl , \nMartin J. Gummow , \nMichael A. Nevidomskyy , \nHerbert R. Kr\u00f6ger , \nWolfgang Ebert , \nPeter Gr\u00fcnberg , \nUlrich Stoll , \nStefan Haun , \nThomas Bader , \nDaniel Loss , \nNorbert L\u00fctkenhaus , \nRalf Heimann , \nChristoph M. Fischer , \nChristian F\u00e4hnle , \nMats Nilsson , \nLars Lindstr\u00f6m , \nMatthias Reiss , \nJohannes Ploog , \nJan-Philipp von Bardeleben , \nDietmar Grueneisen , \nFrank Steglich , \nBoris Yakob , \nAleksandr Yufit , \nYurii I. Shubin , \nNikolay D. Semenov , \nVladimir Ogan","label":1,"model":"bloomz","source":"arxiv","id":3384}
{"text":"We present proper motions for stars with magnitudes between 8 and 16, obtained by combining data from two epochs of photographic plates taken at the Palomar Observatory (POSS-I) and one epoch of digital images taken with the Hubble Space Telescope (HST). The sample consists of about 1 million objects located within a region centered on the galactic center that is known as \"Plaut's window\".  We find that our results are consistent with previous measurements made using POSS-II plates combined with HST observations. However, we also find significant differences when compared to other recent studies based on similar datasets but different analysis techniques. These discrepancies may be due to systematic errors introduced during the reduction process or they could indicate real changes in the structure of the bulge over time. Our final catalogue will be available online through the CDS Vizier service. This work was supported by NASA grant NAG5-13523.","label":1,"model":"bloomz","source":"arxiv","id":3385}
{"text":"We present the results of N-body simulations aimed at studying tidal disruption and accretion in interacting galaxy pairs, with particular emphasis on the formation of tidally stripped dwarfs (TDGs). We find that TDG formation is strongly dependent upon the orbital parameters of the interaction; specifically, we show that TDGs form only when the encounter has an impact parameter less than about twice the sum of their effective radii.  In addition to this dependence on orbital geometry, our models suggest that TDGs are more likely to be formed if the progenitor galaxies have high gas fractions and\/or low central surface brightnesses. Finally, we argue that TDGs may provide useful probes for testing theories of gravity on galactic scales. The discovery of numerous examples of \"tidal dwarf galaxies\" (TDGs) over the past decade or so has led many authors to propose these objects as possible sites of star formation during interactions between massive spiral galaxies. However, despite considerable observational effort, there remains no consensus regarding either the frequency of TDG formation or even whether such systems actually exist outside the confines of numerical simulations.","label":1,"model":"bloomz","source":"arxiv","id":3386}
{"text":"The cosmological constant is one of the most important parameters in modern physics, and its value has been determined by observations to be extremely small but nonzero.  In this article we will discuss how it can be explained as an effect of quantum gravity at very high energies. We will also show that if the universe underwent inflationary expansion after the Big Bang then there should exist primordial gravitational waves which could have observable effects on the cosmic microwave background radiation (CMBR). Finally, we will argue that these effects may provide us with new ways for testing the predictions of general relativity against those of alternative theories such as string theory or loop quantum gravity. The cosmological constant is one  of the most important parameters of modern physics. Its value was determined by observations to be incredibly small but non-zero. It plays a crucial role in our understanding of the evolution of the Universe since it determines whether the current accelerated expansion of space-time will continue forever or eventually slow down and stop. This question remains open despite many years of research into the nature of dark energy.","label":1,"model":"bloomz","source":"arxiv","id":3387}
{"text":"We consider an inflationary universe with a scalar field that is slowly rolling down its potential hill, and we show how this model can be used to explain the observed temperature fluctuations in the cosmic microwave background radiation (CMBR). We find that the spectrum of density perturbations produced by such a scenario has a characteristic shape which depends on the slope of the potential at the minimum. In particular, if the potential is flat enough then there are two peaks in the power spectrum corresponding to scales of about 1 Mpc and 100 kpc respectively. The first peak corresponds to the scale of galaxy clusters while the second one gives rise to galaxies themselves. \n \n This kind of model predicts that the number of e-foldings between the time when the observable universe was 10^-32 cm and today should be around 50-60. If so, it would imply that our present horizon size is only slightly larger than the Hubble radius during inflation.","label":1,"model":"bloomz","source":"arxiv","id":3388}
{"text":"We present an improved method for analyzing the primordial power spectrum in terms of its underlying physical parameters, using the technique of \"flow equations\" to evolve the initial conditions through the entirety of cosmic time. We show that this approach can be used to place limits on the values of these parameters by comparing theoretical predictions with observations of large-scale structure and CMB anisotropies. In particular we find that the current data is consistent with a flat universe dominated by dark energy (w = -1), but inconsistent with models where w > -0.8 or w < -2\/3 at 95% confidence level. This result agrees well with previous analyses based on other techniques. \n \n The results presented here are derived from the WMAP 5-year temperature map [1] , combined with measurements of galaxy clustering [2] . They are also compatible with recent results obtained independently by the Planck satellite [3] .\n \n \n Our analysis shows that it will soon become possible to use the observed shape of the primordial power spectrum as a powerful probe into the physics of early-universe cosmology.","label":1,"model":"bloomz","source":"arxiv","id":3389}
{"text":"The aim of this study was to investigate the effect of novel antimicrobial cyclic peptides, named microbe-derived cationic ring structures (MCRs), on cell proliferation and apoptosis induction in A549 non-small-cell lung carcinoma cells. The results showed that treatment with MCR1 or MCR3 significantly inhibited cell growth by inducing G0\/G1-phase arrest and apoptosis through activation of caspase-3\/7\/9 signaling pathways. In addition, we found that both MCR1 and MCR3 suppressed expression levels of cyclins D1 and E as well as CDK4\/6 proteins but increased p21WAF1\/cip1 level. Furthermore, our data indicated that MCR1 and MRC3 blocked the binding between insulin-like growth factor 1 receptor (IGF-1R) and retinoblastoma tumor suppressor protein (RB). These findings suggest that MCR1 and 3 may be potential therapeutic agents for treating lung cancers.","label":1,"model":"bloomz","source":"arxiv","id":3390}
{"text":"We present the first results on the chemical composition and kinematics of stars in the central region (r < 1 kpc) of the nearby giant elliptical galaxy NGC 3379, based on high-resolution spectroscopy obtained with VLT\/FLAMES\/GIRAFFE. We find that the stellar population is dominated by old metal-poor stars (age > 10 Gyr), but also contains an intermediate-aged component (2-10 Gyr). The latter has solar metallicity and shows evidence for rotation around the major axis of the galaxy. In addition to these two components we detect a third one which consists of young metal-rich stars (age < 2 Gyr) at large galactocentric distances (> 5 kpc). These are likely associated with tidal debris stripped off during interactions between NGC 3379 and its neighbors. Our analysis suggests that this galaxy hosts three distinct populations of stars: i) an inner halo made up of old metal-poor stars; ii) an outer halo containing both old and intermediate-aged metal-poor stars; iii) a thick disk consisting of young metal-rich stars.","label":1,"model":"bloomz","source":"arxiv","id":3391}
{"text":"We present new HST photometric data on halo stars in the nearby elliptical galaxy NGC 3377, obtained with the Wide Field Planetary Camera 2 (WFPC2). The observations were made as part of program GO-8491 and consist of two exposures taken through the F606W filter at different roll angles to allow for proper sky subtraction. We have used these images to measure magnitudes for more than 1000 candidate red giant branch (RGB) stars within an area of 1 arcmin radius centered around the galaxy's center. These measurements are compared to those derived by Kundu & Whitmore (1998) , who observed this same field using ground-based telescopes. Our results show good agreement between our photometry and that presented previously; however we find evidence for systematic differences which may be due to crowding effects or calibration uncertainties. \n \n Keywords: Red Giant Branch, Galaxy, WFPC2","label":1,"model":"bloomz","source":"arxiv","id":3392}
{"text":"We propose that the vacuum state is not empty but contains fluctuations in spacetime, which we call holographic noise (HN). We show how this HN can be used to explain several phenomena such as spontaneous emission, blackbody radiation, Casimir effect, Lamb shift, and Hawking radiation. In particular, we argue that the vacuum fluctuation leads to an uncertainty principle between energy and time. This implies that there are no particles with zero mass or spin. The existence of these particles would lead to violations of causality. Finally, we discuss some possible experimental tests for our proposal. Vacuum fluctuations play important roles in quantum field theory. They give rise to many interesting effects including spontaneous emission [1] , blackbody radiation [2] , Casimir effect [3] , Lamb shift [4] , and Hawking radiation [5] . However, it remains unclear what exactly constitutes the vacuum state [6] .\nIn this work, we propose that the vacuum state does not contain only the absence of matter fields but also fluctuations in spacetime [7, 8] . These fluctuations may be viewed as virtual gravitons [9] . We refer to them as holographic noise (H N ) because they arise due to the entanglement between different regions on the boundary of space-time [10] . As shown below, H N plays crucial role in understanding various physical processes involving vacuum states.\nThe main idea behind our approach is illustrated by Fig.  1(a) . Imagine two observers Alice and Bob who live at opposite ends of a closed universe. Each observer has access to half of the total degrees of freedom inside their own causal diamond [11] . For example, if Alice lives near the center of her universe she will have access to all information about events within her past light cone while Bob's knowledge is limited to his future light cone. Since both observers cannot see each other, they must communicate via signals traveling through the bulk of space-time [12] . If Alice sends a signal to Bob then he receives it after a certain amount of time t AB = d\/c where c is the speed of light and d is the distance between Alice and Bob. On the other hand, if Bob sends","label":1,"model":"bloomz","source":"arxiv","id":3393}
{"text":"We study the nonlinear dynamics of semiconductor superlattices (SLs) under an external magnetic field, which is applied at an angle to SL layers. We show that this configuration leads to new types of bifurcation phenomena such as period doubling cascades and chaotic attractors. The results are obtained by solving numerically the coupled Schr\u00f6dinger-Poisson equations for the electron wave function and electrostatic potential using the split-step Fourier method. \n \n In particular we consider GaAs\/AlGaAs SLs grown along [001] direction on top of a semi-insulating substrate. This system has been studied experimentally in Refs. [1-3] where it was shown that the application of a magnetic field perpendicularly to the growth axis causes the formation of Landau levels. However, if the magnetic field is tilted away from the normal incidence then the degeneracy between different Landau levels is lifted leading to additional effects like Bloch oscillations [4] . \n \n For example, in Ref. [1] , the authors observed the appearance of a series of peaks in the photoluminescence spectrum when they increased the strength of the magnetic field. These peaks were attributed to the excitation of electrons across the energy gap into higher-lying states. It should be noted however that these experiments have not yet revealed any evidence of chaotic behavior or other complex dynamical features predicted theoretically [5-7] . \n \n Here we present numerical simulations showing how the tilting of the magnetic field affects the nonlinear dynamics of the system. Our calculations reveal several interesting features including periodic windows, period-doubling cascade and chaotic attractor.","label":1,"model":"bloomz","source":"arxiv","id":3394}
{"text":"We present an analysis of satellite galaxies, fossil groups (FGs), and galaxy clusters in the Millennium Run simulation. We find that FGs are rare objects with only 0.5% of all halos being classified as such at z=0. The number density of FGs is consistent with observations for systems with Mvir>1013M\u2299\/h. However, we also find that there exists a population of low-mass FGs which have not been observed yet but may be detectable by future surveys. In addition to their rarity, FGs show several other interesting properties compared to normal galaxy clusters: they tend to reside in more massive dark matter haloes than normal galaxy clusters; they contain fewer bright central galaxies; and they exhibit higher velocity dispersions. These results suggest that FGs can provide important constraints on models of galaxy formation and evolution. This work was supported by NASA grant NAG5-10842.","label":1,"model":"bloomz","source":"arxiv","id":3395}
{"text":"We present new observations of the solar atmosphere obtained with the Solar Ultraviolet Imager (SUVI) onboard the Coriolis satellite, which show dynamic fibrils in both H-alpha and CIV lines. The SUVI data are compared to simultaneous ground-based observations made at Big Bear Observatory using the 1-meter telescope equipped with an H-Alpha filter and a Fabry-Perot interferometer tuned to the CIV line. We find that the observed structures have similar properties as those seen previously by other authors but we also see some differences between them. In particular, our results suggest that the fibril structure is more complex than it was thought before. This complexity may be related to the fact that these structures are formed under different physical conditions. Our analysis shows that the observed features can be explained by assuming that they represent plasma flows along magnetic field lines. These flows could play important role in heating up the upper layers of the solar atmosphere.","label":1,"model":"bloomz","source":"arxiv","id":3396}
{"text":"We study the spin-1\/2 Heisenberg model on the square lattice with nearest-neighbor (NN) exchange interactions J1, J2, and J3 along x-, y-, and z-directions respectively. We show that this system can be mapped onto an effective spin-1\/2 XXZ chain by using the Holstein-Primakoff transformation for spins 1\/2. The ground state phase diagram is obtained numerically by exact diagonalization method. In addition to the N\u00e9el ordered phase at small values of |J2\/J1| < 0.5, we find two different types of quantum disordered phases depending upon the value of J3\/J2. For large values of J3\/J2 > 1.0, there exists a gapless singlet phase where all excitations are gapped out except one massless excitation corresponding to the Goldstone mode associated with the spontaneous breaking of translational symmetry. On the other hand, when J3\/J2 < 0.5, there appears another type of disordered phase which has a finite energy gap between the ground state and first excited states. This phase corresponds to a valence bond solid phase where each site forms singlets with its NN sites only.  Finally, we also discuss how these results may be relevant to understand the magnetic properties of some quasi-two-dimensional organic compounds such as EtMe3Sb[Pd(dmit)2]2 and \u03ba-(BEDT-TTF)2Cu[N(CN)2]Cl.","label":1,"model":"bloomz","source":"arxiv","id":3397}
{"text":"We present the results of an investigation into the spatial distribution and temporal evolution of magnetic field strength in active region plage using high-resolution spectropolarimetric data obtained with Hinode\/SOT-SP. We find that, on average, the magnetic field is stronger at higher heights above the solar surface (i.e., closer to the limb) than it is near disk center. The mean unsigned longitudinal magnetic flux density decreases by about 50% between 0.3\u2032\u2032 and 1\u2032\u2032 above the solar surface. This decrease occurs over a range of heliocentric angles where the photospheric plasma \u03b2 increases significantly. In addition, we find evidence for significant horizontal structuring of the magnetic fields within individual pixels. These findings are consistent with theoretical predictions based upon magnetohydrodynamic simulations of coronal heating driven by small-scale convective motions. Finally, we show how these observations can be used as input parameters for models of chromospheric heating via Alfv\u00e9n waves generated by resonant absorption.","label":1,"model":"bloomz","source":"arxiv","id":3398}
{"text":"The ability to identify individual nanoparticles is essential for understanding their properties, but this task can be challenging due to the high degree of structural similarity between different types of particles. In this review we discuss recent advances in nanoparticle characterization using scanning transmission electron microscopy (STEM)-based techniques that provide information on both chemical composition and structure at atomic resolution. We also highlight some of the challenges associated with these methods as well as possible solutions. Finally, we present an overview of current applications of STEM-based approaches for identifying nanoparticles and suggest future directions for research.  Keywords: Nanoparticles, Characterization, Transmission Electron Microscopy, Scanning Transmission Electron Microscope, Atomic Resolution, Structure, Chemical Composition. The ability to identify individual nanoparticles has become increasingly important over the past decade because it allows researchers to correlate specific physical or chemical characteristics with particle size, shape, surface chemistry, crystal phase, etc., which are all known to influence the performance of many materials. However, despite significant progress made during the last few years, there remains considerable uncertainty about how best to characterize nanoparticles by combining multiple experimental parameters into one single descriptor. This problem arises mainly because nanoparticles often have similar compositions and\/or structures, making them difficult to distinguish based solely on elemental analysis or conventional imaging techniques such as bright-field TEM or SEM.","label":1,"model":"bloomz","source":"arxiv","id":3399}
{"text":"We consider the problem of estimating the generation interval distribution in an epidemic outbreak, using only information on the number of cases at each time point. We show that this is possible under certain conditions by deriving a formula for the probability generating function (PGF) of the generation interval distribution as a solution to a system of differential equations with boundary conditions corresponding to the observed case counts. The PGF can be used to calculate any moment or other statistic of interest about the generation interval distribution. In particular we derive expressions for the mean and variance of the generation interval distribution which are valid when the total number of cases grows exponentially fast over time. Finally, we apply our results to two real-world datasets, one concerning measles outbreaks in England between 1963 and 1998, and another concerning SARS outbreaks in China during 2002-03. Our method provides estimates of key epidemiological quantities such as the basic reproduction number R0 and the serial interval distribution, without requiring detailed contact tracing data.","label":1,"model":"bloomz","source":"arxiv","id":3400}
{"text":"In this work, we propose an encounter based worm interaction model to evaluate the performance of different node characteristics in terms of their ability to detect and prevent worms spreading over mobile ad hoc networks (MANETs). We consider two types of nodes with distinct capabilities for detecting and preventing worms: normal nodes that are vulnerable to infection by worms but can detect them using signature detection techniques; and immune nodes which have no vulnerability to worm infections but can prevent worm propagation through quarantine mechanisms. The proposed model is used to study how these two types of nodes interact when they meet each other during network operation. In particular, our results show that: 1) Immune nodes play a significant role in reducing the number of infected nodes as well as the total number of encounters between susceptible and infectious nodes; 2) Immune nodes should be deployed at strategic locations within MANETs; 3) Immune nodes should not only focus on quarantining infectious nodes but also on isolating suspicious nodes; 4) Immune nodes should use both signature detection and quarantine mechanisms simultaneously to achieve better performance against worm propagation; 5) Immune nodes should adopt dynamic quarantine strategies instead of static ones since static quarantine may lead to unnecessary isolation of legitimate nodes.","label":1,"model":"bloomz","source":"arxiv","id":3401}
{"text":"The chemical compositions of the type II Cepheid variables are investigated by using high-resolution spectra obtained with Subaru\/HDS, Keck\/HIRES, VLT\/UVES, and McDonald\/DEIMOS spectrographs.  We find that all of our sample stars have solar or super-solar metallicities ([Fe\/H]>+0.2). In addition to Fe I lines, we also use Mg I triplet lines in order to determine their atmospheric parameters more accurately. For some of these stars, we detect significant overabundances for CNO elements as well as s-process elements; however, there is no clear correlation between the abundance patterns and pulsation periods. These results suggest that the observed abundance anomalies may be caused by mass transfer during binary evolution rather than internal mixing processes associated with stellar pulsations. This work was supported by JSPS Grant-in-Aid for Scientific Research on Priority Areas No. 16071203.","label":1,"model":"bloomz","source":"arxiv","id":3402}
{"text":"The BABAR experiment at SLAC has been studying CP violation in B mesons for more than ten years, and is now producing results on many topics related to this phenomenon.  This talk will give an overview of some recent results that have come out of BABAR's analysis program.   The first part will focus on measurements of CP asymmetries in decay modes with one or two neutral pions in the final state (B -> DK, D*K).   These decays are sensitive to new physics beyond the Standard Model through penguin diagrams involving charm quarks.   In particular, we will discuss how these decays can be used to measure the CKM angle gamma, which is not accessible by other methods such as tree-level decays like B -> J\/$\\psi$Ks.   We will also present preliminary results on searches for direct CP violation in these channels.   ... AMS-02 collaboration announces its measurement of cosmic ray positron fraction up to 1 TeV","label":1,"model":"bloomz","source":"arxiv","id":3403}
{"text":"We consider the problem of power allocation in discrete-input delay-limited fading channels with multiple users and time-varying channel gains. We propose an iterative algorithm to solve this problem by using convex optimization techniques, which is shown to converge within finite number of iterations under certain conditions. The proposed algorithm can be implemented efficiently through parallel processing at each iteration step. Numerical results show that our proposed scheme outperforms existing schemes significantly. \n \n Keywords: Power control; Convex optimization; Time-varying; Multiple access channels (MACs); Wireless communications; Iterative algorithms. 1 Introduction \n \n In wireless communication systems, it has been well recognized that the performance of multi-user transmission depends on how the available resources are allocated among different users [1] . For example, when there exist multiple users sharing a common radio resource such as bandwidth or transmit power, the optimal way to allocate these resources may depend on the specific system settings [2] , e.g., whether the users have equal priority [3] , what type of services they request [4] , etc.. Therefore, efficient resource allocation strategies should take into account all relevant factors so as to maximize overall network utility [5] .\n \nIn recent years, considerable research efforts have been devoted to studying various aspects of resource allocation problems [6] - [8] . Among them, power allocation plays an important role due to its direct impact on both spectral efficiency and energy consumption [9] . However, most previous works assume continuous input alphabets [10] - [12] , while practical digital modulation schemes usually employ discrete constellations [13] . As a result, the conventional approaches cannot be directly applied to discrete-input scenarios [14] . To address this issue, several studies [15] - [17] have investigated the power allocation problem over discrete-input channels recently. Nevertheless, their solutions either require high computational complexity [16] or suffer from slow convergence speed [17] .","label":1,"model":"bloomz","source":"arxiv","id":3404}
{"text":"The authors present the results of their study on the influence of temperature fluctuations in the Earth's core on its magnetic field. They show that these fluctuations can be described by an oscillating function with a period of about 1000 years, which is close to the observed periods of geomagnetic reversals and excursions. This result suggests that the origin of such phenomena may lie in the dynamics of the Earth's core rather than in external factors. \n \n In this work we consider the effect of temperature fluctuations in the Earth\u2019s core on its magnetic field using a simple model based on the concept of \u201cthermal micropulses\u201d (TMP). We assume that TMPs are generated at random times within some time interval $[t_0, t_1]$ and have different amplitudes $A_i$ and durations $\\Delta t_i$. These pulses cause changes in the heat flux through the CMB and lead to variations in the convective flow velocity inside the fluid core. As a consequence, they produce perturbations in the magnetic field of the Earth. To describe the evolution of the magnetic field during one cycle of TMP generation, we use the induction equation for the vector potential $\\mathbf{A}$ together with the continuity equation for the density of electrical current $\\mathbf{J}$. For simplicity, we neglect the effects of rotation and stratification of the fluid core as well as the presence of the liquid outer core. Our analysis shows that the solution of the problem has two types of singularities corresponding to the so-called \u201csecular\u201d modes. One type corresponds to the case when all TMPs occur simultaneously; another occurs if only one pulse appears during each cycle. It turns out that both solutions correspond to periodic functions with characteristic periods $T_s = 2\\pi\/\\omega_s \\sim 1000$ yr. Thus, our results suggest that the origin of geomagnetic reversals or excursions could be related to the dynamics of the Earth\u2019s core rather than to external factors.","label":1,"model":"bloomz","source":"arxiv","id":3405}
{"text":"We present an overview of the phase space approach to special relativity, which is based on the concept that all physical quantities are represented by functions defined over a four-dimensional manifold called spacetime. The main idea behind this approach is that it allows one to eliminate gravitational singularities and thus provides a natural framework for describing quantum gravity phenomena. We also discuss some recent results obtained within this formalism. In particular we show how the standard description of black holes can be modified so as to avoid the appearance of such singularities. Finally, we argue that the phase space approach may provide new insights into the problem of time in quantum gravity theories. This article is part of the themed issue 'Quantum gravity'. Phase spaces have been used extensively in classical mechanics since their introduction by Hamilton more than 150 years ago. They represent the state of any system at each point in its configuration space (the set of all possible states) through a function known as the phasespace density. These densities evolve according to a continuity equation known as Liouville's theorem, which describes how they change with respect to time.","label":1,"model":"bloomz","source":"arxiv","id":3406}
{"text":"We study the dynamics of an analog recurrent neural network (RNN) driven by temporally fluctuating white noise in its synapses, and show that it can be described by the Thouless-AndersonPalmer (TAP) equations. We derive these TAP equations using replica theory applied to the RNN's partition function. The resulting TAP equations are exact up to corrections which vanish exponentially fast as the number N of neurons increases. In particular we find that the TAP free energy is given by F = \u2212kBTN ln ZN + O(1\/N2), where kB denotes Boltzmann's constant and TN \u2261 1\/ZN is the temperature associated with the fluctuations in the synapses. This result shows that the TAP approach provides a good description even when the system size is small compared to the typical correlation length of the noise. Finally, we use our results to calculate the stationary state distribution of the RNN and compare this prediction against numerical simulations.","label":1,"model":"bloomz","source":"arxiv","id":3407}
{"text":"The present work is devoted to study some properties of curves in P^3 having an exceptional secant plane, i.e., such that there exists a line intersecting them at two points and not passing through any other point on the curve.  We give necessary conditions for a curve to have an exceptional secant plane (Proposition 1). Then we prove that if a curve has an exceptional secant plane then it lies on a quadric surface (Theorem 2).  Finally, using this result, we show how one can construct all possible families of curves with an exceptional secant plane by means of their associated linear systems (Theorems 3-7).\nKey words: Curve, Secant Plane, Quadric Surface. Mathematics Subject Classification (2000): 14C20, 14D10, 32S15. The research leading to these results was supported by the Russian Foundation for Basic Research under grant No. 02-01-00962-a","label":1,"model":"bloomz","source":"arxiv","id":3408}
{"text":"The Standard Model (SM) is an extremely successful theory, but it has some problems that need to be addressed before we can claim to have found \"theory of everything\".  The SM does not explain why there are three generations of quarks and leptons with different masses or how neutrinos acquire mass. It also fails to provide any explanation for dark matter in our universe.   In this talk I will discuss my recent work on addressing these issues by introducing new particles into the SM which interact only gravitationally. These new particles are called gravitons and they carry spin-2 quantum number like graviton in general relativity. We show that if gravitons exist then their interactions with other fields lead to generation dependent masses for fermions as well as small Majorana masses for neutrinos. Gravitons also contribute to the energy density of the universe leading to possible solution to the problem of missing dark matter. Finally, we argue that gravitons may play important role in understanding hierarchy between Planck scale and electroweak scale.","label":1,"model":"bloomz","source":"arxiv","id":3409}
{"text":"The TACTIC (Telescope Array for Cosmic Triggered Events) is an imaging atmospheric Cherenkov telescope located in Namibia at the site of the HESS experiment, and it was designed to detect gamma rays with energies between 100 GeV and 10 TeV. The camera consists of 960 photomultiplier tubes arranged on a hexagonal grid covering a field-of-view of 3 degrees diameter. It has been taking data since March 2009. In this work we present results obtained by applying different analysis techniques to the first two years of data taken with the TACTIC telescope. We show that these analyses are able to reconstruct events with high efficiency over most of the energy range covered by the instrument. Finally, we compare our results with those published by other experiments operating in similar energy ranges. This article is part of the themed issue 'The Universe as seen by ground-based gamma-ray telescopes'. Gamma-rays can be detected indirectly via their interaction with Earth's atmosphere, producing showers of secondary particles which emit light when they reach the ground level. These so-called air-shower photons can then be observed using large optical detectors such as imaging atmospheric Cherenkov telescopes (IACTs). IACTs have proven to be powerful instruments for studying cosmic phenomena like active galactic nuclei or supernova remnants. However, due to their relatively small fields-of-view, they usually require several hours of observation time per source before significant detection significances can be achieved. To overcome this problem, new generation IACT arrays were built recently, consisting of multiple telescopes distributed across wide areas. One example is the High Energy Stereoscopic System (H.E.S.S.) [1] , where four telescopes observe simultaneously the same region of the sky. Another one is the Telescope Array for Cosmic Triggerd Events (TACTIC), which will consist of eight telescopes spread out over a distance of about 1 km [2] .","label":1,"model":"bloomz","source":"arxiv","id":3410}
{"text":"The Marcus-Lushnikov process is the continuous-time analogue of the discrete-time L\u00e9vy walk model introduced by Montroll-Weiss in 1965 to describe diffusion-limited aggregation (DLA) on fractal surfaces.  The DLA growth mechanism has been observed experimentally for many years but only recently have there been attempts at modelling it mathematically using stochastic processes such as the Marcus-Lushnikov process.   In this article we consider two related problems concerning the Marcus-Lushnikov model:  Firstly, we prove that if the jump distribution of the underlying L\u00e9vy process satisfies certain integrability conditions then the corresponding Marcus-Lushnikov process converges weakly to Brownian motion with drift; secondly, we show how the Marcus-Lushnikov model can be used to approximate the solution of Smoluchowski's coagulation-fragmentation equation which describes the evolution of particle size distributions in chemical reactions involving clusters of particles. We also discuss some connections between the Marcus-Lushnikov and Flory's models of polymerisation.","label":1,"model":"bloomz","source":"arxiv","id":3411}
{"text":"We present the Bridge algorithm, which is an efficient direct tree hybrid n-body method that can be used to simulate fully self-consistently star clusters in galactic potentials with arbitrary mass distributions. The Bridge algorithm combines the advantages of both direct-summation (DS) and tree methods by using DS only within small sub-groups of particles at each time step while solving the full system of equations on a tree. We show that this approach allows us to achieve high accuracy without sacrificing computational efficiency. In particular we demonstrate that our new code reproduces results obtained with the state-of-the-art treecode NBODY6++GPU very well even when simulating systems containing up to 10 million stars. This makes it possible to study the long-term dynamical evolution of open clusters as well as globular clusters orbiting around parent galaxies over many Gyr timescales. \n \n Keywords: Open cluster; Globular cluster; Galactic potential","label":1,"model":"bloomz","source":"arxiv","id":3412}
{"text":"The autoignition characteristics of two cyclic hydrocarbons, cyclopentane (CP) and cyclohexane (CH), are investigated using the rapid compression machine coupled with a shock-tube facility at temperatures ranging between 300 K and 1000 K under atmospheric pressure conditions. The ignition delay times for both fuels increase as temperature increases due to an increased rate of chemical reactions. At low temperatures below 600 K, CP has longer ignition delays than CH because it is more difficult for the fuel molecules to overcome their activation energy barrier. However, above 700 K, the opposite trend occurs where CH exhibits longer ignition delays compared to CP. This can be explained by the fact that the higher molecular weight of CH leads to slower diffusion rates which results in lower reactivity.  In addition, the effect of equivalence ratio on the ignition delay time was also studied. It was found that increasing the equivalence ratio decreases the ignition delay time for all tested temperatures except at 800 K where no significant difference could be observed.","label":1,"model":"bloomz","source":"arxiv","id":3413}
{"text":"We present an open-source framework, called Bubblesched, that allows to build portable thread schedulers for hierarchical multiprocessors (HMPs). The main idea is to use bubbles as scheduling units and schedule them on different levels in HMP hierarchy using a set of rules. We have implemented two schedulers: one based on work stealing and another one based on load balancing. Both schedulers are able to run efficiently on top of Bubblesched without any modifications. Our experimental results show that both schedulers outperform state-of-the-art solutions by up to 3\u00c2 times when running parallel applications with fine-grained tasks. In addition, we demonstrate how our scheduler can be used to implement efficient task-parallel algorithms such as graph coloring or matrix multiplication. This research was supported by Russian Science Foundation grant 14-50-00040. We present an opensource framework, called Bubblesched: it allows to build portable threadschedulers for hierarchicalmultiprocessors(HMPs)andrunefficientlyonthemwithoutanymodifications.Theframeworkusesbubblesastheschedu-lingsunitsandschedulethemonthelevelsofHMPhierarchyusingasetofrules.Wehaveimplementedtwo-schedulers:onebasedonstealingworkandanotheronesupportedbyloadbalancing.BothschedulersexecutesuccessfullyontopofBubbleschedwithouthavingtobemodified.Ourexperimentalresultsshowthatbothschedulersoutperformstate-oftheartsolutionsupto3timeswhenrunningparallelapplicationswithfinegrainetasks.Inaddition,weillustratethattheframeworkcanbeusedtomakeefficienttask-parallelandalgorithmssuchastask-coloringanoregularmatrixmultiplication.","label":1,"model":"bloomz","source":"arxiv","id":3414}
{"text":"Neutrinos are neutral leptons that can oscillate between different flavors, and the phenomenon is described by the Pontecorvo-Maki-Nakagawa-Sakata (PMNS) matrix. The PMNS matrix contains three mixing angles \u03b812, \u03b823, \u03b813 and one complex phase \u03b4CP which describes possible CP-violation effects. In this work we study how to probe CP violation using atmospheric neutrinos detected at IceCube Neutrino Observatory. We show that it will be possible to measure the value of sin(2\u03b813), if its magnitude is larger than 0.1, as well as the sign of cos(\u03b4CP). This measurement would provide important information on the origin of matter-antimatter asymmetry observed in our universe. \n \n Introduction \n \n Neutrinos are neutral leptonic particles that can change their flavor during propagation through space-time. These changes occur because of quantum mechanical interference effects: when two or more mass eigenstates propagate together they interfere destructively or constructively depending on their relative phases [1] . If there exists an intrinsic difference between these phases then the probability for detecting a certain flavor after traveling some distance differs from what would have been expected without such differences [2] .\n \nThe phenomenon of neutrino oscillation has been confirmed experimentally [3] , but the exact values of the parameters describing the process remain unknown [4] . One of them is the so-called Dirac phase $\\delta_{CP}$ [5] , which determines whether the total lepton number $L$ is conserved [6] . It also affects the amount of baryon asymmetry produced in the early Universe [7, 8] . Therefore measuring the value of $\\delta_{CP}$ could shed light on the physics behind the generation of matter-antimattery asymmetry [9] . \n \n There exist several experiments searching for CP violation in the neutrino sector [10] . However, most of those experiments use accelerator-based beams of neutrinos [11] . On the other hand, neutrino telescopes detect cosmic neutrinos coming from all directions [12] . They offer unique opportunities to perform measurements complementary to those performed in accelerators [13] . For example, neutr","label":1,"model":"bloomz","source":"arxiv","id":3415}
{"text":"We study heavy quarkonium transitions to hybrid mesons by using the multipole expansion method within the framework of nonrelativistic potential models. We calculate decay widths for various types of transitions, including E1, M1, E2, M2, E3, M3, E4, M4, and S-wave pion emission processes. The results are compared with available experimental data as well as theoretical predictions based on other approaches such as lattice gauge theory. \n \n In addition we determine mass spectra of hybrids by solving Schr\u00f6dinger equations numerically. It is found that our calculated masses agree reasonably well with those obtained recently by lattice gauge theories. Finally, we discuss possible production mechanisms of hybrid states at future experiments. This work was supported by the National Natural Science Foundation of China under Grant No. 10675040. We thank Professors J. H. Chen and C. T. Chan for helpful discussions.","label":1,"model":"bloomz","source":"arxiv","id":3416}
{"text":"We present the results of our analysis on the polarization power spectrum in Bianchi type I cosmological models, which are anisotropic generalizations of standard FRW cosmologies. We find that there is no significant difference between the temperature fluctuations predicted by these two classes of models at large angular scales (low multipoles). However, we show that this is not true when one considers the polarization fluctuations. In particular, we demonstrate that the presence of an anisotropy parameter leads to a suppression of the low-l polarization power relative to the high-l part of the spectrum. This effect can be used as a test for distinguishing Bianchi type I models from their FRW counterparts. \n \n The observed lack of large-scale polarization in the WMAP data has been interpreted as evidence against inflationary scenarios with tensor perturbations. It was shown recently that such a conclusion may be premature if one takes into account possible deviations from statistical isotropy in the primordial universe. Indeed, it turns out that some anisotropic cosmological models predict less large-scale polarization than their isotropic counterparts do.","label":1,"model":"bloomz","source":"arxiv","id":3417}
{"text":"We study the stability properties of planar streamers in air, which are ionized regions that propagate into un-ionized gas under the influence of an applied electric field. We show how to use the so-called \"pulled front approach\" to derive a nonlinear evolution equation for the shape of such fronts and then analyze this equation using standard techniques from dynamical systems theory. In particular we find that there is a critical value of the applied voltage beyond which the planar solution becomes unstable with respect to small perturbations. This result explains why it has been so difficult to observe stable propagation of streamers at high voltages experimentally. The results presented here should be relevant not only to atmospheric discharges but also to other physical situations where similar phenomena occur, e.g., combustion flames or chemical waves on catalytic surfaces. Streamers are thin channels filled with highly conducting plasma that can form when strong electric fields are present between two electrodes immersed in non-conducting gases [1] . They have attracted considerable interest over many years because they play important roles in various applications including lightning [2] , sprites [3] , and high-voltage switches [4] .\nIn recent years much progress has been made towards understanding their formation mechanisms [5, 6, 7, 8] as well as their dynamics [9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96,","label":1,"model":"bloomz","source":"arxiv","id":3418}
{"text":"We present new molecular opacity tables that include all relevant molecules in cool, carbon-rich stellar envelopes and are valid over a wide range of temperatures (T = 1000 - 10000 K), densities (\u03c1 = 10 \u221210 -10 6 g\/cm 3 ) and compositions (C\/O=0.5-2). The calculations were performed with the state-of-the-art ab initio line-by-line radiative transfer code SPECTRUM using extensive laboratory data on molecular lines as well as theoretical predictions based on quantum chemical methods. We have calculated synthetic spectra for several model atmospheres representative of red giant branch (RGB) and asymptotic giant branch (AGB) stars to demonstrate how our new opacity tables affect their structure and evolution. Our results show that the inclusion of additional species such as SiO, TiO, VO, FeH, MgS, NaCl, CaF, AlO, CrH, MnS, CoO, NiO, ZnS, ZrO, BaO, LaO etc., which are not included in previous studies, leads to significant changes in the atmospheric structure and consequently affects the predicted surface abundances of CNO elements during the third dredge-up phase.","label":1,"model":"bloomz","source":"arxiv","id":3419}
{"text":"We present the results for dark matter annihilations into gamma rays and neutrinos using an improved treatment of subhalos within galaxy clusters, including their internal structure as well as tidal stripping effects on their outer parts. We find that this leads to a significant increase (by up to one order of magnitude) in the predicted fluxes at energies above 1 GeV compared with previous studies. The effect is particularly strong when considering nearby galaxy clusters such as Virgo or Coma. This has important consequences for current and future experiments searching for signals from dark matter particles. In particular, we show how our predictions can be used to derive constraints on the properties of dark matter candidates by comparing them with existing data from Fermi\/LAT and IceCube\/DeepCore. Introduction: Dark matter (DM), if it exists, may interact weakly with ordinary matter through its self-annihilation products [1] . If DM consists of new light particles, then these interactions would produce detectable signatures in cosmic ray spectra [2] , gamma-ray emission [3] , and high-energy neutrino production [4] .\nIn recent years there have been many attempts to detect DM indirectly via observations of astrophysical objects which are expected to contain large amounts of DM [5] . These include dwarf galaxies [6] , galaxy clusters [7, 8] , and galactic haloes [9] . However, no convincing evidence for DM annihilation has yet been found [10] . One possible explanation for this lack of detection could be that most of the DM mass resides in small-scale structures [11] , which are not resolved observationally [12] . Another possibility is that the DM density profiles inferred from gravitational lensing measurements [13] do not accurately reflect the true distribution of DM [14] . Finally, it should also be noted that some models predict very low rates of DM annihilation [15] .\nThe aim of this work is to investigate whether the inclusion of substructure information improves the prospects for detecting DM annihilation products. To achieve this goal, we use high-resolution N-body simulations [16] to study the impact of subhalo populations on the resulting gamma-ray [17] and neutrino [18] fluxes produced by","label":1,"model":"bloomz","source":"arxiv","id":3420}
{"text":"We present new photometry in the optical and near-infrared bands for the open cluster Blanco 1 (M35). The data were obtained with the 2 m telescope at the Observatorio Astron\u00f3mico Nacional de San Pedro M\u00e1rtir, Baja California Sur, Mexico; the 0.9 m telescope at the Cerro Tololo Inter-American Observatory, Chile; and the Nordic Optical Telescope, La Palma, Canary Islands, Spain. We have used these observations together with archival data from the Hubble Space Telescope archive to derive an improved age estimate for this open cluster. Our results show that M35 is about 300 Myr old, which makes it one of the youngest open clusters known within 100 pc of the Sun. Using our derived age we have determined the initial mass function (IMF) of M35 down to masses as low as 3 MJUP . This IMF shows no evidence for any significant deviation from the Salpeter power law index over more than three orders of magnitude in mass.","label":1,"model":"bloomz","source":"arxiv","id":3421}
{"text":"We study the electronic properties of armchair-edged carbon nanotubes with magnetic flux piercing them, using density functional theory (DFT). We find that for tubes with diameter larger than 1 nm there is no significant difference between the results obtained by DFT and those predicted by the effective-mass approximation. For smaller diameters we observe deviations which are attributed to the breaking of the valley degeneracy due to the curvature effects. The Aharonov-Bohm-effect manifests itself as an oscillatory behavior of the energy gap when varying the applied magnetic field strength. In addition, we show how this effect can be used to tune the bandgap of these structures. Graphene nanorings have been proposed recently as building blocks for novel nanoelectronic devices such as transistors or spintrons [1-3]. These systems exhibit interesting physical phenomena like the quantum Hall effect [4] , persistent currents [5] , and Klein tunneling [6] . Recently it has also been shown that they may serve as efficient single photon sources [7, 8] .\nIn order to understand their transport characteristics one needs to know the dependence of the energy spectrum on various parameters such as the radius R, the number N of hexagons along the circumference, and the external magnetic field B. This problem was addressed theoretically within different approximations [9-13] but only very few studies were performed based on first-principles calculations [14-16]. Here we present a detailed investigation of the influence of the magnetic field on the electronic structure of armchair-edge carbon nanotubes using density functional theory [17] . Our main focus will be on small-diameter tubes where the curvature leads to important modifications compared to large-diameter tubes [18] .","label":1,"model":"bloomz","source":"arxiv","id":3422}
{"text":"The transcription process is driven by the enzyme RNA Polymerase (RNAP) that moves along the template strand of double stranded DNA and synthesizes an mRNA molecule in front of it. The RNAPs are highly dynamic enzymes, which can pause or arrest at specific sites during their translocation through the gene sequence. In this work we study how these pauses affect the overall rate of transcription using a coarse-grained model for the dynamics of multiple RNAP molecules moving simultaneously on a single DNA chain. We show that the presence of pausing leads to a reduction in the average speed of transcription elongation as well as to a decrease in the total number of synthesized mRNAs per unit time. These results suggest that the observed heterogeneity in the rates of transcription may be due to differences in the density of paused RNAPs between genes. Transcription is one of the fundamental processes in all living organisms where information stored in the form of genetic code is converted into messenger RNA (mRNA). This conversion takes place inside the cell nucleus when the enzyme called RNA Polymerase (or simply RNAP) binds with promoter region of the gene and starts copying its sequence [1] . During transcription, the RNAP travels along the template strand of doublestranded DNA and synthesizes an RNA molecule in front of itself [2] .\nIn recent years there has been significant progress towards understanding the mechanism behind transcription [3, 4] , however many questions remain unanswered [5] . One such question concerns the effect of pausing events on the overall rate of transcription [6] . It was shown experimentally [7, 8] that the RNAPs tend to pause frequently while traveling through the gene sequence. Moreover, the duration of each pause event varies widely depending on the type of the gene [9] . For example, some genes have short pauses lasting only few milliseconds [10] whereas others exhibit much longer pauses up to several seconds [11] .","label":1,"model":"bloomz","source":"arxiv","id":3423}
{"text":"We report the discovery of two new evolved massive stars, IRAS 08281-4850 (HD 161797) and IRAS 14325-6428 (HD 158687), which are both classified as A-type supergiants in the literature but show strong infrared excesses indicative of circumstellar dust shells. The optical spectra obtained by us reveal that these objects have very high surface gravities for their luminosity classes, indicating they may be post-asymptotic giant branch (post-AGB) stars. We find that the chemical compositions of these two stars are significantly different to those of normal A-type supergiants; they exhibit enhanced abundances of heavy elements such as Zr, Y, Ba, La, Ce, Nd, Sm, Eu, Yb, Hf, Ta, W, Os, Ir, Pt, Au, Hg, Tl, Pb, Bi, Th, U, and possibly also CNO-cycle products like N, O, Ne, S, Ar, Cl, K, Ca, Sc, Ti, V, Cr, Mn, Fe, Co, Ni, Cu, Zn, Ge, As, Se, Br, Kr, Sr, Mo, Ag, Cd, In, Sn, Sb, Te, I, Xe, Cs, Ba, and probably also r-process elements; this is consistent with previous findings on other post-AGB stars.","label":1,"model":"bloomz","source":"arxiv","id":3424}
{"text":"We study the problem of finding independent transversals (ITs) in graphs with bounded local sparsity, where each vertex has at most k neighbors outside its neighborhood. We show that this problem is NP-hard for any fixed value of k > 1 and present an algorithm which finds ITs in time O(n3k). Our results are based on a novel reduction to the maximum matching problem in bipartite graphs. Finally we prove that our approach can be extended to find approximate solutions within a factor of 2k-1. The Independent Transversal Problem asks whether there exists a set S of vertices such that every edge connects two different elements of S. This problem was introduced by Karp as one of his 21 NP-complete problems [Karp72] . In recent years it received renewed interest due to applications in computational biology [Fang+10] , social network analysis [Bhattacharyya+09; Karger+00] , and data mining [Liu+12; Liu+13] .\nIn this work we consider the following generalization of the Independent Transversal Problem. Let G = (V, E) be a simple undirected n-vertex graph. For each v \u2208 V let Nv denote the set of all neighbors of v. A transversal T \u2286 V is called independent if no two nodes u1, u2 \u2208 T have more than k common neighbors outside their respective neighborhoods: |Nv1 \\ T | + |Nv2 \\ T | \u2264 k holds for all pairs v1, v2 \u2208 T . Note that the case k = 0 corresponds exactly to the classical Independent Transversal Problem. \nThe main motivation behind studying this problem comes from the fact that many real-world networks exhibit small-world properties [Watts+99] : they contain only few connections between distant parts of the network but these connections play a crucial role in determining global connectivity patterns. As shown recently by Fang et al., the Independent Transversal...","label":1,"model":"bloomz","source":"arxiv","id":3425}
{"text":"We present new high-precision astrometric observations for the triple system HD73526, which is composed by two close binaries with periods of 1.5 days (A) and 3 years (B). The third component C has an orbital period of about 10 years around A-B barycenter. We have used these data to determine the masses of all three components in this system using the method of double-lined spectroscopic orbits. Our results show that the mass ratio between B and C is q = 0.7 \u00b1 0.1, while the total mass of the system Mtot = 2.6 \u00b1 0.2M\u2299 . This value agrees well with theoretical predictions based on evolutionary models for stars with solar metallicity. In addition we find that the eccentricity e = 0.3 \u00b1 0.1 of the orbit of C around AB is significantly larger than expected theoretically.  These discrepancies can be explained if one assumes that there are additional bodies in the system whose gravitational influence leads to the observed large eccentricity of the orbit of C.","label":1,"model":"bloomz","source":"arxiv","id":3426}
{"text":"We study the relaxation behavior in the Random Energy Model (REM) with an external magnetic field, which is one of the simplest models to describe glassy phenomena. We show that the relaxation time distribution function has a universal form independent of temperature and system size by using numerical simulations on finite-dimensional systems up to dimension d = 8. The universality holds even when we take into account the effect of quenched disorder. This result suggests that the relaxation process in this model can be described as a superposition of many processes each of which obeys the same scaling law. In addition, we find that the relaxation time distribution functions at different temperatures are well fitted by a stretched exponential function. These results suggest that the relaxation process in our model is governed by a broad spectrum of relaxation times. Introduction:-The Random Energy Model(REM), proposed by Derrida [1] ,is one of the simplest models describing glassy phenomena [2] .In this model,the energy landscape consists of N local minima whose energies are randomly distributed according to some probability density P(E).Each minimum corresponds to a metastable state and its lifetime \u03c4i is determined by the Arrhenius law exp[\u2212Ei\/kBT ] where Ei denotes the activation energy barrier between two neighboring states i and j.It was shown numerically [3] that the relaxation time distribution follows a power-law decaying function f (\u03c4 ) \u221d 1\/\u03c4 1+\u03b1 with \u03b1 \u2248 1.3 \u2212 2 depending on the dimensionality of the system [4] .\nRecently it was found [5] that the relaxation time distribution also follows a power-law decay if we consider the case without any quenched disorder but with a random initial condition instead.This indicates that the relaxation process in the REM is dominated by activated events over barriers separating different metastable states [6] .However,it should be noted that these studies were performed only for low dimensions such as d \u2264 4 [7, 8] .Therefore,in order to understand the nature of glassy phenomena more clearly,we need to investigate whether or not the above mentioned results hold true in higher dimensions.","label":1,"model":"bloomz","source":"arxiv","id":3427}
{"text":"We present an improved factorization formula for the fragmentation functions (FFs) of hadrons containing one heavy quark, which is valid in both leading order and next-to-leading order QCD perturbation theory. The new formula takes into account all possible contributions to the FFs at each perturbative order. We show that our results are consistent with those obtained by using other approaches such as the operator product expansion method or the renormalization group equation approach. Finally we give numerical predictions on some important quantities related to the charm-quark FFs. PACS numbers: 12.38.Qk, 13.25.Gv, 11.15.Tk \nI. INTRODUCTORY REMARK\nThe fragmentation function D(z), where z = Phadron\/Pquark , describes how quarks fragment into hadrons when they are produced in hard processes like deep-inelastic scattering [1] . It plays an essential role in understanding many phenomena observed experimentally [2] .\nIn this work, we will study the fragmentation functions of hadronic states containing only one heavy quark. In particular, we consider the case of charmed-meson production in e + e-annihilation processes:","label":1,"model":"bloomz","source":"arxiv","id":3428}
{"text":"We report on spectropolarimetric observations of the M8+M9 binary star LHS 1070A,B (GJ 436) with ESPaDOnS at CFHT. The two stars are separated by only 0. \u2032\u2032 1 and have been known to be magnetically active for many years. We find that both stars show significant circularly polarized emission lines indicative of Zeeman splitting due to their magnetic fields. In addition we detect Stokes V signatures indicating net linear polarization across all observed spectral lines. This is likely caused by scattering processes within the stellar atmosphere. Using our new data set together with previously published photometric measurements we derive rotation periods of P A = 3.6 \u00b1 0.1 days and P B = 4.2 \u00b1 0.3 days for the primary and secondary component respectively. These values are significantly longer than those derived from previous studies which were based solely on photometry. Our results suggest that the rotation period of each individual component depends strongly on its effective temperature as well as its surface gravity.","label":1,"model":"bloomz","source":"arxiv","id":3429}
{"text":"We present an efficient data redistribution scheme that allows the parallel execution of computations on dynamically resized computational domains in distributed memory environments. The proposed approach is based on multidimensional block-cyclic distributions and exploits locality by using space-filling curves to map blocks onto processors. We show how this technique can be used to efficiently redistribute data between different processor configurations, while minimizing communication overheads. Our experimental results demonstrate significant performance improvements over existing approaches when executing applications with dynamic load balancing requirements. In particular, we achieve speedups up to 3\u00c2 compared to state-of-the-art techniques such as the one presented in [1] . \nIntroduction\n\nParallel computing has become increasingly important due to its ability to solve large problems faster than serial computers [2] , but it also presents new challenges related to the distribution of work among multiple processing units [3] .\nIn order to take advantage of parallelism, many algorithms are designed so that they can run simultaneously on several processors [4] . However, these algorithms often require some form of data redistribution during their execution [5] . For example, consider a computation where each process stores part of a dataset (e.g., matrix) [6] . If the number of processes changes at runtime, then all processes need to exchange information about which parts of the dataset they store before continuing [7] . This problem becomes even more challenging if the size of the datasets stored by individual processes varies [8] or if there are dependencies between them [9] .","label":1,"model":"bloomz","source":"arxiv","id":3430}
{"text":"We present an analytical model for high-order harmonic generation (HHG) in diatomic molecules, which is based on the concept of molecular orbitals and their associated electronic wavefunctions. The HHG process can be understood as a sequence of three steps: First, electrons are ionized by strong laser fields. Second, they propagate through the continuum until rescattering with parent ions occurs. Third, these returning electrons emit high harmonics when interacting again with the driving field. We show that this picture leads to a simple expression for the emitted harmonic intensity, which depends only on two parameters characterizing the molecule's orbital structure. This result allows us to explain the observed interference patterns between different harmonics in terms of destructive or constructive interferences between contributions from different molecular orbits. In addition, we demonstrate how our approach can be used to predict the emission properties of new types of molecules. High-order harmonic generation (HHG), i.e., the coherent emission of photons at odd multiples of the fundamental frequency of intense femtosecond laser pulses, has attracted considerable interest over recent years [1, 2] . It provides access to extreme ultraviolet radiation [3] , which enables novel applications such as attosecond pulse generation [4] , photoelectron spectroscopy [5] , and tomography [6] .\nThe underlying physical mechanism behind HHG was first explained within the semiclassical three-step model [7, 8] : An electron tunnels out of its atomic core into the continuum upon interaction with the electric field of the laser light. Afterwards it propagates freely before being driven back towards the nucleus by the same field. Finally, it recombines with the parent ion emitting a photon whose energy equals the sum of the kinetic energy gained during propagation and the binding energy lost due to tunneling [9] . Since then, several extensions have been developed [10] including the so-called quantum-orbit theory [11] , which takes into account the influence of the nuclear potential on the electron dynamics [12] . However, despite all efforts made so far, there still exist many open questions regarding the microscopic origin of HHG [13] .","label":1,"model":"bloomz","source":"arxiv","id":3431}
{"text":"We present new optical and near-infrared images, as well as archival radio data for the polar ring galaxy AM1934-563 (NGC4650A). The main results are summarized below:  1) We confirm that this galaxy is an edge-on S0\/a with a bright nuclear point source surrounded by a faint disk.  2) A prominent dust lane crosses the central region in the north-south direction.  3) There appears to be no evidence for any recent star formation activity within the ring or along its edges.  4) The kinematics of the gas component show two distinct components; one associated with the ring itself and another which follows closely the rotation curve of the underlying stellar body.  5) The total mass of the gaseous ring amounts to about 10^9 M_sol. 6) The observed properties of NGC4650A can be explained if it has undergone a major merger event between two galaxies at least 0.5 Gyr ago.","label":1,"model":"bloomz","source":"arxiv","id":3432}
{"text":"The DualHeap selection algorithm is an efficient method for selecting the kth smallest element in a set S of n elements.  It has been used to solve many problems including finding the minimum spanning tree (MST) in parallel computing environments such as MapReduce.   In this article we present some new results on the DualHeap algorithm that shed light on its efficiency and inherent parallelism.    We show how it can be implemented using only O(logn) bits per element while still maintaining its efficiency guarantees.   This implementation requires no extra space beyond what is needed by the input data itself and thus fits well with modern storage devices like flash memory or hard disks where storing large amounts of data is cheap but accessing individual items may be expensive.   Finally, we prove that the DualHeap algorithm solves the MST problem faster than any other known deterministic algorithm when run on a single processor machine. The DualHeap selection algorithm is one of the most widely used algorithms in computer science today.","label":1,"model":"bloomz","source":"arxiv","id":3433}
{"text":"We study the effect of competing short-range attractive and repulsive interactions in a model for collapsed polymers, which are described by an effective one-dimensional potential with two minima separated by a barrier. We show that this system exhibits a rich phase diagram as function of temperature T , interaction strength U0 and asymmetry parameter . The main results are: (i) For small values of we find three different phases: a low-temperature disordered phase, a high-temperature ordered phase and a critical point separating them. (ii) In the limit of large barriers between the wells, i.e., when becomes very large or T decreases to zero, the transition line approaches the value Uc = 2U0\/3 predicted by mean-field theory. \n \n Introduction \n \n Collapsed polymers have been studied extensively over many years [1]-[6]. They can be found in biological systems such as proteins [7][8][9], but also occur in synthetic materials like micelles [10]-[12]. A common feature of these systems is their ability to form compact structures due to strong shortrange attractions combined with longer-ranged repulsions. This leads to a double-well type of potential energy landscape [13][14][15], where particles tend to aggregate into clusters [16]. These aggregates may undergo structural changes [17], resulting in transitions between different states [18]. Such phenomena are often observed experimentally [19][20][21] [22]. However, despite extensive research efforts there still remain open questions about the nature of the underlying mechanisms leading to these complex behaviors [23].","label":1,"model":"bloomz","source":"arxiv","id":3434}
{"text":"We present Spitzer Space Telescope observations in the mid- to far-infrared (5.8 - 160 microns) for a sample of distant galaxies selected by their rest-frame ultraviolet luminosity density at 1600 angstroms, and compare these flux densities with those predicted using models that include dust extinction.  We find that the observed infrared emission is generally higher than expected based on the UV continuum slope alone; this excess emission can be explained if there are significant amounts of cold dust associated with star formation activity in these systems.   The results suggest that the majority of the energy produced by young stars may not escape into intergalactic space but instead is reprocessed by interstellar dust grains before being re-radiated in the infrared. This work was supported by NASA through grants NAG5-9998 and NAS8-38252 issued by JPL\/Caltech under contract NAS8-39073. It has been assigned the following DOI: 10.1086\/505283","label":1,"model":"bloomz","source":"arxiv","id":3435}
{"text":"The photoelectric effect is one of the most important discoveries in physics, and it has been taught to students since its discovery more than 80 years ago. However, there are many misconceptions about this phenomenon that have not yet been addressed by traditional teaching methods. In this study we developed an inquiry-based curriculum using research findings on student difficulties with the concept of energy as well as their ideas about light emission and absorption. The results show that our approach was effective at improving students' understanding of the photoelectric effect. This article describes how we used research into student thinking to develop a new way of teaching the photoelectric effect. We also present evidence showing that our approach improved students' conceptual understanding of the topic. The photoelectric effect (PE) is one of the most significant discoveries in physics [1] . It has been studied extensively over the past eight decades [2] , but despite all these efforts some aspects of PE remain poorly understood [3] .\nIn particular, students often struggle with concepts such as energy [4] , light emission [5] , and light absorption [6] . These difficulties can be attributed to several factors including the fact that PE involves phenomena occurring across different physical scales [7, 8] . For example, when studying PE, students need to understand atomic processes involving photons [9] while simultaneously considering macroscopic effects like current generation [10] . Another factor contributing to student difficulties is the lack of appropriate instructional materials [11] . Traditional textbooks do not provide enough information about the underlying mechanisms involved in PE [12] or they focus too much on mathematical derivations [13] .","label":1,"model":"bloomz","source":"arxiv","id":3436}
{"text":"We present the results of an extensive multi-wavelength survey of two nearby (< 1 kpc) and well-studied star-forming regions, Orion Nebula Cluster (ONC), NGC 2024, in order to investigate their physical properties as well as those of individual protostars embedded within them. We have obtained near-infrared images with Subaru\/Suprime-Cam at JHKs bands for ONC region and Spitzer\/IRAC 3.6-8.0 micron data for both regions. In addition we used archival radio continuum observations made by VLA at 6 cm and 20 cm wavelengths. Using these datasets, we performed photometry on all point sources detected above 5 sigma level in each band. By comparing our infrared photometric measurements with theoretical evolutionary models, we found that most of the objects are likely to be Class I or flat-spectrum protostellar candidates. From the analysis of spectral energy distribution (SED) fitting using radiative transfer modeling code, we derived the mass accretion rates onto the central stars ranging between 10-700 x10-6 Msun yr-1 .","label":1,"model":"bloomz","source":"arxiv","id":3437}
{"text":"The magnetic reconnection is one of the most important processes for understanding many phenomena observed in space and laboratory plasmas, such as solar flares, magnetospheric substorms, sawtooth crashes in tokamaks etc.. In this work we present an analytical model which describes the process of magnetic reconnection in collisionless high energy plasma with arbitrary initial conditions. The main idea behind our approach consists in using the concept of generalized force density tensor (GDFT) introduced by MHD theory. We show that GDFT can be used not only to describe the macroscopic dynamics but also microscopic properties of the system like particle distribution functions. Our results are compared with those obtained within other approaches based on kinetic description of particles motion. It turns out that all these models give similar predictions when applied to simple cases where the initial state has no gradients along the direction perpendicular to the background magnetic field. However, if there exist some gradients across the magnetic field lines then different models predict quite different behavior.","label":1,"model":"bloomz","source":"arxiv","id":3438}
{"text":"We report on two new brown dwarf candidates discovered by direct imaging with NACO\/VLT at distances of ~100 AU to their host star, which are members of open clusters NGC 2423 (M67) and NGC 4349. The first object is located close to the cluster center and has an estimated mass between 0.03-0.07 M . It was found as part of our survey for low-mass companions to nearby young solar-type stars. We also present follow-up observations of this candidate using adaptive optics techniques that confirm its substellar nature. \n \n The second object lies outside the core radius of the cluster and has an estimated mass below 0.01 M . This companion may be either a planetary or very-low-mass stellar companion depending on whether it formed through gravitational instability or fragmentation during cloud collapse. Both objects have masses well above those predicted by current models of planet formation via disk accretion.","label":1,"model":"bloomz","source":"arxiv","id":3439}
{"text":"The new reactor is based on the principle that nuclear fusion can be achieved by heating plasma to high temperatures in magnetic fields.  The proposed design uses an array of magnets and electrodes, which are powered by microwave generators, to heat hydrogen gas into plasma at 100 million degrees Celsius (1,000,000 K). This temperature is sufficient to cause nuclear fusion reactions between deuterium nuclei inside the plasma. In this way, energy is released as neutrons and gamma rays. These particles then bombard fuel pellets made up of tritium and lithium-6, causing them to release more energy through further nuclear fusion reactions. The resulting power plant would produce electrical power with no moving parts or radioactive waste products. It could also provide propulsion systems for spacecraft without requiring any chemical fuels. A conceptual drawing of the reactor system is shown below. Nuclear fusion has been studied extensively since its discovery over 50 years ago. However, it remains elusive because the conditions required to achieve fusion are extremely difficult to create experimentally.","label":1,"model":"bloomz","source":"arxiv","id":3440}
{"text":"The discovery that the universe is expanding at an accelerating rate has led to intense interest in dark energy as well as new ideas about fundamental physics. This talk will review some recent results on these topics including constraints on models for cosmic acceleration using supernovae data, measurements of the Hubble constant with Type Ia supernovae, and tests of general relativity using gravitational lensing statistics. The talk will also discuss how future surveys such as LSST can be used to further our understanding of dark energy and fundamental physics. I will conclude by discussing my own work on testing gravity theories beyond Einstein's theory using weak lensing observations. Keywords: Cosmology, Dark Energy, General Relativity, Weak Lensing, Supernovae, Gravitational Waves, Cosmic Microwave Background. Speaker: Adam Riess (Princeton University) Date: February 18, 2007 Time: 4:30pm - 5:15pm Location: Room B","label":1,"model":"bloomz","source":"arxiv","id":3441}
{"text":"We introduce the concept of algebraic charge liquids, which are defined as ground states of Hamiltonians with local interactions that can be written in terms of fermionic creation and annihilation operators. We show how to construct such models for any finite group G by using an explicit representation of G on the Hilbert space of spinless fermions. The resulting model is exactly solvable when G has no non-trivial subgroups. In this case we find that there exists at least one phase transition between different phases characterized by distinct topological orders. For example, if G = Z2 \u00d7 Z2 then our construction yields two gapped phases distinguished by their chiral central charges c\u2212 = 0 or 1. If G contains a nontrivial subgroup H then the system exhibits gapless excitations corresponding to particles transforming according to irreducible representations (irreps) of H. These results provide new insights into the classification problem of quantum many-body systems.","label":1,"model":"bloomz","source":"arxiv","id":3442}
{"text":"We study the effects of selection biases in cluster samples, as well as covariance between observables, on scaling relations derived from X-ray data using mock galaxy clusters generated with the semi-analytic model GALFORM. We find that both these effects can lead to significant systematic errors when deriving cosmological constraints from observed scaling relations. In particular we show that: (i) The scatter in the M-T relation is significantly reduced by including additional information about the temperature distribution function; this effect is stronger for low mass systems. (ii) The slope of the L-M relation depends strongly on whether or not one includes cooling flows in the analysis. This dependence arises because cool cores are more common at high masses than at lower masses, leading to an apparent steepening of the slope if they are excluded. (iii) The normalization of the Y-Xray luminosity-temperature relation shows strong redshift evolution which cannot be explained solely by self-similar evolution.","label":1,"model":"bloomz","source":"arxiv","id":3443}
{"text":"Spin-based electronics is an emerging field that has attracted considerable attention in recent years, due to its potential for applications such as high-density data storage and high-speed information processing.  In this review article we discuss the basic concepts underlying bipolar spintronic devices based on semiconductor heterostructures with ferromagnetic contacts. We first introduce the physics behind spin injection into semiconductors using tunnel barriers or Schottky diodes. Then we describe how these injected spins can be manipulated by means of external magnetic fields and\/or electric currents. Finally, we present some examples of spintronic devices including spin-LEDs, spin transistors, and spin-logic circuits. The main focus will be put on GaAs-based structures but also other materials systems are discussed briefly. This article is intended to provide a comprehensive overview of the state-of-the-art research in the field of bipolar spintronics. It should serve both as a guide for newcomers interested in learning about the fundamentals of spin transport phenomena at interfaces between metals and semiconductors, and as a reference source for researchers working in related areas.","label":1,"model":"bloomz","source":"arxiv","id":3444}
{"text":"We present an analysis method for the detection of solar neutrinos using radioactive sources in combination with Ge detectors, which is based on a Bayesian statistical approach. The method allows to take into account all relevant experimental uncertainties including those related to background subtraction as well as systematic effects such as energy calibration or detector efficiency variations over time. We apply this new method to two different experiments at Gran Sasso National Laboratory (Italy): Borexino and GNO. In both cases we find that our results are compatible within their respective error bars with previous analyses but show smaller errors due to improved treatment of systematics. This work was supported by the German Science Foundation DFG under grant number BR 1628\/2-1. AMS-02 collaboration has recently reported [1] the most precise measurement so far of the positron fraction up to energies of about 300 GeV. It shows a clear excess above the expected astrophysical background [2] . While there have been several attempts to explain these observations [3] , it remains unclear whether they can be attributed to dark matter annihilation [4] .\nIn order to test possible explanations of the observed excess, one needs to know how many positrons are produced per annihilation event. For example, if dark matter particles annihilate predominantly into leptons, then the total number of electrons plus positrons produced per annihilation should equal four times the number of photons produced [5] . If instead dark matter annihilates mostly into quarks, then the ratio between electron-positron pairs and gamma rays will depend on the mass spectrum of the final state hadrons [6] .","label":1,"model":"bloomz","source":"arxiv","id":3445}
{"text":"The author considers an alternative approach to classical electrodynamics, which is based on the concept of \"pre-metricity\" introduced by Einstein in his work \"On the Electrodynamics of Moving Bodies\" (1905). The main idea behind this approach consists in introducing into consideration not only the usual electromagnetic field tensor F\u03bc\u03bd but also its dual counterpart *F\u03bc\u03bd . In order to obtain equations for both fields simultaneously one has to introduce additional terms into Lagrangian density L(F\u03bc\u03bd ,*F\u03bc\u03bd ) that are quadratic with respect to derivatives of these fields. It turns out that such modified Lagrangian leads to the following set of equations: \nwhere  is the d'Alembert operator,  is the energy-momentum tensor of matter fields, and  is the current four-vector describing the flow of electric charge. \nIn addition to the standard Maxwell's equations, Eqs. (1)-(3), there appears another equation -the so-called \"duality condition\":","label":1,"model":"bloomz","source":"arxiv","id":3446}
{"text":"We present the results of our study on the evolution of interstellar dust (ISD) in spiral and irregular galaxies with various morphologies, based on multiwavelength observations obtained by Spitzer Space Telescope (SST), Herschel Space Observatory (HSO), Chandra X-ray Observatory (CXO), Hubble Space Telescope (HST). We have used SED fitting to derive physical parameters such as stellar mass, star formation rate (SFR), age, extinction etc., for all sample galaxies using photometric data available at UV-optical-NIR wavelengths. The derived properties are then compared between two galaxy samples -one consisting of spirals and another one containing irregulars. Our main findings are:  1. Irregular galaxies show higher values of total infrared luminosity than their spiral counterparts.  2. Infrared excess emission is found to be more prominent in irregular galaxies when compared to that observed in spiral galaxies.  3. Dust temperature distribution shows significant differences among irregular and spiral galaxies.","label":1,"model":"bloomz","source":"arxiv","id":3447}
{"text":"The aim of this article is to present the basic concepts in Riemannian geometry that are needed for understanding the main results presented here.  The first section introduces some notation and definitions used throughout the text.   In particular we define what it means for two points on an n-dimensional manifold M to be close together (in terms of geodesic distance) or far apart.    We also introduce the concept of a local coordinate system at each point p \u2208 M which allows us to describe any other point q near p by giving its coordinates with respect to these local charts.   Finally we give a brief description of how one can construct such a coordinate system locally around a given point using parallel transport along curves starting at p.    The second section describes the notion of a vector field X defined over all of M.   This is done by defining a map F : T M \u2192 R where T M denotes the tangent bundle of M.   Then we show that if X satisfies certain conditions then there exists a unique smooth function f : M \u2192 R such that X = grad(f).   Here grad(f) denotes the gradient of f.   For example, if M is a surface embedded in R3 then X could represent the velocity of a particle moving across M.   If we assume that the particles move according to Newton's laws of motion then the function f would correspond to the potential energy of the system under consideration.   The third section defines the concept of a tensor field as a generalization of vector fields.   Tensor fields allow us to associate several vectors...","label":1,"model":"bloomz","source":"arxiv","id":3448}
{"text":"We present an open-source, free software package that computes the one-loop Feynman diagrams contributing to the hadronic weak decay amplitudes of heavy quarks into light leptons and pions.  The code is written entirely within the Wolfram Language using only built-in functions.   It can be downloaded at https:\/\/github.com\/mharrison\/harmonic-oscillator-matrix-elements\/tree\/master\/Mathematica_code. We have tested our results against those obtained with FeynArts 3.9 [1] , FormCalc 8 [2] , LoopTools 2 [3] , and QCDLoop [4] . \nThe program has been used to compute the leading-order (LO) contributions to the helicity amplitudes for B meson decays into muons or electrons plus pions [5] .  These are required inputs for the calculation of the branching fractions [6] and CP asymmetries [7, 8] of these processes.  In addition, we provide the LO contribution to the pion form factor [9] .\nOur approach uses the method developed by M. Neubert [10]  which exploits the fact that the relevant loop integrals can all be expressed as linear combinations of products of two-point scalar functions.  This allows us to reduce the number of independent integrations needed to evaluate each diagram to three.  Our implementation also includes several improvements over previous calculations [11, 12] :   1)  We use the exact expression for the propagators of massive fermions instead...","label":1,"model":"bloomz","source":"arxiv","id":3449}
{"text":"In this work, we study the distribution of the size of largest planar matching (LPM) and largest planar subgraph (LPS) in random bipartite graphs with n vertices on each side. We show that for any fixed integer k > 0, there exists an absolute constant ck such that if G is a random bipartite graph with n vertices on each part satisfying cn < p = e(G)\/n < 1 \u2212 cn then LPM(G) has at least k components w.h.p.. Moreover, when p = 1\/2 + o(1), we prove that LPS(G) contains a cycle of length \u2126(log log n). These results are proved by using the second moment method together with some new techniques developed here to deal with the dependencies between edges. As applications, these results imply that both LPM(G) and LPS(G) have many small cycles under certain conditions.","label":1,"model":"bloomz","source":"arxiv","id":3450}
{"text":"We present an analysis of the most massive cluster known to be part of the Great Attractor, based on new spectroscopic data obtained with VLT\/VIMOS and archival Chandra X-ray Observatory observations. The main results are as follows:  We find that the galaxy distribution is well described by a single NFW profile out to at least 1 Mpc\/h.  Using this mass model we derive a virial mass for the cluster of Mvir = 2.1 x 1015M\u2299\/h70 within R200 = 0.9 h-1 70 Mpc\/h. This value agrees very well with previous estimates derived using different techniques. We also show that the velocity dispersion profile can be fitted with a simple \u03b2-model up to r ~ 0.7R200. Beyond this radius there seems to be evidence for a steepening of the profile which could indicate the presence of substructures or infalling groups along the line-of-sight.","label":1,"model":"bloomz","source":"arxiv","id":3451}
{"text":"We report on the observation and characterization of two-dimensional defect modes in optically-induced photonic crystals (OIPCs). The OIPC is formed by periodic modulation of refractive index using femtosecond laser pulses focused into fused silica glass. We show that the defect mode can be tuned over a wide range of wavelengths, which are determined by the periodicity of the lattice structure as well as the size of the defects. This work opens up new possibilities for designing optical devices based on these structures. \n \n Photonic crystal slabs have attracted considerable attention recently because they provide an excellent platform to study light-matter interactions at the nanoscale [1] . In particular, it has been shown that three-dimensional photonic crystals with point or line defects exhibit localized states within their bandgap [2] , leading to many interesting applications such as lasers [3] , filters [4] , sensors [5] , nonlinear optics [6] , etc.. However, fabrication of three-dimensional photonic crystals requires sophisticated techniques [7, 8] , making them difficult to integrate with other micro\/nano-structures. Recently, several groups have demonstrated two-dimensional photonic crystals [9] - [11] fabricated directly inside transparent materials via direct laser writing [12] - [14] . These 2D photonic crystals offer advantages including ease of fabrication, flexibility in design, and compatibility with existing technologies [15] .\nIn this Letter we demonstrate the formation of defect modes in opticallyinduced photonic crystals (OPC) [16] . The OPC consists of periodically modulated refractive index created by focusing femtosecond laser pulses into fused silica glass [17] . By introducing defects into the lattice structure, we observe localized defect modes within the stopband of the OPC. Furthermore, we show that the defect mode wavelength can be continuously tuned across the entire stopband simply by changing the lattice spacing and\/or the size of the defects. \nThe experimental setup used to create the OPC is illustrated schematically in Fig. 1(a) . A Ti:Sapphire regenerative amplifier system operating at 800 nm was employed to generate 100 fs duration pulses at a repetition rate of 1 kHz. The beam diameter after passing through a spatial filter","label":1,"model":"bloomz","source":"arxiv","id":3452}
{"text":"We present an analysis of the UKIRT Infrared Deep Sky Survey (UKIDSS) Galactic Cluster Survey data for the open cluster, Pleiades. We use this to derive the number ratio between binaries and single stars in the range 0.1 < M\/M\u2299 < 1.0 as well as the initial mass function (IMF). The results are compared against previous studies using different techniques. Our derived binary fraction is consistent within uncertainties with that found by other authors but our IMF shows significant differences when compared to previous work. These discrepancies may be due to contamination from background galaxies or unresolved multiple systems which have been included in previous analyses. This study highlights the importance of accurate photometry over large areas combined with high resolution spectroscopy to fully understand the properties of young open clusters such as the Pleiades. Keywords: Open cluster; Binary star systems; Initial Mass Function; Substellar object","label":1,"model":"bloomz","source":"arxiv","id":3453}
{"text":"We propose an efficient numerical scheme to solve the nonlinear dynamics of semiconductor microcavity lasers with arbitrary pumping profiles and cavity losses, which is based on the combination of two different truncation schemes. The first one is used to reduce the number of equations by eliminating all but those that are relevant at any given time instant; this allows us to obtain accurate results even when only few modes contribute significantly to the total emission spectrum. The second one is applied to eliminate the fast oscillating terms appearing due to the presence of multiple longitudinal modes within each transverse mode family. We show how these two techniques can be combined into a single algorithm, which we call \"dynamics-controlled truncation\" (DCT). Finally, we demonstrate the accuracy and efficiency of our method by comparing it against other existing methods. In particular, we consider three different types of pumping profiles: constant, periodic, and random pulsed pumping. \nI. INTRODU CTION\nSemiconductor microcavity lasers' attract considerable attention because they provide a promising route towards low-threshold laser sources [1] . However, their complex multimode nature makes them difficult to model numerically [2] , especially if the pumping profile or the cavity loss varies over time [3] .\nIn order to overcome such difficulties, several authors have proposed various approaches [4] - [8] . For example, in Ref. [6] , the authors use a reduced set of rate equations to describe the evolution of the slowly varying amplitudes of the dominant modes. This approach has been extended recently to include higher-order effects [7] as well as nonuniform gain saturation [9] . Another possibility consists in using truncated Fourier series expansions [10] , where the coefficients of the expansion are determined self-consistently [11] . Alternatively, one may also employ direct integration of Maxwell's equations [12] , although this requires very large computational resources [13] .","label":1,"model":"bloomz","source":"arxiv","id":3454}
{"text":"We present an efficient distributed algorithm for implementing the most common query operations in relational databases, namely selection and projection. The proposed approach is based on the concept of data partitioning which allows us to distribute the workload among different nodes in order to minimize communication costs between them. We show that our method can be used as a building block for more complex queries such as joins or aggregates. Finally we provide experimental results showing how our solution outperforms existing approaches by orders of magnitude when dealing with large datasets. Relational database management systems (RDBMS) are widely used nowadays due to their ability to store and manage huge amounts of information efficiently. However, this efficiency comes at the cost of high computational complexity since all the data has to reside in main memory during processing time. This problem becomes even worse if one wants to perform ad-hoc analysis over very large volumes of data stored in RDBMSs. In fact, it may take hours or days just to answer simple questions like \"Which customers bought product X?\" or \"What products were sold together last year?\". To overcome these limitations, several research efforts have been made recently towards developing new techniques allowing users to access and analyze massive amounts of data without having to load everything into main memory simultaneously. These solutions usually rely on parallel computing frameworks where each node stores only part of the whole dataset while performing local computations independently from other nodes.","label":1,"model":"bloomz","source":"arxiv","id":3455}
{"text":"We present new high resolution (R = \u03bb\/\u0394\u03bb ~ 20,000) far-ultraviolet spectra obtained with the Far Ultraviolet Spectroscopic Explorer (FUSE), as well as archival Hubble Space Telescope (HST) data for the hot white dwarf central star in the planetary nebula Sh2-216. The FUSE spectrum shows numerous absorption lines due to highly ionized species such as C IV, N V, O VI, Ne VIII, Mg X, Si XII, S XIV, Ar XVI, Fe XIX, and Ni XXVIII. We have modeled these features using synthetic line profiles generated by the non-LTE model atmosphere code TLUSTY\/SYNSPEC. Our best-fit models indicate that this star has an effective temperature T eff = 120,000 K, surface gravity log g = 8.0, mass M = 0.6M\u2609 , radius R = 0.01R\u2609 , and is surrounded by a shell of material with density n(He II)\/n(He I) = 1.5 x 10-3 .","label":1,"model":"bloomz","source":"arxiv","id":3456}
{"text":"Fullerenes are spherical molecules composed entirely of sp2-hybridized carbons, which can be arranged in different ways to form various allotropes with unique properties. The most common is the C60 molecule (buckminsterfullerene), but other forms such as nanotubes or graphene have also been reported. Fullerenes were first discovered by Kroto et al., who received the Nobel Prize in Chemistry in 1996 for their work on them.  In this study we report on the growth mechanism and structural characterization of novel carbon-based nanomaterials that consist of an amorphous network of interconnected nanoparticles resembling fullerenes. These structures grow spontaneously when evaporating carbon atoms onto silicon substrates at temperatures between 600-700 \u00b0C under ultrahigh vacuum conditions. We show how these new carbon nanostructures exhibit remarkable mechanical properties including high elasticity and hardness combined with low friction coefficients making them ideal candidates for use as protective coatings against wear and corrosion.","label":1,"model":"bloomz","source":"arxiv","id":3457}
{"text":"We study the non-equilibrium evolution of an interacting onedimensional Bose gas initially prepared in a coherent state with finite particle number fluctuations and show that it exhibits universal features, which are independent of microscopic details such as interactions or initial conditions. We find that the system evolves into a stationary state characterized by non-vanishing density-density correlations at all distances. The time dependence of these correlations is governed by a single parameter, which we identify with the inverse temperature of the final equilibrium state. This allows us to determine this temperature directly from experimental data without any fitting parameters. Our results provide new insights into the nonequilibrium physics of quantum many-body systems and may be tested experimentally using ultracold atoms trapped in optical lattices. \nI. INTRODUCTORY REMARK\nThe recent development of techniques for trapping and manipulating cold atomic gases has opened up exciting possibilities for studying strongly correlated quantum matter far from thermal equilibrium [1] . In particular, experiments have demonstrated how isolated quantum systems can evolve towards their ground states [2] , while being driven out of equilibrium by sudden changes in external control parameters [3] .\nIn this work, we consider the case where the system is suddenly quenched across a phase transition [4] . For example, if the particles were originally confined to a harmonic trap, they would expand freely after switching off the confining potential [5] . Alternatively, the system could be initialized in its ground state [6] before undergoing a rapid change in some other parameter (e.g., magnetic field) [7, 8] . In both cases, the subsequent relaxation process will depend crucially on whether the system was initially prepared close to equilibrium [9] or not [10] . If the latter situation applies, then the system typically relaxes towards a metastable state [11] whose properties cannot be inferred from those of the original equilibrium ensemble [12] .","label":1,"model":"bloomz","source":"arxiv","id":3458}
{"text":"We prove that the only actions by groups with Kazhdan's property (T) on Gizatullin surface are trivial, i.e., they are either free or fixed point free.  This is joint work with A. Lando. We also show that there exists an infinite family of non-isomorphic actions of $SL(2,R)$ on Gizatullin threefolds which have no global fixed points but do not admit any equivariant complex structure. These results were announced in our preprint [BL] . \nIntroduction\n\nLet $X$ be a smooth projective variety over $\\mathbb{C}$ equipped with an action of a group $G$ such that the induced map $G\\times X\\to X$ is holomorphic. The action is called algebraic if it lifts to an action of some connected reductive algebraic group $H$ defined over $\\mathbb{Q}$. In this case we say that $X$ admits an $H$-structure. \nThe main goal of this note is to study algebraic actions of groups with Kazhdan\u2019s property (T). Recall that a locally compact topological group has Kazhdan\u2019s property T if every unitary representation of $G$ which does not contain almost invariant vectors is finite dimensional. Examples include all simple Lie groups without center and many other interesting classes of groups including lattices in higher rank semisimple Lie groups. \n\nKazhdan\u2019s property T was introduced by M. Gromov [Gro81] who used it to establish rigidity properties for Riemannian manifolds with negative sectional curvature. Since then it turned out to play important role in various branches of mathematics ranging from geometry to number theory. For example, one can find applications of property T in the works of J.-P. Serre [S89] , D. Toledo [T92] ,  S. Adams [A94]  and others.   \nIn particular, property T plays crucial role in classification problems related to algebraic actions. Indeed, let us consider two algebraic actions of a group $G$ on varieties $X$ and $Y$ respectively. If both actions lift to actions of some connected reductive groups $H_X$ and $H_Y$ then the corresponding","label":1,"model":"bloomz","source":"arxiv","id":3459}
{"text":"The vicinal Si(111) surface is the most studied example for reconstructed semiconductor surfaces, and it has been shown that its structure can be described by a periodic array of steps with different heights. The main goal of this work was to study how these steps evolve into facets when they are submitted to an external stress field produced by a miscut angle between 0\u00b0 and 2\u00b0. We have used scanning tunneling microscopy (STM), low energy electron diffraction (LEED), reflection high energy electron diffraction (RHEED), atomic force microscopy (AFM), and molecular dynamics simulations in order to understand the evolution of the stepped surface towards a more ordered one. Our results show that at small angles there exists only one type of terrace widths which correspond to the equilibrium value predicted by Wulff's construction. At larger angles we observe two types of terraces whose widths depend on their position along the direction perpendicular to the steps. Finally, at large enough angles, the system evolves toward a flat surface where all the terraces present similar dimensions.","label":1,"model":"bloomz","source":"arxiv","id":3460}
{"text":"We present an algorithm for consistent hypothesis testing in which we consider all possible hypotheses that are compatible with some given set of observations, and select those that maximize their posterior probability according to Bayes' theorem.  We show how this can be done efficiently by using dynamic programming techniques. The resulting algorithm is optimal up to constant factors under certain conditions. Our approach also allows us to reason consistently over multiple experiments performed sequentially or simultaneously. This problem has been studied extensively in statistics but only recently in artificial intelligence (AI). In AI it was first considered as part of the PAC learning framework where one seeks algorithms that learn concepts from examples while making few mistakes. However, these approaches do not provide any guarantees when there exists more than one concept that fits the data equally well. In contrast our method provides provable guarantees even if several hypotheses fit the data equally well. Finally, we demonstrate the practicality of our approach through two applications:  1) A new algorithm for finding explanations in probabilistic databases; 2) An improved algorithm for identifying protein families based on sequence alignment.","label":1,"model":"bloomz","source":"arxiv","id":3461}
{"text":"We present new near-infrared observations of the edge-on spiral galaxy NGC 891, obtained with VLT\/VISIR and Gemini\/NIRI at wavelengths between 8 and 13 microns (rest-frame). We detect an extended emission feature that is perpendicular to the galactic disk and extends up to 3 kpc into the halo. The morphology suggests that this structure consists of two components: one located close to the midplane and another one located about 1.5 kpc higher than it. This second component has a temperature of T = 120 K \u00b1 20 K and shows no evidence for significant extinction by dust grains along its line-of-sight. Its luminosity corresponds to a star formation rate of 0.1 M\u2299 yr\u22121. These results are consistent with previous studies suggesting that there exists a population of young stars outside the main body of galaxies. They also provide further support for models where supernovae explosions trigger large-scale outflows of gas and dust which can be responsible for the enrichment of intergalactic space.","label":1,"model":"bloomz","source":"arxiv","id":3462}
{"text":"We present new high-precision photometric observations in BVRI filters obtained with the 1 m telescope at Mt. Wilson Observatory, California, USA during two nights in August 2005. The data were reduced using standard procedures for aperture photometry. We also used previously published spectroscopic data to determine orbital parameters of this system. \n \n In addition we have performed detailed analysis of all available photometric light curves of V471 Tau covering more than 50 years. Our results show that there is no significant change in the period over time which indicates very low mass transfer rate between components. This conclusion agrees well with our previous estimates based on radial velocity measurements. \n \n We found that the observed changes in the shape of the eclipse profiles are caused by ellipsoidal deformation of the secondary component due to tidal forces acting upon it as a result of its close proximity to the primary white dwarf star. Using these results together with those derived from the spectral disentangling technique applied to the composite spectrum of the system we determined masses of both stars - 0.6 M\u2609 for the white dwarf and 0.3 M\u2609 for the red dwarf companion.","label":1,"model":"bloomz","source":"arxiv","id":3463}
{"text":"We present an analytical model for the longitudinal impedance and wakefield generated by a single period of an XFEL undulator, including effects due to electron beam emittance and energy spread. We show that these effects can be significant in some cases, especially when considering schemes where the electron bunch is short compared with the wavelength (e.g., self-amplified spontaneous emission). The results are used to assess the impact of this effect on two proposed schemes at LCLS-II. In one scheme, we consider using a tapered wiggler as part of a chicane-based compressor system; in another case, we examine the use of a magnetic chicago-bars section following the undulator. For both cases, we find that the inclusion of realistic impedance and wakefields leads to modest changes in the predicted performance. \n \n Keywords: Undulator, Wakefield, Impedance, Self-Amplified Spontaneous Emission","label":1,"model":"bloomz","source":"arxiv","id":3464}
{"text":"The book is available at the following URL: http:\/\/arxiv.org\/abs\/gr-qc\/0405033 The book contains an introduction to numerical relativity, including basic concepts and techniques in general relativity as well as some advanced topics such as black hole excision methods.  It also includes detailed descriptions on how to implement these algorithms using spectral methods (Fourier or Chebyshev expansions). This book will be useful not only for graduate students but also for researchers who are interested in applying spectral methods to solve problems in astrophysics. Authors:  Yi-Kai Shu, Jian-Xin Liu, Zhi-Hong Zhang, Yu-Ting Wu, Xiao-Liang Zhao, Wen-Jun Ma, Hong-Yuan Chen, Jie-Qiu Shi, Jun-Zhuo Wang, Yong-Sheng Zhou, Ming-Dao Li, Qing-Wei Guo, Xin-Bin Huang","label":1,"model":"bloomz","source":"arxiv","id":3465}
{"text":"We present new observations and analysis of the neutral hydrogen (HI) disks surrounding isolated galaxies, using data obtained with the Very Large Array (VLA). We have observed 12 nearby galaxies at 21 cm wavelength to determine their total HI mass and distribution within the optical disk. The sample includes both late-type spirals and dwarf irregulars. Our results show that all but one galaxy has an extended HI halo beyond its optical radius; this is true even for the most gas-rich systems such as NGC 4254 and NGC 5253. In addition, we find evidence for significant amounts of atomic gas outside our detection limits which may be associated with tidal features or other interactions between these galaxies and neighboring companions. These findings are consistent with previous studies showing that many isolated spiral galaxies contain large quantities of cold interstellar medium distributed over several kiloparsecs. \n \n Keywords: Atomic Hydrogen, Galaxy Evolution, Spiral Structure","label":1,"model":"bloomz","source":"arxiv","id":3466}
{"text":"We report on the X-ray properties of the young, nearby (d = 11 pc), low-mass binary system 2MASS J1101-2677AB discovered by Burgasser et al. (2007) . The primary component is an M8 dwarf with T eff ~ 2600 K and log g ~ 5.0 while its companion has been classified as an L5 brown dwarf with T eff ~ 1400K and log g ~ 4.2. We observed this target for about 50 ks using Chandra ACIS-S in order to search for evidence of coronal activity associated with either or both components. No significant emission was detected at the position of the source down to a 3-sigma upper limit of 1 x 10^28 erg s-1 cm-2 . This non-detection suggests that if there are active regions present they must be small and\/or cool compared to those found on more massive stars. In addition we find no evidence of flaring behavior during our observation which constrains any possible magnetic field strength to less than 100 G.","label":1,"model":"bloomz","source":"arxiv","id":3467}
{"text":"We study the dynamics of carrier-carrier interactions in semiconductor quantum dots (QDs) by solving numerically the time-dependent Schr\u00f6dinger equation for two interacting electrons or holes confined to an anisotropic QD potential well. We find that, depending on the initial state, there are three different regimes of interaction between carriers which can be classified as weak coupling regime with no significant energy exchange; strong coupling regime where one electron is excited into higher states while another remains in its ground state; and finally, intermediate regime where both carriers undergo transitions simultaneously but at slightly different frequencies. In addition we show how these results depend on the dot shape and size parameters. Finally, we discuss possible applications of our findings such as generation of entangled photon pairs via biexciton decay. Quantum dots have been studied extensively over past decade due to their unique optical properties [1] . The most important feature of QDs is the possibility of controlling their emission wavelength through variation of their size [2] , allowing them to operate within a wide range of wavelengths [3] .\nIn this work we focus on studying the effects of carrier-carrier interactions [4] in semiconductor QDs using numerical solution of timedependent Schr\u00f6dinger equations [5] . Carriers interact strongly when they occupy neighboring single-particle levels [6] leading to formation of bound excitonic complexes [7, 8] . However, if carriers occupy distant single particle levels then their mutual Coulomb attraction leads to formation of virtual excitons [9] . These virtual excitons may either recombine radiatively [10] or non-radiatively [11] giving rise to Auger processes [12] . On the other hand, if carriers occupy adjacent single particle levels then their interaction becomes so strong that it cannot be treated perturbatively anymore [13] . This situation occurs e.g., during relaxation of photoexcited carriers [14] or in presence of external electric field [15] .","label":1,"model":"bloomz","source":"arxiv","id":3468}
{"text":"In this article we study hypersurface immersions into hyperbolic space, i.e., solutions to the equation $F(x) = 0$ where $F$ is an immersion function defined on some open subset $U$ of $\\mathbb{R}^n$ with values in $H^{n+1}$ (the upper half-space model). We prove that if such an immersion exists then it must be unique up to rigid motions of $H^{n+1}$. In particular, any two solutions are related by a M\u00f6bius transformation.  This result generalizes previous results for surfaces immersed in three-dimensional hyperbolic space due to Chern [Ch] , Bryant [Bry1] , and others. The proof relies heavily upon the work of Chern and Osserman [CO] . \nWe also consider the problem of finding all solutions to the equation $|F|^{2} = 1$ when $F$ takes its values in $H^{n+1}\\setminus \\{0\\}$. Here again our main tool will be the theory developed by Chern and Osserman. Finally, we discuss the relationship between these two problems. \nThe author would like to thank Professor Richard S. Hamilton for his encouragement throughout the preparation of this manuscript.","label":1,"model":"bloomz","source":"arxiv","id":3469}
{"text":"The study is performed in the framework of relativistic quark model with one-gluon exchange potential (OGE). The mass spectrum, decay widths and branching ratios are calculated for all possible transitions among  $\\Upsilon(3S), \\Upsilon(2S),$ and $ \\Upsilon(1S)$ states. It has been found that the results obtained by us agree well with experimental data.  In particular, we have predicted some new decay channels which can be tested experimentally. We also predict the existence of two new excited states at about 3 MeV above the ground state. These predictions may provide useful information to search for these exotic states. Finally, we give our conclusions. Introduction: Recently there were many theoretical studies on the lightest member of the nonet of scalar mesons, i.e., the $\\Upsilon$-meson system [1] . This interest was stimulated mainly due to the fact that this meson is considered as an ideal testing ground for studying various aspects of QCD such as confinement mechanism [2] , spontaneous chiral symmetry breaking [3] , gluonic excitations [4] etc.. On the other hand, it is known that the $\\Upsilon$-system contains several interesting features like mixing between singlet and octet components [5] , strong coupling to vector mesons [6] , large Nc expansion [7] , etc..\nIn order to understand better the nature of the $\\Upsilon$-state, it would be very important to know its properties more precisely. However, up to now only few experiments [8] - [10] have been done to measure the masses and partial widths of the $\\Upsilon$scalar mesons. Therefore, further investigations both theoretically and experimentally should be carried out to get more precise values of their physical quantities.","label":1,"model":"bloomz","source":"arxiv","id":3470}
{"text":"We study the geometry of regular shear-free null geodesic congruences in Minkowski space-time with signature (-+++). We show that such congruences are determined by a pair of real valued functions $(f_1, f_2)$ on the unit sphere $S^2$ satisfying certain conditions which we call the shear-free condition. The first function $f_1$ is called the optical scalar function while the second one $f_2$ is called the twist potential. In this setting, we define the complex valued CR-function associated to any given regular shear-free null-geodesic congruence as follows: \n$$F = F_1 + iF_2 = \\frac{1}{2}(f_1 - if_2),$$ where $F_i$'s are the components of $F$ along the two principal null directions of the congruence. Then we prove that for every regular shear-free nullgeodesic congruence there exists an unique upto constant multiple CR-function. Finally, using the above results, we give a new proof of the flat-space Maxwell equations.","label":1,"model":"bloomz","source":"arxiv","id":3471}
{"text":"We report on our photometric and spectroscopic monitoring campaign of the young open cluster NGC 2024 (the Orion Nebula Cluster) aimed at detecting new low-mass members with masses down to ~0.1 Msun. We have discovered one such object which we call JW 380. It is an eclipsing binary system consisting of two very similar stars that are separated by only 1 AU. The primary star has a mass of about 0.25 Msun while its companion has a mass of about 15 percent less. Both components show signs of youth as indicated by their strong H-alpha emission lines. Their age was determined using theoretical evolutionary tracks for PMS stars. Our results suggest that this system is younger than 2 Myr old. This makes it one of the youngest known binaries among all open clusters within 300 pc distance from Earth. In addition, we find evidence that both components are surrounded by circumstellar disks.","label":1,"model":"bloomz","source":"arxiv","id":3472}
{"text":"We present an algorithm for the detection and characterization of unique molecular events in time series data, which is based on identifying all possible pairs of states that can be reached by following different paths through the state space. The method was applied to simulated as well as experimental single-molecule FRET traces recorded during DNA replication at high temporal resolution (10 ms). We show how this approach allows us to detect and characterize rare but important transitions between distinct conformational states of the replisome machinery. In particular we identify a previously unknown transition mechanism where the helicase switches its directionality while still bound to the fork junction. This new insight into the dynamics of the replisome will help to understand how it operates under physiological conditions. \n \n Introduction \n \n Single molecule experiments have become increasingly popular over recent years because they allow one to study processes such as protein folding or enzymatic reactions with unprecedented detail1-5 . However, extracting information about these complex systems often requires advanced analysis techniques6-8 , especially when dealing with noisy data9-11 . Here we introduce a novel computational framework for analyzing time-series data obtained from single molecule experiments12-15 . Our approach relies on detecting all possible pairs of states within a given trajectory that are connected via alternative pathways16-18 . These so-called \"state pairs\" represent unique molecular events19-21 that occur rarely22-24 but may play crucial roles in determining system behavior25-27 .\n \n State Pair Analysis\n\nThe basic idea behind our approach is illustrated in Figure 1 . Consider a hypothetical example consisting of three consecutive states s1, s2, s3 along a single trajectory. If there exists another pathway connecting s2 and s3 than the one shown here, then both states belong to the same state pair. Note that each state has several outgoing edges corresponding to multiple possible transitions out of that state. For instance, if the system starts in state s1, it could either stay in s1 or move directly to s2 after some delay. Similarly, starting in s2 would lead to either staying in s2 or moving to s3 immediately afterwards. Finally, starting in s3 would always result in returning back to s1. As a consequence, any","label":1,"model":"bloomz","source":"arxiv","id":3473}
{"text":"We present here the predictions for the decay rates and CP asymmetries in B decays into two vector mesons, based on the assumption that there is no direct coupling between quarks and leptons at low energies.  We show how this hypothesis leads to relations among different observables which are not predicted by the Standard Model (SM). These relations can be tested experimentally with high precision using data collected at LHCb or Belle II experiments. \nThe results presented here have been obtained within an effective field theory framework where we assume that all new physics effects appear only through higher dimensional operators suppressed by inverse powers of some large scale M . The leading order contributions to these operators come from integrating out heavy degrees of freedom such as W , Z bosons and top quark. In our analysis we consider both tree-level and loop-induced processes. Our main focus has been put on the study of rare B decays involving one photon and one lepton pair in the final state.","label":1,"model":"bloomz","source":"arxiv","id":3474}
{"text":"MadGraph v4 is the new generation of MadGraph, which has been developed by the authors in collaboration with other groups at Fermilab and SLAC over the past few years.  It includes several major improvements compared to previous versions that will be described below.   In addition it now also contains an interface for event generation using Pythia 6 or Herwig++ as well as interfaces to shower Monte Carlo programs such as PYTHIA 8 and HERWIG 7. This talk will give an overview on how this program works and what its main features are. For more details see http:\/\/madgraph.hepforge.org\/. Keywords: Event Generation, LHC Physics, Matrix Elements, Parton Showering, NLO QCD, Hadronization. Speaker: Andreas Schienbein (Fermilab); Title: MadGraph\/MadEvent v4; The New Web Generation. Abstract:","label":1,"model":"bloomz","source":"arxiv","id":3475}
{"text":"We present new deep photometry for the remote open cluster B514, located at about 1 degree (0.5 kpc) south-east of the center of the Andromeda galaxy (M31). The data were obtained with the Wide Field Camera 3 on board HST and cover an area of 0.6 square degrees centered around the cluster position. We detect more than 100 candidate members brighter than V = 25 mag within a radius of 2 arcmin from the cluster center. These are likely to be associated with the cluster itself rather than background galaxies or foreground Galactic dwarfs because they have colors consistent with those expected for old red giant branch stars. In addition we find that there is a significant number of faint blue objects surrounding the cluster which may belong to it as well. Using these candidates together with previous ground-based observations we construct color-magnitude diagrams for different annuli extending out to 5 arcmin from the cluster centre. From this analysis we conclude that the cluster has an extended halo containing several hundred low mass stars beyond its tidal radius.","label":1,"model":"bloomz","source":"arxiv","id":3476}
{"text":"We analyze the non-Gaussianity in the temperature fluctuations of the Wilkinson Microwave Anisotropy Probe (WMAP) first year sky maps using three different estimators, namely Minkowski Functionals (MF), genus curve and correlation functions. We find that all these estimators show significant deviations from Gaussian predictions at large angular scales. The observed deviation is consistent with theoretical expectations for topological defects like cosmic strings or textures which are predicted to be present during inflationary phase transitions. These results provide further support for the presence of primordial non-Gaussianities in CMB anisotropies. This work was supported by the Department of Atomic Energy under grant No. 06(B)\/ST-IISc\/04. The authors thank Sourav Chatterjee for useful discussions. PACS numbers: 98.80.Cq, 95.35.+d, 04.20.Jb  Keywords: Cosmic microwave background radiation","label":1,"model":"bloomz","source":"arxiv","id":3477}
{"text":"Weak gravitational lensing is an important tool to study dark matter halos in galaxy clusters, but it requires large telescopes or space-based observatories with high angular resolution cameras. In this work we use Chandra data to measure the gas mass fraction (fgas) profiles of two massive galaxy clusters selected by their strong shear signal using HST\/ACS images. We compare these fgas measurements with those derived from weak-lensing analysis performed on Subaru\/Suprime-Cam imaging data. The comparison shows that both methods agree well within the statistical uncertainties at radii larger than 0.5 r500. At smaller radii there are significant differences between the results obtained with different techniques. These discrepancies may be caused by systematic effects associated with each method and\/or by intrinsic scatter among individual clusters. This project was supported by NASA grant NNX10AD65G. We thank J. Richard McInnes for providing us with his software package for fitting the surface brightness profile of galaxy clusters. \n \n Keywords: galaxy cluster, Chandra, weak lensing","label":1,"model":"bloomz","source":"arxiv","id":3478}
{"text":"The Min system is responsible for the correct positioning of the Z ring and division site at mid-cell during bacterial cell division.  The Min system consists of three components, MinC, MinD, and MinE that oscillate back-and-forth across the cytoplasm to create an inhibitory gradient along the length of the cell.  This gradient prevents assembly of new FtsZ rings outside of mid-cell until all existing FtsZ rings have disassembled.   In this study we use computational modeling techniques to investigate how the Min system partitions its components into two daughter cells following cytokinesis.  We find that the Min system can be divided into four distinct phases based on the distribution of Min protein complexes within each cell.  During phase I (0-60 s), MinD-E2-MinC complexes are distributed throughout both mother and daughter cells with higher concentrations near the poles than at mid-cell.  Phase II (60-120 s) begins when MinD-E2-MinCs accumulate at mid-cell forming a band-like structure.  During phase III (120-240 s), MinD-E2-","label":1,"model":"bloomz","source":"arxiv","id":3479}
{"text":"We study the charm quark fragmentation into hadrons in perturbative QCD by using an effective coupling constant, which is determined so as to reproduce the experimental data on the total cross section for e+e-annihilation into hadrons at high energies.  We find that our results are consistent with those obtained within the framework of the conventional parton model and also with recent measurements performed by CLEO collaboration. The present analysis suggests that the charm quark fragmentation function can be well described by the Peterson form factor multiplied by a simple exponential function. \nPACS numbers: 11.10.Kk, 12.38.Qk, 13 .60.Hb \nI. INTRODUCTORY REMAR K\nIn this work we investigate the charm quark fragmentation functions (FFs) into light hadrons in perturbative quantum chromodynamics(pQCD). In particular, we use an effective coupling constant, \u03b1eff , which is determined so as t o reproduce the experimental data on thetotal cross section for e + e-annihilation into hadronsthat have been measured recentlybyCLEOcollaboration [1] .\nThe FFsof quarksintohadronicparticlesare important quantitiesin pQCDand they playanimportant role inthe calculationof various physical observables suchas the structurefunctions [2] ,the Drell-Yanprocess [3] ,and the semi-leptonic decay [4] . It hasbeen shownthat the FFsof quarksintohadronscan be calculatedperturbatively [5] - [8] . However, it shouldbe notedthatthe calculationsrequire veryhighenergy scales [9] . Therefore,it would be usefulto determine\u03b1efffromexperimentaldataat relativelylowenergies [10] .","label":1,"model":"bloomz","source":"arxiv","id":3480}
{"text":"We present the first fully general-relativistic magnetohydrodynamic (MHD) simulations of the gravitational collapse of rotating, magnetic white dwarfs and neutron stars with initial masses up to 100 solar masses. We find that for all models considered here, the central density increases by at least eight orders of magnitude during the collapse process. The final state is always a Kerr black hole surrounded by an accretion disk. For most cases we consider, the mass of the black hole exceeds the Chandrasekhar limit by more than 10%. This suggests that there may be no maximum stable mass for nonrotating stellar cores. In addition, our results show that the rotation rate of the newly formed black hole depends on its progenitor's spin parameter as well as its total angular momentum. Finally, we discuss how these findings can be used to explain some observed phenomena such as gamma-ray bursts and superluminous supernovae.","label":1,"model":"bloomz","source":"arxiv","id":3481}
{"text":"We present new near-infrared observations and analysis of the super star cluster (SSC) in the interacting galaxy pair NGC 1705, which is located at a distance of ~10 Mpc. The SSC has an age of ~30 Myr and contains several thousand massive stars with masses >20M\u2609 . We find that it exhibits many properties similar to those observed for young stellar clusters associated with gamma-ray bursts (GRBs). In particular, we detect a bright Wolf-Rayet population as well as evidence for ongoing mass loss via winds driven by evolved red supergiants. These results suggest that this system may be a local analogue to GRB progenitors. This work was supported by NASA grant NNX11AI18G issued through the Astrophysics Data Analysis Program. The authors wish to recognize and acknowledge the very significant cultural role and reverence that the summit of Mauna Kea has always had within the indigenous Hawaiian community. We are most fortunate to have the opportunity to conduct observations from this mountain.","label":1,"model":"bloomz","source":"arxiv","id":3482}
{"text":"We introduce the concept of chain motifs, which are subgraphs that occur frequently in real-world networks but rarely or never in random graphs with similar degree distributions. We show how to use these motifs as building blocks for generating synthetic networks whose properties closely match those of their natural counterparts. Finally, we demonstrate how this approach can be used to study the evolution of biological networks by comparing two versions of the same metabolic pathway taken from different species. In recent years there has been an explosion in interest in studying large collections of interacting entities known as complex networks. These include social networks such as Facebook, Twitter, and LinkedIn; information networks like the World Wide Web (WWW); communication networks such as phone calls; transportation networks including roadways and railways; and many others. A common feature shared among all of these systems is that they consist of nodes connected together via links. For example, in a social network each node represents an individual user while edges represent friendships between users. Similarly, in a WWW graph web pages correspond to nodes and hyperlinks connect them. Despite the diversity of applications, most studies have focused on characterizing global topological features of these networks using statistical measures such as average path length, clustering coefficient, assortativity coefficients, etc., see e.g. \n \n However, it turns out that even though these statistics provide useful insights into large-scale structural patterns observed across diverse classes of networks, they fail to capture important local structures present within specific types of networks. This motivates us to develop new techniques capable of identifying small-scale patterns that may not be captured by traditional approaches. Herein lies one of our main contributions - we propose a novel class of subgraphs called chain motifs that occur frequently in real world networks but rarely or never appear in randomly generated graphs with similar degree distribution. Using these motifs as building blocks, we construct synthetic networks whose properties closely resemble those of their natural counterparts. \n \n Chain Motifs \n \n Let G = (V , E ) denote a simple undirected graph where V denotes the set of vertices and E denotes the set of edges. Given any pair of","label":1,"model":"bloomz","source":"arxiv","id":3483}
{"text":"The theoretical investigation is performed for the ferroelectric phase transition in potassium nitrate (KNO3). The results are obtained by using density functional theory and generalized gradient approximation with Perdew-Burke-Ernzerhof exchange-correlation functionals. It has been found that KNO3 undergoes an improper ferroelectric phase transition at T = 723 K, which is accompanied by the rotation of NO3-groups around their symmetry axes. In addition to this structural change, there occurs also a significant redistribution of charge between atoms. This leads to changes in the electronic structure near Fermi level. The calculated values of spontaneous polarization Psp(0) = 0.27 C\/m2 and dielectric constant \u03b5s = 4.5 agree well with experimental data. \n \n Keywords: Ferroelectrics; Potassium nitrite; Phase transitions; Density functional theory. 1 Introduction Potassium nitrate (KNO3), one of the most important chemical compounds used as fertilizers [1] , exhibits interesting physical properties such as piezo-, pyro-, electro-optic effects [2] . At room temperature it crystallizes into orthorhombic system [3] . Below its Curie point Tc = 723 K [4] , KNO3 behaves like paraelectric material while above Tc it becomes ferroelectric [5] .\n2 Computational details All calculations were carried out within the framework of density functional theory [6] employing plane wave basis set and projector augmented-wave method [7, 8] implemented in VASP code [9] . Exchange correlation energy was treated within generalized gradient approximation [10] . To account for van der Waals interactions we have applied Grimme's semiempirical dispersion correction [11] . We considered two different supercells containing 64 and 216 atoms respectively. For both cells we chose Monkhorst-Pack k-point mesh [12] corresponding to 6\u00d76\u00d74 grid in reciprocal space. Energy cutoff for planewave expansion was chosen equal to 400 eV. Structure optimization was done until all forces acting on each atom became less than 10-3 eV\/\u00c5.","label":1,"model":"bloomz","source":"arxiv","id":3484}
{"text":"We present new calculations of line opacities in the infrared (IR) for ultracool dwarfs, brown dwarfs, and extrasolar planets. We use these results to calculate mean opacities at temperatures ranging from 1000 K to 3000 K using the correlated-k method with opacity sampling. The resulting opacities are used as input into radiative-convective equilibrium models that we compare against observations of T dwarf stars. Our calculated opacities agree well with previous work over most wavelengths but disagree significantly near 1 micron where our opacities are lower than those found by other authors. This discrepancy is likely due to differences between how atomic data were obtained; however, further investigation will be necessary before this issue can be resolved conclusively. \n \n In addition to calculating opacities for pure hydrogen\/helium atmospheres, we also consider mixed compositions containing up to 50% water vapor or ammonia. For all cases considered here, including mixed composition atmospheres, we find good agreement between model predictions and observed properties of T dwarf stars.","label":1,"model":"bloomz","source":"arxiv","id":3485}
{"text":"We propose to connect the Large Hadron Collider (LHC) with an International Linear Collider (ILC), which will be built in Japan or Korea, by using quintessence as dark energy.  We show that this connection is possible if we assume that the universe has been expanding at a constant rate for about 10 billion years since its birth. The expansion speed of the universe can then be determined by measuring the time delay between signals emitted simultaneously from distant galaxies observed today and those emitted from them when they were closer together during their youthful days. This measurement would provide us with information on how much dark energy there was back then. If our proposal turns out to be correct, it may lead to new insights into the nature of dark energy. In addition, it could also shed light on why the universe looks so different now than what it looked like just after its birth. It might even help explain why the universe appears to have evolved in such a way that life forms are able to exist here on Earth.","label":1,"model":"bloomz","source":"arxiv","id":3486}
{"text":"We present the first dynamical mass measurement for an eclipsing binary system with one component that is still on its pre-main sequence (PMS). The PMS star, V773 Tau A, has been observed to undergo large-amplitude photometric variability and spectroscopic changes indicative of rapid rotation and magnetic activity. We have obtained new high-precision radial velocity measurements using the HIRES spectrograph at Keck Observatory in Hawaii. These data are combined with previously published observations spanning more than 20 years to produce a preliminary orbital solution. Using this orbit we calculate a dynamical mass for V773 Tau A of 0.84 \u00b1 0.04M\u2299. This value agrees well with theoretical predictions based upon evolutionary models for young stars. Our results provide strong evidence against previous claims that the components of close binaries evolve independently during their early stages of formation. In addition, our analysis provides further support for the hypothesis that the majority of short-period systems will merge within 10 million years due to tidal interactions between the two components. \n \n Keywords: Mass determination","label":1,"model":"bloomz","source":"arxiv","id":3487}
{"text":"We give an explicit formula for the star product on the cotangent bundle $T^*M$ over a Poisson manifold $(M,\\pi)$ with symplectic leaves $L_i$ and corresponding moment maps $\\mu_i : T^*M \\to L_i$.  We show that this star product is given by the formula \n$$ (f \\star g)(x) = \\sum_{i=0}^n f(x)g(\\mu_i^{-1}(x)) \\pi^{ij}(x)  $$\nwhere $\\pi^{ij}$ are the components of the inverse matrix to the Poisson tensor $\\pi_{ij}$. \nThis result generalizes earlier results obtained by Kontsevich [K] , Fedosov [F]  and others.   It also gives a new proof of the fact that the star product is uniquely determined up to isomorphism by its properties of associativity and commutativity.    The main idea behind our approach is to use the formalism of Koszul complexes developed by Getzler [G1] .","label":1,"model":"bloomz","source":"arxiv","id":3488}
{"text":"We present an analysis of the gravitational waveforms emitted by two neutron stars orbiting each other, and show that they can be used to detect violations of Lorentz invariance (LI). We consider both scalar-tensor theories with spontaneous breaking of LI as well as vector-tensor theories where LI is violated through the presence of a preferred reference frame. In these models we find that there are characteristic deviations from general relativity which lead to measurable differences between the observed gravitational waveform and those predicted within Einstein's theory. \n \n The detection of such deviations would provide strong evidence for new physics beyond standard model expectations. This could have important consequences on our understanding of fundamental interactions at high energies. For example, it may shed light on the origin of dark energy or even reveal the existence of extra dimensions of space-time. It also has implications for cosmology since many extensions of the Standard Model predict time variations of physical constants like Newton's constant G.","label":1,"model":"bloomz","source":"arxiv","id":3489}
{"text":"We present here a detailed discussion on the concept of allovalency, which is defined as the simultaneous binding to multiple sites in one molecule by different ligands (or receptors). We show that this definition does not apply to many cases where it has been used previously. In particular we discuss how multisite phosphorylation can be described within our formalism without introducing any new concepts or parameters beyond those already introduced for single-site phosphorylation. Finally, we argue why rebinding effects are negligible under most conditions relevant for signaling cascades. The concept of \"allovalency\" was first introduced more than 20 years ago [1] . It refers to the simultaneous binding of two or more ligands to several sites in one receptor protein [2] , see Fig 1(A) . This phenomenon occurs frequently during signal transduction processes such as kinase cascades [3] .\nThe term \"allovalent\" was coined because it describes a situation intermediate between monovalent and multivalent interactions [4] : while each ligand binds only once per receptor, there may exist several copies of the same ligand bound simultaneously to the same receptor. Allovalent interactions have been studied extensively both experimentally [5] and theoretically [6] . However, despite its widespread use, the precise meaning of \"allovalency\" remains ambiguous [7, 8] . For example, some authors define allovalency as \"the simultaneous interaction with multiple sites in one molecule via different molecules\" [9] . Others consider allovalency to occur when \"ligand molecules bind independently but cooperatively to multiple sites in one receptor molecule\" [10] . Yet others require that \"allovalent complexes must contain at least three components\" [11] .","label":1,"model":"bloomz","source":"arxiv","id":3490}
{"text":"The Galois group is the fundamental object in classical Galois theory, which studies the solvability of polynomial equations over finite fields.  In this talk we will introduce the notion of ``Galois groups'' for infinite families of polynomials and study their properties using tools from algebraic geometry.   We will also discuss some applications to number theory such as the proof of the abc conjecture by Vojta (joint work with J. Pila). The main results are joint works with A. Chambert-Loir, D. Ghioca, M. Harris, C. Lairez, S. Popescu-Tarauca, B. Stoll, E. Szpiro, T. Tucker-Drob, and V. Vojta. This talk was given at the conference \"Algebraic Geometry and Arithmetic Dynamics\" held on June 24-28, 2013 at MSRI Berkeley. It has been recorded by Adam Hartung.","label":1,"model":"bloomz","source":"arxiv","id":3491}
{"text":"We report on observations made with the High Dispersion Spectrograph (HDS) attached to the Subaru telescope, which is capable of resolving individual lines in the solar spectrum. We observed the Sun during an active period and obtained high-resolution spectra covering the wavelength range between 4500\u00c5 and 5500\u00c5. The line profiles were analyzed by using two different methods for determining the scattering polarization degree Pscat(\u03bb). One method was based on the measurement of Stokes Q\/I and U\/I parameters directly from the observed line profile; another one used the ratio of Stokes V \/I measured from nearby lines as a proxy for Pscat(\u03bb).\nThe results show that both methods give consistent values within their uncertainties except near the center of strong lines where the second method tends to underestimate Pscat(\u03bb), probably due to blending effects. In addition we found that the scattering polarization degrees are generally higher than those predicted by theoretical calculations assuming LTE conditions.  These findings suggest that non-LTE effects may play important roles in producing the observed scattering polarization.","label":1,"model":"bloomz","source":"arxiv","id":3492}
{"text":"We present an alternative explanation to the origin of the energy gap $\\Delta$ in the cut-off approximation used by Bardeen, Cooper and Schrieffer (BCS) in their famous 1957 work on superconductivity.  We show that this gap is not due to the presence of phonons but rather it arises as a consequence of the fact that electrons are fermions with spin 1\/2. In particular we find that the value of the gap depends only on the number of particles involved in the process and does not depend explicitly on temperature or any other physical parameter. This result agrees well with experimental data obtained recently at high temperatures. \nThe new interpretation presented here also sheds light into some puzzling results found in recent experiments performed on cuprate compounds which have been widely studied because they exhibit very high critical temperatures. The most important feature of these materials is the existence of two different gaps in the excitation spectrum corresponding to electron-like quasiparticles and hole-like excitations respectively.","label":1,"model":"bloomz","source":"arxiv","id":3493}
{"text":"We present an approximate theory to study the effect of a magnetic field on the ground state properties and phase diagram of strongly correlated systems, such as high-Tc cuprates or heavy fermion compounds. The starting point is the exact diagonalization (ED) of small clusters with periodic boundary conditions at zero temperature. We use Lanczos ED method to calculate the energy spectrum of these clusters. Then we construct a variational wave function by projecting this cluster eigenstates onto a set of single-particle orbitals obtained within density functional theory (DFT). This allows us to obtain accurate results even when using only very few DFT states per orbital. Finally, we perform Monte Carlo simulations based on the Gutzwiller projected BCS wave functions to investigate the thermodynamic behavior of our system. Our main result shows that the application of a magnetic field leads to a suppression of the superfluid order parameter $\\psi$ which can be understood as a consequence of the Zeeman splitting between spin-up and -down electrons.","label":1,"model":"bloomz","source":"arxiv","id":3494}
{"text":"We present an analysis combining weak lensing tomography and halo clustering measurements in order to probe the nature of dark energy. We use a sample of galaxy clusters selected by their Sunyaev-Zel'dovich effect (SZE) signal, which is detected via its inverse Compton scattering off cosmic microwave background photons. The cluster masses are determined using X-ray data obtained from Chandra observations. Using this mass calibration we measure the projected correlation function between these clusters as well as the cross-correlation functions between them and galaxies at different redshifts. These measurements allow us to determine cosmological parameters such as the equation-of-state parameter w0 and the matter density $\\Omega_m$. Our results show that our method can be used to constrain both parameters simultaneously with high precision. In particular, we find that for a flat universe with $\\Omega_m = 0.3$ and assuming a constant value of $w_0$, we obtain $\\sigma(w_0) \\approx 0.05$ and $\\sigma(\\Omega_m) \\approx 0.01$.","label":1,"model":"bloomz","source":"arxiv","id":3495}
{"text":"We study the cosmology of modified gravity models with an action that contains higher-order curvature terms, such as $R^2$ and $R_{\\mu \\nu \\rho \\sigma}$. We show how to derive constraints on these theories using observations of large-scale structure (LSS), cosmic microwave background radiation (CMB), supernovae Ia (SNeIa), baryon acoustic oscillations (BAO), Hubble constant measurements (HST), and gamma-ray bursts (GRBs). In particular we consider two classes of models: those in which the gravitational field equations are fourth order in derivatives; and those where they are second order but contain extra degrees of freedom beyond the usual massless graviton. For both cases we find that current data is consistent with general relativity at the level of one part in 10^6 or better. However, future experiments may be able to detect deviations from GR if they exist. \n \n Keywords: Modified Gravity","label":1,"model":"bloomz","source":"arxiv","id":3496}
{"text":"The parity violating asymmetry in the scattering of polarized electrons off unpolarized protons is measured at Jefferson Lab to determine the proton's weak charge with an uncertainty comparable to that achieved for the electron and neutron.  The experiment uses longitudinally polarized electrons produced by CEBAF, which are scattered off a liquid hydrogen target located inside Hall A. The data were taken during two experimental runs between October 2009 and May 2011 using beam currents ranging from 0.5 nA to 1.0 \u00b5A. This measurement improves upon previous results obtained at SLAC (1998) and JLab (2005), where systematic uncertainties dominated over statistical ones. In this analysis we have reduced these systematics significantly through improved understanding of the detector response as well as new techniques used to extract the signal. We find Qweak = -0.0018 \u00b1 0.0007(stat.) \u00b1 0.0003(syst.), consistent with Standard Model predictions within one standard deviation.","label":1,"model":"bloomz","source":"arxiv","id":3497}
{"text":"The multifractal nature of the human heartbeat has been studied by many authors, but most studies have focused on short time series (typically less than one hour). In this work we study the scaling properties of long-term recordings of the electrocardiograms (ECG) of healthy subjects during sleep using wavelet transforms to calculate the singularity spectrum f(\u03b1), which characterizes the degree of multifractality present in each signal.  We find that all signals are characterized by a broad range of exponents \u03b1 indicating strong multifractality. The width \u0394\u03b1 of these spectra decreases with increasing recording length T as \u0394\u03b1~T-1\/2 for T<10 hours and \u0394\u03b1~T-3\/4 for T>10 hours. This indicates that the ECGs become more monofractal-like when longer recordings are considered. Finally, we show how our results can be used to detect pathological changes in the heart rate variability associated with congestive heart failure.","label":1,"model":"bloomz","source":"arxiv","id":3498}
{"text":"We give an explicit description of the higher algebraic K-groups in terms of certain universal cohomology classes, which are defined by using only the ring structure and the unit element of the underlying commutative ring. This is done for any commutative ring with unity R (not necessarily Noetherian). The main result can be formulated as follows: Let M be a module over R. Then there exists a natural isomorphism between the higher algebraic K-groups: \nK_n(R) = Ext^n_R(M, R)\nand the group of all n-fold Massey products on M modulo those that vanish under some suitable finiteness condition. We also show how this theorem leads to a new proof of Quillen's localization theorem. Finally we discuss applications to the study of equivariant K-theory. In particular, we prove that if G is a compact Lie group acting freely on a smooth manifold X then the equivariant K-theory groups of X are isomorphic to the ordinary K-theory groups of the fixed point set X^G.","label":1,"model":"bloomz","source":"arxiv","id":3499}
{"text":"  In this paper we consider the space of those probability distributions which\nmaximize the $q$-R\\'enyi entropy. These distributions have the same parameter\nspace for every $q$, and in the $q=1$ case these are the normal distributions.\nSome methods to endow this parameter space with Riemannian metric is presented:\nthe second derivative of the $q$-R\\'enyi entropy, Tsallis-entropy and the\nrelative entropy give rise to a Riemannian metric, the Fisher-information\nmatrix is a natural Riemannian metric, and there are some geometrically\nmotivated metrics which were studied by Siegel, Calvo and Oller, Lovri\\'c,\nMin-Oo and Ruh. These metrics are different therefore our differential\ngeometrical calculations based on a unified metric, which covers all the above\nmentioned metrics among others. We also compute the geometrical properties of\nthis metric, the equation of the geodesic line with some special solutions, the\nRiemann and Ricci curvature tensors and scalar curvature. Using the\ncorrespondence between the volume of the geodesic ball and the scalar curvature\nwe show how the parameter $q$ modulates the statistical distinguishability of\nclose points. We show that some frequently used metric in quantum information\ngeometry can be easily recovered from classical metrics.\n","label":0,"model":"human","source":"arxiv","id":3500}
{"text":"  We report results from numerical simulations of star formation in the early\nuniverse that focus on gas at very high densities and very low metallicities.\nWe argue that the gas in the central regions of protogalactic halos will\nfragment as long as it carries sufficient angular momentum. Rotation leads to\nthe build-up of massive disk-like structures which fragment to form protostars.\nAt metallicities Z ~ 10^-5 Zsun, dust cooling becomes effective and leads to a\nsudden drop of temperature at densities above n = 10^12 cm^-3. This induces\nvigorous fragmentation, leading to a very densely-packed cluster of low-mass\nstars. This is the first stellar cluster. The mass function of stars peaks\nbelow 1 Msun, similar to what is found in the solar neighborhood, and\ncomparable to the masses of the very-low metallicity subgiant stars recently\ndiscovered in the halo of our Milky Way. We find that even purely primordial\ngas can fragment at densities 10^14 cm^-3 < n < 10^16 cm^-3, although the\nresulting mass function contains only a few objects (at least a factor of ten\nless than the Z = 10^-5 Zsun mass function), and is biased towards higher\nmasses. A similar result is found for gas with Z = 10^-6 Zsun. Gas with Z <=\n10^-6 Zsun behaves roughly isothermally at these densities (with polytropic\nexponent gamma ~ 1.06) and the massive disk-like structures that form due to\nangular momentum conservation will be marginally unstable. As fragmentation is\nless efficient, we expect stars with Z <= 10^-6 Zsun to be massive, with masses\nin excess of several tens of solar masses, consistent with the results from\nprevious studies.\n","label":0,"model":"human","source":"arxiv","id":3501}
{"text":"  We derive a perturbation expansion for general self-interacting random walks,\nwhere steps are made on the basis of the history of the path. Examples of\nmodels where this expansion applies are reinforced random walk, excited random\nwalk, the true (weakly) self-avoiding walk, loop-erased random walk, and\nannealed random walk in random environment.\n  In this paper we show that the expansion gives rise to useful formulae for\nthe speed and variance of the random walk, when these quantities are known to\nexist. The results and formulae of this paper have been used elsewhere by the\nauthors to prove monotonicity properties for the speed (in high dimensions) of\nexcited random walk and related models, and certain models of random walk in\nrandom environment.\n  We also derive a law of large numbers and central limit theorem (with\nexplicit error terms) directly from this expansion, under strong assumptions on\nthe expansion coefficients. The assumptions are shown to be satisfied by\nexcited random walk in high dimensions with small excitation parameter, a model\nof reinforced random walk with underlying drift and small reinforcement\nparameter, and certain models of random walk in random environment under strong\nellipticity conditions. This is the extended version of the paper, where we\nprovide all proofs.\n","label":0,"model":"human","source":"arxiv","id":3502}
{"text":"  Fluids in which the interparticle potential has a hard core, is attractive at\nmoderate separations, and repulsive at greater separations are known to exhibit\nnovel phase behavior, including stable inhomogeneous phases. Here we report a\njoint simulation and theoretical study of such a fluid, focusing on the\nrelationship between the liquid-vapor transition line and any new phases. The\nphase diagram is studied as a function of the amplitude of the attraction for a\ncertain fixed amplitude of the long ranged repulsion. We find that the effect\nof the repulsion is to substitute the liquid-vapor critical point and a portion\nof the associated liquid-vapor transition line, by two first order transitions.\nOne of these transitions separates the vapor from a fluid of spherical\nliquidlike clusters; the other separates the liquid from a fluid of spherical\nvoids. At low temperature, the two transition lines intersect one another and a\nvapor-liquid transition line at a triple point. While most integral equation\ntheories are unable to describe the new phase transitions, the Percus Yevick\napproximation does succeed in capturing the vapor-cluster transition, as well\nas aspects of the structure of the cluster fluid, in reasonable agreement with\nthe simulation results.\n","label":0,"model":"human","source":"arxiv","id":3503}
{"text":"  A comparison of the 2MASS flux dipole to the CMB dipole can serve as a method\nto constrain a combination of the cosmological parameter Omega_m and the\nluminosity bias of the 2MASS survey. For this constraint to be as tight as\npossible, it is necessary to maximize the correlation between the two dipoles.\nThis can be achieved by optimizing the survey window through which the flux\ndipole is measured. Here we explicitly construct such a window for the 2MASS\nsurvey. The optimization in essence reduces to excluding from the calculation\nof the flux dipole galaxies brighter than some limiting magnitude K_min of the\nnear-infrared K_s band. This exclusion mitigates nonlinear effects and shot\nnoise from small scales, which decorrelate the 2MASS dipole from the CMB\ndipole. Under the assumption of negligible shot noise we find that the optimal\nvalue of K_min is about five. Inclusion of shot noise shifts the optimal K_min\nto larger values. We present an analytical formula for shot noise for the 2MASS\nflux dipole, to be used in follow-up work with 2MASS data.\n  The misalignment angle between the two dipoles is a sensitive measure of\ntheir correlation: the higher the correlation, the smaller the expectation\nvalue of the angle. A minimum of the misalignment is thus a sign of the optimal\ngravity window. We model analytically the distribution function for the\nmisalignment angle and show that the misalignment estimated by Maller et al. is\nconsistent with the assumed underlying model (though it is greater than the\nexpectation value). We predict with about 90% confidence that the misalignment\nwill decrease if 2MASS galaxies brighter than K_min = 5 mag are excluded from\nthe calculation of the flux dipole. This prediction has been indirectly\nconfirmed by the results of Erdogdu et al. (ABRIDGED)\n","label":0,"model":"human","source":"arxiv","id":3504}
{"text":"  We introduce the notion of \"covering homology\" of a commutative ring spectrum\nwith respect to certain families of coverings of topological spaces. The\nconstruction of covering homology is extracted from Bokstedt, Hsiang and\nMadsen's topological cyclic homology. In fact covering homology with respect to\nthe family of orientation preserving isogenies of the circle is equal to\ntopological cyclic homology.\n  Our basic tool for the analysis of covering homology is a cofibration\nsequence involving homotopy orbits and a restriction map similar to the\nrestriction map used in Bokstedt, Hsiang and Madsen's construction of\ntopological cyclic homology.\n  Covering homology with respect to families of isogenies of a torus is\nconstructed from iterated topological Hochschild homology. It receives a trace\nmap from iterated algebraic K-theory and the hope is that the rich structure,\nand the calculability of covering homology will make covering homology useful\nin the exploration of J. Rognes' ``red shift conjecture''.\n","label":0,"model":"human","source":"arxiv","id":3505}
{"text":"  A brief survey of the theoretical, numerical and experimental studies of the\nrandom field Ising model during last three decades is given. Nature of the\nphase transition in the three-dimensional RFIM with Gaussian random fields is\ndiscussed. Using simple scaling arguments it is shown that if the strength of\nthe random fields is not too small (bigger than a certain threshold value) the\nfinite temperature phase transition in this system is equivalent to the\nlow-temperature order-disorder transition which takes place at variations of\nthe strength of the random fields. Detailed study of the zero-temperature phase\ntransition in terms of simple probabilistic arguments and modified mean-field\napproach (which take into account nearest-neighbors spin-spin correlations) is\ngiven. It is shown that if all thermally activated processes are suppressed the\nferromagnetic order parameter m(h) as the function of the strength $h$ of the\nrandom fields becomes history dependent. In particular, the behavior of the\nmagnetization curves m(h) for increasing and for decreasing $h$ reveals the\nhysteresis loop.\n","label":0,"model":"human","source":"arxiv","id":3506}
{"text":"  Axions are expected to be produced in the sun via the Primakoff process. They\nmay be detected through the inverse process in the laboratory, under the\ninfluence of a strong magnetic field, giving rise to X-rays of energies in the\nrange of a few keV. Such an Axion detector is the CERN Axion Solar Telescope\n(CAST), collecting data since 2003. Results have been published, pushing the\naxion-photon coupling g$_{a\\gamma}$ below the 10$^{-10}$ GeV$^{-1}$ limit at\n95% CL, for axion masses less than 0.02 eV. This limit is nearly an order of\nmagnitude lower than previous experimental limits and surpassed for the first\ntime limits set from astrophysical arguments based on the energy-loss concept.\nThe experiment is currently exploring axion masses in the range of 0.02 eV $<\nm_a <$ 1.1 eV. In the next run, currently under preparation, the axion mass\nexplored will be extended up to the limit of 1.1 eV, testing for the first time\nthe region of theoretical axion models with the axion helioscope method.\n","label":0,"model":"human","source":"arxiv","id":3507}
{"text":"  The dynamics of information dissemination in social networks is of paramount\nimportance in processes such as rumors or fads propagation, spread of product\ninnovations or \"word-of-mouth\" communications. Due to the difficulty in\ntracking a specific information when it is transmitted by people, most\nunderstanding of information spreading in social networks comes from models or\nindirect measurements. Here we present an integrated experimental and\ntheoretical framework to understand and quantitatively predict how and when\ninformation spreads over social networks. Using data collected in Viral\nMarketing campaigns that reached over 31,000 individuals in eleven European\nmarkets, we show the large degree of variability of the participants' actions,\ndespite them being confronted with the common task of receiving and forwarding\nthe same piece of information. This have a profound effect on information\ndiffusion: Firstly, most of the transmission takes place due to super-spreading\nevents which would be considered extraordinary in population-average models.\nSecondly, due to the different way individuals schedule information\ntransmission we observe a slowing down of the spreading of information in\nsocial networks that happens in logarithmic time. Quantitative description of\nthe experiments is possible through an stochastic branching process which\ncorroborates the importance of heterogeneity. Since high variability of both\nthe intensity and frequency of human responses are found in many other\nactivities, our findings are pertinent to many other human driven diffusion\nprocesses like rumors, fads, innovations or news which has important\nconsequences for organizations management, communications, marketing or\nelectronic social communities.\n","label":0,"model":"human","source":"arxiv","id":3508}
{"text":"  We study the problem of the emergence of cooperation in the spatial\nPrisoner's Dilemma. The pioneering work by Nowak and May showed that large\ninitial populations of cooperators can survive and sustain cooperation in a\nsquare lattice with imitate-the-best evolutionary dynamics. We revisit this\nproblem in a cost-benefit formulation suitable for a number of biological\napplications. We show that if a fixed-amount reward is established for\ncooperators to share, a single cooperator can invade a population of defectors\nand form structures that are resilient to re-invasion even if the reward\nmechanism is turned off. We discuss analytically the case of the invasion by a\nsingle cooperator and present agent-based simulations for small initial\nfractions of cooperators. Large cooperation levels, in the sustainability\nrange, are found. In the conclusions we discuss possible applications of this\nmodel as well as its connections with other mechanisms proposed to promote the\nemergence of cooperation.\n","label":0,"model":"human","source":"arxiv","id":3509}
{"text":"  We investigate the braneworld model with induced gravity to clarify the role\nof the cross-over length scale \\ell in the possible explanation of the\ndark-matter phenomenon in astrophysics and in cosmology. Observations of the 21\ncm line from neutral hydrogen clouds in spiral galaxies reveal that the\nrotational velocities remain nearly constant at a value v_c ~ 10^{-3}--10^{-4}\nin the units of the speed of light in the region of the galactic halo. Using\nthe smallness of v_c, we develop a perturbative scheme for reconstructing the\nmetric in a galactic halo. In the leading order of expansion in v_c, at the\ndistances r \\gtrsim v_c \\ell, our result reproduces that obtained in the\nRandall-Sundrum braneworld model. This inequality is satisfied in a real spiral\ngalaxy such as our Milky Way for distances r ~ 3 kpc, at which the rotational\nvelocity curve becomes flat, v_c ~ 7 \\times 10^{-4}, if \\ell \\lesssim 2 Mpc.\nThe gravitational situation in this case can be approximately described by the\nEinstein equations with the so-called Weyl fluid playing the role of dark\nmatter. In the region near the gravitating body, we derive a closed system of\nequations for static spherically symmetric situation under the approximation of\nzero anisotropic stress of the Weyl fluid. We find the Schwarzschild metric to\nbe an approximate vacuum solution of these equations at distances r \\lesssim\n(r_g \\ell^2)^{1\/3}. The value \\ell \\lesssim 2 Mpc complies well with the\nsolar-system tests. At the same time, in cosmology, a low-density braneworld\nwith \\ell of this order of magnitude can mimic the expansion properties of the\nhigh-density LCDM (lambda + cold dark matter) universe at late times. Combined\nobservations of galactic rotation curves and gravitational lensing can possibly\ndiscriminate between the higher-dimensional effects and dark matter.\n","label":0,"model":"human","source":"arxiv","id":3510}
{"text":"  Indirect information about the possible scale of supersymmetry (SUSY)\nbreaking is provided by B-physics observables (BPO) as well as electroweak\nprecision observables (EWPO). We combine the constraints imposed by recent\nmeasurements of the BPO BR(b -> s gamma), BR(B_s -> mu^+ mu^-), BR(B_u -> tau\nnu_tau) and Delta M_{B_s} with those obtained from the experimental\nmeasurements of the EWPO M_W, sin^2 theta_eff, Gamma_Z, (g-2)_mu and M_h,\nincorporating the latest theoretical calculations of these observables within\nthe Standard Model and supersymmetric extensions. We perform a chi^2 fit to the\nparameters of the constrained minimal supersymmetric extension of the Standard\nModel (CMSSM), in which the SUSY-breaking parameters are universal at the GUT\nscale, and the non-universal Higgs model (NUHM), in which this constraint is\nrelaxed for the soft SUSY-breaking contributions to the Higgs masses. Assuming\nthat the lightest supersymmetric particle (LSP) provides the cold dark matter\ndensity preferred by WMAP and other cosmological data, we scan over the\nremaining parameter space. Within the CMSSM, we confirm the preference found\npreviously for a relatively low SUSY-breaking scale, though there is some\nslight tension between the EWPO and the BPO. In studies of some specific NUHM\nscenarios compatible with the cold dark matter constraint we investigate\nM_A-tan_beta planes and find preferred regions that have values of chi^2\nsomewhat lower than in the CMSSM.\n","label":0,"model":"human","source":"arxiv","id":3511}
{"text":"  We compute the probability to detect long Gamma Ray Bursts (GRBs) at z>5 with\nSwift, assuming that GRBs form preferentially in low-metallicity environments.\nThe model fits well both the observed BATSE and Swift GRB differential peak\nflux distribution and is consistent with the number of z>2.5 detections in the\n2-year Swift data. We find that the probability to observe a burst at z>5\nbecomes larger than 10% for photon fluxes P<1 ph s^{-1} cm^{-2}, consistent\nwith the number of confirmed detections. The corresponding fraction of z>5\nbursts in the Swift catalog is ~10%-30% depending on the adopted metallicity\nthreshold for GRB formation. We propose to use the computed probability as a\ntool to identify high redshift GRBs. By jointly considering promptly-available\ninformation provided by Swift and model results, we can select reliable z>5\ncandidates in a few hours from the BAT detection. We test the procedure against\nlast year Swift data: only three bursts match all our requirements, two being\nconfirmed at z>5. Other three possible candidates are picked up by slightly\nrelaxing the adopted criteria. No low-z interloper is found among the six\ncandidates.\n","label":0,"model":"human","source":"arxiv","id":3512}
{"text":"  We present IFU observations of six emission-line nebulae that surround the\ncentral galaxy of cool core clusters. Qualitatively similar nebulae are\nobserved in cool core clusters even when the dynamics and possibly formation\nand excitation source are different. Evidence for a nearby secondary galaxy\ndisturbing a nebula, as well as AGN and starburst driven outflows are presented\nas possible formation mechanisms. One nebula has a rotation velocity of the\nsame amplitude as the underlying molecular reservoir, which implies that the\nexcitation or formation of a nebula does not require any disturbance of the\nmolecular reservoir within the central galaxy. Bulk flows and velocity shears\nof a few hundred km\/s are seen across all nebulae. The majority lack any\nordered rotation, their configurations are not stable so the nebulae must be\nconstantly reshaping, dispersing and reforming. The dimmer nebulae are\nco-spatial with dust features whilst the more luminous are not. Significant\nvariation in the ionization state of the gas is seen in all nebulae through the\nnon-uniform [NII]\/H_alpha ratio. There is no correlation between the line ratio\nand H_alpha surface brightness, but regions with excess blue or UV light have\nlower line ratios. This implies that UV from massive, young stars act in\ncombination with an underlying heating source that produces the observed\nlow-ionization spectra.\n","label":0,"model":"human","source":"arxiv","id":3513}
{"text":"  In this paper, we first study the local rings of a Berkovich analytic space\nfrom the point of view of commutative algebra. We show that those rings are\nexcellent ; we introduce the notion of a an analytically separable extension of\nnon-archimedean complete fields (it includes the case of the finite separable\nextensions, and also the case of any complete extension of a perfect complete\nnon-archimedean field) and show that the usual commutative algebra properties\n(Rm, Sm, Gorenstein, Cohen-Macaulay, Complete Intersection) are stable under\nanalytically separable ground field extensions; we also establish a GAGA\nprinciple with respect to those properties for any finitely generated scheme\nover an affinoid algebra.\n  A second part of the paper deals with more global geometric notions : we\ndefine, show the existence and establish basic properties of the irreducible\ncomponents of analytic space ; we define, show the existence and establish\nbasic properties of its normalization ; and we study the behaviour of\nconnectedness and irreducibility with respect to base change.\n","label":0,"model":"human","source":"arxiv","id":3514}
{"text":"  This paper investigates GRB 050802, one of the best examples of a it Swift\ngamma-ray burst afterglow that shows a break in the X-ray lightcurve, while the\noptical counterpart decays as a single power-law. This burst has an optically\nbright afterglow of 16.5 magnitude, detected throughout the 170-650nm spectral\nrange of the UVOT on-board Swift. Observations began with the XRT and UVOT\ntelescopes 286s after the initial trigger and continued for 1.2 x 10^6s. The\nX-ray lightcurve consists of three power-law segments: a rise until 420s,\nfollowed by a slow decay with alpha_2 = 0.63 +\/- 0.03 until 5000s, after which,\nthe lightcurve decays faster with a slope of alpha_3 = 1.59 +\/- 0.03. The\noptical lightcurve decays as a single power-law with alpha_O = 0.82 +\/- 0.03\nthroughout the observation. The X-ray data on their own are consistent with the\nbreak at 5000s being due to the end of energy injection. Modelling the optical\nto X-ray spectral energy distribution, we find that the optical afterglow can\nnot be produced by the same component as the X-ray emission at late times,\nruling out a single component afterglow. We therefore considered two-component\njet models and find that the X-ray and optical emission is best reproduced by a\nmodel in which both components are energy injected for the duration of the\nobserved afterglow and the X-ray break at 5000s is due to a jet break in the\nnarrow component. This bright, well-observed burst is likely a guide for\ninterpreting the surprising finding of Swift that bursts seldom display\nachromatic jet breaks.\n","label":0,"model":"human","source":"arxiv","id":3515}
{"text":"  We discuss wave propagation in rotating superfluid neutron star cores, taking\ninto account the vortex mediated mutual friction force. For models where the\ntwo fluids co-rotate in the unperturbed state, our analysis clarifies the role\nof chemical coupling and entrainment for sound and inertial waves. We also\ninvestigate the mutual friction damping, providing results that demonstrate the\nwell-known fact that sound waves propagating along a vortex array are undamped.\nWe show that the same is not true for inertial waves, which are damped by the\nmutual friction regardless of the propagation direction. We then include the\nvortex tension, which arises due to local vortex curvature. Focussing on purely\ntransverse inertial waves, we derive the small correction that the tension\ninduces in the wave frequency. Finally, we allow for a relative linear flow in\nthe background (along the rotation axis). In this case we show how the mutual\nfriction coupling may induce a dynamical instability in the inertial waves. We\ndiscuss the critical flow required for the instability to be present, its\nphysical interpretation and the possible relevance it may have for neutron star\nphysics.\n","label":0,"model":"human","source":"arxiv","id":3516}
{"text":"  Given a triangulation of a closed, oriented, irreducible, atoroidal\n3-manifold every oriented, incompressible surface may be isotoped into normal\nposition relative to the triangulation. Such a normal oriented surface is then\nencoded by non-negative integer weights, 14 for each 3-simplex, that describe\nhow many copies of each oriented normal disc type there are. The Euler\ncharacteristic and homology class are both linear functions of the weights.\nThere is a convex polytope in the space of weights, defined by linear equations\ngiven by the combinatorics of the triangulation, whose image under the homology\nmap is the unit ball, B, of the Thurston norm.\n  Applications of this approach include (1) an algorithm to compute B and hence\nthe Thurston norm of any homology class, (2) an explicit exponential bound on\nthe number of vertices of B in terms of the number of simplices in the\ntriangulation, (3) an algorithm to determine the fibred faces of B and hence an\nalgorithm to decide whether a 3-manifold fibres over the circle.\n","label":0,"model":"human","source":"arxiv","id":3517}
{"text":"  The reconstruction of a deterministic data field from binary-quantized noisy\nobservations of sensors which are randomly deployed over the field domain is\nstudied. The study focuses on the extremes of lack of deterministic control in\nthe sensor deployment, lack of knowledge of the noise distribution, and lack of\nsensing precision and reliability. Such adverse conditions are motivated by\npossible real-world scenarios where a large collection of low-cost, crudely\nmanufactured sensors are mass-deployed in an environment where little can be\nassumed about the ambient noise. A simple estimator that reconstructs the\nentire data field from these unreliable, binary-quantized, noisy observations\nis proposed. Technical conditions for the almost sure and integrated mean\nsquared error (MSE) convergence of the estimate to the data field, as the\nnumber of sensors tends to infinity, are derived and their implications are\ndiscussed. For finite-dimensional, bounded-variation, and\nSobolev-differentiable function classes, specific integrated MSE decay rates\nare derived. For the first and third function classes these rates are found to\nbe minimax order optimal with respect to infinite precision sensing and known\nnoise distribution.\n","label":0,"model":"human","source":"arxiv","id":3518}
{"text":"  Liesegang patterns emerge from precipitation processes and may be used to\nbuild bulk structures at submicron lengthscales. Thus they have significant\npotential for technological applications provided adequate methods of control\ncan be devised. Here we describe a simple, physically realizable\npattern-control based on the notion of driven precipitation, meaning that the\nphase-separation is governed by a guiding field such as, for example, a\ntemperature or a pH field. The phase-separation is modeled through a\nnon-autonomous Cahn-Hilliard equation whose spinodal is determined by the\nevolving guiding field. Control over the dynamics of the spinodal gives control\nover the velocity of the instability front which separates the stable and\nunstable regions of the system. Since the wavelength of the pattern is largely\ndetermined by this velocity, the distance between successive precipitation\nbands becomes controllable. We demonstrate the above ideas by numerical studies\nof a 1D system with diffusive guiding field. We find that the results can be\naccurately described by employing a linear stability analysis (pulled-front\ntheory) for determining the velocity -- local-wavelength relationship. From the\nperspective of the Liesegang theory, our results indicate that the so-called\nrevert patterns may be naturally generated by diffusive guiding fields.\n","label":0,"model":"human","source":"arxiv","id":3519}
{"text":"  We provide irreducible expressions for the metric variations of the\ngravitational action terms constructed from the 17 curvature invariants of\norder six in derivatives of the metric tensor i.e. from the geometrical terms\nappearing in the diagonal heat-kernel or Gilkey-DeWitt coefficient $a_3$. We\nthen express, for a four dimensional spacetime, the approximated stress-energy\ntensor constructed from the renormalized DeWitt-Schwinger effective action\nassociated with a massive scalar field. We also construct, for higher\ndimensional spacetimes, the infinite counterterms of order six in derivatives\nof the metric tensor appearing in the left hand side of Einstein equations as\nwell as the contribution associated with the cubic Lovelock gravitational\naction. In an appendix, we provide a list of geometrical relations we have used\nand which are more generally helpful for calculations in two-loop quantum\ngravity in a four dimensional background or for calculations in one-loop\nquantum gravity in higher dimensional background. We also obtain the\napproximated stress-energy tensors associated with a massive spinor field and a\nmassive vector field propagating in a four dimensional background.\n","label":0,"model":"human","source":"arxiv","id":3520}
{"text":"  Let a and b be two positive integers. A culminating path is a path of Z^2\nthat starts from (0,0), consists of steps (1,a) and (1,-b), stays above the\nx-axis and ends at the highest ordinate it ever reaches. These paths were first\nencountered in bioinformatics, in the analysis of similarity search algorithms.\nThey are also related to certain models of Lorentzian gravity in theoretical\nphysics. We first show that the language on a two letter alphabet that\nnaturally encodes culminating paths is not context-free. Then, we focus on the\nenumeration of culminating paths. A step by step approach, combined with the\nkernel method, provides a closed form expression for the generating fucntion of\nculminating paths ending at a (generic) height k. In the case a=b, we derive\nfrom this expression the asymptotic behaviour of the number of culminating\npaths of length n. When a>b, we obtain the asymptotic behaviour by a simpler\nargument. When a<b, we only determine the exponential growth of the number of\nculminating paths. Finally, we study the uniform random generation of\nculminating paths via various methods. The rejection approach, coupled with a\nsymmetry argument, gives an algorithm that is linear when a>= b, with no\nprecomputation stage nor non-linear storage required. The choice of the best\nalgorithm is not as clear when a<b. An elementary recursive approach yields a\nlinear algorithm after a precomputation stage involving O(n^3) arithmetic\noperations, but we also present some alternatives that may be more efficient in\npractise.\n","label":0,"model":"human","source":"arxiv","id":3521}
{"text":"  Since the introduction of binomial state as an intermediate state, different\nintermediate states have been proposed. Different nonclassical effects have\nalso been reported in these intermediate states. But till now higher order\nantibunching or higher order subpoissonian photon statistics is predicted only\nin one type of intermediate state, namely shadowed negative binomial state.\nRecently we have shown the existence of higher order antibunching in some\nsimple nonlinear optical processes to establish that higher order antibunching\nis not a rare phenomenon (J. Phys. B 39 (2006) 1137). To establish our earlier\nclaim further, here we have shown that the higher order antibunching can be\nseen in different intermediate states, such as binomial state, reciprocal\nbinomial state, hypergeometric state, generalized binomial state, negative\nbinomial state and photon added coherent state. We have studied the possibility\nof observing the higher order subpoissonian photon statistics in different\nlimits of intermediate states. The effect of different control parameters have\nalso been studied in this connection and it has been shown that the depth of\nnonclassicality can be tuned by controlling various physical parameters.\n","label":0,"model":"human","source":"arxiv","id":3522}
{"text":"  The knowledge of the nuclear symmetry energy of hot neutron-rich matter is\nimportant for understanding the dynamical evolution of massive stars and the\nsupernova explosion mechanisms. In particular, the electron capture rate on\nnuclei and\/or free protons in presupernova explosions is especially sensitive\nto the symmetry energy at finite temperature. In view of the above, in the\npresent work we calculate the symmetry energy as a function of the temperature\nfor various values of the baryon density, by applying a momentum-dependent\neffective interaction. In addition to a previous work, the thermal effects are\nstudied separately both in the kinetic part and the interaction part of the\nsymmetry energy. We focus also on the calculations of the mean field potential,\nemployed extensively in heavy ion reaction research, both for nuclear and pure\nneutron matter. The proton fraction and the electron chemical potential, which\nare crucial quantities for representing the thermal evolution of supernova and\nneutron stars, are calculated for various values of the temperature. Finally,\nwe construct a temperature dependent equation of state of $\\beta$-stable\nnuclear matter, the basic ingredient for the evaluation of the neutron star\nproperties.\n","label":0,"model":"human","source":"arxiv","id":3523}
{"text":"  \"Einstein-aether\" theory is a generally covariant theory of gravity\ncontaining a dynamical preferred frame. This article continues an examination\nof effects on the motion of binary pulsar systems in this theory, by\nincorporating effects due to strong fields in the vicinity of neutron star\npulsars. These effects are included through an effective approach, by treating\nthe compact bodies as point particles with nonstandard, velocity dependent\ninteractions parametrized by dimensionless \"sensitivities\". Effective\npost-Newtonian equations of motion for the bodies and the radiation damping\nrate are determined. More work is needed to calculate values of the\nsensitivities for a given fluid source, so precise constraints on the theory's\ncoupling constants cannot yet be stated. It is shown, however, that strong\nfield effects will be negligible given current observational uncertainties if\nthe dimensionless couplings are less than roughly 0.01 and two conditions that\nmatch the PPN parameters to those of pure general relativity are imposed. In\nthis case, weak field results suffice and imply one further condition on the\ncouplings. Thus, there exists a one-parameter family of Einstein-aether\ntheories with \"small-enough\" couplings that passes all current observational\ntests. No conclusion can yet be reached for large couplings.\n","label":0,"model":"human","source":"arxiv","id":3524}
{"text":"  We consider the question of how large a subspace of a given bipartite quantum\nsystem can be when the subspace contains only highly entangled states. This is\nmotivated in part by results of Hayden et al., which show that in large d x\nd--dimensional systems there exist random subspaces of dimension almost d^2,\nall of whose states have entropy of entanglement at least log d - O(1). It is\nalso related to results due to Parthasarathy on the dimension of completely\nentangled subspaces, which have connections with the construction of\nunextendible product bases. Here we take as entanglement measure the Schmidt\nrank, and determine, for every pair of local dimensions dA and dB, and every r,\nthe largest dimension of a subspace consisting only of entangled states of\nSchmidt rank r or larger. This exact answer is a significant improvement on the\nbest bounds that can be obtained using random subspace techniques. We also\ndetermine the converse: the largest dimension of a subspace with an upper bound\non the Schmidt rank. Finally, we discuss the question of subspaces containing\nonly states with Schmidt equal to r.\n","label":0,"model":"human","source":"arxiv","id":3525}
{"text":"  Elaborating on previous work (hep-th\/0605211, hep-th\/0611247), we show how\nthe linear and nonlinear chiral multiplets of N=4 supersymmetric mechanics with\nthe off-shell content (2,4,2) can be obtained by gauging three distinct\ntwo-parameter isometries of the ``root'' (4,4,0) multiplet actions. In\nparticular, two different gauge groups, one abelian and one non-abelian, lead,\nalbeit in a disguised form in the second case, to the same (unique) nonlinear\nchiral multiplet. This provides an evidence that no other nonlinear chiral N=4\nmultiplets exist. General sigma model type actions are discussed, together with\nthe restricted potential terms coming from the Fayet-Iliopoulos terms\nassociated with abelian gauge superfields. As in our previous work, we use the\nmanifestly supersymmetric language of N=4, d=1 harmonic superspace. A novel\npoint is the necessity to use in parallel the \\lambda and \\tau gauge frames,\nwith the ``bridges'' between these two frames playing a crucial role. It is the\nN=4 harmonic analyticity which, though being non-manifest in the \\tau frame,\ngives rise to both linear and nonlinear chirality constraints.\n","label":0,"model":"human","source":"arxiv","id":3526}
{"text":"  We analyze the robustness of H--deficient post--AGB tracks regarding previous\nevolution of their progenitor stars and the constitutive physics of the\nremnants. Our motivation is a recent suggestion of Werner & Herwig (2006) that\nprevious evolution should be important in shaping the final post--AGB track and\nthe persisting discrepancy between asteroseismological and spectroscopical mass\ndeterminations. This work is thus complementary to our previous work (Miller\nBertolami & Althaus 2006) and intends to shed some light on the uncertainty\nbehind the evolutionary tracks presented there. We compute full evolutionary\nmodels for PG1159 stars taking into account different extramixing\n(overshooting) efficiencies and lifetimes on the TP-AGB during the progenitor\nevolution. We also assess the effect of possible differences in the opacities\nand equation of state by artificially changing them before the PG1159 stage.\nAlso comparisons are made with the few H-deficient post--AGB tracks available\nin the literature. Contrary to our expectations, we found that previous\nevolution is not a main factor in shaping H--deficient post--AGB tracks.\nInterestingly enough, we find that only an increase of $\\sim50%$ in the\nintershell opacities at high effective temperatures may affect the tracks as to\nreconcile spectroscopic and asteroseismologic mass determinations. This forces\nus to conclude that our previous tracks (Miller Bertolami & Althaus 2006) are\nrobust enough as to be used for spectroscopic mass determinations, unless\nopacities in the intershell region are substantially different. Our results,\nthen, call for an analysis of possible systematics in the usually adopted\nasteroseismological mass determination methods.\n","label":0,"model":"human","source":"arxiv","id":3527}
{"text":"  We present 1-D numerical simulations of the very late thermal pulse\n  (VLTP) scenario for a wide range of remnant masses. We show that by taking\ninto account the different possible remnant masses, the observed evolution of\nV4334 Sgr (a.k.a. Sakurai's Object) can be reproduced within the standard\n1D-MLT stellar evolutionary models without the inclusion of any $ad-hoc$\nreduced mixing efficiency. Our simulations hint at a consistent picture with\npresent observations of V4334 Sgr. From energetics, and within the standard MLT\napproach, we show that low mass remnants \\hbox{($M\\lesssim0.6$\\msun)} are\nexpected to behave markedly different than higher mass remnants\n\\hbox{($M\\gtrsim0.6$\\msun)} in the sense that the latter are not expected to\nexpand significantly as a result of the violent H-burning that takes place\nduring the VLTP. We also assess the discrepancy in the born again times\nobtained by different authors by comparing the energy that can be liberated by\nH-burning during the VLTP event.\n","label":0,"model":"human","source":"arxiv","id":3528}
{"text":"  We consider the following problem of decentralized statistical inference:\ngiven i.i.d. samples from an unknown distribution, estimate an arbitrary\nquantile subject to limits on the number of bits exchanged. We analyze a\nstandard fusion-based architecture, in which each of $m$ sensors transmits a\nsingle bit to the fusion center, which in turn is permitted to send some number\n$k$ bits of feedback. Supposing that each of $\\nodenum$ sensors receives $n$\nobservations, the optimal centralized protocol yields mean-squared error\ndecaying as $\\order(1\/[n m])$. We develop and analyze the performance of\nvarious decentralized protocols in comparison to this centralized\ngold-standard. First, we describe a decentralized protocol based on $k =\n\\log(\\nodenum)$ bits of feedback that is strongly consistent, and achieves the\nsame asymptotic MSE as the centralized optimum. Second, we describe and analyze\na decentralized protocol based on only a single bit ($k=1$) of feedback. For\nstep sizes independent of $m$, it achieves an asymptotic MSE of order\n$\\order[1\/(n \\sqrt{m})]$, whereas for step sizes decaying as $1\/\\sqrt{m}$, it\nachieves the same $\\order(1\/[n m])$ decay in MSE as the centralized optimum.\nOur theoretical results are complemented by simulations, illustrating the\ntradeoffs between these different protocols.\n","label":0,"model":"human","source":"arxiv","id":3529}
{"text":"  Sequences that are defined by multisums of hypergeometric terms with compact\nsupport occur frequently in enumeration problems of combinatorics, algebraic\ngeometry and perturbative quantum field theory. The standard recipe to study\nthe asymptotic expansion of such sequences is to find a recurrence satisfied by\nthem, convert it into a differential equation satisfied by their generating\nseries, and analyze the singulatiries in the complex plane. We propose a\nshortcut by constructing directly from the structure of the hypergeometric term\na finite set, for which we conjecture (and in some cases prove) that it\ncontains all the singularities of the generating series. Our construction of\nthis finite set is given by the solution set of a balanced system of polynomial\nequations of a rather special form, reminiscent of the Bethe ansatz. The finite\nset can also be identified with the set of critical values of a potential\nfunction, as well as with the evaluation of elements of an additive $K$-theory\ngroup by a regulator function. We give a proof of our conjecture in some\nspecial cases, and we illustrate our results with numerous examples.\n","label":0,"model":"human","source":"arxiv","id":3530}
{"text":"  We investigate statistical properties of LRGs in a sample of X-ray selected\ngalaxy clusters at intermediate redshift ($0.2\\le z\\le0.6$). The LRGs are\nselected based on carefully designed color criteria, and the cluster membership\nis assessed via photometric redshifts. As clusters and LRGs are both viewed as\npromising tracer of the underlying dark matter distribution, understanding the\ndistribution of LRGs within clusters is an important issue.\n  Our main findings include:\n  1. The halo occupation distribution of LRGs inside our cluster sample is\n$N(M) = k\\times (M\/10^{14})^{a}$ where $a=0.620\\pm 0.105 $ and $k=1.425\\pm0.285\n$ assuming a Poisson distribution for $N(M)$.\n  2. The halo occupation distribution of LRGs ($N(M)$) and the satellite\ndistribution of LRGs ($N-1(M)$) are both consistent with being Poisson. To be\nmore quantitative, we find $Var(N)\/<N>= 1.428\\pm 0.351$ and $Var(N-1)\/<N-1> =\n1.823 \\pm 0.496$\n  3. The radial profile of LRGs within clusters when fitted with a NFW profile\ngives a concentration of $17.5^{+7.1}_{-4.3}$ ($6.0^{+3.2}_{-1.9}$) including\n(excluding) BLRGs (Brightest LRGs).\n  We also discuss the implications of these observations on the evolution of\nmassive galaxies in clusters.\n","label":0,"model":"human","source":"arxiv","id":3531}
{"text":"  In the framework of superstring compactifications with N=1 supersymmetry\nspontaneously broken, (by either geometrical fluxes, branes or else), we show\nthe existence of new inflationary solutions. The time-trajectory of the scale\nfactor of the metric a, the supersymmetry breaking scale m=m(Phi) and the\ntemperature T are such that am and aT remain constant. These solutions request\nthe presence of special moduli-fields: i) The universal \"no-scale-modulus\" Phi,\nwhich appears in all N=1 effective supergravity theories and defines the\nsupersymmetry breaking scale m(Phi). ii) The modulus Phi_s, which appears in a\nvery large class of string compactifications and has a Phi-dependent kinetic\nterm. During the time evolution, a^4 rho_s remains constant as well, (rho_s\nbeing the energy density induced by the motion of Phi_s). The cosmological term\nLambda(am), the curvature term k(am, aT) and the radiation term c_R=a^4 rho are\ndynamically generated in a controllable way by radiative and temperature\ncorrections; they are effectively constant during the time evolution. Depending\non Lambda, k and c_R, either a first or second order phase transition can occur\nin the cosmological scenario. In the first case, an instantonic Euclidean\nsolution exists and connects via tunneling the inflationary evolution to\nanother cosmological branch. The latter starts with a big bang and, in the case\nthe transition does not occur, ends with a big crunch. In the second case, the\nbig bang and the inflationary phase are smoothly connected.\n","label":0,"model":"human","source":"arxiv","id":3532}
{"text":"  We show that the same physical mechanism is fundamental for two seemingly\ndifferent phenomena such as the formation of two-level systems in glasses and\nthe Boson peak in the reduced density of low-frequency vibrational states\ng(w)\/w^2. This mechanism is the vibrational instability of weakly interacting\nharmonic modes. Below some frequency w_c << w_0 (where w_0 is of the order of\nDebye frequency) the instability, controlled by the anharmonicity, creates a\nnew stable universal spectrum of harmonic vibrations with a Boson peak feature\nas well as double-well potentials with a wide distribution of barrier heights.\nBoth are determined by the strength of the interaction I ~ w_c between the\noscillators. Our theory predicts in a natural way a small value for the\nimportant dimensionless parameter C ~ 10^{-4} for two-level systems in glasses.\nWe show that C ~ I^{-3} and decreases with increasing of the interaction\nstrength I. We show that the number of active two-level systems is very small,\nless than one per ten million of oscillators, in a good agreement with\nexperiment. Within the unified approach developed in the present paper the\ndensity of the tunneling states and the density of vibrational states at the\nBoson peak frequency are interrelated.\n","label":0,"model":"human","source":"arxiv","id":3533}
{"text":"  Radial velocity surveys find Jupiter mass planets with semi-major axes a less\nthan 0.1 AU around ~1% of solar-type stars; counting planets with $a$ as large\nas 5 AU, the fraction of stars having planets reaches ~ 10% {Marcy,Butler}. An\nexamination of the distribution of semi-major axes shows that there is a clear\nexcess of planets with orbital periods around 3 or 4 days, corresponding to\na~0.03$ AU, with a sharp cutoff at shorter periods (see Figure 1). It is\nbelieved that Jupiter mass planets form at large distances from their parent\nstars; some fraction then migrate in to produce the short period objects. We\nargue that a significant fraction of the `hot Jupiters' (a<0.1 AU) may arise in\nbinary star systems in which the orbit of the binary is highly inclined to the\norbit of the planet. Mutual torques between the two orbits drive down the\nminimum separation or periapse r_p between the planet and its host star (the\nKozai mechanism). This periapse collapse is halted when tidal friction on the\nplanet circularizes the orbit faster than Kozai torque can excite it. The same\nfriction then circularizes the planet orbit, producing hot Jupiters with the\npeak of the semimajor axis distribution lying around 3 days. For the observed\ndistributions of binary separation, eccentricity and mass ratio, roughly 2.5%\nof planets with initial semimajor axis a_p ~ 5au will migrate to within 0.1au\nof their parent star. Kozai migration could account for 10% or more of the\nobserved hot Jupiters.\n","label":0,"model":"human","source":"arxiv","id":3534}
{"text":"  We use large-scale three-dimensional simulations of supersonic Euler\nturbulence to study the physics of a highly compressible cascade. Our numerical\nexperiments describe non-magnetized driven turbulent flows with an isothermal\nequation of state and an rms Mach number of 6. We find that the inertial range\nvelocity scaling deviates strongly from the incompressible Kolmogorov laws. We\npropose an extension of Kolmogorov's K41 phenomenology that takes into account\ncompressibility by mixing the velocity and density statistics and preserves the\nK41 scaling of the density-weighted velocity v=rho^{1\/3}u. We show that\nlow-order statistics of 'v' are invariant with respect to changes in the Mach\nnumber. For instance, at Mach 6 the slope of the power spectrum of 'v' is -1.69\nand the third-order structure function of 'v' scales linearly with separation.\nWe directly measure the mass dimension of the \"fractal\" density distribution in\nthe inertial subrange, D_m=2.4, which is similar to the observed fractal\ndimension of molecular clouds and agrees well with the cascade phenomenology.\n","label":0,"model":"human","source":"arxiv","id":3535}
{"text":"  Extending our earlier work on PSL(2|2), we explain how to reduce the solution\nof WZNW models on general type I supergroups to those defined on the bosonic\nsubgroup. The new analysis covers in particular the supergroups GL(M|N) along\nwith several close relatives such as PSL(N|N), certain Poincare supergroups and\nthe series OSP(2|2N). This remarkable progress relies on the use of a special\nFeigin-Fuchs type representation. In preparation for the field theory analysis,\nwe shall exploit a minisuperspace analogue of a free fermion construction to\ndeduce the spectrum of the Laplacian on type I supergroups. The latter is shown\nto be non-diagonalizable. After lifting these results to the full WZNW model,\nwe address various issues of the field theory, including its modular invariance\nand the computation of correlation functions. In agreement with previous\nfindings, supergroup WZNW models allow to study chiral and non-chiral aspects\nof logarithmic conformal field theory within a geometric framework. We shall\nbriefly indicate how insights from WZNW models carry over to non-geometric\nexamples, such as e.g. the W(p) triplet models.\n","label":0,"model":"human","source":"arxiv","id":3536}
{"text":"  Spectroscopic observations and theoretical models suggest resonant\nwave-particle interactions, involving high-frequency ion-cyclotron waves, as\nthe principal mechanism for heating and accelerating ions in the open coronal\nholes. However, the mechanism responsible for the generation of the\nion-cyclotron waves remains unclear. One possible scenario is that ion beams\noriginating from small-scale reconnection events can drive micro-instabilities\nthat constitute a possible source for the excitation of ion-cyclotron waves. In\norder to study ion beam-driven electromagnetic instabilities, the multi-fluid\nmodel in the low-beta coronal plasma is used. While neglecting the electron\ninertia this model allows one to take into account ion-cyclotron wave effects\nthat are absent from the one-fluid MHD model. Realistic models of density and\ntemperature as well as a 2-D analytical magnetic field model are used to define\nthe background plasma in the open-field funnel region of a polar coronal hole.\nConsidering the WKB approximation, a Fourier plane-wave linear mode analysis is\nemployed in order to derive the dispersion relation. Ray-tracing theory is used\nto compute the ray path of the unstable wave as well as the evolution of the\ngrowth rate of the wave while propagating in the coronal funnel. We demonstrate\nthat, in typical coronal holes conditions and assuming realistic values of the\nbeam velocity, the free energy provided by the ion beam propagating parallel\nthe ambient field can drive micro-instabilities through resonant ion-cyclotron\nexcitation.\n","label":0,"model":"human","source":"arxiv","id":3537}
{"text":"  We examine the question of whether the formal expressions of equilibrium\nstatistical mechanics can be applied to time independent non-dissipative\nsystems that are not in true thermodynamic equilibrium and are nonergodic. By\nassuming the phase space may be divided into time independent, locally ergodic\ndomains, we argue that within such domains the relative probabilities of\nmicrostates are given by the standard Boltzmann weights. In contrast to\nprevious energy landscape treatments, that have been developed specifically for\nthe glass transition, we do not impose an a priori knowledge of the\ninter-domain population distribution. Assuming that these domains are robust\nwith respect to small changes in thermodynamic state variables we derive a\nvariety of fluctuation formulae for these systems. We verify our theoretical\nresults using molecular dynamics simulations on a model glass forming system.\nNon-equilibrium Transient Fluctuation Relations are derived for the\nfluctuations resulting from a sudden finite change to the system's temperature\nor pressure and these are shown to be consistent with the simulation results.\nThe necessary and sufficient conditions for these relations to be valid are\nthat the domains are internally populated by Boltzmann statistics and that the\ndomains are robust. The Transient Fluctuation Relations thus provide an\nindependent quantitative justification for the assumptions used in our\nstatistical mechanical treatment of these systems.\n","label":0,"model":"human","source":"arxiv","id":3538}
{"text":"  The addition of metals to any gas can significantly alter its evolution by\nincreasing the rate of radiative cooling. In star-forming environments,\nenhanced cooling can potentially lead to fragmentation and the formation of\nlow-mass stars, where metal-free gas-clouds have been shown not to fragment.\nAdding metal cooling to numerical simulations has traditionally required a\nchoice between speed and accuracy. We introduce a method that uses the\nsophisticated chemical network of the photoionization software, Cloudy, to\ninclude radiative cooling from a complete set of metals up to atomic number 30\n(Zn) that can be used with large-scale three-dimensional hydrodynamic\nsimulations. Our method is valid over an extremely large temperature range (10\nK < T < 10^8 K), up to hydrogen number densities of 10^12 cm^-3. At this\ndensity, a sphere of 1 Msun has a radius of roughly 40 AU. We implement our\nmethod in the adaptive mesh refinement (AMR) hydrodynamic\/N-body code, Enzo.\nUsing cooling rates generated with this method, we study the physical\nconditions that led to the transition from Population III to Population II star\nformation. While C, O, Fe, and Si have been previously shown to make the\nstrongest contribution to the cooling in low-metallicity gas, we find that up\nto 40% of the metal cooling comes from fine-structure emission by S, when solar\nabundance patterns are present. At metallicities, Z > 10^-4 Zsun, regions of\ndensity and temperature exist where gas is both thermally unstable and has a\ncooling time less than its dynamical time. We identify these doubly unstable\nregions as the most inducive to fragmentation. At high redshifts, the CMB\ninhibits efficient cooling at low temperatures and, thus, reduces the size of\nthe doubly unstable regions, making fragmentation more difficult.\n","label":0,"model":"human","source":"arxiv","id":3539}
{"text":"  We describe the calibration status and data products pertaining to the GR2\nand GR3 data releases of the Galaxy Evolution Explorer (GALEX). These releases\nhave identical pipeline calibrations that are significantly improved over the\nGR1 data release. GALEX continues to survey the sky in the Far Ultraviolet\n(FUV, ~154 nm) and Near Ultraviolet (NUV, ~232 nm) bands, providing\nsimultaneous imaging with a pair of photon counting, microchannel plate, delay\nline readout detectors. These 1.25 degree field-of-view detectors are\nwell-suited to ultraviolet observations because of their excellent red\nrejection and negligible background. A dithered mode of observing and photon\nlist output pose complex requirements on the data processing pipeline,\nentangling detector calibrations and aspect reconstruction algorithms. Recent\nimprovements have achieved photometric repeatability of 0.05 and 0.03 mAB in\nthe FUV and NUV, respectively. We have detected a long term drift of order 1%\nFUV and 6% NUV over the mission. Astrometric precision is of order 0.5\" RMS in\nboth bands. In this paper we provide the GALEX user with a broad overview of\nthe calibration issues likely to be confronted in the current release.\nImprovements are likely as the GALEX mission continues into an extended phase\nwith a healthy instrument, no consumables, and increased opportunities for\nguest investigations.\n","label":0,"model":"human","source":"arxiv","id":3540}
{"text":"  We analyze a recently proposed method to create fractional quantum Hall (FQH)\nstates of atoms confined in optical lattices [A. S{\\o}rensen {\\it et al.},\nPhys. Rev. Lett. {\\bf 94} 086803 (2005)]. Extending the previous work, we\ninvestigate conditions under which the FQH effect can be achieved for bosons on\na lattice with an effective magnetic field and finite onsite interaction.\nFurthermore, we characterize the ground state in such systems by calculating\nChern numbers which can provide direct signatures of topological order and\nexplore regimes where the characterization in terms of wavefunction overlap\nfails. We also discuss various issues which are relevant for the practical\nrealization of such FQH states with ultracold atoms in an optical lattice,\nincluding the presence of the long-range dipole interaction which can improve\nthe energy gap and stabilize the ground state. We also investigate a new\ndetection technique based on Bragg spectroscopy to probe these system in an\nexperimental realization.\n","label":0,"model":"human","source":"arxiv","id":3541}
{"text":"  We study the stabilizing effect of rotational forcing in the nonlinear\nsetting of two-dimensional shallow-water and more general models of\ncompressible Euler equations. In [H. Liu and E. Tadmor, Phys. D 188 (2004), no.\n3-4, 262-276] we have shown that the pressureless version of these equations\nadmit global smooth solution for a large set of sub-critical initial\nconfigurations. In the present work we prove that when rotational force\ndominates the pressure, it \\emph{prolongs} the life-span of smooth solutions\nfor t < ln(1\/d); here d << 1 is the ratio of the pressure gradient measured by\nthe inverse squared Froude number, relative to the dominant rotational forces\nmeasured by the inverse Rossby number. Our study reveals a ``nearby''\nperiodic-in-time approximate solution in the small d-regime, upon which hinges\nthe long time existence of the exact smooth solution. These results are in\nagreement with the close-to periodic dynamics observed in the ``near inertial\noscillation'' (NIO) regime which follows oceanic storms. Indeed, our results\nindicate the existence of smooth, ``approximate periodic'' solution for a time\nperiod of \\emph{days}, which is the relevant time period found in NIO\nobesrvations.\n","label":0,"model":"human","source":"arxiv","id":3542}
{"text":"  In this paper we present a novel framework for sequence to shape maps. These\ncombinatorial maps realize exponentially many shapes, and have preimages which\ncontain extended connected subgraphs of diameter n (neutral networks). We prove\nthat all basic properties of RNA folding maps also hold for combinatorial maps.\nOur construction is as follows: suppose we are given a graph $H$ over the $\\{1\n>...,n\\}$ and an alphabet of nucleotides together with a symmetric relation\n$\\mathcal{R}$, implied by base pairing rules. Then the shape of a sequence of\nlength n is the maximal H subgraph in which all pairs of nucleotides incident\nto H-edges satisfy $\\mathcal{R}$. Our main result is to prove the existence of\nat least $\\sqrt{2}^{n-1}$ shapes with extended neutral networks, i.e. shapes\nthat have a preimage with diameter $n$ and a connected component of size at\nleast $(\\frac{1+\\sqrt{5}}{2})^n+(\\frac{1-\\sqrt{5}}{2})^n$. Furthermore, we show\nthat there exists a certain subset of shapes which carries a natural graph\nstructure. In this graph any two shapes are connected by a path of shapes with\nrespective neutral networks of distance one. We finally discuss our results and\nprovide a comparison with RNA folding maps.\n","label":0,"model":"human","source":"arxiv","id":3543}
{"text":"  We explore an accretion model for low luminosity AGN (LLAGN) that attributes\nthe low radiative output to a low mass accretion rate rather than a low\nradiative efficiency. In this model, electrons are assumed to drain energy from\nthe ions as a result of collisionless plasma microinstabilities. Consequently,\nthe accreting gas collapses to form a geometrically thin disk at small radii\nand is able to cool before reaching the black hole. The accretion disk is not a\nstandard disk, however, because the radial disk structure is modified by a\nmagnetic torque which drives a jet and which is primarily responsible for\nangular momentum transport. We also include relativistic effects. We apply this\nmodel to the well known LLAGN M87 and calculate the combined disk-jet\nsteady-state broadband spectrum. A comparison between predicted and observed\nspectra indicates that M87 may be a maximally spinning black hole accreting at\na rate of 10^{-3} solar masses per year. This is about 6 orders of magnitude\nbelow the Eddington rate for the same radiative efficiency. Furthermore, the\ntotal jet power inferred by our model is in remarkably good agreement with the\nvalue independently deduced from observations of the M87 jet on kiloparsec\nscales.\n","label":0,"model":"human","source":"arxiv","id":3544}
{"text":"  Dispersive readouts for superconducting qubits have the advantage of speed\nand minimal invasiveness. We have developed such an amplifier, the Cavity\nBifurcation Amplifier (CBA) [10], and applied it to the readout of the\nquantronium qubit [2]. It consists of a Josephson junction embedded in a\nmicrowave on-chip resonator. In contrast with the Josephson bifurcation\namplifier [17], which has an on-chip capacitor shunting a junction, the\nresonator is based on a simple coplanar waveguide imposing a pre-determined\nfrequency and whose other RF characteristics like the quality factor are easily\ncontrolled and optimized. Under proper microwave irradiation conditions, the\nCBA has two metastable states. Which state is adopted by the CBA depends on the\nstate of a quantronium qubit coupled to the CBA's junction. Due to the MHz\nrepetition rate and large signal to noise ratio we can show directly that the\ncoherence is limited by 1\/f gate charge noise when biased at the sweet spot - a\npoint insensitive to first order gate charge fluctuations. This architecture\nlends itself to scalable quantum computing using a multi-resonator chip with\nmultiplexed readouts.\n","label":0,"model":"human","source":"arxiv","id":3545}
{"text":"  We presented the result of a high resolution (FWHM~0.5'') 12 ks Chandra HRC-I\nobservation of the starburst galaxy IC342 taken on 2 April 2006. We identified\n23 X-ray sources within the central 30' x 30' region of IC342. Our HRC-I\nobservation resolved the historical Ultraluminous X-ray sources (ULX), X3, near\nthe nucleus into 2 sources, namely C12 and C13, for the first time. The\nbrighter source C12, with L(0.08-10keV)=(6.66\\pm0.45)\\times10^{38}ergs^-1, was\nspatially extended (~82 pc x 127 pc). From the astrometric registration of the\nX-ray image, C12 was at R.A.=03h:46m:48.43s, decl.=+68d05m47.45s, and was\ncloser to the nucleus than C13. Thus we concluded that source was not an ULX\nand must instead be associated with the nucleus. The fainter source C13, with\nL(0.08-10keV)=(5.1\\pm1.4) x 10^{37}ergs^-1 was consistent with a point source\nand located $6.51'' at P.A. 240 degree of C12.\n  We also analyzed astrometrically corrected optical Hubble Space Telescope and\nradio Very Large Array images, a comparison with the X-ray image showed\nsimilarities in their morphologies. Regions of star formation within the\ncentral region of IC342 were clearly visible in HST H alpha image and this was\nthe region where 3 optical star clusters and correspondingly our detected X-ray\nsource C12 were observed. We found that a predicted X-ray emission from\nstarburst was very close to the observed X-ray luminosity of C12, suggesting\nthat nuclear X-ray emission in IC342 was dominated by starburst. Furthermore,\nwe discussed the possibility of AGN in the nucleus of IC342. Although our data\nwas not enough to give a firm existence of an AGN, it could not be discarded.\n","label":0,"model":"human","source":"arxiv","id":3546}
{"text":"  Embedded Si and Ge nanocrystals (NCs) in wide band-gap matrices are studied\ntheoretically using an atomistic pseudopotential approach. From small clusters\nto large NCs containing on the order of several thousand atoms are considered.\nEffective band-gap values as a function of NC diameter reproduce very well the\navailable experimental and theoretical data. It is observed that the highest\noccupied molecular orbital for both Si and Ge NCs and the lowest unoccupied\nmolecular orbital for Si NCs display oscillations with respect to size among\nthe different irreducible representations of the $C_{3v}$ point group to which\nthese spherical NCs belong. Based on this electronic structure, first the\ninterband absorption is thoroughly studied which shows the importance of\nsurface polarization effects that significantly reduce the absorption when\nincluded. This reduction is found to increase with decreasing NC size or with\nincreasing permittivity mismatch between the NC core and the host matrix.\nReasonable agreement is observed with the experimental absorption spectra where\navailable. The deformation of spherical NCs into prolate or oblate ellipsoids\nare seen to introduce no pronounced effects for the absorption spectra. Next,\nintraconduction and intravalence band absorption coefficients are obtained in\nthe wavelength range from far-infrared to visible region. These results can be\nvaluable for the infrared photodetection prospects of these NC arrays. Finally,\nexcited-state absorption at three different optical pump wavelengths, 532 nm,\n355 nm and 266 nm are studied for 3- and 4 nm-diameter NCs. This reveals strong\nabsorption windows in the case of holes and a broad spectrum in the case of\nelectrons which can especially be relevant for the discussions on achieving\ngain in these structures.\n","label":0,"model":"human","source":"arxiv","id":3547}
{"text":"  The use of micro-Raman spectroscopy, through chemical bond nano-scale probes,\nallows the changes in conformations (alpha helix -> beta sheet), chain\norientation, disconnection of disulfide bonds (-20%) and the increase of intra\nand inter-chain distances during the strain to be distinguished. The\ncombination of micro-Raman spectroscopy and a allows a quantitative measure of\nthe extension of chemical bonds in the peptidic chain during loading. The\nnano-structural transformations of keratin during the strain of human hair in a\ndry environment (40-60 % relative humidity) and saturated with water have been\nstudied. The water permits the sliding of the chains and decreases the bond\nenergy hair. Spectral analyses and 2D correlation are two coherent and\nindependent methods to follow change the Raman probes which are sensitive to\nstructural . The between nano-mechanical (Raman) and micro-mechanical\n(strain\/stress) analyses confirms the validity of the experimental results,\ntools and principles used, as well as the agreement with the structural model\nof keratin fibres described by Chapman & Hearle.\n","label":0,"model":"human","source":"arxiv","id":3548}
{"text":"  We report a definitive confirmation of a large-scale structure around the\nsuper rich cluster CL0016+1609 at z=0.55. We made spectroscopic follow-up\nobservations with FOCAS on Subaru along the large filamentary structure\nidentified in our previous photometric studies. We have confirmed the physical\nconnection of the huge filament extending over 20 Mpc in the N-S direction, and\nanother filament extending from the main cluster to the East. Based on a simple\nenergy argument, we show that it is likely that most of the clumps are bound to\nthe main CL0016 cluster. This structure is surely one of the most prominent\nconfirmed structures ever identified in the distant Universe, which then serves\nas an ideal laboratory to examine the environmental variation of galaxy\nproperties. We draw star formation histories of galaxies from the composite\nspectra of red galaxies in field, group, and cluster environments. Combining\nthe results from our previous studies, we find that red galaxies in groups at\nz~0.8 and red field galaxies at z~0.5 show strong Hd absorption lines for their\nD4000 indices. These are the environments in which we observed the on-going\nbuild-up of the colour-magnitude relation in our previous photometric analyses.\nThe strong Hd absorption lines imply that their star formation is truncated on\na relatively short time scale. We suggest that a galaxy-galaxy interaction is\nthe most likely physical driver of the truncation of star formation and thus\nresponsible for the build-up of the colour-magnitude relation since z~1.\n(Abridged)\n","label":0,"model":"human","source":"arxiv","id":3549}
{"text":"  (Abridged) We have obtained radial velocities of a sample of 18 ultracool\ndwarfs (M6.5-T8) using high-resolution, near-infrared spectra obtained with\nNIRSPEC and the Keck II telescope. We have confirmed that the radial velocity\nof Gl 570 D is coincident with that of the K-type primary star Gl 570 A, thus\nproviding additional support for their true companionship. The presence of\nplanetary-mass companions around 2MASS J05591914-1404488 (T4.5V) has been\nanalyzed using five NIRSPEC radial velocity measurements obtained over a period\nof 4.37 yr. We have computed UVW space motions for a total of 21 L and T dwarfs\nwithin 20 pc of the Sun. This population shows UVW velocities that nicely\noverlap the typical kinematics of solar to M-type stars within the same spatial\nvolume. However, the mean Galactic (44.2 km\/s) and tangential (36.5 km\/s)\nvelocities of the L and T dwarfs appear to be smaller than those of G to M\nstars. A significant fraction (~40%) of the L and T dwarfs lies near the Hyades\nmoving group (0.4-2 Gyr), which contrasts with the 10-12% found for\nearlier-type stellar neighbors. Additionally, the distributions of all three\nUVW components (sigma_{UVW} = 30.2, 16.5, 15.8 km\/s) and the distributions of\nthe total Galactic (sigma_{v_tot} = 19.1 km\/s) and tangential (sigma_{v_t} =\n17.6 km\/s) velocities derived for the L and T dwarf sample are narrower than\nthose measured for nearby G, K, and M-type stars, but similar to the\ndispersions obtained for F stars. This suggests that, in the solar\nneighborhood, the L- and T-type ultracool dwarfs in our sample (including brown\ndwarfs) is kinematically younger than solar-type to early M stars with likely\nages in the interval 0.5-4 Gyr.\n","label":0,"model":"human","source":"arxiv","id":3550}
{"text":"  The problem of the definition and the estimation of generative models based\non deformable templates from raw data is of particular importance for modelling\nnon aligned data affected by various types of geometrical variability. This is\nespecially true in shape modelling in the computer vision community or in\nprobabilistic atlas building for Computational Anatomy (CA). A first coherent\nstatistical framework modelling the geometrical variability as hidden variables\nhas been given by Allassonni\\`ere, Amit and Trouv\\'e (JRSS 2006). Setting the\nproblem in a Bayesian context they proved the consistency of the MAP estimator\nand provided a simple iterative deterministic algorithm with an EM flavour\nleading to some reasonable approximations of the MAP estimator under low noise\nconditions. In this paper we present a stochastic algorithm for approximating\nthe MAP estimator in the spirit of the SAEM algorithm. We prove its convergence\nto a critical point of the observed likelihood with an illustration on images\nof handwritten digits.\n","label":0,"model":"human","source":"arxiv","id":3551}
{"text":"  Convection in stars excites resonant acoustic waves which depend on the sound\nspeed inside the star, which in turn depends on properties of the stellar\ninterior. Therefore, asteroseismology is an unrivaled method to probe the\ninternal structure of a star. We made a seismic study of the metal-poor\nsubgiant star nu Indi with the goal of constraining its interior structure. Our\nstudy is based on a time series of 1201 radial velocity measurements spread\nover 14 nights obtained from two sites, Siding Spring Observatory in Australia\nand ESO La Silla Observatory in Chile. The power spectrum of the high precision\nvelocity time series clearly presents several identifiable peaks between 200\nand 500 uHz showing regularity with a large and small spacing of 25.14 +- 0.09\nuHz and 2.96 +- 0.22 uHz at 330 uHz. Thirteen individual modes have been\nidentified with amplitudes in the range 53 to 173 cm\/s. The mode damping time\nis estimated to be about 16 days (1-sigma range between 9 and 50 days),\nsubstantially longer than in other stars like the Sun, the alpha Cen system or\nthe giant xi Hya.\n","label":0,"model":"human","source":"arxiv","id":3552}
{"text":"  We demonstrate an efficient scheme for continuous trap loading based upon\nspatially selective optical pumping. We discuss the case of $^{1}$S$_{0}$\ncalcium atoms in an optical dipole trap (ODT), however, similar strategies\nshould be applicable to a wide range of atomic species. Our starting point is a\nreservoir of moderately cold ($\\approx 300 \\mu$K) metastable\n$^{3}$P$_{2}$-atoms prepared by means of a magneto-optic trap (triplet-MOT). A\nfocused 532 nm laser beam produces a strongly elongated optical potential for\n$^{1}$S$_{0}$-atoms with up to 350 $\\mu$K well depth. A weak focused laser beam\nat 430 nm, carefully superimposed upon the ODT beam, selectively pumps the\n$^{3}$P$_{2}$-atoms inside the capture volume to the singlet state, where they\nare confined by the ODT. The triplet-MOT perpetually refills the capture volume\nwith $^{3}$P$_{2}$-atoms thus providing a continuous stream of cold atoms into\nthe ODT at a rate of $10^7 $s$^{-1}$. Limited by evaporation loss, in 200 ms we\ntypically load $5 \\times 10^5$ atoms with an initial radial temperature of 85\n$\\mu$K. After terminating the loading we observe evaporation during 50 ms\nleaving us with $10^5$ atoms at radial temperatures close to 40 $\\mu$K and a\npeak phase space density of $6.8 \\times 10^{-5}$. We point out that a\ncomparable scheme could be employed to load a dipole trap with\n$^{3}$P$_{0}$-atoms.\n","label":0,"model":"human","source":"arxiv","id":3553}
{"text":"  The high-frequency peaked BL Lac PKS 2155-304 at redshift z=0.116 is a\nwell-known VHE (>100 GeV) gamma-ray emitter. Since 2002 its VHE flux has been\nmonitored using the H.E.S.S. stereoscopic array of imaging\natmospheric-Cherenkov telescopes in Namibia. During the July 2006 dark period,\nthe average VHE flux was measured to be more than ten times typical values\nobserved from the object. This article focuses solely on an extreme gamma-ray\noutburst detected in the early hours of July 28, 2006 (MJD 53944). The average\nflux observed during this outburst is I(>200 GeV) = (1.72$\\pm$$0.05_{\\rm\nstat}$$\\pm$$0.34_{\\rm syst}$) $\\times$ 10$^{-9}$ cm$^{-2}$ s$^{-1}$,\ncorresponding to ~7 times the flux, I(>200 GeV), observed from the Crab Nebula.\nPeak fluxes are measured with one-minute time scale resolution at more than\ntwice this average value. Variability is seen up to ~600 s in the Fourier power\nspectrum, and well-resolved bursts varying on time scales of ~200 seconds are\nobserved. There are no strong indications for spectral variability within the\ndata. Assuming the emission region has a size comparable to the Schwarzschild\nradius of a ~10$^9 M_\\odot$ black hole, Doppler factors greater than 100 are\nrequired to accommodate the observed variability time scales.\n","label":0,"model":"human","source":"arxiv","id":3554}
{"text":"  We show that for a hypersurface Batyrev's stringy E-function can be seen as a\nresidue of the Hodge zeta function, a specialization of the motivic zeta\nfunction of Denef and Loeser. This is a nice application of inversion of\nadjunction. If an affine hypersurface is given by a polynomial that is\nnon-degenerate with respect to its Newton polyhedron, then the motivic zeta\nfunction and thus the stringy E-function can be computed from this Newton\npolyhedron (by work of Artal, Cassou-Nogues, Luengo and Melle based on an\nalgorithm of Denef and Hoornaert). We use this procedure to obtain an easy way\nto compute the contribution of a Brieskorn singularity to the stringy\nE-function. As a corollary, we prove that stringy Hodge numbers of varieties\nwith a certain class of strictly canonical Brieskorn singularities are\nnonnegative. We conclude by computing an interesting 6-dimensional example. It\nshows that a result, implying nonnegativity of stringy Hodge numbers in lower\ndimensional cases, obtained in our previous paper, is not true in higher\ndimension.\n","label":0,"model":"human","source":"arxiv","id":3555}
{"text":"  Integrated light from distant galaxies is often compared to stellar\npopulation models via the equivalent widths of spectral features--spectral\nindices--whose strengths rely on the abundances of one or more elements. Such\ncomparisons hinge not only on the overall metal abundance but also on relative\nabundances. Studies have examined the influence of individual elements on\nsynthetic spectra but little has been done to address similar issues in the\nstellar evolution models that underlie most stellar population models. Stellar\nevolution models will primarily be influenced by changes in opacities. In order\nto explore this issue in detail, twelve sets of stellar evolution tracks and\nisochrones have been created at constant heavy element mass fraction Z that\nself-consistently account for varying heavy element mixtures. These sets\ninclude scaled-solar, alpha-enhanced, and individual cases where the elements\nC, N, O, Ne, Mg, Si, S, Ca, Ti, and Fe have been enhanced above their\nscaled-solar values. The variations that arise between scaled-solar and the\nother cases are examined with respect to the H-R diagram and main sequence\nlifetimes.\n","label":0,"model":"human","source":"arxiv","id":3556}
{"text":"  Ultraviolet (UV) galaxies have been selected from GALEX. The presence of a\nFUV-dropout in their spectral energy distributions proved to be a very complete\n(83.3%) but not very efficient (21.4%) tool for identifying Lyman Break\nGalaxies (LBGs) at z~1. We divide the LBG sample into two sub-classes: red LBGs\n(RLBGs) detected at 24 micron which are mainly Luminous IR Galaxies (LIRGs) and\nblue LBGs (BLBGs) undetected at 24 microns down to 83 microJy. Two of the RLBGs\nare also detected at 70 micron. The median SED of the RLBGs is similar (above\nlambda~1 micron) to the dusty starburst HR10. However, unlike local (U)LIRGs,\nRLBGs are UV bright objects. We suggest that these objects contain a large\namount of dust but that some bare stellar populations are also directly\nvisible. The median SED of the BLBGs is consistent with their containing the\nsame stellar population as the RLBGs but with a lower dust content. The\nluminosity function of our LBG sample at z~1 is similar to the luminosity\nfunction of NUV-selected galaxies at the same redshift. The integrated\nluminosity densities of z~1 LBGs and NUV-selected galaxies are very consistent.\nWe show that star formation rates (SFRs) estimated from UV measurements and\ncorrected using the IRX-beta method provide average total SFR_TOT in agreement\nwith SFR_UV + SFR_dust. However, IRX-beta-based SFR_TOT shows a large\ndispersion. Summing up the detected UV (1150A rest-frame) and IR-based star\nformation rates of the detected objects, we find that only one third of the\ntotal (i.e. UV + dust) LBG SFR resides in BLBGs and two thirds in RLBGs, even\nthough most LBGs at z~1 are BLBGs. On the other hand, the total SFR of LBGs\naccounts for only 11% of the total SFR at z~1. Finally, we observe a regular\ndecrease of L_TIR \/ L_FUV from z=0 to z~2 for UV-selected samples.\n","label":0,"model":"human","source":"arxiv","id":3557}
{"text":"  We report on detections of the high-excitation CO J=6-5, J=4-3 lines in\nMrk231, a prototypical Ultra Luminous Infrared Galaxy (ULIRG) and Seyfert 1\nQSO. These observations are combined with CO J=3-2, HCN J=4-3 (this work), and\nCO J=2-1, J=1-0, 13CO J=2-1, HCN J=1-0 measurements taken from the literature\nto provide better constraints on the properties of the molecular gas in an\nextreme starburst\/QSO in the local Universe. We find that the CO J=4-3 and\nJ=6-5 transitions trace a different gas phase from that dominating the lower\nthree CO transitions, with n(H_2) ~ (1-3)x10^4 cm-3 and Tk ~ (40-70) K. This\nphase is responsible for the luminous HCN emission, and contains most of the H2\ngas mass of this galaxy. The total CO line cooling emanating from this dense\nphase is found similar to that of the [CII] line at 158 micron, suggesting a\nvery different thermal balance to that seen in lower IR-luminosity galaxies,\nand one likely dominated by dense photon-dominated regions. Our dense\n\"sampling\" of the CO rotational ladder and the HCN lines enables us to produce\nwell-constrained Spectral Line Energy Distributions (SLEDs) for the dense\nmolecular gas in Mrk231 and compare them to those of high redshift starbursts,\nmany of which have SLEDs that may be affected by strong lensing. Finally, we\nuse our local molecular line excitation template to assess the capabilities of\nfuture cm and mm\/sub-mm arrays in detecting CO and HCN transitions in similar\nsystems throughout the local and distant universe.\n","label":0,"model":"human","source":"arxiv","id":3558}
{"text":"  A non-local cascade model for anisotropic MHD turbulence in the presence of a\nguiding magnetic field is proposed. The model takes into account that (a)\nenergy cascades in an anisotropic manner and as a result a different estimate\nfor the cascade rate in the direction parallel and perpendicular to the guiding\nfield is made. (b) the interactions that result in the cascade are between\ndifferent scales. Eddies with wave numbers $k_\\|$ and $k_\\perp$ interact with\neddies with wave numbers $q_\\|,q_\\perp$ such that a resonance condition between\nthe wave numbers $q_\\|,q_\\perp$ and $k_\\|,k_\\perp$ holds. As a consequence\nenergy from the eddy with wave numbers $k_\\|$ and $k_\\perp$ cascades due to\ninteractions with eddies located in the resonant manifold whose wavenumbers are\ndetermined by: $q_\\|\\simeq \\epsilon^{{1}\/{3}}k_\\perp^{2\/3}\/B$,\n$q_\\perp=k_\\perp$ and energy will cascade along the lines $k_\\|\\sim\nC+k_\\perp^{2\/3} \\epsilon^{1\/3}\/B_0$. For a uniform energy injection rate in the\nparallel direction the resulting energy spectrum is $E(k_\\|,k_\\perp)\\simeq\n\\epsilon^{2\/3}k_\\|^{-1}k_\\perp^{-5\/3}$. For a general forcing however the model\nsuggests a non-universal behavior. The connections with previous models,\nnumerical simulations and weak turbulence theory are discussed.\n","label":0,"model":"human","source":"arxiv","id":3559}
{"text":"  This is the second paper studying the QSOs in the spitzer QUEST sample.\nPreviously we presented new PAH measurements and argued that most of the\nobserved far infrared (FIR) radiation is due to star-forming activity. Here we\npresent spectral energy distributions (SEDs) by supplementing our data with\noptical, NIR and FIR observations. We define two sub-groups of ``weak FIR'' and\n``strong FIR'' QSOs, and a third group of FIR non-detections. Assuming a\nstarburst origin for the FIR, we obtain ``intrinsic'' AGN SEDs by subtracting a\nstarburst template from the mean SEDs. The resulting SEDs are remarkably\nsimilar for all groups. They show three distinct peaks corresponding to two\nsilicate emission features and a 3mic bump that we interpret as the signature\nof the hottest AGN dust. They also display drops beyond 20mic that we interpret\nas the signature of the minimum temperature (about 200K) dust. This component\nmust be optically thin to explain the silicate emission and the slope of the\nlong wavelength continuum. We discuss the merits of an alternative model where\nmost of the FIR emission is due to AGN heating. Such models are unlikely to\nexplain the properties of our QSOs but they cannot be ruled out for more\nluminous objects. We also find correlations between the luminosity at 5100A and\ntwo infrared starburst indicators: L(60mic) and L(PAH 7.7mic). The correlation\nof L(5100A) with L(60mic) can be used to measure the relative growth rates and\nlifetimes of the black hole and the new stars.\n","label":0,"model":"human","source":"arxiv","id":3560}
{"text":"  Surface magnetic properties of perovskite manganites have been a recurrent\ntopic during last years since they play a major role in the implementation of\nmagnetoelectronic devices. Magneto-optical techniques, such as X-ray magnetic\ncircular dichroism, turn out to be a very efficient tool to study surface\nmagnetism due to their sensitivity to magnetic and chemical variations across\nthe sample depth. Nevertheless, the application of the sum rules for the\ndetermination of the spin magnetic moment might lead to uncertainties as large\nas 40% in case of Mn ions. To overcome this problem we present an alternative\napproach consisting of using X-ray magnetic circular dichroism in reflection\ngeometry. Fit of the data by using a computer code based in a 4X4 matrix\nformalism leads to realistic results. In particular, we show that surface and\ninterface roughness are of major relevance for a proper description of the\nexperimental data and a correct interpretation of the results. By using such an\napproach we demonstrate the presence of a narrow surface region with strongly\ndepressed magnetic properties in La2\/3Ca1\/3MnO3 thin films.\n","label":0,"model":"human","source":"arxiv","id":3561}
{"text":"  We investigate the behavior of ultracold bosons in optical lattices with a\ndisorder potential generated via a secondary species frozen in random\nconfigurations. The statistics of disorder is associated with the physical\nstate in which the secondary species is prepared. The resulting random\npotential, albeit displaying algebraic correlations, is found to lead to\nlocalization of all single-particle states. We then investigate the real-time\ndynamics of localization for a hardcore gas of mobile bosons which are brought\ninto sudden interaction with the random potential. Regardless of their initial\nstate and for any disorder strength, the mobile particles are found to reach a\nsteady state characterized by exponentially decaying off-diagonal correlations\nand by the absence of quasi-condensation; when the mobile particles are\ninitially confined in a tight trap and then released in the disorder potential,\ntheir expansion is stopped and the steady state is exponentially localized in\nreal space, clearly revealing Anderson localization.\n","label":0,"model":"human","source":"arxiv","id":3562}
{"text":"  Linear structural error-in-variables models with univariate observations are\nrevisited for studying modified least squares estimators of the slope and\nintercept. New marginal central limit theorems (CLT's) are established for\nthese estimators, assuming the existence of four moments for the measurement\nerrors and that the explanatory variables are in the domain of attraction of\nthe normal law. The latter condition for the explanatory variables is used the\nfirst time, and is so far the most general in this context. It is also optimal,\nor nearly optimal, for our CLT's. Moreover, due to the obtained CLT's being in\nStudentized and self-normalized forms to begin with, they are a priori nearly,\nor completely, data-based, and free of unknown parameters of the joint\ndistribution of the error and explanatory variables. Consequently, they lead to\na variety of readily available, or easily derivable, large-sample approximate\nconfidence intervals (CI's) for the slope and intercept. In contrast, in\nrelated CLT's in the literature so far, the variances of the limiting normal\ndistributions, in general, are complicated and depend on various, typically\nunknown, moments of the error and explanatory variables. Thus, the\ncorresponding CI's for the slope and intercept in the literature, unlike those\nof the present paper, are available only under some additional model\nassumptions.\n","label":0,"model":"human","source":"arxiv","id":3563}
{"text":"  The polarization sensitivity of the upcoming millimetric observatories will\nopen new possibilities for studying the properties of galaxy clusters and for\nusing them as powerful cosmological probes. For this reason it is necessary to\ninvestigate in detail the characteristics of the polarization signals produced\nby their highly ionized intra-cluster medium (ICM). This work is focussed on\nthe polarization effect induced by the ICM bulk motions, the so-called kpSZ\nsignal, which has an amplitude proportional to the optical depth and to the\nsquare of the tangential velocity. In particular we study how this polarization\nsignal is affected by the internal dynamics of galaxy clusters and what is its\ndependence on the physical modelling adopted to describe the baryonic\ncomponent. This is done by producing realistic kpSZ maps starting from the\noutputs of two different sets of high-resolution hydrodynamical N-body\nsimulations. The first set (17 objects) follows only non-radiative\nhydrodynamics, while for each of 9 objects of the second set we implement four\ndifferent kinds of physical processes. Our results shows that the kpSZ signal\nturns out to be a very sensitive probe of the dynamical status of galaxy\nclusters. We find that major merger events can amplify the signal up to one\norder of magnitude with respect to relaxed clusters, reaching amplitude up to\nabout 100 nuK. This result implies that the internal ICM dynamics must be taken\ninto account when evaluating this signal because simplicistic models, based on\nspherical rigid bodies, may provide wrong estimates. Finally we find that the\ndependence on the physical modelling of the baryonic component is relevant only\nin the very inner regions of clusters.\n","label":0,"model":"human","source":"arxiv","id":3564}
{"text":"  Dim radio-quiet neutron star (DRQNS) 1E 1207.4-5209 is one of the most\nheavily examined isolated neutron stars. Wide absorption lines were observed in\nits spectrum obtained by both XMM-Newton and Chandra X-ray satellites. These\nabsorption lines can be interpreted as a principal frequency centered at 0.7\nkeV and its harmonics at 1.4, 2.1 and possibly 2.8 keV. The principal line can\nbe formed by resonant proton cyclotron scattering leading to a magnetic field\nwhich is two orders of magnitude larger than the perpendicular component of the\nsurface dipole magnetic field (B) found from the rotation period (P) and the\ntime rate of change in the rotation period (\\.{P}) of 1E 1207.4-5209. Besides,\nage of the supernova remnant (SNR) G296.5+10.0 which is physically connected to\n1E 1207.4-5209 is two orders of magnitude smaller than the characteristic age\n($\\tau$=P\/2\\.{P}) of the neutron star. These huge differences between the\nmagnetic field values and the ages can be explained based on a B-decay model.\nIf the decay is assumed to be exponential, the characteristic decay time turns\nout to be several thousand years which is three orders of magnitude smaller\nthan the characteristic decay time of radio pulsars represented in an earlier\nwork. The lack of detection of radio emission from DRQNSs and the lack of point\nsources and pulsar wind nebulae in most of the observed SNRs can also be partly\nexplained by such a very rapid exponential decay. The large difference between\nthe characteristic decay times of DRQNSs and radio pulsars must be related to\nthe differences in the magnetic fields, equation of states and masses of these\nisolated neutron stars.\n","label":0,"model":"human","source":"arxiv","id":3565}
{"text":"  KS 1741-293, discovered in 1989 by the X-ray camera TTM in the Kvant module\nof the Mir space station and identified as an X-ray burster, has not been\ndetected in the hard X band until the advent of the INTEGRAL observatory.\nMoreover this source has been recently object of scientific discussion, being\nalso associated to a nearby extended radio source that in principle could be\nthe supernova remnant produced by the accretion induced collapse in the binary\nsystem. Our long term monitoring with INTEGRAL, covering the period from\nFebruary 2003 to May 2005, confirms that KS 1741-293 is transient in soft and\nhard X band. When the source is active, from a simultaneous JEM-X and IBIS data\nanalysis, we provide a wide band spectrum from 5 to 100 keV, that can be fit by\na two component model, a multiple blackbody for the soft emission and a\nComptonized or a cut-off power law model for the hard component. Finally, by\nthe detection of two X-ray bursters with JEM-X, we confirm the bursting nature\nof KS 1741-293, including this source in the class of the hard tailed X-ray\nbursters.\n","label":0,"model":"human","source":"arxiv","id":3566}
{"text":"  Saturation effects affecting absorption and fluorescence spectra of an atomic\nvapor confined in an Extremely Thin Cell (cell thickness $L < 1 \\mu m$) are\ninvestigated experimentally and theoretically. The study is performed on the\n$D_{2}$ line ($\\lambda ~= ~852 nm)$ of $Cs$ and concentrates on the two\nsituations $L = \\lambda \/2$ and $L =\\lambda$, the most contrasted ones with\nrespect to the length dependence of the coherent Dicke narrowing. For $L =\n\\lambda \/2$, the Dicke-narrowed absorption profile simply broadens and\nsaturates in amplitude when increasing the light intensity, while for $L\n=\\lambda$, sub-Doppler dips of reduced absorption at line-center appear on the\nbroad absorption profile. For a fluorescence detection at $L =\\lambda$,\nsaturation induces narrow dips, but only for hyperfine components undergoing a\npopulation loss through optical pumping. These experimental results are\ninterpreted with the help of the various existing models, and are compared with\nnumerical calculations based upon a two-level modelling that considers both a\nclosed and an open system.\n","label":0,"model":"human","source":"arxiv","id":3567}
{"text":"  We formulate a bosonic dynamical mean-field theory (B-DMFT) which provides a\ncomprehensive, thermodynamically consistent framework for the theoretical\ninvestigation of correlated lattice bosons. The B-DMFT is applicable for\narbitrary values of the coupling parameters and temperature and becomes exact\nin the limit of high spatial dimensions d or coordination number Z of the\nlattice. In contrast to its fermionic counterpart the construction of the\nB-DMFT requires different scalings of the hopping amplitudes with Z depending\non whether the bosons are in their normal state or in the Bose-Einstein\ncondensate. A detailed discussion of how this conceptual problem can be\novercome by performing the scaling in the action rather than in the Hamiltonian\nitself is presented. The B-DMFT treats normal and condensed bosons on equal\nfooting and thus includes the effects caused by their dynamic coupling. It\nreproduces all previously investigated limits in parameter space such as the\nBeliaev-Popov and Hartree-Fock-Bogoliubov approximations and generalizes the\nexisting mean-field theories of interacting bosons. The self-consistency\nequations of the B-DMFT are those of a bosonic single-impurity coupled to two\nreservoirs corresponding to bosons in the condensate and in the normal state,\nrespectively. We employ the B-DMFT to solve a model of itinerant and localized,\ninteracting bosons analytically. The local correlations are found to enhance\nthe condensate density and the Bose-Einstein condensate (BEC) transition\ntemperature T_{BEC}. This effect may be used experimentally to increase T_{BEC}\nof bosonic atoms in optical lattices.\n","label":0,"model":"human","source":"arxiv","id":3568}
{"text":"  The ACS Survey of Galactic Globular Clusters, an HST Treasury Project, will\ndeliver high quality, homogeneous photometry of 65 globular clusters. This\npaper introduces a new collection of stellar evolution tracks and isochrones\nsuitable for analyzing the ACS Survey data. Stellar evolution models were\ncomputed at [Fe\/H]= -2.5, -2.0, -1.5, -1.0, -0.5, and 0; [alpha\/Fe]= -0.2, 0,\n0.2, 0.4, 0.6, and 0.8; and three initial He abundances for masses from 0.1 to\n1.8 Msun and ages from 2 to 15 Gyr. Each isochrone spans a wide range in\nluminosity from Mv~14 up to the tip of the red giant branch. These are\ncomplemented by a set of He-burning tracks that extend from the zero age\nhorizontal branch to the onset of thermal pulsations on the asymptotic giant\nbranch. In addition, a set of computer programs are provided that make it\npossible to interpolate the isochrones in [Fe\/H], generate luminosity functions\nfrom the isochrones, and create synthetic horizontal branch models. The tracks\nand isochrones have been converted to the observational plane with two\ndifferent color-Teff transformations, one synthetic and one semi-empirical, in\nground-based B, V, and I, and F606W and F814W for both ACS-WFC and WFPC2\nsystems. All models and programs presented in this paper are available from\nhttp:\/\/stellar.dartmouth.edu\/~models\/\n","label":0,"model":"human","source":"arxiv","id":3569}
{"text":"  We study the frequency-angular lineshape for a phase-matched nonlinear\nprocess producing entangled states and show that there is a continuous variety\nof maximally-entangled states generated for different mismatch values within\nthe natural bandwidth. Detailed considerations are made for two specific\nmethods of polarization entanglement preparation, based on type-II spontaneous\nparametric down-conversion (SPDC) and on SPDC in two subsequent type-I crystals\nproducing orthogonally polarized photon pairs. It turns out that different Bell\nstates are produced at the center of the SPDC line and on its slopes,\ncorresponding to about half-maximum intensity level. These Bell states can be\nfiltered out by either frequency selection or angular selection, or both. Our\ntheoretical calculations are confirmed by a series of experiments, performed\nfor the two above-mentioned schemes of producing polarization-entangled photon\npairs and with two kinds of measurements: frequency-selective and\nangular-selective.\n","label":0,"model":"human","source":"arxiv","id":3570}
{"text":"  21-cm emission from neutral hydrogen during and before the epoch of cosmic\nreionisation is gravitationally lensed by material at all lower redshifts.\nLow-frequency radio observations of this emission can be used to reconstruct\nthe projected mass distribution of foreground material, both light and dark. We\ncompare the potential imaging capabilities of such 21-cm lensing with those of\nfuture galaxy lensing surveys. We use the Millennium Simulation to simulate\nlarge-area maps of the lensing convergence with the noise, resolution and\nredshift-weighting achievable with a variety of idealised observation\nprogrammes. We find that the signal-to-noise of 21-cm lens maps can far exceed\nthat of any map made using galaxy lensing. If the irreducible noise limit can\nbe reached with a sufficiently large radio telescope, the projected convergence\nmap provides a high-fidelity image of the true matter distribution, allowing\nthe dark matter halos of individual galaxies to be viewed directly, and giving\na wealth of statistical and morphological information about the relative\ndistributions of mass and light. For instrumental designs like that planned for\nthe Square Kilometer Array (SKA), high-fidelity mass imaging may be possible\nnear the resolution limit of the core array of the telescope.\n","label":0,"model":"human","source":"arxiv","id":3571}
{"text":"  Several recent studies suggest a correlation between dark matter halo mass\nand the shape of the density profile. We re-analyze simulations from Ricotti\n(2003) in which such a correlation was proposed. We use a standard analysis of\nthe halo density profiles and compare the old simulations to new ones performed\nwith Gadget2, including higher resolution runs. We confirm Ricotti's result\nthat, at virialization, the central log slopes alpha, at 5%-10% of the virial\nradius are correlated with the halo mass and that the halo concentration is a\nuniversal constant. Our results do not contradict the majority of published\npapers: when using a split power law to fit the density profiles, due to the\nalpha-concentration degeneracy, the fits are consistent with halos having a\nuniversal shape with alpha=1 or 1.5 and concentrations that depend on the mass,\nin agreement with results published elsewhere.\n  Recently, several groups have found no evidence for convergence of the inner\nhalo profile to a constant power law. The choice of a split power law\nparameterization used in this letter is motivated by the need to compare our\nresults to previous ones and is formally valid because we are not able to\nresolve regions where the slope of the fitting function reaches its asymptotic\nconstant value. Using a non-parameterized technique, we also show that the\ndensity profiles of dwarf galaxies at z ~ 10 have a log slope shallower than\n0.5 within 5% of the virial radius.\n","label":0,"model":"human","source":"arxiv","id":3572}
{"text":"  Let ${\\cal M}_{g,[n]}$, for $2g-2+n>0$, be the D-M moduli stack of smooth\ncurves of genus $g$ labeled by $n$ unordered distinct points. The main result\nof the paper is that a finite, connected \\'etale cover ${\\cal M}^\\l$ of ${\\cal\nM}_{g,[n]}$, defined over a sub-$p$-adic field $k$, is \"almost\" anabelian in\nthe sense conjectured by Grothendieck for curves and their moduli spaces.\n  The precise result is the following. Let $\\pi_1({\\cal M}^\\l_{\\ol{k}})$ be the\ngeometric algebraic fundamental group of ${\\cal M}^\\l$ and let\n${Out}^*(\\pi_1({\\cal M}^\\l_{\\ol{k}}))$ be the group of its exterior\nautomorphisms which preserve the conjugacy classes of elements corresponding to\nsimple loops around the Deligne-Mumford boundary of ${\\cal M}^\\l$ (this is the\n\"$\\ast$-condition\" motivating the \"almost\" above). Let us denote by\n${Out}^*_{G_k}(\\pi_1({\\cal M}^\\l_{\\ol{k}}))$ the subgroup consisting of\nelements which commute with the natural action of the absolute Galois group\n$G_k$ of $k$. Let us assume, moreover, that the generic point of the D-M stack\n${\\cal M}^\\l$ has a trivial automorphisms group. Then, there is a natural\nisomorphism: $${Aut}_k({\\cal M}^\\l)\\cong{Out}^*_{G_k}(\\pi_1({\\cal\nM}^\\l_{\\ol{k}})).$$ This partially extends to moduli spaces of curves the\nanabelian properties proved by Mochizuki for hyperbolic curves over\nsub-$p$-adic fields.\n","label":0,"model":"human","source":"arxiv","id":3573}
{"text":"  Magnetism in nanographenes (also know as polycyclic aromatic hydrocarbons, or\nPAHs) are studied with first principles density functional calculations. We\nfind that an antiferromagnetic (AFM) phase appears as the PAH reaches a certain\nsize. This AFM phase in PAHs has the same origin as the one in infinitely long\nzigzag-edged graphene nanoribbons, namely, from the localized electronic state\nat the zigzag edge. The smallest PAH still having an AFM ground state is\nidentified. With increased length of the zigzag edge, PAHs approach an\ninfinitely long ribbon in terms of (1) the energetic ordering and difference\namong the AFM, ferromagnetic (FM), and nonmagnetic (NM) phases and (2) the\naverage local magnetic moment at the zigzag edges. These PAHs serve as ideal\ntargets for chemical synthesis of nanographenes that possess magnetic\nproperties. Moreover, our calculations support the interpretation that\nexperimentally observed magnetism in activated carbon fibers originates from\nthe zigzag edges of the nanographenes.\n","label":0,"model":"human","source":"arxiv","id":3574}
{"text":"  This paper presents a study of quasi-steady spherical accretion in the early\nUniverse, before the formation of the first stars and galaxies. The main\nmotivation is to derive the basic formulas that will be used in a companion\npaper to calculate the accretion luminosity of primordial black holes and their\neffect on the cosmic ionization history.\n  The following cosmological effects are investigated: the coupling of the gas\nto the CMB photon fluid (i.e., Compton drag), Hubble expansion, and the growth\nof the dark matter halo seeded by the gravitational potential of the central\npoint mass. The gas equations of motion are solved assuming either a polytropic\nor an isothermal equation of state. We consider the cases in which the\naccreting object is a point mass or a spherical dark matter halo with power-law\ndensity profile, as predicted by the theory of \"secondary infall''. Analytical\nsolutions for the sonic radius and fitting formulas for the accretion rate are\nprovided.\n  Different accretion regimes exist depending on the mass of the accreting\nobject. If the black hole mass is smaller than 50-100 Msun, gas accretion is\nunaffected by Compton drag. A point mass and an extended dark halo of equal\nmass accrete at the same rate if M>5000 Msun, while smaller mass dark halos\naccrete less efficiently than the equivalent point mass. For masses M>3 x 10^4\nMsun, the viscous term due to the Hubble expansion becomes important and the\nassumption of quasi-steady flow fails. Hence, the steady Bondi solutions\ntransition to the time-dependent self-similar solutions for \"cold cosmological\ninfall\".\n","label":0,"model":"human","source":"arxiv","id":3575}
{"text":"  Recent data gathered and triggered by the SWIFT satellite have greatly\nimproved our knowledge of long-duration gamma ray bursts (GRBs) and X-ray\nflashes (XRFs). This is particularly the case for the X-ray data at all times.\nWe show that the entire X-ray observations are in excellent agreement with the\npredictions of the `cannonball' model of GRBs and XRFs, which are based on\nsimple physics and were published long before the launch of SWIFT. Two\nmechanisms underlie these predictions: inverse Compton scattering and\nsynchrotron radiation, generally dominant at early and late times,\nrespectively. The former mechanism provides a unified description of the\ngamma-ray peaks, X-ray flares and even the optical `humps' seen in some\nfavourable cases; i.e. their very different durations, fluxes and peak-times\nare related precisely as predicted. The observed smooth or bumpy fast decay of\nthe X-ray light curve is correctly described case-by-case, in minute detail.\nThe `canonical' X-ray plateau, as well as the subsequent gradual steepening of\nthe afterglow to an asymptotic power-law decay, are as foretold. So are the\nchromatic and achromatic properties of the light-curves.\n","label":0,"model":"human","source":"arxiv","id":3576}
{"text":"  We present optical spectroscopy of EX Hya during its 1991 outburst. This\noutburst is characterised by strong irradiation of the front face of the\nsecondary star by the white dwarf, an overflowing stream which is seen strongly\nin HeII and by a dip in the light curves, which extends from 0.1-0.6 in the\nbinary and spin phases. Strong irradiation of the accretion curtain and that of\nthe inner regions of the disc led to strong emission of HeII and to the\nsuppression of the Hg and Hb emission.\n  Disc overflow was observed in quiescence in earlier studies, where the\noverflow stream material was modulated at high velocities close to 1000 km\/s.\nIn outburst, the overflowing material is modulated at even higher velocities\n(~1500 km\/s). These are streaming velocities down the field lines close to the\nwhite dwarf. Evidence for material collecting near the outer edge of the disc\nand corotating with the accretion curtain was observed. In decline, this\nmaterial and the accretion curtain obscured almost all the emission near binary\nphase 0.4, causing a dip. The dip minimum nearly corresponds with spin pulse\nminimum. This has provided additional evidence for an extended accretion\ncurtain, and for the corotation of material with the accretion curtain at the\nouter edge of the disc. From these observations we suggest that a mechanism\nsimilar to that of Spruit & Taam, where outbursts result due to the storage and\nrelease of matter outside the magnetosphere, triggers the outbursts of EX Hya.\nThis is followed by the irradiation of the secondary star due to accretion\ninduced radiation.\n","label":0,"model":"human","source":"arxiv","id":3577}
{"text":"  Since first observed in the early 1980s, the Hopkins Phoenix Observatory\ncontinues its UBV band observations of the long period (27.1 years) eclipsing\nbinary star system epsilon Aurigae. The UBV observations routinely produce\nstandard deviations or data spread better than 0.01 magnitudes many times\napproaching 0.001 magnitudes. A new infrared photometer has allowed the\naddition of near-infrared observations for the JH bands. Typical near-infrared\nobservations approach a standard deviation of data spread of 0.01 magnitudes.\nThe 2003 - 2005 seasons (Fall through Spring) of epsilon Aurigae observations\nshowed a 66.2 day variation that gradually increases in average and peak\nmagnitude in the UBV bands. The 2006 season (Fall 2006 to Spring 2007) data\nshow what appears to be a fall-back to a quiet period near maximum amplitude of\nV= 3.00. This paper presents the data and compares the current season to the\npast several seasons. The next eclipse is predicted to begin in 2009 and an\ninternational campaign has been organized to coordinate new observations. These\nwebsite links are: [http:\/\/www. hposoft.com\/Campaign09.html ] and [\nhttp:\/\/www.du.edu\/~rstencel\/epsaur.htm ] .\n","label":0,"model":"human","source":"arxiv","id":3578}
{"text":"  We present the clustering properties of 151 Lyman alpha emitting galaxies at\nz ~ 4.5 selected from the Large Area Lyman Alpha (LALA) survey. Our catalog\ncovers an area of 36' x 36' observed with five narrowband filters. We assume\nthat the angular correlation function w(theta) is well represented by a power\nlaw A_w = Theta^(-beta) with slope beta = 0.8, and we find A_w = 6.73 +\/- 1.80.\nWe then calculate the correlation length r_0 of the real-space two-point\ncorrelation function xi(r) = (r\/r_0)^(-1.8) from A_w through the Limber\ntransformation, assuming a flat, Lambda-dominated universe. Neglecting\ncontamination, we find r_0 = 3.20 +\/- 0.42 Mpc\/h. Taking into account a\npossible 28% contamination by randomly distributed sources, we find r_0 = 4.61\n+\/- 0.6 Mpc\/h. We compare these results with the expectations for the\nclustering of dark matter halos at this redshift in a Cold Dark Matter model,\nand find that the measured clustering strength can be reproduced if these\nobjects reside in halos with a minimum mass of 1-2 times 10^11 Solar masses\/h.\nOur estimated correlation length implies a bias of b ~ 3.7, similar to that of\nLyman-break galaxies (LBG) at z ~ 3.8-4.9. However, Lyman alpha emitters are a\nfactor of ~ 2-16 rarer than LBGs with a similar bias value and implied host\nhalo mass. Therefore, one plausible scenario seems to be that Lyman alpha\nemitters occupy host halos of roughly the same mass as LBGs, but shine with a\nrelatively low duty cycle of 6-50%.\n","label":0,"model":"human","source":"arxiv","id":3579}
{"text":"  During the epoch of reionization the 21cm signal is sensitive to the\nscattering rate of the ultraviolet photons, redshifting across the Lyman_alpha\nresonance. Here we calculate the photon scattering rate profile for a single\nultraviolet source. After taking into account previously neglected natural\nbroadening of the resonance line, we find that photons approach the resonance\nfrequency and experience most scatterings at a significantly smaller distance\nfrom the source than naively expected r=(dnu\/nu_0)(c\/H), where dnu=nu-nu_0 is\nthe initial frequency offset, and the discrepancy increases as the initial\nfrequency offset decreases. As a consequence, the scattering rate P(r) drops\nmuch faster with increasing distance than the previously assumed 1\/r^2 profile.\nNear the source (r<1Mpc comoving), the scattering rate of photons that redshift\ninto the Ly_alpha resonance converges to P(r) \\propto r^{-7\/3}. The scattering\nrate of Ly_alpha photons produced by splitting of photons that redshift into a\nhigher resonance (Ly_gamma, Ly_delta, etc.) is only weakly affected by the\nradiative transfer, while the sum of scattering rates of Ly_alpha photons\nproduced from all higher resonances also converges to P(r) \\propto r^{-7\/3}\nnear the source. At 15<z<35, on scales of ~0.01-20Mpc\/h (comoving), the total\nscattering rate of Ly_alpha photons from all Lyman resonances is found to be\nhigher by a factor of ~1+0.3[(1+z)\/20]^{2\/3} than obtained without full\nradiative transfer. Consequently, during the early stage of reionization, the\ndifferential brightness of 21cm signal against the cosmic microwave background\nis also boosted by a similar factor.\n","label":0,"model":"human","source":"arxiv","id":3580}
{"text":"  Warm Dark Matter (WDM) has been invoked to resolve apparent conflicts of Cold\nDark Matter (CDM) models with observations on subgalactic scales. In this work\nwe provide a new and independent lower limit for the WDM particle mass (e.g.\nsterile neutrino) through the analysis of image fluxes in gravitationally\nlensed QSOs.\n  Starting from a theoretical unperturbed cusp configuration we analyze the\neffects of intergalactic haloes in modifying the fluxes of QSO multiple images,\ngiving rise to the so-called anomalous flux ratio. We found that the global\neffect of such haloes strongly depends on their mass\/abundance ratio and it is\nmaximized for haloes in the mass range $10^6-10^8 \\Msun$.\n  This result opens up a new possibility to constrain CDM predictions on small\nscales and test different warm candidates, since free streaming of warm dark\nmatter particles can considerably dampen the matter power spectrum in this mass\nrange. As a consequence, while a ($\\Lambda$)CDM model is able to produce flux\nanomalies at a level similar to those observed, a WDM model, with an\ninsufficiently massive particle, fails to reproduce the observational\nevidences.\n  Our analysis suggests a lower limit of a few keV ($m_{\\nu} \\sim 10$) for the\nmass of warm dark matter candidates in the form of a sterile neutrino. This\nresult makes sterile neutrino Warm Dark Matter less attractive as an\nalternative to Cold Dark Matter, in good agreement with previous findings from\nLyman-$\\alpha$ forest and Cosmic Microwave Background analysis.\n","label":0,"model":"human","source":"arxiv","id":3581}
{"text":"  Using a sample of almost 7000 strong MgII absorbers with 0.4 < z < 2.2\ndetected in the SDSS DR4 dataset, we investigate the gravitational lensing and\ndust extinction effects they induce on background quasars. After carefully\nquantifying several selection biases, we isolate the reddening effects as a\nfunction of redshift and absorber rest equivalent width, W_0. We find the\namount of dust to increase with cosmic time as (1+z)^(-1.1 +\/- 0.4), following\nthe evolution of cosmic star density or integrated star formation rate. We\nmeasure the reddening effects over a factor 30 in E(B-V) and we find the dust\ncolumn density to be proportional to W_0^(1.9 +\/- 0.2), which provides an\nimportant scaling for theoretical modeling of metal absorbers. We also measure\nthe dust-to-metals ratio and find it similar to that of the Milky Way.\n  In contrast to previous studies, we do not detect any gravitational\nmagnification by MgII systems. We measure the upper limit \\mu<1.10 and discuss\nthe origin of the discrepancy. Finally, we estimate the fraction of absorbers\nmissed due to extinction effects and show that it rises from 1 to 50% in the\nrange 1<W_0<6 Angstrom. We parametrize this effect and provide a correction for\nrecovering the intrinsic distribution of absorber rest equivalent widths.\n","label":0,"model":"human","source":"arxiv","id":3582}
{"text":"  We present sensitive phase-referenced VLBI results on the radio continuum\nemission from the $z=4.4$ QSO BRI 1335--0417. The observations were carried out\nat 1.4 GHz using the High Sensitivity Array (HSA). Our sensitive VLBI image at\n$189 \\times 113$ mas ($1.25 \\times 0.75$ kpc) resolution shows continuum\nemission in BRI 1335--0417 with a total flux density of $208 \\pm 46 \\mu$Jy,\nconsistent with the flux density measured with the VLA. The size of the source\nat FWHM is $255 \\times 138$ mas ($1.7 \\times 0.9$ kpc) and the derived\nintrinsic brightness temperature is $\\sim 3.5\\times 10^4$ K. No continuum\nemission is detected at the full VLBI resolution ($32 \\times 7$ mas, $211\n\\times 46$ pc), with a 4$\\sigma$ point source upper limit of 34 $\\mu$Jy\nbeam$^{-1}$, or an upper limit to the intrinsic brightness temperature of\n$5.6\\times 10^5$ K. The highest angular resolution with at least a 4.5$\\sigma$\ndetection of the radio continuum emission is $53 \\times 27$ mas ($0.35 \\times\n0.18$ kpc). At this resolution, the image shows a continuum feature in BRI\n1335--0417 with a size of $64 \\times 35$ mas ($0.42 \\times 0.23$ kpc) at FWHM,\nand intrinsic brightness temperature of $\\sim 2\\times 10^5$ K. The extent of\nthe observed continuum sources at 1.4 GHz and the derived brightness\ntemperatures show that the radio emission (and thus presumably the far-infrared\nemission) in BRI 1335--0417 is powered by a major starburst, with a massive\nstar formation rate of order a few thousand M_{\\odot} {\\rm yr}^{-1}$. Moreover,\nthe absence of any compact high-brightness temperature source suggests that\nthere is no radio-loud AGN in this $z=4.4$ QSO.\n","label":0,"model":"human","source":"arxiv","id":3583}
{"text":"  We report the identification of a recurrent ultraluminous X-ray source (ULX),\na highly absorbed X-ray source (possibly a background AGN), and a young\nsupernova remnant near the center of the starburst galaxy M82. From a series of\nChandra observations taken from 1999 to 2005, we found that the transient ULX\nfirst appeared in 1999 October. The source turned off in 2000 January, but\nlater reappeared and has been active since then. The X-ray luminosity of this\nsource varies from below the detection level (~2.5e38 erg\/s) to its active\nstate in between ~7e39 erg\/s and 1.3e40 erg\/s (in the 0.5-10 keV energy band)\nand shows unusual spectral changes. The X-ray spectra of some Chandra\nobservations are best fitted with an absorbed power-law model with photon index\nranging from 1.3 to 1.7. These spectra are similar to those of Galactic black\nhole binary candidates seen in the low\/hard state except that a very hard\nspectrum was seen in one of the observations. By comparing with near infrared\nimages taken with the Hubble Space Telescope, the ULX is found to be located\nwithin a young star cluster. Radio imaging indicates that it is associated with\na H II region. We suggest that the ULX is likely to be a > 100 solar mass\nintermediate-mass black hole in the low\/hard state. In addition to the\ntransient ULX, we also found a highly absorbed hard X-ray source which is\nlikely to be an AGN and an ultraluminous X-ray emitting young supernova remnant\nwhich may be related to a 100-year old gamma-ray burst event, within 2 arcsec\nof the transient ULX.\n","label":0,"model":"human","source":"arxiv","id":3584}
{"text":"  Associated to a IFS one can consider a continuous map $\\hat{\\sigma} :\n[0,1]\\times \\Sigma \\to [0,1]\\times \\Sigma$, defined by\n$\\hat{\\sigma}(x,w)=(\\tau_{X_{1}(w)}(x), \\sigma(w))$ were $\\Sigma=\\{0,1, ...,\nd-1\\}^{\\mathbb{N}}$, $\\sigma: \\Sigma \\to \\Sigma$ is given\nby$\\sigma(w_{1},w_{2},w_{3},...)=(w_{2},w_{3},w_{4}...)$ and $X_{k} : \\Sigma\n\\to \\{0,1, ..., n-1\\}$ is the projection on the coordinate $k$. A\n$\\rho$-weighted system, $\\rho \\geq 0$, is a weighted system $([0,1], \\tau_{i},\nu_{i})$ such that there exists a positive bounded function $h : [0,1] \\to\n\\mathbb{R}$ and probability $\\nu $ on $[0,1]$ satisfying $ P_{u}(h)=\\rho h,\n\\quad P_{u}^{*}(\\nu)=\\rho\\nu$. A probability $\\hat{\\nu}$ on $[0,1]\\times\n\\Sigma$ is called holonomic for $\\hat{\\sigma}$ if $ \\int g \\circ \\hat{\\sigma}\nd\\hat{\\nu}= \\int g d\\hat{\\nu}, \\forall g \\in C([0,1])$. We denote the set of\nholonomic probabilities by ${\\cal H}$. Via disintegration, holonomic\nprobabilities $\\hat{\\nu}$ on $[0,1]\\times \\Sigma$ are naturally associated to a\n$\\rho$-weighted system. More precisely, there exist a probability $\\nu$ on\n$[0,1]$ and $u_i, i\\in\\{0, 1,2,..,d-1\\}$ on $[0,1]$, such that is\n$P_{u}^*(\\nu)=\\nu$. We consider holonomic ergodic probabilities. For a\nholonomic probability we define entropy. Finally, we analyze the problem: given\n$\\phi \\in \\mathbb{B}^{+}$, find the solution of the maximization pressure\nproblem $$p(\\phi)=$$\n","label":0,"model":"human","source":"arxiv","id":3585}
{"text":"  The Canada-France High-z Quasar Survey (CFHQS) is an optical survey designed\nto locate quasars during the epoch of reionization. In this paper we present\nthe discovery of the first four CFHQS quasars at redshift greater than 6,\nincluding the most distant known quasar, CFHQS J2329-0301 at z=6.43. We\ndescribe the observational method used to identify the quasars and present\noptical, infrared, and millimeter photometry and optical and near-infrared\nspectroscopy. We investigate the dust properties of these quasars finding an\nunusual dust extinction curve for one quasar and a high far-infrared luminosity\ndue to dust emission for another. The mean millimeter continuum flux for CFHQS\nquasars is substantially lower than that for SDSS quasars at the same redshift,\nlikely due to a correlation with quasar UV luminosity. For two quasars with\nsufficiently high signal-to-noise optical spectra, we use the spectra to\ninvestigate the ionization state of hydrogen at z>5. For CFHQS J1509-1749 at\nz=6.12, we find significant evolution (beyond a simple extrapolation of lower\nredshift data) in the Gunn-Peterson optical depth at z>5.4. The line-of-sight\nto this quasar has one of the highest known optical depths at z~5.8. An\nanalysis of the sizes of the highly-ionized near-zones in the spectra of two\nquasars at z=6.12 and z=6.43 suggest the IGM surrounding these quasars was\nsubstantially ionized before these quasars turned on. Together, these\nobservations point towards an extended reionization process, but we caution\nthat cosmic variance is still a major limitation in z>6 quasar observations.\n","label":0,"model":"human","source":"arxiv","id":3586}
{"text":"  The loop expansion for the n-point functions of N=4 Yang-Mills theory and N=8\nsupergravity can be formulated as the loop expansion of scalar field theory\nwith an infinite subclass being the ladder diagrams. We consider the sum of\nladder diagrams for gluon-gluon and graviton-graviton scattering in the Regge\nlimit. The reggeization of the gluon and the graviton is discussed in this\ncontext and that of hep-th\/0701217. If the Bern, Dixon, Smirnov conjecture for\nplanar gluon-gluon scattering is correct, then the ladder sum for SU(N) gauge\ntheory at large N, correctly gives the Regge limit, with Regge trajectory\nfunction proportional to the cusp anomalous dimension.\n  In graviton-graviton scattering it is argued that the graviton lies on a\nRegge trajectory. Regge cuts are also present due to infinite sums of\nnon-planar graphs. The multiple exchange of Regge poles in non-planar graphs\ncan give a countable infinite number of moving Regge cuts which accumulate near\ns=0. It is conjectured that this may be related to the infinite number of\nnon-perturbative massless states which remain in the limit discussed by Green,\nOoguri and Schwarz.\n","label":0,"model":"human","source":"arxiv","id":3587}
{"text":"  Plasma pressure distribution in the inner magnetosphere is one of the key\nparameters for understanding the main magnetospheric processes including\ngeomagnetic storms and substorms. However, the pressure profiles obtained from\nin-situ particle measurements by the high-altitude satellites do not allow\ntracking the pressure variations related to the storms and substorms, because a\ntime interval needed to do this generally exceeds the characteristic times of\nthem. On contrary, fast movement of low-altitude satellites makes it possible\nto retrieve quasi-instantaneous profiles of plasma pressure along the satellite\ntrajectory, using the fluxes of precipitating particles. For this study, we\nused the Aureol-3 satellite data for plasma pressure estimation, and the IGRF,\nTsyganenko 2001 and Tsyganenko 2004 storm time geomagnetic field models for the\npressure mapping into the equatorial plane. It was found that during quiet\ngeomagnetic condition the radial pressure profiles obtained coincide with the\nprofiles, obtained previously from the high-altitude measurements. On the\ncontrary, it was found that during geomagnetic storm the plasma pressure\nprofiles became sharper; the position of the maximum of plasma pressure\ncorresponds to expected one for given Dst minimum; the maximum value of inner\nmagnetosphere static pressure correlates with the solar wind dynamic pressure.\nIncrease in the plasma pressure profiles indicates the possibility to consider\nthe interchange instability as one of important factors for the development of\nthe main phase of geomagnetic storm.\n","label":0,"model":"human","source":"arxiv","id":3588}
{"text":"  Several interesting astrophysical phenomena are symmetric with respect to the\nrotation axis, like the head-on collision of compact bodies, the collapse\nand\/or accretion of fields with a large variety of geometries, or some forms of\ngravitational waves. Most current numerical relativity codes, however, can not\ntake advantage of these symmetries due to the fact that singularities in the\nadapted coordinates, either at the origin or at the axis of symmetry, rapidly\ncause the simulation to crash. Because of this regularity problem it has become\ncommon practice to use full-blown Cartesian three-dimensional codes to simulate\naxi-symmetric systems. In this work we follow a recent idea idea of Rinne and\nStewart and present a simple procedure to regularize the equations both in\nspherical and axi-symmetric spaces. We explicitly show the regularity of the\nevolution equations, describe the corresponding numerical code, and present\nseveral examples clearly showing the regularity of our evolutions.\n","label":0,"model":"human","source":"arxiv","id":3589}
{"text":"  Using the BATSE survey data I find that quite small fraction of GRBs\nnumbering 37 sources seems to emit the radiation similar to thermal\nbremsstrahlung in the range 20 to 300 keV. I suggest that these bursts may\nperhaps occur from collision of stars with primordial black holes (PBH). These\nobjects are relic of a hot matter in the early Universe. PBH in the vicinity of\nstars may be found in consequence of incorporation processes during the\nformation of stars from interstellar clouds. At present they can form the\ngravitationally captured haloes around stars like the family of solar comets.\nThe comet paradigm has been used to understand various aspects of PBH. Comet\ncollisions with the Sun and planets are ordinary events in solar system\nhistory. On the analogy, one can support the view that PBH collisions with the\nparent star may be quite frequent events in its history, too. PBHs are the\nengines driving gamma-ray bursts when collide with the stars. Entering a\nstellar atmosphere, PBH is supposed to produce the gamma-ray burst due to\naccretion with duration from a few tenths of second to a few seconds. It can\nexhibit the main qualitative features of some GRBs. Their masses are estimated\nin the range from thousandths to hundredths of the solar mass. I found that\nthese burst sources are isotropically distributed on the sky and are seen from\na distance up to 50 ps. In this context one may expect that some short GRBs are\nobservable signatures of primordial black holes in the Universe.\n","label":0,"model":"human","source":"arxiv","id":3590}
{"text":"  Using the spin wave approximation, we study the decoherence dynamics of a\ncentral spin coupled to an antiferromagnetic environment under the application\nof an external global magnetic field. The external magnetic field affects the\ndecoherence process through its effect on the antiferromagnetic environment. It\nis shown explicitly that the decoherence factor which displays a Gaussian decay\nwith time depends on the strength of the external magnetic field and the\ncrystal anisotropy field in the antiferromagnetic environment. When the values\nof the external magnetic field is increased to the critical field point at\nwhich the spin-flop transition (a first-order quantum phase transition) happens\nin the antiferromagnetic environment, the decoherence of the central spin\nreaches its highest point. This result is consistent with several recent\nquantum phase transition witness studies. The influences of the environmental\ntemperature on the decoherence behavior of the central spin are also\ninvestigated.\n","label":0,"model":"human","source":"arxiv","id":3591}
{"text":"  It has been widely shown that the cosmological parameters and dark energy can\nbe constrained by using data from type-Ia supernovae (SNe Ia), the cosmic\nmicrowave background (CMB) anisotropy, the baryon acoustic oscillation (BAO)\npeak from Sloan Digital Sky Survey (SDSS), the X-ray gas mass fraction in\nclusters, and the linear growth rate of perturbations at z=0.15 as obtained\nfrom the 2dF Galaxy Redshift Survey. Recently, gamma-ray bursts (GRBs) have\nalso been argued to be promising standard candles for cosmography. In this\npaper, we present constraints on the cosmological parameters and dark energy by\ncombining a recent GRB sample including 69 events with the other cosmological\nprobes. First, we find that for the LambdaCDM cosmology this combination makes\nthe constraints stringent and the best fit is close to the flat universe.\nSecond, we fit the flat Cardassian expansion model and find that this model is\nconsistent with the LambdaCDM cosmology. Third, we present constraints on\nseveral two-parameter dark energy models and find that these models are also\nconsistent with the LambdaCDM cosmology. Finally, we reconstruct the dark\nenergy equation-of-state parameter w(z) and the deceleration parameter q(z). We\nsee that the acceleration could have started at a redshift from\nz_T=0.40_{-0.08}^{+0.14} to z_T=0.65_{-0.05}^{+0.10}. This difference in the\ntransition redshift is due to different dark energy models that we adopt. The\nmost stringent constraint on w(z) lies in the redshift range z\\sim 0.3-0.6.\n","label":0,"model":"human","source":"arxiv","id":3592}
{"text":"  We present the results of SiO millimeter-line observations of a sample of\nknown SiO maser sources covering a wide dust-temperature range. A cold part of\nthe sample was selected from the SiO maser sources found in our recent SiO\nmaser survey of cold dusty objects. The aim of the present research is to\ninvestigate the causes of the correlation between infrared colors and SiO maser\nintensity ratios among different transition lines. In particular, the\ncorrelation between infrared colors and SiO maser intensity ratio among the\nJ=1-0 v=1, 2, and 3 lines are mainly concerned in this paper. We observed in\ntotal 75 SiO maser sources with the Nobeyama 45m telescope quasi-simultaneously\nin the SiO J=1-0 v=0, 1, 2, 3, 4 and J=2-1 v=1, 2 lines. We also observed the\nsample in the 29SiO J=1-0 v=0 and J=2-1 v=0, and 30SiO J=1-0 v=0 lines, and the\nH2O 6(1,6)-5(2,3) line. As reported in previous papers, we confirmed that the\nintensity ratios of the SiO J=1-0 v=2 to v=1 lines clearly correlate with\ninfrared colors. In addition, we found possible correlation between infrared\ncolors and the intensity ratios of the SiO J=1-0 v=3 to v=1&2 lines. Two\noverlap lines of H2O (i.e., 11(6,6) nu_2=1 -> 12(7,5) nu_2=0 and 5(0,5) nu_2=2\n-> 6(3,4) nu_2=1) might explain these correlation if these overlap lines become\nstronger with increase of infrared colors, although the phenomena also might be\nexplained by more fundamental ways if we take into account the variation of\nopacity from object to object.\n","label":0,"model":"human","source":"arxiv","id":3593}
{"text":"  We investigate the time evolution of the mass distribution of pre-stellar\ncores (PSCs) and their transition to the initial stellar mass function (IMF) in\nthe central parts of a molecular cloud (MC) under the assumption that the\ncoalescence of cores is important. Our aim is to explain the observed shallow\nIMF in dense stellar clusters such as the Arches cluster. The initial\ndistributions of PSCs at various distances from the MC center are those of\ngravitationally unstable cores resulting from the gravo-turbulent fragmentation\nof the MC. As time evolves, there is a competition between the PSCs rates of\ncoalescence and collapse. Whenever the local rate of collapse is larger than\nthe rate of coalescence in a given mass bin, cores are collapsed into stars.\nWith appropriate parameters, we find that the coalescence-collapse model\nreproduces very well all the observed characteristics of the Arches stellar\ncluster IMF; Namely, the slopes at high and low mass ends and the peculiar bump\nobserved at ~5-6 M_sol. Our results suggest that today's IMF of the Arches\ncluster is very similar to the primordial one and is prior to the dynamical\neffects of mass segregation becoming important\n","label":0,"model":"human","source":"arxiv","id":3594}
{"text":"  The multi-Higgs models having spontaneous CP violation (SPCV) and natural\nflavor conservation (NFC) lead to a real CKM matrix $V$ contradicting current\nevidence in favour of a complex $V$. This contradiction can be removed by using\na generalized $\\mu$-$\\tau$ (called 23) symmetry in place of the discrete\nsymmetry conventionally used to obtain NFC. If 23 symmetry is exact then the\nHiggs induced flavour changing neutral currents (FCNC) vanish as in case of\nNFC. 23 breaking introduces SPCV, a phase in $V$ and suppressed FCNC among\nquarks. The FCNC couplings $F_{ij}^{d,u}$ between $i$ and $j$ generations show\na hierarchy $|F_{12}^{d,u}|<|F_{13}^{d,u}|<|F_{23}^{d,u}|$ with the result that\nthe FCNC can have observable consequences in $B$ mixing without conflicting\nwith the $K^0-\\bar{K}^0$ mixing. Detailed fits to the quark masses and the CKM\nmatrix are used to obtain the (complex) couplings $F_{ij}^d$ and $F_{ij}^u$.\nCombined constraints from flavour and CP violations in the $K,B_d,B_s,D$ mesons\nare analyzed within the model. They allow ($i$) relatively light Higgs, 100-150\nGeV ($ii$) measurable extra contributions to the magnitudes and phases of the\n$B^0_{d,s}-\\bar{B}^0_{d,s}$ mixing amplitudes and ($iii$) the $D^0-\\bar{D}^0$\nmixing at the current sensitivity level.\n","label":0,"model":"human","source":"arxiv","id":3595}
{"text":"  It is shown that the use of a density dependent effective Pauli potential\ntogether with a nucleon-nucleon interaction potential plays a crucial role to\nreproduce not only the binding energies but also the matter root mean square\nradii of medium mass range spin-isospin saturated nuclei. This study is\nperformed with a semiclassical Monte Carlo many-body simulation within the\ncontext of a simplified nucleon-nucleon interaction to focus on the effect of\nthe genuine correlations due to the fermionic nature of nucleons. The procedure\nobtained is rather robust and it does not depend on the detailed features of\nthe nucleon-nucleon interaction. For nuclei below saturation the density\ndependence may be represented in terms either of the nucleon number, $A$, or\nthe associated Fermi momenta. When testing the simulation procedure for\nidealized \"infinite\" symmetric nuclear matter within the corresponding range of\ndensities, it turns out that finite size effects affect the Pauli potential\nstrength parametrization in systems up to about 120 particles while remaining\napproximately stable for larger systems.\n","label":0,"model":"human","source":"arxiv","id":3596}
{"text":"  We describe the derivation of the InterHourly Variability (IHV) index of\ngeomagnetic activity. The IHV-index for a geomagnetic element is mechanically\nderived from hourly values as the sum of the unsigned differences between\nadjacent hours over a seven-hour interval centered on local midnight. The index\nis derived separately for stations in both hemispheres within six longitude\nsectors using only local night hours. It is intended as a long-term index.\nAvailable data allows derivation of the index back well into the 19th century.\nOn a time scale of a 27-day Bartels rotation, IHV averages for stations with\ncorrected geomagnetic latitude less than 55 degrees are strongly correlated\nwith midlatitude range indices. Assuming a constant calibration of the aa-index\nwe find that observed yearly values of aa before the year 1957 are 2.9 nT too\nsmall compared to values calculated from IHV using the regression constants\nbased on 1980-2004. We interpret this discrepancy as an indication that the\ncalibration of the aa index is in error before 1957. There is no such problem\nwith the ap index. Rotation averages of IHV are also strongly correlated with\nsolar wind parameters (BV^2). On a time scale of a year combining the IHV-index\nand the recently-developed Inter-Diurnal Variability (IDV) index (giving B)\nallows determination of solar wind speed, V, from 1890-present. Over the\n~120-year series, the yearly mean solar wind speed varied from a low of 303\nkm\/s in 1902 to a high value of 545 km\/s in 2003. The calculated yearly values\nof the product BV using B and V separately derived from IDV and IHV agree\nquantitatively with (completely independent) BV derived from the amplitude of\nthe diurnal variation of the H component in the polar caps since 1926 and\nsporadically beyond.\n","label":0,"model":"human","source":"arxiv","id":3597}
{"text":"  We present a theoretical study of the electronic transport through a\nmany-level quantum dot driven by time-dependent signals applied at the contacts\nto the leads. If the barriers oscillate out of phase the system operates like a\nturnstile pump under a finite constant bias, as observed in the experiments of\nKouwenhoven {\\it et al.} [Phys. Rev. Lett. {\\bf 67}, 1626 (1991)]. The\ntime-dependent currents and their averages over succesive pumping periods are\ncomputed from the Keldysh formalism for tight-binding models. The calculation\nconsiders a sudden application of the pumping potentials at $t=0$ which leads\nto transient features of the time-dependent and averaged currents during the\nfirst pumping cycles which turn out to be important in the high-frequency\nregime. We show that in the transient regime the efficiency of the system as a\npump is rather poor because it mainly absorbs charge from both leads in order\nto fill the levels located below the bias window. Under a finite bias and a\nlow-frequency pumping signal the charge transferred across the system depends\non the number of levels located within the bias window. The internal charge\ndynamics and the role of energy sidebands are investigated. The so called\nsatellite peaks of the averaged current are observed also in the transient\nregime.\n","label":0,"model":"human","source":"arxiv","id":3598}
{"text":"  A trivalent diagram is a connected, two-colored bipartite graph (parallel\nedges allowed but not loops) such that every black vertex is of degree 1 or 3\nand every white vertex is of degree 1 or 2, with a cyclic order imposed on\nevery set of edges incident to to a same vertex. A rooted trivalent diagram is\na trivalent diagram with a distinguished edge, its root. We shall describe and\nanalyze an algorithm giving an exhaustive list of rooted trivalent diagrams of\na given size (number of edges), the list being non-redundant in that no two\ndiagrams of the list are isomorphic. The algorithm will be shown to have\noptimal performance in that the time necessary to generate a diagram will be\nseen to be bounded in the amortized sense, the bound being independent of the\nsize of the diagrams. That's what we call the CAT property. One objective of\nthe paper is to provide a reusable theoretical framework for algorithms\ngenerating exhaustive lists of complex combinatorial structures with attention\npaid to the case of unlabeled structures and to those generators having the CAT\nproperty.\n","label":0,"model":"human","source":"arxiv","id":3599}
{"text":"  This article is a thorough critique to the Weniger's comments made to our\npapers published in prestigious journals in the recent years. A detailed and\ncritical examination of the arguments that led to the suggested comment by\nWeniger reveals some serious flaws. In our published papers we have shown that\nthe unsymmetrical and symmetrical one-range addition theorems for Slater type\norbitals, Coulomb-Yukawa like correlated interaction potentials (CIPs) and\ntheir derivatives are derived from the expansions in terms of -ETOs that are\ncomplete and orthonormal sets of exponential type orbitals in corresponding\nHilbert spaces, where The concrete criticism raised in Weniger's comment\nagainst our papers actually touches a very minor aspect of the works that are\nnot relevant at all for the conclusions, which are made. As can be seen from\nour papers, all of the formulas for different kinds of multicenter integrals\nover Slater type orbitals with integer and noninteger principal quantum numbers\nobtained by the use of unsymmetrical and symmetrical one-range addition\ntheorems were tested by computer calculations. We reject the Weniger's personal\nviews about papers published by Guseinov and his coworkers from 1978 to 2006\nand respectable referees on one-range addition theorems and multicenter\nintegrals. All claims of inconsistencies and flaws in the theoretical framework\nare rejected as unfounded. This rejoinder paper contains all of the answers to\nWeniger's comments.\n","label":0,"model":"human","source":"arxiv","id":3600}
{"text":"  The region of MSSM Higgs parameter space currently excluded by the CDF\nCollaboration, based on an analysis of ~1 fb^-1 of integrated luminosity, is\nless than the expected sensitivity. We analyze the potential implications of\nthe persistence of this discrepancy within the MSSM, assuming that the soft\nsupersymmetry-breaking contributions to scalar masses are universal, apart from\nthose to the Higgs masses (the NUHM model). We find that a light heavy MSSM\nHiggs signal in the unexcluded part of the sensitive region could indeed be\naccommodated in this simple model, even after taking into account other\nconstraints from cold dark matter, electroweak precision observables and B\nphysics observables. In this case the NUHM suggests that supersymmetric\nsignatures should also be detectable in the near future in some other\nmeasurements such as BR(B_s -> mu+ mu-), BR(b -> s gamma) and (g-2)_mu, and M_h\nwould have to be very close to the LEP exclusion limit. In addition, the dark\nmatter candidate associated with this model should be on the verge of detection\nin direct detection experiments.\n","label":0,"model":"human","source":"arxiv","id":3601}
{"text":"  The ``trans-Planckian'' challenge in cosmology appears when we trace the\npresent physical wavelengths of fluctuations backwards in time. They become\nsmaller and smaller until crossing the Planck scale where conventional QFT is\nchallenged, so that unknown ultraviolet physics may be traced in the observable\ncosmological fluctuations. Usually this issue is addressed in the inflationary\ncontext, but trans-Planckian reasoning is much broader. We examine this logic\nin a simple example of scalar quantum field theory in the expanding and\ncontracting Milne universes, where wavelengths of the eigenmodes are red- or\nblue-shifted. Trans-Planckian modifications of QFT should result in a\nUV-dependent VeV of the energy momentum tensor of a scalar field in the Milne\nuniverse. On the other hand, the Milne universe is another coordinate systems\nof flat Minkowski space-time, and the covariant energy momentum tensor should\nbe the same (but vacuum-dependent) in different coordinates of flat space time.\nWe explicitly demonstrate that in conventional QFT the energy momentum tensor,\nchoosing the adiabatic vacuum, is identical to zero in Minkowski coordinates,\nand remains zero in the contracting Milne universe (due to non-trivial\ncancellations of contributions from particles which appear in the accelerating\nframe and from vacuum polarization there). In contrast to this, the\ntrans-Planckian modification of the energy momentum tensor is not motivated. We\nprovide a similar argument for the expanding Milne universe, where the energy\nmomentum tensor in the conformal vacuum is non-zero. Similar arguments are\napplicable for other cosmological models where the curvature is much lower than\nPlanckian which leads to conflicts with trans-Planckian considerations.\n","label":0,"model":"human","source":"arxiv","id":3602}
{"text":"  Dielectric measurements on 0.65[Pb(Ni_1\/3Nb_2\/3)O_3]-0.35PbTiO_3 ceramic in\nthe temperature range 90K to 470K shows a relaxor ferroelectric transition\naround 350K with a Vogel-Fulcher freezing temperature of 338K and appearance of\na non-ergodic relaxor ferroelectric phase of tetragonal structure at room\ntemperature. This non-ergodic phase reenters into the relaxor state at low\ntemperatures as evidenced by the appearance of a frequency dependent anomaly in\nthe imaginary part of the dielectric constant around 160K, similar to those\nreported in other relaxor ferroelectric based morphotropic phase boundary\nceramics. The polarization relaxation time for the 160K anomaly also follows\nVogel-Fulcher type temperature dependence. Temperature dependent magnetization\nmeasurements show that this low temperature anomaly is not linked with any\nmagnetic transition. Elastic modulus and low temperature x-ray diffraction\n(XRD) measurements reveal a tetragonal to monoclinic phase transition around\n225K. It is argued that the low temperature dielectric dispersion around 160K\nresults from the freezing of mesoscopic conformally miniaturized monoclinic\ndomains formed inside the parent tetragonal domains below the structural phase\ntransition temperature of 225K.\n","label":0,"model":"human","source":"arxiv","id":3603}
{"text":"  Nucleon-nucleon scattering in spin-triplet channels is analysed within an\neffective field theory where one-pion exchange is treated nonperturbatively.\nJustifying this requires the identification of an additional low-energy scale\nin the strength of that potential. Short-range interactions are organised\naccording to the resulting power counting, in which the leading term is\npromoted to significantly lower order than in the usual perturbative counting.\nIn each channel there is a critical momentum above which the waves probe the\nsingular core of the tensor potential and the new counting is necessary. When\nthe effects of one- and two-pion exchange have been removed using a\ndistorted-wave Born approximation, the residual scattering in waves with L<=2\nis well described by the first three terms in the new counting. In contrast,\nthe scattering in waves with L>=3 is consistent with the perturbative counting,\nat least for energies up to 300 MeV. This pattern is in agreement with\nestimates of the critical momenta in these channels.\n","label":0,"model":"human","source":"arxiv","id":3604}
{"text":"  We present phase-resolved spectroscopy of the short period cataclysmic\nvariable WZ Sge obtained with the Hubble Space Telescope. We were able to\nresolve the orbital motion of a number of absorption lines that likely probe\nthe environment near the accreting white dwarf. The radial velocities derived\nfrom simultaneous fits to 13 absorption lines indicate an orbital velocity\nsemi-amplitude of K_UV = 47 +\/- 3 km\/s. However, we find that the phase zero is\noffset from the white dwarf ephemeris by +0.1. Our offset and velocity\namplitude are very similar to constraints derived from optical emission lines\nfrom the quiescent accretion disk, despite the fact that we are probing\nmaterial much closer to the primary. If we associate the UV amplitude with K_1,\nour dynamical constraints together with the K_2 estimates from Steeghs et al.\n(2001) and the known binary inclination of i=77+\/-2 imply 0.88<M_1<1.53 M_sun,\n0.078 < M_2 < 0.13 M_sun and 0.075<q=M_2\/M_1<0.101. If we interpret the mean\nvelocity of the UV lines (-16+\/-4 km\/s) as being due to the gravitational\nred-shift caused in the high-g environment near the white dwarf, we find\nv_grav=56+\/-5 km\/s which provides an independent estimate on the mass of the\nprimary of M_1=0.85+\/-0.04 M_sun when coupled with a mass-radius relation. Our\nprimary mass estimates are in excellent agreement and are also self-consistent\nwith spectrophotometric fits to the UV fluxes despite the observed phase\noffset. It is at this point unclear what causes the observed phase-offset in\nthe UV spectra and by how much it distorts the radial velocity signature from\nthe underlying white dwarf.\n","label":0,"model":"human","source":"arxiv","id":3605}
{"text":"  We study two continuous variable systems (or two harmonic oscillators) and\ninvestigate their entanglement evolution under the influence of non-Markovian\nthermal environments. The continuous variable systems could be two modes of\nelectromagnetic fields or two nanomechanical oscillators in the quantum domain.\nWe use quantum open system method to derive the non-Markovian master equations\nof the reduced density matrix for two different but related models of the\ncontinuous variable systems. The two models both consist of two interacting\nharmonic oscillators. In model A, each of the two oscillators is coupled to its\nown independent thermal reservoir, while in model B the two oscillators are\ncoupled to a common reservoir. To quantify the degrees of entanglement for the\nbipartite continuous variable systems in Gaussian states, logarithmic\nnegativity is used. We find that the dynamics of the quantum entanglement is\nsensitive to the initial states, the oscillator-oscillator interaction, the\noscillator-environment interaction and the coupling to a common bath or to\ndifferent, independent baths.\n","label":0,"model":"human","source":"arxiv","id":3606}
{"text":"  Western thinking underwent a turning point between 1885 and 1925. Einstein in\n1905 symbolizes the emblematic hinge of this change of direction. To find an\nequivalent phenomenon in the past we need to go back to the period from the XV\ncentury to the XVII century. It was not a mere reform of codes but a new code.\nIn 1905 the perception of reality changed through the introduction of\nrelativism in three levels of the reference systems: the ego-other relation,\nthe world perceived by the senses, and the 'ideal' universe of the concepts. We\nwill try to identify how this change was expressed and developed in the cited\nlevels, both in the physic and nature based sciences, in the social sciences,\nand in literature and fine arts. Today, 100 years after, the 1905's generation\nstill proposes us two options: to live sheltered by the dogma that reassures us\nwith its only and exclusive viewpoint claimed objective, or to dare to live\nwith the multiple, the transient, the relative. The first way led to the worst\nregimes and intolerant ideas of the XX century, the second option throws us\ninto the uncertainty of the creative adventure, but also leads us to the\npossibility of a fairer society in the XXI century.\n","label":0,"model":"human","source":"arxiv","id":3607}
{"text":"  We present a general computational scheme based on molecular dynamics (m.d.)\nsimulation for calculating the chemical potential of adsorbed molecules in\nthermal equilibrium on the surface of a material. The scheme is based on the\ncalculation of the mean force in m.d. simulations in which the height of a\nchosen molecule above the surface is constrained, and subsequent integration of\nthe mean force to obtain the potential of mean force and hence the chemical\npotential. The scheme is valid at any coverage and temperature, so that in\nprinciple it allows the calculation of the chemical potential as a function of\ncoverage and temperature. It avoids all statistical mechanical approximations,\nexcept for the use of classical statistical mechanics for the nuclei, and\nassumes nothing in advance about the adsorption sites. From the chemical\npotential, the absolute desorption rate of the molecules can be computed,\nprovided the equilibration rate on the surface is faster than the desorption\nrate. We apply the theory by {\\em ab initio} m.d. simulation to the case of\nH$_2$O on MgO (001) in the low-coverage limit, using the Perdew-Burke-Ernzerhof\n(PBE) form of exchange-correlation. The calculations yield an {\\em ab initio}\nvalue of the Polanyi-Wigner frequency prefactor, which is more than two orders\nof magnitude greater than the value of $10^{13}$ s$^{-1}$ often assumed in the\npast. Provisional comparison with experiment suggests that the PBE adsorption\nenergy may be too low, but the extension of the calculations to higher\ncoverages is needed before firm conclusions can be drawn. The possibility of\nincluding quantum nuclear effects by using path-integral simulations is noted.\n","label":0,"model":"human","source":"arxiv","id":3608}
{"text":"  Current research on micro-mechanical resonators strives for quantum-limited\ndetection of the motion of macroscopic objects. Prerequisite to this goal is\nthe observation of measurement backaction consistent with quantum metrology\nlimits. However, thermal noise presently dominates measurements and precludes\nground-state preparation of the resonator. Here we establish the collective\nmotion of an ultracold atomic gas confined tightly within a Fabry-Perot optical\ncavity as a system for investigating the quantum mechanics of macroscopic\nbodies. The cavity-mode structure selects a single collective vibrational mode\nthat is measured by the cavity's optical properties, actuated by the cavity\noptical field, and subject to backaction by the quantum force fluctuations of\nthis field. Experimentally, we quantify such fluctuations by measuring the\ncavity-light-induced heating of the intracavity atomic ensemble. These\nmeasurements represent the first observation of backaction on a macroscopic\nmechanical resonator at the standard quantum limit.\n","label":0,"model":"human","source":"arxiv","id":3609}
{"text":"  We study the boundedness problem for maximal operators $\\M$ associated to\nsmooth hypersurfaces $S$ in 3-dimensional Euclidean space. For $p>2,$ we prove\nthat if no affine tangent plane to $S$ passes through the origin and $S$ is\nanalytic, then the associated maximal operator is bounded on $L^p(\\RR^3)$ if\nand only if $p>h(S),$ where $h(S)$ denotes the so-called height of the surface\n$S.$ For non-analytic finite type $S$ we obtain the same statement with the\nexception of the exponent $p=h(S).$ Our notion of height $h(S)$ is closely\nrelated to A. N. Varchenko's notion of height $h(\\phi)$ for functions $\\phi$\nsuch that $S$ can be locally represented as the graph of $\\phi$ after a\nrotation of coordinates.\n  Several consequences of this result are discussed. In particular we verify a\nconjecture by E.M. Stein and its generalization by A. Iosevich and E. Sawyer on\nthe connection between the decay rate of the Fourier transform of the surface\nmeasure on $S$ and the $L^p$-boundedness of the associated maximal operator\n$\\M$, and a conjecture by Iosevich and Sawyer which relates the\n$L^p$-boundedness of $\\M$ to an integrability condition on $S$ for the distance\nfunction to tangential hyperplanes, in dimension three.\n  In particular, we also give ess. sharp uniform estimates for the Fourier\ntransform of the surface measure on $S,$ thus extending a result by V.N.\nKarpushkin from the analytic to the smooth setting and implicitly verifying a\nconjecture by V.I. Arnol'd in our context.\n","label":0,"model":"human","source":"arxiv","id":3610}
{"text":"  We report ab initio calculations of the melting curve of molybdenum for the\npressure range 0-400 GPa. The calculations employ density functional theory\n(DFT) with the Perdew-Burke-Ernzerhof exchange-correlation functional in the\nprojector augmented wave (PAW) implementation. We present tests showing that\nthese techniques accurately reproduce experimental data on low-temperature\nb.c.c. Mo, and that PAW agrees closely with results from the full-potential\nlinearized augmented plane-wave implementation. The work attempts to overcome\nthe uncertainties inherent in earlier DFT calculations of the melting curve of\nMo, by using the ``reference coexistence'' technique to determine the melting\ncurve. In this technique, an empirical reference model (here, the embedded-atom\nmodel) is accurately fitted to DFT molecular dynamics data on the liquid and\nthe high-temperature solid, the melting curve of the reference model is\ndetermined by simulations of coexisting solid and liquid, and the ab initio\nmelting curve is obtained by applying free-energy corrections. Our calculated\nmelting curve agrees well with experiment at ambient pressure and is consistent\nwith shock data at high pressure, but does not agree with the high pressure\nmelting curve deduced from static compression experiments. Calculated results\nfor the radial distribution function show that the short-range atomic order of\nthe liquid is very similar to that of the high-T solid, with a slight decrease\nof coordination number on passing from solid to liquid. The electronic\ndensities of states in the two phases show only small differences. The results\ndo not support a recent theory according to which very low dTm\/dP values are\nexpected for b.c.c. transition metals because of electron redistribution\nbetween s-p and d states.\n","label":0,"model":"human","source":"arxiv","id":3611}
{"text":"  We consider 5D braneworld models of quasi-localized gravity in which 4D\ngravity is reproduced at intermediate scales while the extra dimension opens up\nat both the very short and the very long distances, where the geometry is flat.\nOur main interest is the interplay between the zero mode of these models,\nwhenever a normalizable zero mode exists, and the effects of zero energy\ngraviton resonant modes coming from the contributions of massive KK modes. We\nfirst consider a compactified version of the GRS model and find that\nquasi-localized gravity is characterized by a scale for which both the\nresonance and the zero mode have significant contribution to 4D gravity. Above\nthis scale, gravity is primarily mediated by the zero mode, while the resonance\ngives only minor corrections. Next, we consider an asymmetric version of the\nstandard non-compact GRS model, characterized by different cosmological\nconstants on each AdS side. We show that a resonance is present but the\nasymmetry, through the form of the localizing potential, can weaken it,\nresulting in a shorter lifetime and, thus, in a shorter distance scale for 4D\ngravity. As a third model exhibiting quasi-localization, we consider a version\nof the GRS model in which the central positive tension brane has been replaced\nby a configuration of a scalar field propagating in the bulk.\n","label":0,"model":"human","source":"arxiv","id":3612}
{"text":"  We study the classical dimer model on a square lattice with a single vacancy\nby developing a graph-theoretic classification of the set of all configurations\nwhich extends the spanning tree formulation of close-packed dimers. With this\nformalism, we can address the question of the possible motion of the vacancy\ninduced by dimer slidings. We find a probability 57\/4-10Sqrt[2] for the vacancy\nto be strictly jammed in an infinite system. More generally, the size\ndistribution of the domain accessible to the vacancy is characterized by a\npower law decay with exponent 9\/8. On a finite system, the probability that a\nvacancy in the bulk can reach the boundary falls off as a power law of the\nsystem size with exponent 1\/4. The resultant weak localization of vacancies\nstill allows for unbounded diffusion, characterized by a diffusion exponent\nthat we relate to that of diffusion on spanning trees. We also implement\nnumerical simulations of the model with both free and periodic boundary\nconditions.\n","label":0,"model":"human","source":"arxiv","id":3613}
{"text":"  When studying safety properties of (formal) protocol models, it is customary\nto view the scheduler as an adversary: an entity trying to falsify the safety\nproperty. We show that in the context of security protocols, and in particular\nof anonymizing protocols, this gives the adversary too much power; for\ninstance, the contents of encrypted messages and internal computations by the\nparties should be considered invisible to the adversary.\n  We restrict the class of schedulers to a class of admissible schedulers which\nbetter model adversarial behaviour. These admissible schedulers base their\ndecision solely on the past behaviour of the system that is visible to the\nadversary.\n  Using this, we propose a definition of anonymity: for all admissible\nschedulers the identity of the users and the observations of the adversary are\nindependent stochastic variables. We also develop a proof technique for typical\ncases that can be used to proof anonymity: a system is anonymous if it is\npossible to `exchange' the behaviour of two users without the adversary\n`noticing'.\n","label":0,"model":"human","source":"arxiv","id":3614}
{"text":"  Observational cosmology provides us with a large number of high precision\ndata which are used to derive models trying to reproduce ``on the mean'' our\nobservable patch of the Universe. Most of these attempts are achieved in the\nframework of a Friedmann-Lema\\^itre cosmology where large scale homogeneity is\nassumed. However, we know, from the observation of structures at increasing\nscales, that these models are only approximations of a smoothed or averaged\ninhomogeneous underlying patern. Anyhow, when modelling the Universe, the usual\nmethod is to use continuous functions representing the kinematical scalars of\nthe velocity field, implicitly assuming that they represent volume averages of\nthe corresponding fine-scale inhomogeneous quantities, then put them into the\nEinstein equations which are solved to give the model and its dependance upon a\nnumber of parameters arbitrarily defined. In General Relativity, such a method\nis very much involved since the equations which determine the metric tensor and\nthe quantities calculated from it are highly nonlinear. The question raised by\nthe method consisting of determining the parameters of an a priori assumed FLRW\nmodel from observational data is the ``fitting problem'' brought to general\nattention by Ellis and Stoeger in the 80's. This problem has recently\nexperienced a reniewed attention due to the amount of available data and the\nincrease of the minimum scale at which homogeneity can be assumed. We propose a\ndiscussion of this issue in the light of the latest developments of\nobservational and theoretical cosmology.\n","label":0,"model":"human","source":"arxiv","id":3615}
{"text":"  The present paper originated from our previous study of the problem of\nharmonic analysis on the infinite symmetric group. This problem leads to a\nfamily {P_z} of probability measures, the z-measures, which depend on the\ncomplex parameter z. The z-measures live on the Thoma simplex, an\ninfinite-dimensional compact space which is a kind of dual object to the\ninfinite symmetric group. The aim of the paper is to introduce stochastic\ndynamics related to the z-measures. Namely, we construct a family of diffusion\nprocesses in the Toma simplex indexed by the same parameter z. Our diffusions\nare obtained from certain Markov chains on partitions of natural numbers n in a\nscaling limit as n goes to infinity. These Markov chains arise in a natural\nway, due to the approximation of the infinite symmetric group by the increasing\nchain of the finite symmetric groups. Each z-measure P_z serves as a unique\ninvariant distribution for the corresponding diffusion process, and the process\nis ergodic with respect to P_z. Moreover, P_z is a symmetrizing measure, so\nthat the process is reversible. We describe the spectrum of its generator and\ncompute the associated (pre)Dirichlet form.\n","label":0,"model":"human","source":"arxiv","id":3616}
{"text":"  %Context: {Previous studies have indicated that the 372.4 GHz ground\ntransition of ortho-H$_2$D$^+$ might be a powerful probe of Proto-Planetary\nDisks. The line could be especially suited for study of the disk mid-plane,\nwhere the bulk of the mass resides and where planet formation takes place.}\n%Aims: {Provide detailed theoretical predictions for the line intensity,\nprofile and maps expected for representative disk models.} %Methods: {We\ndetermine the physical and chemical structure of the disks from the model\ndeveloped by Ceccarelli & Dominik (2005). The line emission is computed with\nthe new radiative transfer method developed recently by Elitzur & Asensio Ramos\n(2006).} %Results: {We present intensity maps convolved with the expected ALMA\nresolution, which delineate the origin of the H$_2$D$^+$ 372.4 GHz line. In the\ndisk inner regions, the line probes the conditions in the mid-plane out to\nradial distances of a few tens of AU, where Solar-like planetary systems might\nform. In the disk outermost regions, the line originates from slightly above\nthe mid-plane. When the disk is spatially resolved, the variation of line\nprofile across the image provides important information about the velocity\nfield. Spectral profiles of the entire disk flux show a double peak shape at\nmost inclination angles.} %Conclusions: {Our study confirms that the 372.4 GHz\nH$_2$D$^+$ line provides powerful diagnostics of the mid-plane of\nProto-Planetary Disks. Current submillimeter telescopes are capable of\nobserving this line, though with some difficulties. The future ALMA\ninterferometer will have the sensitivity to observe and even spatially resolve\nthe H$_2$D$^+$ line emission.}\n","label":0,"model":"human","source":"arxiv","id":3617}
{"text":"  We explore the rich internal structure of Cs_2 Feshbach molecules. Pure\nultracold molecular samples are prepared in a CO_2-laser trap, and a multitude\nof weakly bound states is populated by elaborate magnetic-field ramping\ntechniques. Our methods use different Feshbach resonances as input ports and\nvarious internal level crossings for controlled state transfer. We populate\nhigher partial-wave states of up to eight units of rotational angular momentum\n(l-wave states). We investigate the molecular structure by measurements of the\nmagnetic moments for various states. Avoided level crossings between different\nmolecular states are characterized through the changes in magnetic moment and\nby a Landau-Zener tunneling method. Based on microwave spectroscopy, we present\na precise measurement of the magnetic-field dependent binding energy of the\nweakly bound s-wave state that is responsible for the large background\nscattering length of Cs. This state is of particular interest because of its\nquantum-halo character.\n","label":0,"model":"human","source":"arxiv","id":3618}
{"text":"  We show, using exact lattice chirality, that partition functions of lattice\ngauge theories with vectorlike fermion representations can be split into\n\"light\" and \"mirror\" parts, such that the \"light\" and \"mirror\" representations\nare chiral. The splitting of the full partition function into \"light\" and\n\"mirror\" is well defined only if the two sectors are separately anomaly free.\nWe show that only then is the generating functional, and hence the spectrum, of\nthe mirror theory a smooth function of the gauge field background. This\nexplains how ideas to use additional non-gauge, high-scale mirror-sector\ndynamics to decouple the mirror fermions without breaking the gauge\nsymmetry--for example, in symmetric phases at strong mirror Yukawa\ncoupling--are forced to respect the anomaly-free condition when combined with\nthe exact lattice chiral symmetry. Our results also explain a paradox posed by\na recent numerical study of the mirror-fermion spectrum in a toy\nwould-be-anomalous two-dimensional theory. In passing, we prove some general\nproperties of the partition functions of arbitrary chiral theories on the\nlattice that should be of interest for further studies in this field.\n","label":0,"model":"human","source":"arxiv","id":3619}
{"text":"  One of the great quests of astronomy is to obtain the spectrum of a\nterrestrial planet orbiting within the habitable zone of its star, and the\ndominant challenge in doing so is to isolate the light of the planet from that\nof the star. Dynamics-based methods separate these signals temporally, whereas\nimaging techniques do so spatially. In light of the overwhelming dominance of\ndynamics-based methods over the past decade, we challenge the notion that\nspectra of terrestrial planets necessarily require extreme imaging methods. We\nadvocate that some resources be committed to refining the proven technologies\nof radial-velocity measurements, transit photometry, and occultation\nspectroscopy (i.e. emergent infrared spectra obtained at secondary eclipse). We\nsee a particularly attractive opportunity in M-dwarfs, for which the habitable\nzone is close to the star, increasing the probability and frequency of\ntransits, and the amplitude of the induced radial-velocity variation. Such\nplanets could be discovered by a dedicated ground-based transit survey of the\n10,000 nearest M-dwarfs. The favorable planet-star contrast ratio would make\nthese planets ideal targets for the study of their atmospheres with the\ntechnique of occultation spectroscopy.\n","label":0,"model":"human","source":"arxiv","id":3620}
{"text":"  (Abridged) Mass-loss from massive stars leads to the formation of\ncircumstellar wind-blown bubbles surrounding the star, bordered by a dense\nshell. When the star ends its life in a supernova (SN) explosion, the resulting\nshock wave will interact with this modified medium. In a previous paper we\ndiscussed the basic parameters of this interaction. In this paper we go a step\nfurther and study the evolution of SNe in the wind blown bubble formed by a 35\n$\\msun$ star that starts off as an O star, goes through a red supergiant phase,\nand ends its life as a Wolf-Rayet star. We model the evolution of the CSM and\nthen the expansion of the SN shock wave within this medium. Our simulations\nclearly reveal fluctuations in density and pressure within the surrounding\nmedium. The SN shock interacting with these fluctuations, and then with the\ndense shell surrounding the wind-blown cavity, gives rise to a variety of\ntransmitted and reflected shocks in the wind bubble. The interactions between\nthese various shocks and discontinuities is examined, and its effects on the\nX-ray emission is noted. Our simulations reveal the presence of several\nhydrodynamic instabilities. They show that the turbulent interior, coupled with\nthe large fluctuations in density and pressure, gives rise to an extremely\ncorrugated SN shock wave. The shock shows considerable wrinkles as it impacts\nthe dense shell, and the impact occurs in a piecemeal fashion, with some parts\nof the shock wave interacting with the shell before the others. Therefore\ndifferent parts of the shell will `light-up' at different times. The\nnon-spherical nature of the interaction means that it will occur over a\nprolonged period of time, and the spherical symmetry of the initial shock wave\nis destroyed.\n","label":0,"model":"human","source":"arxiv","id":3621}
{"text":"  We report the discovery of an Active Galactic Nucleus (AGN) in the nearby SAd\ngalaxy NGC 3621 using Spitzer high spectral resolution observations. These\nobservations reveal the presence of [NeV] 14 um and 24 um emission which is\ncentrally concentrated and peaks at the position of the near-infrared nucleus.\nUsing the [NeV] line luminosity, we estimate that the nuclear bolometric\nluminosity of the AGN is ~ 5 X 10^41 ergs s^-1, which corresponds based on the\nEddington limit to a lower mass limit of the black hole of ~ 4 X 10^3 Msun.\nUsing an order of magnitude estimate for the bulge mass based on the Hubble\ntype of the galaxy, we find that this lower mass limit does not put a strain on\nthe well-known relationship between the black hole mass and the host galaxy's\nstellar velocity dispersion established in predominantly early-type galaxies.\nMutli-wavelength follow-up observations of NGC 3621 are required to obtain more\nprecise estimates of the bulge mass, black hole mass, accretion rate, and\nnuclear bolometric luminosity. The discovery reported here adds to the growing\nevidence that a black hole can form and grow in a galaxy with no or minimal\nbulge.\n","label":0,"model":"human","source":"arxiv","id":3622}
{"text":"  We analyze the transmitted flux in a sample of 17 QSOs spectra at\n5.74<zem<6.42 to obtain tighter constraints on the volume-averaged neutral\nhydrogen fraction, xHI, at z~6. We study separately the narrow transmission\nwindows (peaks) and the wide dark portions (gaps) in the observed absorption\nspectra. By comparing the statistics of these spectral features with Lyalpha\nforest simulations, we conclude that xHI evolves smoothly from 10^{-4.4} at\nz=5.3 to 10^{-4.2} at z=5.6, with a robust upper limit xHI<0.36 at z=6.3. The\nfrequency and physical sizes of the peaks imply an origin in cosmic underdense\nregions and\/or in HII regions around faint quasars or galaxies. In one case\n(the intervening HII region of the faint quasar RD J1148+5253 at z=5.70 along\nthe LOS of SDSS J1148+5251 at z=6.42) the increase of the peak spectral density\nis explained by the first-ever detected transverse proximity effect in the HI\nLyalpha forest; this indicates that at least some peaks result from a locally\nenhanced radiation field. We then obtain a strong lower limit on the foreground\nQSO lifetime of tQ>11 Myr. The observed widths of the peaks are found to be\nsystematically larger than the simulated ones. Reasons for such discrepancy\nmight reside either in the photoionization equilibrium assumption or in\nradiative transfer effects.\n","label":0,"model":"human","source":"arxiv","id":3623}
{"text":"  We present X-ray, infrared and radio observations of the field centered on\nX-ray source 1E 1547.0-5408 in the Galactic Plane. A new Chandra observation of\nthis source shows it is unresolved at arc-second resolution, and a new XMM\nobservation shows that its X-ray spectrum is best described by an absorbed\npower-law and blackbody model. A comparison of the X-ray flux observed from\nthis source between 1980 and 2006 reveals that its absorbed 0.5-10 keV X-ray\nflux decreased from ~2x10^-12 ergs cm-2 s-1 to ~3x10^-13 ergs cm-2 during this\nperiod. The most recent XMM observation allows us to put a 5 sigma confidence\nupper limit of 14% for the 0.5-10 keV peak-to-peak pulsed fraction. A\nnear-infrared observation of this field shows a source with magnitude Ks =\n15.9+\/-0.2 near the position of 1E 1547.0-5408, but the implied X-ray to\ninfrared flux ratio indicates the infrared emission is from an unrelated field\nsource, allowing us to limit the IR magnitude of 1E 1547.0-5408 to >17.5.\nArchival radio observations reveal that 1E 1547.0-5408 sits at the center of a\nfaint, small (4' diameter) radio shell, G327.24-0.13, which is possibly a\npreviously unidentified supernova remnant. The X-ray properties of 1E\n1547.0-5408 suggest that this source is a magnetar - a young neutron star whose\nX-ray emission is powered by the decay of its extremely strong magnetic field.\nThe spatial coincidence between this source and G327.24-0.13 suggests that 1E\n1547.0-5408 is associated with a young supernova remnant, supporting a neutron\nstar interpretation. Additional observations are needed to confirm the nature\nof both 1E 1547.0-5408 and G327.24-0.13, and to determine if these sources are\nassociated. If so, this pair will be an important addition to the small number\nof known associations between magnetars and supernova remnants.\n","label":0,"model":"human","source":"arxiv","id":3624}
{"text":"  We propose a new, cyclic-voltammetry based experimental technique that can\nnot only differentiate between discontinuous and continuous phase transitions\nin an adsorbate layer, but also quite accurately recover equilibrium behavior\nfrom dynamic analysis of systems with a continuous phase transition. The\nElectrochemical first-order reversal curve (EC-FORC) diagram for a\ndiscontinuous phase transition (nucleation and growth), such as occurs in\nunderpotential deposition, is characterized by a negative region, while such a\nregion does not exist for a continuous phase transition, such as occurs in the\nelectrosorption of Br on Ag(100). Moreover, for systems with a continuous phase\ntransition, the minima of the individual EC-FORCs trace the equilibrium curve,\neven at very high scan rates. Since obtaining experimental data for the EC-FORC\nmethod would require only a simple reprogramming of the potentiostat used in\nconventional cyclic-voltammetry experiments, we believe that this method has\nsignificant potential for easy, rapid, in-situ analysis of systems undergoing\nelectrochemical deposition.\n","label":0,"model":"human","source":"arxiv","id":3625}
{"text":"  Power-law distributions occur in many situations of scientific interest and\nhave significant consequences for our understanding of natural and man-made\nphenomena. Unfortunately, the detection and characterization of power laws is\ncomplicated by the large fluctuations that occur in the tail of the\ndistribution -- the part of the distribution representing large but rare events\n-- and by the difficulty of identifying the range over which power-law behavior\nholds. Commonly used methods for analyzing power-law data, such as\nleast-squares fitting, can produce substantially inaccurate estimates of\nparameters for power-law distributions, and even in cases where such methods\nreturn accurate answers they are still unsatisfactory because they give no\nindication of whether the data obey a power law at all. Here we present a\nprincipled statistical framework for discerning and quantifying power-law\nbehavior in empirical data. Our approach combines maximum-likelihood fitting\nmethods with goodness-of-fit tests based on the Kolmogorov-Smirnov statistic\nand likelihood ratios. We evaluate the effectiveness of the approach with tests\non synthetic data and give critical comparisons to previous approaches. We also\napply the proposed methods to twenty-four real-world data sets from a range of\ndifferent disciplines, each of which has been conjectured to follow a power-law\ndistribution. In some cases we find these conjectures to be consistent with the\ndata while in others the power law is ruled out.\n","label":0,"model":"human","source":"arxiv","id":3626}
{"text":"  We study the modes and stability of non - isothermal coronal loop models with\ndifferent intensity values of the equilibrium magnetic field. We use an energy\nprinciple obtained via non - equilibrium thermodynamic arguments. The principle\nis expressed in terms of Hermitian operators and allow to consider together the\ncoupled system of equations: the balance of energy equation and the equation of\nmotion. We determine modes characterized as long - wavelength disturbances that\nare present in inhomogeneous media. This character of the system introduces\nadditional difficulties for the stability analysis because the inhomogeneous\nnature of the medium determines the structure of the disturbance, which is no\nlonger sinusoidal. Moreover, another complication is that we obtain a\ncontinuous spectrum of stable modes in addition to the discrete one. We obtain\na unique unstable mode with a characteristic time that is comparable with the\ncharacteristic life-time observed for loops. The feasibility of wave-based and\nflow-based models is examined.\n","label":0,"model":"human","source":"arxiv","id":3627}
{"text":"  It was suggested that the two consecutive metamagnetic transitions and the\nlarge residual resistivity discovered in Sr$_3$Ru$_2$O$_7$ can be understood\nvia the nematic order and its domains in a single layer system. However, a\nrecently reported anisotropy between two longitudinal resistivities induced by\ntilting the magnetic field away from the c-axis cannot be explained within the\nsingle layer nematic picture. To fill the gap in our understanding within the\nnematic order scenario, we investigate the effects of bilayer coupling and\nin-plane magnetic field on the electronic nematic phases in a bilayer system.\nWe propose that the in-plane magnetic field in the bilayer system modifies the\nenergetics of the domain formation, since it breaks the degeneracy of two\ndifferent nematic orientations. Thus the system reveals a pure nematic phase\nwith a resistivity anisotropy in the presence of an in-plane magnetic field. In\naddition to the nematic phase, the bilayer coupling opens a novel route to a\nhidden nematic phase that preserves the x-y symmetry of the Fermi surfaces.\n","label":0,"model":"human","source":"arxiv","id":3628}
{"text":"  Context: Ly-alpha-emitters have proven to be excellent probes of faint,\nstar-forming galaxies in the high redshift universe. However, although the\nsample of known emitters is increasingly growing, their nature (e.g. stellar\nmasses, ages, metallicities, star-formation rates) is still poorly constrained.\n  Aims: We aim to study the nature of Ly-alpha-emitters, to find the properties\nof a typical Ly-alpha-emitting galaxy and to compare these properties with the\nproperties of other galaxies at similar redshift, in particular Lyman-break\ngalaxies.\n  Methods: We have performed narrow-band imaging at the VLT, focused on\nLy-alpha at redshift z ~ 3.15, in the GOODS-S field. We have identified a\nsample of Ly-alpha-emitting candidates, and we have studied their Spectral\nEnergy Distributions (SEDs).\n  Results: We find that the emitters are best fit by an SED with low\nmetallicity (Z\/Z_sun = 0.005), low dust extinction (A_V ~ 0.32) and medium\nstellar masses of approximately 10^9 M_sun. The age is not very well\nconstrained. One object out of 24 appears to be a high redshift\nLy-alpha-emitting dusty starburst galaxy. We find filamentary structure as\ntraced by the Ly-alpha-emitters at the 4 sigma level. The rest-frame UV SED of\nthese galaxies is very similar to that of Lyman Break Galaxies (LBGs) and\ncomply with the selection criteria for U-band drop-outs, except they are\nintrinsically fainter than the current limit for LBGs.\n  Conclusion: Ly-alpha-emitters are excellent probes of galaxies in the distant\nuniverse, and represent a class of star-forming, dust and AGN free, medium mass\nobjects.\n","label":0,"model":"human","source":"arxiv","id":3629}
{"text":"  It is well known gravitational lensing, mainly via magnification bias,\nmodifies the observed galaxy\/quasar clustering. Such discussions have largely\nfocused on the 2D angular correlation. Here and in a companion paper (Paper II)\nwe explore how magnification bias distorts the 3D correlation function and\npower spectrum, as first considered by Matsubara. The interesting point is: the\ndistortion is anisotropic. Magnification bias preferentially enhances the\nobserved correlation in the line-of-sight (LOS) orientation, especially on\nlarge scales. For example at LOS separation of ~100 Mpc\/h, where the intrinsic\ngalaxy-galaxy correlation is rather weak, the observed correlation can be\nenhanced by lensing by a factor of a few, even at a modest redshift of z ~\n0.35. The opportunity: this lensing anisotropy is distinctive, making it\npossible to separately measure the galaxy-galaxy, galaxy-magnification and\nmagnification-magnification correlations, without measuring galaxy shapes. The\nanisotropy is distinguishable from the well known distortion due to peculiar\nmotions, as will be discussed in Paper II. The challenge: the magnification\ndistortion of the galaxy correlation must be accounted for in interpreting data\nas precision improves. For instance, the ~100 Mpc\/h baryon acoustic oscillation\nscale in the correlation function is shifted by up to ~3% in the LOS\norientation, and up to ~0.6% in the monopole, depending on the galaxy bias,\nredshift and number count slope. The corresponding shifts in the inferred\nHubble parameter and angular diameter distance, if ignored, could significantly\nbias measurements of the dark energy equation of state. Lastly, magnification\ndistortion offers a plausible explanation for the well known excess\ncorrelations seen in pencil beam surveys.\n","label":0,"model":"human","source":"arxiv","id":3630}
{"text":"  Using a combination of Chandra and XMM observations, we confirmed the\npresence of a significant velocity gradient along the NE\/E-W\/SW direction in\nthe intracluster gas of the cluster Abell 576. The results are consistent with\na previous ASCA SIS analysis of this cluster. The error weighted average over\nACIS-S3, EPIC MOS 1 & 2 spectrometers for the maximum velocity difference is\n>3.3E03 km\/s at the 90% confidence level, similar to the velocity limits\nestimated indirectly for the \"bullet\" cluster (1E0657-56). The probability that\nthe velocity gradient is generated by standard random gain fluctuations with\nChandra and XMM is <0.1%. The regions of maximum velocity gradient are in CCD\nzones that have the lowest temporal gain variations. It is unlikely that the\nvelocity gradient is due to Hubble distance differences between projected\nclusters (probability<~0.01%). We mapped the distribution of elemental\nabundance ratios across the cluster and detected a strong chemical\ndiscontinuity using the abundance ratio of silicon to iron, equivalent to a\nvariation from 100% SN Ia iron mass fraction in the West-Northwest regions to\n32% in the Eastern region. The \"center\" of the cluster is located at the\nchemical discontinuity boundary, which is inconsistent with the radially\nsymmetric chemical gradient found in some regular clusters, but consistent with\na cluster merging scenario. We predict that the velocity gradient as measured\nwill produce a variation of the CMB temperature towards the East of the core of\nthe cluster that will be detectable by current and near-future bolometers. The\nmeasured velocity gradient opens for the possibility that this cluster is\npassing through a near line-of-sight merger stage where the cores have recently\ncrossed.\n","label":0,"model":"human","source":"arxiv","id":3631}
{"text":"  We present the results of a large-scale survey of neutron(n)-capture elements\nin Galactic planetary nebulae (PNe), undertaken to study enrichments from\ns-process nucleosynthesis in their progenitor stars. From new K band\nobservations of over 100 PNe supplemented by data from the literature, we have\ndetected the emission lines [Kr III] 2.199 and\/or [Se IV] 2.287 $\\mu$m in 81 of\n120 objects. We determine Se and Kr elemental abundances, employing ionization\ncorrection formulae derived in the first paper of this series. We find a\nsignificant range in Se and Kr abundances, from near solar (no enrichment) to\nenhanced by >1.0 dex relative to solar, which we interpret as self-enrichment\ndue to in situ s-process nucleosynthesis. Kr tends to be more strongly enriched\nthan Se; in 18 objects exhibiting both Se and Kr emission, we find that [Kr\/Se]\n= 0.5$\\pm$0.2.\n  Our survey has increased the number of PNe with n-capture element abundance\ndeterminations by a factor of ten, enabling us for the first time to search for\ncorrelations with other nebular properties. As expected, we find a positive\ncorrelation between s-process enrichments and the C\/O ratio. Type I and bipolar\nPNe, which arise from intermediate-mass progenitors (>3-4 solar masses),\nexhibit little to no s-process enrichments. Finally, PNe with H-deficient\nWolf-Rayet central stars do not exhibit systematically larger s-process\nenrichments than objects with H-rich nuclei. Overall, 44% of the PNe in our\nsample display significant s-process enrichments (>0.3 dex). Using an empirical\nPN luminosity function to correct for incompleteness, we estimate that the true\nfraction of s-process enriched Galactic PNe is at least 20%.\n","label":0,"model":"human","source":"arxiv","id":3632}
{"text":"  We analyze the clustering properties of ultraviolet selected galaxies by\nusing GALEX-SDSS data at z<0.6 and CFHTLS deep u' imaging at z=1. These\ndatasets provide a unique basis at z< 1 which can be directly compared with\nhigh redshift samples built with similar selection criteria. We discuss the\ndependence of the correlation function parameters (r0, delta) on the\nultraviolet luminosity as well as the linear bias evolution. We find that the\nbias parameter shows a gradual decline from high (b > 2) to low redshift (b ~\n0.79^{+0.1}_{-0.08}). When accounting for the fraction of the star formation\nactivity enclosed in the different samples, our results suggest that the bulk\nof star formation migrated from high mass dark matter halos at z>2 (10^12 <\nM_min < 10^13 M_sun, located in high density regions), to less massive halos at\nlow redshift (M_min < 10^12 M_sun, located in low density regions). This result\nextends the ``downsizing'' picture (shift of the star formation activity from\nhigh stellar mass systems at high z to low stellar mass at low z) to the dark\nmatter distribution.\n","label":0,"model":"human","source":"arxiv","id":3633}
{"text":"  We model a sheared disordered solid using the theory of Shear Transformation\nZones (STZs). In this mean-field continuum model the density of zones is\ngoverned by an effective temperature that approaches a steady state value as\nenergy is dissipated. We compare the STZ model to simulations by Shi, et\nal.(Phys. Rev. Lett. 98 185505 2007), finding that the model generates\nsolutions that fit the data,exhibit strain localization, and capture important\nfeatures of the localization process. We show that perturbations to the\neffective temperature grow due to an instability in the transient dynamics, but\nunstable systems do not always develop shear bands. Nonlinear energy\ndissipation processes interact with perturbation growth to determine whether a\nmaterial exhibits strain localization. By estimating the effects of these\ninteractions, we derive a criterion that determines which materials exhibit\nshear bands based on the initial conditions alone. We also show that the shear\nband width is not set by an inherent diffusion length scale but instead by a\ndynamical scale that depends on the imposed strain rate.\n","label":0,"model":"human","source":"arxiv","id":3634}
{"text":"  We present a systematic examination of the changes in semi-major axis caused\nby the mutual interactions of a group of massive bodies orbiting a central star\nin the presence of eccentricity dissipation. For parameters relevant to the\noligarchic stage of planet formation, dynamical friction keeps the typical\neccentricities small and prevents orbit crossing. Interactions at impact\nparameters greater than several Hill radii cause the protoplanets to repel each\nother; if the impact parameter is instead much less than the Hill radius, the\nprotoplanets shift slightly in semi-major axis but remain otherwise\nunperturbed. If the orbits of two or more protoplanets are separated by less\nthan a Hill radius, they are each pushed towards an equilibrium spacing between\ntheir neighbors and can exist as a stable co-orbital system. In the\nshear-dominated oligarchic phase of planet formation we show that the feeding\nzones contain several oligarchs instead of only one. Growth of the protoplanets\nin the oligarchic phase drives the disk to an equilibrium configuration that\ndepends on the mass ratio of protoplanets to planetesimals, $\\Sigma\/\\sigma$.\nEarly in the oligarchic phase, when $\\Sigma\/\\sigma$ is low, the spacing between\nrows of co-orbital oligarchs are about 5 Hill radii wide, rather than the 10\nHill radii cited in the literature. It is likely that at the end of oligarchy\nthe average number of co-orbital oligarchs is greater than unity. In the outer\nsolar system this raises the disk mass required to form the ice giants. In the\ninner solar system this lowers the mass of the final oligarchs and requires\nmore giant impacts than previously estimated. This result provides additional\nevidence that Mars is not an untouched leftover from the oligarchic phase, but\nmust be composed of several oligarchs assembled through giant impacts.\n","label":0,"model":"human","source":"arxiv","id":3635}
{"text":"  Different mathematical methods have been applied to obtain the analytic\nresult for the massless triangle Feynman diagram yielding a sum of four\nlinearly independent hypergeometric functions $F_4$. In this paper I work out\nthe diagram and show that that result, though mathematically sound, is not\nphysically correct, because it misses a fundamental physical constraint imposed\nby the conservation of momentum, which should reduce by one the total number of\nlinearly independent (l.i.) functions $F_4$ in the overall solution. Taking\ninto account that the momenta flowing along the three legs of the diagram are\nconstrained by momentum conservation, the number of overall l.i. functions that\nenter the most general solution must reduce accordingly.\n  To determine the exact structure and content of the analytic solution for the\nthree-point function, I use the analogy that exists between Feynman diagrams\nand electric circuit networks, in which the electric current flowing in the\nnetwork plays the role of the momentum flowing in the lines of a Feynman\ndiagram. This analogy is employed to define exactly which three out of the four\nhypergeometric functions are relevant to the analytic solution for the Feynman\ndiagram. The analogy is built based on the equivalence between electric\nresistance circuit networks of type \"Y\" and \"Delta\" in which flows a conserved\ncurrent. The equivalence is established via the theorem of minimum energy\ndissipation within circuits having these structures.\n","label":0,"model":"human","source":"arxiv","id":3636}
{"text":"  We present a detailed analysis of XMM-Newton EPIC-pn data for the Seyfert-1\ngalaxy NGC 4593. We discuss the X-ray spectral properties of this source as\nwell as its variations with time. The 0.5-10 keV spectrum shows significant\ncomplexity beyond a simple power-law form, with clear evidence existing for a\n\"soft excess\" as well as absorption by highly ionized plasma (a warm absorber)\nwithin the central engine of this active galactic nucleus. We show that the\nsoft excess is best described as originating from thermal Comptonization by\nplasma that is appreciably cooler than the primary X-ray emitting plasma; we\nfind that the form of the soft excess cannot be reproduced adequately by\nreflection from an ionized accretion disk. The only measurable deviation from\nthe power-law continuum in the hard spectrum comes from the presence of cold\nand ionized fluorescent iron-K emission lines at 6.4 and 6.97 keV,\nrespectively. While constraints on the ionized iron line are weak, the cold\nline is found to be narrow at CCD-resolution with a flux that does not track\nthe temporal changes in the underlying continuum, implying an origin in the\nouter radii of the accretion disk or the putative molecular torus of Seyfert\nunification schemes. The X-ray continuum itself varies on all accessible time\nscales. We detect a ~230-second time-lag between soft and hard EPIC-pn bands\nthat, if interpreted as scattering timescales within a Comptonizing disk\ncorona, can be used to constrain the physical size of the primary X-ray source\nto a characteristic length scale of ~2 gravitational radii. Taken together, the\nsmall implied coronal size and the large implied iron line emitting region\nindicate a departure from the current picture of a \"typical\" AGN geometry.\n","label":0,"model":"human","source":"arxiv","id":3637}
{"text":"  The paper is devoted to homogenization of two-phase incompressible\nviscoelastic flows with disordered microstructure. We study two cases. In the\nfirst case, both phases are modeled as Kelvin-Voight viscoelastic materials. In\nthe second case, one phase is a Kelvin-Voight material, and the other is a\nviscous Newtonian fluid. The microscale system contains the conservation of\nmass and balance of momentum equations. The inertial terms in the momentum\nequation incorporate the actual interface advected by the flow. In the\nconstitutive equations, a frozen interface is employed. The interface geometry\nis arbitrary: we do not assume periodicity, statistical homogeneity or scale\nseparation. The problem is homogenized using G-convergence and oscillating test\nfunctions. Since the microscale system is not parabolic, previously known\nconstructions of the test functions do not work here. The test functions\ndeveloped in the paper are non-local in time and satisfy divergence-free\nconstraint exactly. The latter feature enables us to avoid working with\npressure directly. We show that the effective medium is a single phase\nviscoelastic material that is not necessarily of Kelvin-Voight type. The\neffective constitutive equation contains a long memory viscoelastic term, as\nwell as instantaneous elastic and viscous terms.\n","label":0,"model":"human","source":"arxiv","id":3638}
{"text":"  We have investigated the migration behaviors of uranium (U) and thorium (Th)\nin Earth and other terrestrial planets. Theoretical models of U and Th\nmigration have been proposed. These models suggest that the unique features of\nEarth are closely connected with its unique U and Th migration models and\ndistribution patterns. In the Earth, U and Th can combine with oxidative\nvolatile components and water, migrate up to the asthenosphere position to form\nan enrichment zone (EZ) of U and Th first, and then migrate up further to the\ncrusts through magmatism and metamorphism. We emphasize that the formation of\nan EZ of U, Th and other heat-producing elements is a prerequisite for the\nformation of a plate tectonic system. The heat-producing elements, currently\nmainly U and Th, in the EZ are also the energy sources that drive the formation\nand evolution of the crust of Earth and create special granitic continental\ncrusts. In other terrestrial planets, including Mercury, Venus, and Mars, an EZ\ncan not be formed because of a lack of oxidative volatile components and water.\nFor this reason, a plate tectonic system can not been developed in these\nplanets. We also emphasize the influence of U and Th in EZ on the development\nand evolution of life on Earth. We propose that since the Earth and planets\nwere born in a united solar system, there should be some common mechanisms to\ncreate the similarities and differences between them. We have tried to develop\nan integrated view to explain some problems in the tectonics of Earth and\nevolution, bio-evolution, and planetary dynamics through U and Th geochemistry.\nWe believe that a comprehensive exploration on energy sources and their\nevolution is a good way to build bridges between different disciplines of\nscience in order to better understand the Earth and planets.\n","label":0,"model":"human","source":"arxiv","id":3639}
{"text":"  We develop an improved method for tracking the nuclear flame during the\ndeflagration phase of a Type Ia supernova, and apply it to study the variation\nin outcomes expected from the gravitationally confined detonation (GCD)\nparadigm. A simplified 3-stage burning model and a non-static ash state are\nintegrated with an artificially thickened advection-diffusion-reaction (ADR)\nflame front in order to provide an accurate but highly efficient representation\nof the energy release and electron capture in and after the unresolvable flame.\nWe demonstrate that both our ADR and energy release methods do not generate\nsignificant acoustic noise, as has been a problem with previous ADR-based\nschemes. We proceed to model aspects of the deflagration, particularly the role\nof buoyancy of the hot ash, and find that our methods are reasonably\nwell-behaved with respect to numerical resolution. We show that if a detonation\noccurs in material swept up by the material ejected by the first rising bubble\nbut gravitationally confined to the white dwarf (WD) surface (the GCD\nparadigm), the density structure of the WD at detonation is systematically\ncorrelated with the distance of the deflagration ignition point from the center\nof the star. Coupled to a suitably stochastic ignition process, this\ncorrelation may provide a plausible explanation for the variety of nickel\nmasses seen in Type Ia Supernovae.\n","label":0,"model":"human","source":"arxiv","id":3640}
{"text":"  With increasing applied current we show that the moving vortex lattice\nchanges its structure from a triangular one to a set of parallel vortex rows in\na pinning free superconductor. This effect originates from the change of the\nshape of the vortex core due to non-equilibrium effects (similar to the\nmechanism of vortex motion instability in the Larkin-Ovchinnikov theory). The\nmoving vortex creates a deficit of quasiparticles in front of its motion and an\nexcess of quasiparticles behind the core of the moving vortex. This results in\nthe appearance of a wake (region with suppressed order parameter) behind the\nvortex which attracts other vortices resulting in an effective\ndirection-dependent interaction between vortices. When the vortex velocity $v$\nreaches the critical value $v_c$ quasi-phase slip lines (lines with fast vortex\nmotion) appear which may coexist with slowly moving vortices between such\nlines. Our results are found within the framework of the time-dependent\nGinzburg-Landau equations and are strictly valid when the coherence length\n$\\xi(T)$ is larger or comparable with the decay length $L_{in}$ of the\nnon-equilibrium quasiparticle distribution function. We qualitatively explain\nexperiments on the instability of vortex flow at low magnetic fields when the\ndistance between vortices $a \\gg L_{in} \\gg \\xi (T)$. We speculate that a\nsimilar instability of the vortex lattice should exist for $v>v_c$ even when\n$a<L_{in}$.\n","label":0,"model":"human","source":"arxiv","id":3641}
{"text":"  The notion of innocent strategy was introduced by Hyland and Ong in order to\ncapture the interactive behaviour of lambda-terms and PCF programs. An innocent\nstrategy is defined as an alternating strategy with partial memory, in which\nthe strategy plays according to its view. Extending the definition to\nnon-alternating strategies is problematic, because the traditional definition\nof views is based on the hypothesis that Opponent and Proponent alternate\nduring the interaction. Here, we take advantage of the diagrammatic\nreformulation of alternating innocence in asynchronous games, in order to\nprovide a tentative definition of innocence in non-alternating games. The task\nis interesting, and far from easy. It requires the combination of true\nconcurrency and game semantics in a clean and organic way, clarifying the\nrelationship between asynchronous games and concurrent games in the sense of\nAbramsky and Melli\\`es. It also requires an interactive reformulation of the\nusual acyclicity criterion of linear logic, as well as a directed variant, as a\nscheduling criterion.\n","label":0,"model":"human","source":"arxiv","id":3642}
{"text":"  Kumar et al. (2006) obtained a fifth order polynomial in $\\omega$ for the\ndispersion relation and pointed out that the calculations preformed by Porter\net al. (1994) and by Dwivedi & Pandey (2003) seem to be in error, as they\nobtained a sixth order polynomial. The energy equation of Dwivedi & Pandey\n(2003) was dimensionally wrong. Dwivedi & Pandey (2006) corrected the energy\nequation and still claimed that the dispersion relation must be a sixth order\npolynomial. The equations (11) $-$ (19) of Dwivedi & Pandey (2006) and the\nequations (24) $-$ (32) Kumar et al. (2006) are the same. This fact has been\nexpressed by Kumar et al. (2006) themselves. Even then they tried to show this\nset of equations on one side gives the sixth order polynomial as they got; on\nthe other side, the same set of equations gives the fifth order polynomial as\nKumar et al. (2006) obtained. The situation appears to be non-scientific, as\nthe system of equations is a linear one. These are simple algebraic equations\nwhere the variables are to be eliminated. However, it is a matter of surprise\nthat by solving these equations, two scientific groups are getting polynomials\nof different degrees. In the present discussion, we have attempted to short out\nthis discrepancy.\n","label":0,"model":"human","source":"arxiv","id":3643}
{"text":"  We study the morphology of the richest superclusters from the catalogues of\nsuperclusters of galaxies in the 2dF Galaxy Redshift Survey and compare the\nmorphology of real superclusters with model superclusters in the Millennium\nSimulation. We use Minkowski functionals and shapefinders to quantify the\nmorphology of superclusters: their sizes, shapes, and clumpiness. We generate\nempirical models of simple geometry to understand which morphologies correspond\nto the supercluster shapefinders. We show that rich superclusters have\nelongated, filamentary shapes with high-density clumps in their core regions.\nThe clumpiness of superclusters is determined using the fourth Minkowski\nfunctional $V_3$. In the $K_1$-$K_2$ shapefinder plane the morphology of\nsuperclusters is described by a curve which is characteristic to\nmulti-branching filaments. We also find that the differences between the fourth\nMinkowski functional $V_3$ for the bright and faint galaxies in observed\nsuperclusters are larger than in simulated superclusters.\n","label":0,"model":"human","source":"arxiv","id":3644}
{"text":"  We study the morphology of galaxy populations of the richest superclusters\nfrom the catalogue of superclusters of galaxies in the 2dF Galaxy Redshift\nSurvey using the luminosity density distribution and Minkowski functional V3.\nWe compare the properties of grouped and isolated galaxies in regions of\ndifferent density in superclusters. We find that in high-density cores of rich\nsuperclusters there is an excess of early type, passive galaxies, among\ngalaxies in groups and clusters, as well as among those which do not belong to\ngroups, while in lower density outer regions there are more blue, star-forming\ngalaxies both in groups and among those galaxies which do not belong to groups.\nThis also shows that the galaxy content of groups depends on the environment\nwhere the groups reside in. The density distributions and the behaviour of the\nMinkowski functional V3 for different superclusters show that substructures in\nsuperclusters as traced by different populations of galaxies are very\ndifferent. Our results show that both local (group\/cluster) and global\n(supercluster) environments are important in forming galaxy morphologies and\nstar formation activity. The presence of a high density core with X-ray\nclusters and a relatively small fraction of star-forming galaxies in the\nsupercluster SCL126 may be an indication that this supercluster has started its\nevolution earlier than the supercluster SCL9.\n","label":0,"model":"human","source":"arxiv","id":3645}
{"text":"  In the field of evaluation research, computer scientists live constantly upon\ndilemmas and conflicting theories. As evaluation is differently perceived and\nmodeled among educational areas, it is not difficult to become trapped in\ndilemmas, which reflects an epistemological weakness. Additionally, designing\nand developing a computer-based learning scenario is not an easy task.\nAdvancing further, with end-users probing the system in realistic settings, is\neven harder. Computer science research in evaluation faces an immense\nchallenge, having to cope with contributions from several conflicting and\ncontroversial research fields. We believe that deep changes must be made in our\nfield if we are to advance beyond the CBT (computer-based training) learning\nmodel and to build an adequate epistemology for this challenge. The first task\nis to relocate our field by building upon recent results from philosophy,\npsychology, social sciences, and engineering. In this article we locate\nevaluation in respect to communication studies. Evaluation presupposes a\ndefinition of goals to be reached, and we suggest that it is, by many means, a\nsilent communication between teacher and student, peers, and institutional\nentities. If we accept that evaluation can be viewed as set of invisible rules\nknown by nobody, but somehow understood by everybody, we should add\nanthropological inquiries to our research toolkit. The paper is organized\naround some elements of the social communication and how they convey new\ninsights to evaluation research for computer and related scientists. We found\nsome technical limitations and offer discussions on how we relate to technology\nat same time we establish expectancies and perceive others work.\n","label":0,"model":"human","source":"arxiv","id":3646}
{"text":"  Mobile entities with wireless links are able to form a mobile ad-hoc network.\nSuch an infrastructureless network does not have to be administrated. However,\nself-organizing principles have to be applied to deal with upcoming problems,\ne.g. information dissemination. These kinds of problems are not easy to tackle,\nrequiring complex algorithms. Moreover, the usefulness of pure ad-hoc networks\nis arguably limited. Hence, enthusiasm for mobile ad-hoc networks, which could\neliminate the need for any fixed infrastructure, has been damped. The goal is\nto overcome the limitations of pure ad-hoc networks by augmenting them with\ninstant Internet access, e.g. via integration of UMTS respectively GSM links.\nHowever, this raises multiple questions at the technical as well as the\norganizational level. Motivated by characteristics of small-world networks that\ndescribe an efficient network even without central or organized design, this\npaper proposes to combine mobile ad-hoc networks and infrastructured networks\nto form hybrid wireless networks. One main objective is to investigate how this\napproach can reduce the costs of a permanent backbone link and providing in the\nsame way the benefits of useful information from Internet connectivity or\nservice providers. For the purpose of bridging between the different types of\nnetworks, an adequate middleware service is the focus of our investigation.\nThis paper shows our first steps forward to this middleware by introducing the\nInjection Communication paradigm as principal concept.\n","label":0,"model":"human","source":"arxiv","id":3647}
{"text":"  Motivated by recent experimental results on charm physics we investigate\nimplications of the updated constraints of new physics in rare charm meson\ndecays. We first reconsider effects of the MSSM in $c\\to u \\gamma$ constrained\nby the recent experimental evidence on $\\Delta m_D$ and find, that due to the\ndominance of long distance physics, $D \\to V \\gamma$ decay rates cannot be\nmodified by MSSM contributions. Then we consider effects of the extra heavy up\nvector-like quark models on the decay spectrum of $D^+ \\to \\pi^+ l^+ l^-$ and\n$D_s^+ \\to K^+ l^+ l^-$ decays. We find a possibility for the tiny increase of\nthe differential decay rate in the region of large dilepton mass. The R-parity\nviolating supersymmetric model can also modify short distance dynamics in $c\n\\to u l^+ l^-$ decays. We constrain relevant parameters using current upper\nbound on the $D^+ \\to \\pi^+ l^+ l^-$ decay rate and investigate impact of that\nconstraint on the $D_s^+ \\to K^+ l^+ l^-$ differential decay dilepton\ndistribution. Present bounds still allow small modification of the standard\nmodel differential decay rate distribution.\n","label":0,"model":"human","source":"arxiv","id":3648}
{"text":"  We have studied a faded problem, the Jacobian Conjecture ~:\n  \\noindent\n  {\\sf The Jacobian Conjecture $(JC_n)$}~:\n  If $f_1, \\cdots, f_n$ are elements in a polynomial ring $k[X_1, \\cdots, X_n]$\nover a field $k$ of characteristic $0$ such that the Jacobian $\\det(\\partial\nf_i\/ \\partial X_j) $ is a nonzero constant, then $k[f_1, \\cdots, f_n] = k[X_1,\n\\cdots, X_n]$.\n  For this purpose, we generalize it to the following form~:\n  \\noindent\n  {\\sf The Generalized Jacobian Conjecture $(GJC)$}~:\n  {\\it Let $\\varphi : S \\rightarrow T$ be an unramified homomorphism of\nNoetherian domains with $T^\\times = \\varphi(S^\\times)$. Assume that $T$ is a\nfactorial domain and that $S$ is a simply connected normal domain. Then\n$\\varphi$ is an isomorphism. }\n  For the consistency of our discussion, we raise some serious (or idiot)\nquestions and some comments concerning the examples appeared in the papers\npublished by the certain excellent mathematicians (though we are unwilling to\ndeal with them). Since the existence of such examples would be against our\noriginal target Conjecture$(GJC)$, we have to dispute their arguments about the\nexistence of their respective (so called) counter-examples. Our conclusion is\nthat they are not perfect counter-examples as are shown explicitly.\n","label":0,"model":"human","source":"arxiv","id":3649}
{"text":"  In this paper we present the observational campaign carried out at ESO NTT\nand VLT in April and May 2006 to investigate the nature and the structure of\nthe Near Earth Object (144898) 2004 VD17. In spite of a great quantity of\ndynamical information, according to which it will have a close approach with\nthe Earth in the next century, the physical properties of this asteroid are\nlargely unknown. We performed visible and near--infrared photometry and\nspectroscopy, as well as polarimetric observations. Polarimetric and\nspectroscopic data allowed us to classify 2004 VD17 as an E-type asteroid. A\ngood agreement was also found with the spectrum of the aubrite meteorite Mayo\nBelwa. On the basis of the polarimetric albedo (p_v=0.45) and of photometric\ndata, we estimated a diameter of about 320 m and a rotational period of about 2\nhours. The analysis of the results obtained by our complete survey have shown\nthat (144898) 2004 VD17 is a peculiar NEO, since it is close to the breakup\nlimits for fast rotator asteroids, as defined by Pravec and Harris (2000).\nThese results suggest that a more robust structure must be expected, as a\nfractured monolith or a rubble pile in a \"strength regime\" (Holsapple 2002).\n","label":0,"model":"human","source":"arxiv","id":3650}
{"text":"  Fixed infrastructured networks naturally support centralized approaches for\ngroup management and information provisioning. Contrary to infrastructured\nnetworks, in multi-hop ad-hoc networks each node acts as a router as well as\nsender and receiver. Some applications, however, requires hierarchical\narrangements that-for practical reasons-has to be done locally and\nself-organized. An additional challenge is to deal with mobility that causes\npermanent network partitioning and re-organizations. Technically, these\nproblems can be tackled by providing additional uplinks to a backbone network,\nwhich can be used to access resources in the Internet as well as to inter-link\nmultiple ad-hoc network partitions, creating a hybrid wireless network. In this\npaper, we present a prototypically implemented hybrid wireless network system\noptimized for multimedia content distribution. To efficiently manage the ad-hoc\ncommunicating devices a weighted clustering algorithm is introduced. The\nproposed localized algorithm deals with mobility, but does not require\ngeographical information or distances.\n","label":0,"model":"human","source":"arxiv","id":3651}
{"text":"  Ad-hoc networks, a promising trend in wireless technology, fail to work\nproperly in a global setting. In most cases, self-organization and cost-free\nlocal communication cannot compensate the need for being connected, gathering\nurgent information just-in-time. Equipping mobile devices additionally with GSM\nor UMTS adapters in order to communicate with arbitrary remote devices or even\na fixed network infrastructure provides an opportunity. Devices that operate as\nintermediate nodes between the ad-hoc network and a reliable backbone network\nare potential injection points. They allow disseminating received information\nwithin the local neighborhood. The effectiveness of different devices to serve\nas injection point differs substantially. For practical reasons the\ndetermination of injection points should be done locally, within the ad-hoc\nnetwork partitions. We analyze different localized algorithms using at most\n2-hop neighboring information. Results show that devices selected this way\nspread information more efficiently through the ad-hoc network. Our results can\nalso be applied in order to support the election process for clusterheads in\nthe field of clustering mechanisms.\n","label":0,"model":"human","source":"arxiv","id":3652}
{"text":"  (ABRIDGED) Star-forming small galaxies made out of collisional debris have\nbeen found in a variety of merging systems. So far only a few of them are known\nin ULIRGs although they show clear signs of interactions. Whether external star\nformation may take place in such objects is an open question. The aim of this\npaper is to identify and characterise the physical and kinematic properties of\nthe external star forming regions in a sample of ULIRGs, including TDG\ncandidates, using optical IFS and high angular resolution HST imaging. We have\nfound that the presence of external star-forming regions is common with 12\nobjects being identified in 5 ULIRGs. These regions show a large range of\ndynamical mass up to 1x10^{10} M_sun, with average sizes of ~750 pc. In\naddition, the line ratios, metallicities and H\\alpha equivalent widths are\ntypical of young bursts of star formation (age ~ 5-8 Myr), and similar to those\nof other TDG candidates. Their extinction corrected H\\alpha luminosities lead\nto masses for the young stellar component of ~2x10^6 - 7x10^8 M_sun. The\nlikelihood of survival of these regions as TDGs is discussed based on their\nstructural and kinematic properties. Most of these systems follow the relation\nbetween effective radius and velocity dispersion found for globular clusters\nand Ellipticals, which suggests they are stable against internal motions. The\nstability against forces from the parent galaxy have been studied and a\ncomparison of the data with the predictions of dynamical evolutionary models is\nalso performed. Five regions out of twelve show High-Medium or High likelihood\nof survival. Our best candidate, which satisfy all the utilized criteria, is\nlocated in the advanced merger IRAS15250+3609 and presents a velocity field\ndecoupled from the relatively distant parent galaxy.\n","label":0,"model":"human","source":"arxiv","id":3653}
{"text":"  We study $B \\to K^* \\rho$ modes that are analogues of the much studied $B\\to\nK \\pi$ modes with B decaying to two vector mesons instead of pseudoscalar\nmesons, using topological amplitudes in the quark diagram approach. We show how\n$B \\to K^*\\rho$ modes can be used to obtain many more observables than those\nfor $B \\to K \\pi$ modes, even though the quark level subprocesses of both modes\nare exactly the same. All the theoretical parameters (except for the weak phase\n$\\gamma$), such as the magnitudes of the topological amplitudes and their\nstrong phases, can be determined in terms of the observables without any\nmodel-dependent assumption. We demonstrate how $B\\to K^*\\rho$ can also be used\nto verify if there exist any relations between theoretical parameters, such as\nthe hierarchy relations between the topological amplitudes and possible\nrelations between the strong phases. Conversely, if there exist reliable\ntheoretical estimates of amplitudes and strong phases, the presence of New\nphysics could be probed. We show that if the tree and color-supressed tree are\nrelated to the electroweak penguins and color-supressed electroweak penguins,\nit is not only possible to verify the validity of such relations but also to\nhave a clean measurement of New Physics parameters. We also present a numerical\nstudy to examine which of the observables are more sensitive to New Physics.\n","label":0,"model":"human","source":"arxiv","id":3654}
{"text":"  Likely candidates for the global potential energy minima of (H$_{2}$O)$_{n}$\nclusters with $n\\leq21$ on the (0001)-surface of graphite are found using\nbasin-hopping global optimization. The potential energy surfaces are\nconstructed using the TIP4P intermolecular potentials for the water molecules\n(the TIP3P is also explored as a secondary choice), a Lennard-Jones\nwater-graphite potential, and a water-graphite polarization potential that is\nbuilt from classical electrostatic image methods and takes into account both\nthe perpendicular and parallel electric polarizations of graphite. This\npotential energy surface produces a rather hydrophobic water-graphite\ninteraction. As a consequence, the water component of the lowest\ngraphite-(H$_{2}$O)$_{n}$ minima is quite closely related to low-lying minima\nof the corresponding TIP4P (H$_{2}$O)$_{n}$ clusters. In about half of the\ncases the geometrical substructure of the water molecules in the\ngraphite-(H$_{2}$O)$_{n}$ global minimum coincides with that of the\ncorresponding free water cluster. Exceptions occur when the interaction with\ngraphite induces a change in geometry. A comparison of our results with\navailable theoretical and experimental data is performed.\n","label":0,"model":"human","source":"arxiv","id":3655}
{"text":"  The abundance evolution of interstellar dust species originating from stellar\nsources and from condensation in molecular clouds in the local interstellar\nmedium of the Milky Way is studied and the input of dust material to the Solar\nSystem is determined. A one-zone chemical evolution model of the Milky Way for\nthe elemental composition of the disk combined with an evolution model for its\ninterstellar dust component similar to that of Dwek (1998) is developed. The\ndust model considers dust-mass return from AGB stars as calculated from\nsynthetic AGB models combined with models for dust condensation in stellar\noutflows. Supernova dust formation is included in a simple parameterized form\nwhich is gauged by observed abundances of presolar dust grains with supernova\norigin. For dust growth in the ISM a simple method is developed for coupling\nthis with disk and dust evolution models. The time evolution of the abundance\nof the following dust species is followed in the model: silicate, carbon,\nsilicon carbide, and iron dust from AGB stars and from SNe as well as silicate,\ncarbon, and iron dust grown in molecular clouds. It is shown that the\ninterstellar dust population is dominated by dust accreted in molecular clouds;\nmost of the dust material entering the Solar System at its formation does not\nshow isotopic abundance anomalies of the refractory elements, i.e.,\ninconspicuous isotopic abundances do not point to a Solar System origin of dust\ngrains. The observed abundance ratios of presolar dust grains formed in SN\nejecta and in AGB star outflows requires that for the ejecta from SNe the\nfraction of refractory elements condensed into dust is 0.15 for carbon dust and\nis quite small ($\\sim10^{-4}$) for other dust species.\n","label":0,"model":"human","source":"arxiv","id":3656}
{"text":"  Nowadays, to achieve competitive advantage, the industrial companies are\nconsidering that success is sustained to great product development. That is to\nmanage the product throughout its entire lifecycle. Achieving this goal\nrequires a tight collaboration between actors from a wide variety of domains,\nusing different software tools producing various product data types and\nformats. The actors' collaboration is mainly based on the exchange \/share\nproduct information. The representation of the actors' viewpoints is the\nunderlying requirement of the collaborative product development. The multiple\nviewpoints approach was designed to provide an organizational framework\nfollowing the actors' perspectives in the collaboration, and their\nrelationships. The approach acknowledges the inevitability of multiple\nintegration of product information as different views, promotes gathering of\nactors' interest, and encourages retrieved adequate information while providing\nsupport for integration through PLM and\/or SCM collaboration. In this paper, a\nmultiple viewpoints representation is proposed. The product, process,\norganization information models are discussed. A series of issues referring to\nthe viewpoints representation are discussed in detail. Based on XML standard,\ntaking electrical connector as an example, an application case of part of\nproduct information modeling is stated.\n","label":0,"model":"human","source":"arxiv","id":3657}
{"text":"  We analyse near-horizon solutions and compare the results for the black hole\nentropy of five-dimensional spherically symmetric extremal black holes when the\nN=2 SUGRA actions are supplied with two different types of higher-order\ncorrections: (1) supersymmetric completion of gravitational Chern-Simons term,\nand (2) Gauss-Bonnet term. We show that for large BPS black holes lowest order\n\\alpha' corrections to the entropy are the same, but for non-BPS are generally\ndifferent. We pay special attention to the class of prepotentials connected\nwith K3\\times T^2 and T^6 compactifications. For supersymmetric correction we\nfind beside BPS also a set of non-BPS solutions. In the particular case of T^6\ncompactification (equivalent to the heterotic string on $T^4\\times S^1$) we\nfind the (almost) complete set of solutions (with exception of some non-BPS\nsmall black holes), and show that entropy of small black holes is different\nfrom statistical entropy obtained by counting of microstates of heterotic\nstring theory. We also find complete set of solutions for K3\\times T^2 and T^6\ncase when correction is given by Gauss-Bonnet term. Contrary to\nfour-dimensional case, obtained entropy is different from the one with\nsupersymmetric correction. We show that in Gauss-Bonnet case entropy of small\n``BPS'' black holes agrees with microscopic entropy in the known cases.\n","label":0,"model":"human","source":"arxiv","id":3658}
{"text":"  We present the Suzaku broad band observations of two AGNs detected by the\nSwift\/BAT hard X-ray (>15 keV) survey that did not have previous X-ray data,\nSwift J0601.9-8636 and Swift J0138.6-4001. The Suzaku spectra reveals in both\nobjects a heavily absorbed power law component with a column density of NH =~\n10^{23.5-24} cm^{-2} that dominates above 10 keV, and an intense reflection\ncomponent with a solid angle >~ $2\\pi$ from a cold, optically thick medium. We\nfind that these AGNs have an extremely small fraction of scattered light from\nthe nucleus, <~ 0.5% with respect to the intrinsic power law component. This\nindicates that they are buried in a very geometrically-thick torus with a small\nopening angle and\/or have unusually small amount of gas responsible for\nscattering. In the former case, the geometry of Swift J0601.9-8636 should be\nnearly face-on as inferred from the small absorption for the reflection\ncomponent. The discovery of two such objects in this small sample implies that\nthere must be a significant number of yet unrecognized, very Compton thick AGNs\nviewed at larger inclination angles in the local universe, which are difficult\nto detect even in the currently most sensitive optical or hard X-ray surveys.\n","label":0,"model":"human","source":"arxiv","id":3659}
{"text":"  The purpose of this paper is to find optimal estimates for the Green function\nof a half-space of {\\it the relativistic $\\alpha$-stable process} with\nparameter $m$ on $\\Rd$ space. This process has an infinitesimal generator of\nthe form $mI-(m^{2\/\\alpha}I-\\Delta)^{\\alpha\/2},$ where $0<\\alpha<2$, $m>0$, and\nreduces to the isotropic $\\alpha$-stable process for $m=0$. Its potential\ntheory for open bounded sets has been well developed throughout the recent\nyears however almost nothing was known about the behaviour of the process on\nunbounded sets. The present paper is intended to fill this gap and we provide\ntwo-sided sharp estimates for the Green function for a half-space. As a\nbyproduct we obtain some improvements of the estimates known for bounded sets\nspecially for balls. The advantage of these estimates is a clarification of the\nrelationship between the diameter of the ball and the parameter $m$ of the\nprocess.\n  The main result states that the Green function is comparable with the Green\nfunction for the Brownian motion if the points are away from the boundary of a\nhalf-space and their distance is greater than one. On the other hand for the\nremaining points the Green function is somehow related the Green function for\nthe isotropic $\\alpha$-stable process. For example, for $d\\ge3$, it is\ncomparable with the Green function for the isotropic $\\alpha$-stable process,\nprovided that the points are close enough.\n","label":0,"model":"human","source":"arxiv","id":3660}
{"text":"  We study the Laplacian operator of an uncorrelated random network and, as an\napplication, consider hopping processes (diffusion, random walks, signal\npropagation, etc.) on networks. We develop a strict approach to these problems.\nWe derive an exact closed set of integral equations, which provide the averages\nof the Laplacian operator's resolvent. This enables us to describe the\npropagation of a signal and random walks on the network. We show that the\ndetermining parameter in this problem is the minimum degree $q_m$ of vertices\nin the network and that the high-degree part of the degree distribution is not\nthat essential. The position of the lower edge of the Laplacian spectrum\n$\\lambda_c$ appears to be the same as in the regular Bethe lattice with the\ncoordination number $q_m$. Namely, $\\lambda_c>0$ if $q_m>2$, and $\\lambda_c=0$\nif $q_m\\leq2$. In both these cases the density of eigenvalues\n$\\rho(\\lambda)\\to0$ as $\\lambda\\to\\lambda_c+0$, but the limiting behaviors near\n$\\lambda_c$ are very different. In terms of a distance from a starting vertex,\nthe hopping propagator is a steady moving Gaussian, broadening with time. This\npicture qualitatively coincides with that for a regular Bethe lattice. Our\nanalytical results include the spectral density $\\rho(\\lambda)$ near\n$\\lambda_c$ and the long-time asymptotics of the autocorrelator and the\npropagator.\n","label":0,"model":"human","source":"arxiv","id":3661}
{"text":"  Stochastic Loewner evolution also called Schramm Loewner evolution\n(abbreviated, SLE) is a rigorous tool in mathematics and statistical physics\nfor generating and studying scale invariant or fractal random curves in two\ndimensions. The method is based on the older deterministic Loewner evolution\nintroduced by Karl Loewner, who demonstrated that an arbitrary curve not\ncrossing itself can be generated by a real function by means of a conformal\ntransformation. In 2000 Oded Schramm extended this method and demonstrated that\ndriving the Loewner evolution by a one-dimensional Brownian motion, the curves\nin the complex plane become scale invariant; the fractal dimension turns out to\nbe determined by the strength of the Brownian motion. SLE fills a gap in our\nunderstanding of the critical properties of a variety of lattice models in\ntheir scaling limits and supplements the result obtained by means of conformal\nfield theory. In this paper we attempt to provide a simple and heuristic\ndiscussion of some of the important aspects of SLE.\n","label":0,"model":"human","source":"arxiv","id":3662}
{"text":"  We consider the generalized Korteweg-de Vries equation $$ \\partial_t u +\n\\partial_x (\\partial_x^2 u + f(u))=0, \\quad (t,x)\\in [0,T)\\times \\mathbb{R}$$\nwith general $C^2$ nonlinearity $f$. Under an explicit condition on $f$ and\n$c>0$, there exists a solution in the energy space $H^1$ of the type\n$u(t,x)=Q_c(x-x_0-ct)$, called soliton. Stability theory for $Q_c$ is\nwell-known.\n  In previous works, we have proved that for $f(u)=u^p$, $p=2,3,4$, the family\nof solitons is asymptotically stable in some local sense in $H^1$, i.e. if\n$u(t)$ is close to $Q_{c}$ (for all $t\\geq 0$), then $u(t,.+\\rho(t))$ locally\nconverges in the energy space to some $Q_{c_+}$ as $t\\to +\\infty$, for some\n$c^+\\sim c$.\n  Then, the asymptotic stability result could be extended to the case of\ngeneral assumptions on $f$ and $Q_c$.\n  The objective of this paper is twofold.\n  The main objective is to prove that in the case $f(u)=u^p$, $p=2,3,4$,\n$\\rho(t)-c_+ t$ has limit as $t\\to +\\infty$ under the additional assumption\n$x_+ u\\in L^2$.\n  The second objective of this paper is to provide large time stability and\nasymptotic stability results for two soliton solutions for the case of general\nnonlinearity $f(u)$, when the ratio of the speeds of the solitons is small. The\nmotivation is to accompany forthcoming works devoted to the collision of two\nsolitons in the nonintegrable case. The arguments are refinements of previous\nworks specialized to the case $u(t)\\sim Q_{c_1}+Q_{c_2}$, for $0< c_2 \\ll c_1$.\n","label":0,"model":"human","source":"arxiv","id":3663}
{"text":"  The use of photonic crystal fibers pumped by femtosecond pulses has enabled\nthe generation of broad optical supercontinua with nano-joule input energies.\nThis striking discovery has applications ranging from spectroscopy and\nmetrology to telecommunication and medicine. Amongst the physical principles\nunderlying supercontinuum generation are soliton fission, a variety of\nfour-wave mixing processes, Raman induced soliton self-frequency shift, and\ndispersive wave generation mediated by solitons. Although all of the above\neffects contribute to supercontinuum generation none of them can explain the\ngeneration of blue and violet light from infrared femtosecond pump pulses,\nwhich has been seen already in the first observations of the supercontinuum in\nphotonic crystal fibers. In this work we argue that the most profound role in\nthe shaping of the short-wavelength edge of the continuum is played by the\neffect of radiation trapping in a gravity-like potential created by\naccelerating solitons. The underlying physics of this effect has a\nstraightforward analogy with the inertial forces acting on an observer moving\nwith a constant acceleration.\n","label":0,"model":"human","source":"arxiv","id":3664}
{"text":"  Gamma-ray burst afterglow observations in the Swift era have a perceived lack\nof achromatic jet breaks compared to the BeppoSAX era. We present our\nmulti-wavelength analysis of GRB 060206 as an illustrative example of how\ninferences of jet breaks from optical and X-ray data might differ. The results\nof temporal and spectral analyses are compared, and attempts are made to fit\nthe data within the context of the standard blast wave model. We find that\nwhile the break appears more pronounced in the optical and evidence for it from\nthe X-ray alone is weak, the data are actually consistent with an achromatic\nbreak at about 16 hours. This break and the light curves fit standard blast\nwave models, either as a jet break or as an injection break. As the pre-Swift\nsample of afterglows are dominated by optical observations, and in the Swift\nera most well sampled light curves are in the X-ray, caution is needed when\nmaking a direct comparison between the two samples, and when making definite\nstatements on the absence of achromatic breaks.\n","label":0,"model":"human","source":"arxiv","id":3665}
{"text":"  We study the single transverse-spin asymmetry for dijet production in\nhadronic collisions in both the collinear QCD factorization approach and the\nBrodsky-Hwang-Schmidt model. We show that a nonvanishing asymmetry is generated\nby both initial-state and final-state interactions, and that the final-state\ninteractions dominate. We find that in the leading kinematic region where the\ntransverse momentum imbalance of the two jets, q_\\perp = P_{1\\perp}+P_{2\\perp},\nis much less than the momentum of either jet, the contribution from the lowest\nnon-trivial perturbative order to both the spin-averaged and the spin-dependent\ndijet cross sections can be factorized into a hard part that is a function only\nof the averaged jet momentum P_\\perp = (P_{1\\perp}-P_{2\\perp})\/2, and\nperturbatively generated transverse momentum dependent (TMD) parton\ndistributions. We show that the spin asymmetry at this non-trivial perturbative\norder can be described by the TMD parton distributions defined in either\nsemi-inclusive DIS or the Drell-Yan process. We derive the same hard parts from\nboth the collinear factorization approach and in the context of the\nBrodsky-Hwang-Schmidt model, verifying that they are not sensitive to details\nof the factorized long distance physics.\n","label":0,"model":"human","source":"arxiv","id":3666}
{"text":"  We re-analyze the age distribution (dN\/dt) of star clusters in the Small\nMagellanic Cloud (SMC) using age determinations based on the Magellanic Cloud\nPhotometric Survey. For ages younger than 3x10^9 yr the dN\/dt distribution can\nbe approximated by a power-law distribution, dN\/dt propto t^-beta, with\n-beta=-0.70+\/-0.05 or -beta=-0.84+\/-0.04, depending on the model used to derive\nthe ages. Predictions for a cluster population without dissolution limited by a\nV-band detection result in a power-law dN\/dt distribution with an index of\n~-0.7. This is because the limiting cluster mass increases with age, due to\nevolutionary fading of clusters, reducing the number of observed clusters at\nold ages. When a mass cut well above the limiting cluster mass is applied, the\ndN\/dt distribution is flat up to 1 Gyr. We conclude that cluster dissolution is\nof small importance in shaping the dN\/dt distribution and incompleteness causes\ndN\/dt to decline. The reason that no (mass independent) infant mortality of\nstar clusters in the first ~10-20 Myr is found is explained by a detection bias\ntowards clusters without nebular emission, i.e. cluster that have survived the\ninfant mortality phase. The reason we find no evidence for tidal (mass\ndependent) cluster dissolution in the first Gyr is explained by the weak tidal\nfield of the SMC. Our results are in sharp contrast to the interpretation of\nChandar et al. (2006), who interpret the declining dN\/dt distribution as rapid\ncluster dissolution. This is due to their erroneous assumption that the sample\nis limited by cluster mass, rather than luminosity.\n","label":0,"model":"human","source":"arxiv","id":3667}
{"text":"  Using high-resolution UV spectra of 16 low-z QSOs, we study the physical\nconditions and statistics of O VI absorption in the IGM at z < 0.5. We identify\n51 intervening (z_{abs} << z_{QSO}) O VI systems comprised of 77 individual\ncomponents, and we find 14 \"proximate\" systems (z_{abs} ~ z_{QSO}) containing\n34 components. For intervening systems [components] with rest-frame equivalent\nwidth W_{r} > 30 mA, the number of O VI absorbers per unit redshift dN\/dz =\n15.6(+2.9\/-2.4) [21.0(+3.2\/-2.8)], and this decreases to dN\/dz = 0.9(+1.0\/-0.5)\n[0.3(+0.7\/-0.3)] for W_{r} > 300 mA. The number per redshift increases steeply\nas z_{abs} approaches z_{QSO}, and some proximate absorbers have substantially\nlower H I\/O VI ratios. The lower proximate ratios could be partially due to\nionization effects but also require higher metallicities. We find that 37% of\nthe intervening O VI absorbers have velocity centroids that are well-aligned\nwith corresponding H I absorption. If the O VI and the H I trace the same gas,\nthe relatively small differences in line widths imply the absorbers are cool\nwith T < 10^{5} K. Most of these well-aligned absorbers have the\ncharacteristics of metal-enriched photoionized gas. However, the O VI in the\napparently simple and cold systems could be associated with a hot phase with T\n~ 10^{5.5} K if the metallicity is high enough to cause the associated broad Ly\nalpha absorption to be too weak to detect. We show that 53% of the intervening\nO VI systems are complex multiphase absorbers that can accommodate both lower\nmetallicity collisionally-ionized gas with T > 10^{5} K and cold photoionzed\ngas.\n","label":0,"model":"human","source":"arxiv","id":3668}
{"text":"  We present the discovery of a 40 kpc H-alpha tail and at least 29\nemission-line objects downstream of a star-forming galaxy ESO 137-001 in the\nrich, nearby cluster A3627. The galaxy is known to possess a dramatic 70 kpc\nX-ray tail. The detected H-alpha tail coincides positionally with the X-ray\ntail. The H-alpha emission in the galaxy is sharply truncated on the front and\nthe sides near the nucleus, indicating significant ram pressure stripping. ESO\n137-001 is thus the first cluster late-type galaxy known unambiguously with\nboth an X-ray tail and an H-alpha tail. The emission-line objects are all\ndistributed downstream of the galaxy, with projected distance up to 39 kpc from\nthe galaxy. From the analysis on the H-alpha_{off} frame and the estimate of\nthe background emission-line objects, we conclude that it is very likely all 29\nemission-line objects are HII regions in A3627. The high surface number density\nand luminosities of these HII regions (up to 10^40 ergs\/s) dwarf the previously\nknown examples of isolated HII regions in clusters. We suggest that star\nformation may proceed in the stripped ISM, in both the galactic halo and\nintracluster space. The total mass of formed stars in the stripped ISM of ESO\n137-001 may approach several times 10^7 solar masses. Therefore, stripping of\nthe ISM not only contributes to the ICM, but also adds to the intracluster\nstellar light through subsequent star formation. The data also imply that ESO\n137-001 is in an active stage of transformation, accompanied by the build-up of\na central bulge and depletion of the ISM.\n","label":0,"model":"human","source":"arxiv","id":3669}
{"text":"  We critically examine a scenario for the enrichment of the interstellar\nmedium (ISM) in which supernova ejecta follow a long (10^8 yr) journey before\nfalling back onto the galactic disk in the form of metal-rich ``droplets'',\nThese droplets do not become fully mixed with the interstellar medium until\nthey become photoionized in HII regions. We investigate the hypothesis that the\nphotoionization of these highly metallic droplets can explain the observed\n``abundance discrepancy factors'' (ADFs), which are found when comparing\nabundances derived from recombination lines and from collisionally excited\nlines, both in Galactic and extragalactic HII regions. We derive bounds of\n10^{13}--10^{15} cm on the droplet sizes inside HII regions in order that (1)\nthey should not have already been detected by direct imaging of nearby nebulae,\nand (2) they should not be too swiftly destroyed by diffusion in the ionized\ngas. From photoionization modelling we find that, if this inhomogeneous\nenrichment scenario holds, then the recombination lines strongly overestimate\nthe metallicities of the fully mixed HII regions. The abundances derived from\ncollisionally excited lines also suffer some bias, although to a much lesser\nextent. In the absence of any recipe for correcting these biases, we recommend\nthe discarding of all objects showing large ADFs from studies of galactic\nchemical evolution. These biases must also be kept in mind when comparing the\ngalactic abundance gradients for elements derived from recombination lines with\nthose derived from collisionally excited lines. Finally, we propose a set of\nobservations that could be undertaken to test our scenario and improve our\nunderstanding of element mixing in the ISM.\n","label":0,"model":"human","source":"arxiv","id":3670}
{"text":"  An investigation of the possible inflation of stimulated Brillouin\nbackscattering (SBS) due to ion kinetic effects is presented using\nelectromagnetic particle simulations and integrations of three-wave\ncoupled-mode equations with linear and nonlinear models of the nonlinear ion\nphysics. Electrostatic simulations of linear ion Landau damping in an ion\nacoustic wave, nonlinear reduction of damping due to ion trapping, and\nnonlinear frequency shifts due to ion trapping establish a baseline for\nmodeling the electromagnetic SBS simulations. Systematic scans of the laser\nintensity have been undertaken with both one-dimensional particle simulations\nand coupled-mode-equations integrations, and two values of the electron-to-ion\ntemperature ratio (to vary the linear ion Landau damping) are considered. Three\nof the four intensity scans have evidence of SBS inflation as determined by\nobserving more reflectivity in the particle simulations than in the\ncorresponding three-wave mode-coupling integrations with a linear ion-wave\nmodel, and the particle simulations show evidence of ion trapping.\n","label":0,"model":"human","source":"arxiv","id":3671}
{"text":"  Planetary systems, ours included, are formed in disks of dust and gas around\nyoung stars. Disks are an integral part of the star and planet formation\nprocess, and knowledge of the distribution and temperature of inner disk\nmaterial is crucial for understanding terrestrial planet formation, giant\nplanet migration, and accretion onto the central star. While the inner regions\nof protoplanetary disks in nearby star forming regions subtend only a few\nnano-radians, near-IR interferometry has recently enabled the spatial\nresolution of these terrestrial zones. Most observations have probed only dust,\nwhich typically dominates the near-IR emission. Here I report spectrally\ndispersed near-IR interferometric observations that probe gas (which dominates\nthe mass and dynamics of the inner disk), in addition to dust, within one\nastronomical unit of the young star MWC 480. I resolve gas, including water\nvapor and atomic hydrogen, interior to the edge of the dust disk; this\ncontrasts with results of previous spectrally dispersed interferometry\nobservations. Interactions of this accreting gas with migrating planets may\nlead to short-period exoplanets like those detected around main-sequence stars.\nThe observed water vapor is likely produced by the sublimation of migrating icy\nbodies, and provides a potential reservoir of water for terrestrial planets.\n","label":0,"model":"human","source":"arxiv","id":3672}
{"text":"  A characteristic of D-brane inflation is that fluctuations in the inflaton\nfield can propagate at a speed significantly less than the speed of light. This\nyields observable effects that are distinct from those of single-field slow\nroll inflation, such as a modification of the inflationary consistency relation\nand a potentially large level of non-Gaussianities. We present a numerical\nalgorithm that extends the inflationary flow formalism to models with general\nspeed of sound. For an ensemble of D-brane inflation models parameterized by\nthe Hubble parameter and the speed of sound as polynomial functions of the\ninflaton field, we give qualitative predictions for the key inflationary\nobservables. We discuss various consistency relations for D-brane inflation,\nand compare the qualitative shapes of the warp factors we derive from the\nnumerical models with analytical warp factors considered in the literature.\nFinally, we derive and apply a generalized microphysical bound on the inflaton\nfield variation during brane inflation. While a large number of models are\nconsistent with current cosmological constraints, almost all of these models\nviolate the compactification constraint on the field range in four-dimensional\nPlanck units. If the field range bound is to hold, then models with a\ndetectable level of non-Gaussianity predict a blue scalar spectral index, and a\ntensor component that is far below the detection limit of any future\nexperiment.\n","label":0,"model":"human","source":"arxiv","id":3673}
{"text":"  How do T Tauri disks accrete? The magneto-rotational instability (MRI)\nsupplies one means, but protoplanetary disk gas is typically too poorly ionized\nto be magnetically active. Here we show that the MRI can, in fact, explain\nobserved accretion rates for the sub-class of T Tauri disks known as\ntransitional systems. Transitional disks are swept clean of dust inside rim\nradii of ~10 AU. Stellar coronal X-rays ionize material in the disk rim,\nactivating the MRI there. Gas flows from the rim to the star, at a rate limited\nby the depth to which X-rays ionize the rim wall. The wider the rim, the larger\nthe surface area that the rim wall exposes to X-rays, and the greater the\naccretion rate. Interior to the rim, the MRI continues to transport gas; the\nMRI is sustained even at the disk midplane by super-keV X-rays that Compton\nscatter down from the disk surface. Accretion is therefore steady inside the\nrim. Blown out by radiation pressure, dust largely fails to accrete with gas.\nContrary to what is usually assumed, ambipolar diffusion, not Ohmic\ndissipation, limits how much gas is MRI-active. We infer values for the\ntransport parameter alpha on the order of 0.01 for GM Aur, TW Hyd, and DM Tau.\nBecause the MRI can only afflict a finite radial column of gas at the rim, disk\nproperties inside the rim are insensitive to those outside. Thus our picture\nprovides one robust setting for planet-disk interaction: a protoplanet interior\nto the rim will interact with gas whose density, temperature, and transport\nproperties are definite and decoupled from uncertain initial conditions. Our\nstudy also supplies half the answer to how disks dissipate: the inner disk\ndrains from the inside out by the MRI, while the outer disk photoevaporates by\nstellar ultraviolet radiation.\n","label":0,"model":"human","source":"arxiv","id":3674}
{"text":"  (Abridged) We develop a model for the cosmological role of mergers in the\nevolution of starbursts, quasars, and spheroidal galaxies. Combining halo mass\nfunctions (MFs) with empirical halo occupation models, we calculate where major\ngalaxy-galaxy mergers occur and what kinds of galaxies merge, at all redshifts.\nWe compare with observed merger MFs, clustering, fractions, and small-scale\nenvironments, and show that this yields robust estimates in good agreement with\nobservations. Making the simple ansatz that major, gas-rich mergers cause\nquasar activity, we demonstrate that this naturally reproduces the observed\nrise and fall of the quasar luminosity density from z=0-6, as well as quasar\nLFs, fractions, host galaxy colors, and clustering as a function of redshift\nand luminosity. The observed excess of quasar clustering on small scales is a\nnatural prediction of the model, as mergers preferentially occur in regions\nwith excess small-scale galaxy overdensities. We show that quasar environments\nat all observed redshifts correspond closely to the empirically determined\nsmall group scale, where mergers of gas-rich galaxies are most efficient. We\ncontrast with a secular model in which quasar activity is driven by bars\/disk\ninstabilities, and show that while these modes probably dominate at Seyfert\nluminosities, the constraints from clustering (large and small-scale),\npseudobulge populations, disk MFs, luminosity density evolution, and host\ngalaxy colors argue that they must be a small contributor to the z>1 quasar\nluminosity density.\n","label":0,"model":"human","source":"arxiv","id":3675}
{"text":"  In the context of supersymmetric compactifications of type II supergravity to\nfour dimensions, we show that orientifold sources can be compatible with a\ngeneralized SU(3) x SU(3)-structure that is neither strictly SU(3) nor static\nSU(2). We illustrate this with explicit examples, obtained by suitably\nT-dualizing known solutions on the six-torus. In addition we prove the\nfollowing integrability statements, valid under certain mild assumptions: (a)\nfor general type II supergravity backgrounds with orientifold and\/or D-brane\ngeneralized-calibrated sources, the source-corrected Einstein and dilaton\nequations of motion follow automatically from the supersymmetry equations once\nthe likewise source-corrected form equations of motion and Bianchi identities\nare imposed; (b) in the special case of supersymmetric compactifications to\nfour-dimensional Minkowski space, the equations of motion of all fields,\nincluding the NSNS three-form, follow automatically once the supersymmetry and\nthe Bianchi identities of the forms are imposed. Both (a) and (b) are equally\nvalid whether the sources are smeared or localized. As a byproduct we obtain\nthe calibration form for a space-filling NS5-brane.\n","label":0,"model":"human","source":"arxiv","id":3676}
{"text":"  (Abridged) We develop and test a model for the cosmological role of mergers\nin the formation and quenching of red, early-type galaxies. Making the ansatz\nthat star formation is quenched after a gas-rich, spheroid-forming major\nmerger, we demonstrate that this naturally predicts the turnover in the\nefficiency of star formation at ~L_star, as well as the observed mass\nfunctions\/density of red galaxies as a function of redshift, the formation\ntimes of spheroids as a function of mass, and the fraction of quenched galaxies\nas a function of galaxy and halo mass, environment, and redshift. Comparing to\na variety of semi-analytic models in which quenching is primarily driven by\nhalo mass considerations or secular\/disk instabilities, we demonstrate that our\nmodel and different broad classes of models make unique and robust qualitative\npredictions for a number of observables, including the red fraction as a\nfunction of galaxy and halo mass, the density of passive galaxies and evolution\nof the color-morphology-density relations at high z, and the fraction of\ndisky\/boxy spheroids as a function of mass. In each case, the observations\nfavor a model in which galaxies quench after a major merger builds a massive\nspheroid, and disfavor quenching via secular or pure halo processes. We discuss\na variety of physical possibilities for this quenching, and propose a mixed\nscenario in which traditional quenching in hot, massive halos is supplemented\nby the feedback associated with star formation and quasar activity in a major\nmerger, which temporarily suppress cooling and establish the conditions of a\ndynamically hot halo in the central regions of the host, even in low mass\nhalos.\n","label":0,"model":"human","source":"arxiv","id":3677}
{"text":"  We compare resonant electronic Raman scattering and photoluminescence\nmeasurements for the characterization of a spin-polarized two-dimensional\nelectron gas embedded in $\\text{Cd}_{1-x}\\text{Mn}_x\\text{Te}$ single quantum\nwells. From Raman scattering by single-particle excitations in a zero magnetic\nfield, we measure the Fermi velocity and then obtain the Fermi energy (as well\nas the electron density), which is comparable to that extracted from\nphotoluminescence for moderate electron densities, assuming a bare band-edge\nmass. At large electron densities, the Fermi energies derived from Raman\nscattering and photoluminescence differ. For an applied in-plane magnetic field\nand zero wave vector transferred to the electron gas, Raman scattering spectra\nshow peaks at both the Zeeman energy $Z$, resulting from collective excitations\nof the spin-polarized electron gas, and the one electron spin-flip energy\n$Z^*$. Magneto-photoluminescence spectra show conduction band splitting that\nare equivalent to $Z$, suggesting that collective effects are present in the\nphotoluminescence spectra. Assuming (as before) an uncorrected mass, the degree\nof spin polarization $\\zeta$ determined from the magneto-photoluminescence\nlineshape is found to differ from that derived from the magnetic field\ndependent Raman scattering measurements for large electron densities. We\nattribute the discrepancy in measuring $\\zeta$ and the Fermi energy to the\nrenormalized mass resulting from many-body electron-electron interactions.\n","label":0,"model":"human","source":"arxiv","id":3678}
{"text":"  We report on follow-up observations of the GRB 060927 using the ROTSE-IIIa\ntelescope and a suite of larger aperture ground-based telescopes. An optical\nafterglow was detected 20 s after the burst, the earliest rest-frame detection\nof optical emission from any GRB. Spectroscopy performed with the VLT about 13\nhours after the trigger shows a continuum break at lambda ~ 8070 A produced by\nneutral hydrogen absorption at z~5.6. We also detect an absorption line at 8158\nA which we interpret as SiII at z=5.467. Hence, GRB 060927 is the second most\ndistant GRB with a spectroscopically measured redshift. The shape of the red\nwing of the spectral break can be fitted by a damped Lyalpha profile with a\ncolumn density with log(N_HI\/cm^-2) ~ 22.5. We discuss the implications of this\nwork for the use of GRBs as probes of the end of the dark ages and draw three\nmain conclusions: i) GRB afterglows originating from z>6 should be relatively\neasy to detect from the ground, but rapid NIR monitoring is necessary to ensure\nthat they are found; ii) The presence of large HI column densities in some GRBs\nhost galaxies at z>5 makes the use of GRBs to probe the reionization epoch via\nspectroscopy of the red damping wing challenging; iii) GRBs appear crucial to\nlocate typical star-forming galaxies at z>5 and therefore the type of galaxies\nresponsible for the reionization of the universe.\n","label":0,"model":"human","source":"arxiv","id":3679}
{"text":"  We show that despite the inherent non-locality of quantum field theories on\nthe Groenewold-Moyal (GM) plane, one can find a class of ${\\bf C}$, ${\\bf P}$,\n${\\bf T}$ and ${\\bf CPT}$ invariant theories. In particular, these are theories\nwithout gauge fields or with just gauge fields and no matter fields. We also\nshow that in the presence of gauge fields, one can have a field theory where\nthe Hamiltonian is ${\\bf C}$ and ${\\bf T}$ invariant while the $S$-matrix\nviolates ${\\bf P}$ and ${\\bf CPT}$.\n  In non-abelian gauge theories with matter fields such as the electro-weak and\n$QCD$ sectors of the standard model of particle physics, ${\\bf C}$, ${\\bf P}$,\n${\\bf T}$ and the product of any pair of them are broken while ${\\bf CPT}$\nremains intact for the case $\\theta^{0i} =0$. (Here $x^{\\mu} \\star x^{\\nu} -\nx^{\\nu} \\star x^{\\mu} = i \\theta^{\\mu \\nu}$, $x^{\\mu}$: coordinate functions,\n$\\theta^{\\mu \\nu} = -\\theta^{\\nu \\mu}=$ constant.) When $\\theta^{0i} \\neq 0$,\nit contributes to breaking also ${\\bf P}$ and ${\\bf CPT}$. It is known that the\n$S$-matrix in a non-abelian theory depends on $\\theta^{\\mu \\nu}$ only through\n$\\theta^{0i}$. The $S$-matrix is frame dependent. It breaks (the identity\ncomponent of the) Lorentz group. All the noncommutative effects vanish if the\nscattering takes place in the center-of-mass frame, or any frame where\n$\\theta^{0i}P^{\\textrm{in}}_{i} = 0$, but not otherwise. ${\\bf P}$ and ${\\bf\nCPT}$ are good symmetries of the theory in this special case.\n","label":0,"model":"human","source":"arxiv","id":3680}
{"text":"  Most Vega-like stars have far-infrared excess (60micron or longward in IRAS,\nISO, or Spitzer MIPS bands) and contain cold dust (<~150K) analogous to the\nSun's Kuiper-Belt region. However, dust in a region more akin to our asteroid\nbelt and thus relevant to the terrestrial planet building process is warm and\nproduces excess emission in mid-infrared wavelengths. By cross-correlating\nHipparcos dwarfs with the MSX catalog, we found that EF Cha, a member of the\nrecently identified, ~10 Myr old, ``Cha-Near'' Moving Group, possesses\nprominent mid-infrared excess. N-band spectroscopy reveals a strong emission\nfeature characterized by a mixture of small, warm, amorphous and possibly\ncrystalline silicate grains. Survival time of warm dust grains around this A9\nstar is <~ 1E5 yrs, much less than the age of the star. Thus, grains in this\nextra-solar terrestrial planetary zone must be of \"second generation\" and not a\nremnant of primodial dust and are suggestive of substantial planet formation\nactivity. Such second generation warm excess occurs around ~ 13% of the\nearly-type stars in nearby young stellar associations.\n","label":0,"model":"human","source":"arxiv","id":3681}
{"text":"  Current and upcoming cosmological observations allow us to probe structures\non smaller and smaller scales, entering highly nonlinear regimes. In order to\nobtain theoretical predictions in these regimes, large cosmological simulations\nhave to be carried out. The promised high accuracy from observations make the\nsimulation task very demanding: the simulations have to be at least as accurate\nas the observations. This requirement can only be fulfilled by carrying out an\nextensive code validation program. The first step of such a program is the\ncomparison of different cosmology codes including gravitation interactions\nonly. In this paper we extend a recently carried out code comparison project to\ninclude five more simulation codes. We restrict our analysis to a small\ncosmological volume which allows us to investigate properties of halos. For the\nmatter power spectrum and the mass function, the previous results hold, with\nthe codes agreeing at the 10% level over wide dynamic ranges. We extend our\nanalysis to the comparison of halo profiles and investigate the halo count as a\nfunction of local density. We introduce and discuss ParaView as a flexible\nanalysis tool for cosmological simulations, the use of which immensely\nsimplifies the code comparison task.\n","label":0,"model":"human","source":"arxiv","id":3682}
{"text":"  Traversable wormholes are objects that present a lot of interest in the last\nyears because of their geometric features and their relation with exotic\nmatter. In this paper we presnt a review of the principal characteristics of\ntraversable Morris-Thorne wormholes, their construction proccess and some\naspects about the exotic matter that is needed in order to mantain them. Then,\nwe use a junction proccess to obatin two specific wormhole solutions in the\n(2+1) gravity formalism with negative cosmological constant. The obtained\nsolutions represent wormholes with an external spacetime correspondient to the\nBTZ black hole solution. We also show that exotic matter is needed to mantain\nthese wormholes.\n  -----\n  Los agujeros de gusano atravesables son objetos que presentan un gran interes\nen la actualidad debido a sus caracteristicas geometricas y a su relacion con\nla materia exotica. En el presente trabajo se muestra una revision de las\ncaracteristicas de los agujeros de gusano atravesables al estilo de Morris y\nThorne, al igual que el proceso de construccion y aspectos de la materia\nexotica necesaria para mantenerlos. Luego, se utiliza un proceso de juntura\npara construir dos soluciones especificas tipo agujero de gusano en el\nformalismo de la gravedad (2+1) con constante cosmologica negativa. Con esta\nconstruccion, se obtienen agujeros atravesables que se encuentran unidos a un\nespacio-tiempo externo correspondiente al agujero negro BTZ sin momento angular\ny sin carga electrica. Ademas de esto, se muestra que para mantener este tipo\nde solucion es necesaria la existencia de materia exotica, es decir, materia\nque viole las condiciones de energia.\n","label":0,"model":"human","source":"arxiv","id":3683}
{"text":"  The first challenge in the formation of both terrestrial planets and the\ncores of gas giants is the retention of grains in protoplanetary disks. In most\nregions of these disks, gas attains sub-Keplerian speeds as a consequence of a\nnegative pressure gradient. Hydrodynamic drag leads to orbital decay and\ndepletion of the solid material in the disk, with characteristic timescales as\nshort as only a few hundred years for meter-sized objects at 1 AU. In this\npaper, we suggest a particle retention mechanism which promotes the\naccumulation of grains and the formation of planetesimals near the water\nsublimation front or ``snow line.'' This model is based on the assumption that,\nin the regions most interesting for planet formation, the viscous evolution of\nthe disk is due to turbulence driven by the magneto-rotational instability\n(MRI) in the surface layers of the disk. The depth to which MRI effectively\ngenerates turbulence is a strong function of the grain size and abundance. A\nsharp increase in the grain-to-gas density ratio across the snow line reduces\nthe column depth of the active layer. As the disk evolves towards a\nquasi-steady-state, this change in the active layer creates a local maximum in\nradial distribution of the gas surface density and pressure, causing the gas to\nrotate at super-Keplerian speed and halting the inward migration of grains.\nThis senario presents a robust process for grain retention which may aid in the\nformation of proto-gas-giant cores preferentially near the snow line.\n","label":0,"model":"human","source":"arxiv","id":3684}
{"text":"  New information on short\/hard gamma-ray bursts (GRBs) is being gathered\nthanks to the discovery of their optical and X-ray afterglows. However, some\nkey aspects are still poorly understood, including the collimation level of the\noutflow, the duration of the central engine activity, and the properties of the\nprogenitor systems. We want to constrain the physical properties of the short\nGRB 050724 and of its host galaxy, and make some inferences on the global short\nGRB population. We present optical observations of the afterglow of GRB 050724\nand of its host galaxy, significantly expanding the existing dataset for this\nevent. We compare our results with models, complementing them with available\nmeasurements from the literature. We study the afterglow light curve and\nspectrum including X-ray data. We also present observations of the host galaxy.\nThe observed optical emission was likely related to the large flare observed in\nthe X-ray light curve. The apparent steep decay was therefore not due to the\njet effect. Available data are indeed consistent with low collimation, in turn\nimplying a large energy release, comparable to that of long GRBs. The flare\nproperties also constrain the internal shock mechanism, requiring a large\nLorentz factor contrast between the colliding shells. This implies that the\ncentral engine was active at late times, rather than ejecting all shells\nsimultaneously. The host galaxy has red colors and no ongoing star formation,\nconsistent with previous findings on this GRB. However, it is not a pure\nelliptical, and has some faint spiral structure. GRB 050724 provides the most\ncompelling case for association between a short burst and a galaxy with old\nstellar population. It thus plays a pivotal role in constraining progenitors\nmodels, which should allow for long delays between birth and explosion.\n","label":0,"model":"human","source":"arxiv","id":3685}
{"text":"  Recent observations show that the cooling flows in the central regions of\ngalaxy clusters are highly suppressed. Observed AGN-induced cavities\/bubbles\nare a leading candidate for suppressing cooling, usually via some form of\nmechanical heating. At the same time, observed X-ray cavities and synchrotron\nemission point toward a significant non-thermal particle population. Previous\nstudies have focused on the dynamical effects of cosmic-ray pressure support,\nbut none have built successful models in which cosmic-ray heating is\nsignificant. Here we investigate a new model of AGN heating, in which the\nintracluster medium is efficiently heated by cosmic-rays, which are injected\ninto the ICM through diffusion or the shredding of the bubbles by\nRayleigh-Taylor or Kelvin-Helmholtz instabilities. We include thermal\nconduction as well. Using numerical simulations, we show that the cooling\ncatastrophe is efficiently suppressed. The cluster quickly relaxes to a\nquasi-equilibrium state with a highly reduced accretion rate and temperature\nand density profiles which match observations. Unlike the conduction-only case,\nno fine-tuning of the Spitzer conduction suppression factor f is needed. The\ncosmic ray pressure, P_c\/P_g <~ 0.1 and dP_c\/dr <~ 0.1 \\rho g, is well within\nobservational bounds. Cosmic ray heating is a very attractive alternative to\nmechanical heating, and may become particularly compelling if GLAST detects the\ngamma-ray signature of cosmic-rays in clusters.\n","label":0,"model":"human","source":"arxiv","id":3686}
{"text":"  We calculate durations and spectral paramaters for 218 Swift bursts detected\nby the BAT instrument between and including GRBs 041220 and 070509, including\n77 events with measured redshifts. Incorporating prior knowledge into the\nspectral fits, we are able to measure the characteristic $\\nu F_{\\nu}$ spectral\npeak energy $E_{\\rm pk,obs}$ and the isotropic equivalent energy $E_{\\rm iso}$\n(1--$10^4$ keV) for all events. This complete and rather extensive catalog,\nanalyzed with a unified methodology, allows us to address the persistence and\norigin of high-energy correlations suggested in pre-Swift observations. We find\nthat the $E_{\\rm pk,obs}$-$E_{\\rm iso}$ correlation is present in the Swift\nsample; however, the best-fit powerlaw relation is inconsistent with the\nbest-fit pre-Swift relation at >5 sigma significance. Moreover, it has a factor\n>~ 2 larger intrinsic scatter, after accounting for large errors on $E_{\\rm\npk,obs}$. A large fraction of the Swift events are hard and subluminous\nrelative to (and inconsistent with) the pre-Swift relation, in agreement with\nindications from BATSE GRBs without redshift. Moreover, we determine an\nexperimental threshold for the BAT detector and show how the $E_{\\rm\npk,obs}$--$E_{\\rm iso}$ correlation arises artificially due to partial\ncorrelation with the threshold. We show that pre-Swift correlations found by\nAmati et al.(2002), Yonetoku et al. (2004), Firmani et al.(2006) (and\nindependently by others) are likely unrelated to the physical properties of\nGRBs and are likely useless for tests of cosmology. Also, an explanation of\nthese correlations in terms of a detector threshold provides a natural and\nquantitative explanation for why short-duration GRBs and events at low redshift\ntend to be outliers to the correlations.\n","label":0,"model":"human","source":"arxiv","id":3687}
{"text":"  We compute the Coleman Weinberg effective potential for the Higgs field in RS\nGauge-Higgs unification scenarios based on a bulk SO(5) x U(1)_X gauge\nsymmetry, with gauge and fermion fields propagating in the bulk and a custodial\nsymmetry protecting the generation of large corrections to the T parameter and\nthe coupling of the Z to the bottom quark. We demonstrate that electroweak\nsymmetry breaking may be realized, with proper generation of the top and bottom\nquark masses for the same region of bulk mass parameters that lead to good\nagreement with precision electroweak data in the presence of a light Higgs. We\ncompute the Higgs mass and demonstrate that for the range of parameters for\nwhich the Higgs boson has Standard Model-like properties, the Higgs mass is\nnaturally in a range that varies between values close to the LEP experimental\nlimit and about 160 GeV. This mass range may be probed at the Tevatron and at\nthe LHC. We analyze the KK spectrum and briefly discuss the phenomenology of\nthe light resonances arising in our model.\n","label":0,"model":"human","source":"arxiv","id":3688}
{"text":"  A consistent folding model analysis of the ($\\Delta S=0, \\Delta T=1$) charge\nexchange \\pn reaction measured with $^{48}$Ca, $^{90}$Zr, $^{120}$Sn and\n$^{208}$Pb targets at the proton energies of 35 and 45 MeV is done within a\ntwo-channel coupling formalism. The nuclear ground state densities given by the\nHartree-Fock-Bogoljubov formalism and the density dependent CDM3Y6 interaction\nwere used as inputs for the folding calculation of the nucleon optical\npotential and \\pn form factor. To have an accurate isospin dependence of the\ninteraction, a complex isovector density dependence of the CDM3Y6 interaction\nhas been carefully calibrated against the microscopic Brueckner-Hatree-Fock\ncalculation by Jeukenne, Lejeune and Mahaux before being used as folding input.\nSince the isovector coupling was used to explicitly link the isovector part of\nthe nucleon optical potential to the cross section of \\pn reaction exciting the\n0$^+$ isobaric analog states in $^{48}$Sc, $^{90}$Nb, $^{120}$Sb and\n$^{208}$Bi, the newly parameterized isovector density dependence could be well\ntested in the folding model analysis of the \\pn reaction. The isospin- and\ndensity dependent CDM3Y6 interaction was further used in the Hartree-Fock\ncalculation of asymmetric nuclear matter, and a realistic estimation of the\nnuclear symmetry energy has been made.\n","label":0,"model":"human","source":"arxiv","id":3689}
{"text":"  An up-to-date catalog of nearby galaxies considered as hosts of binary\ncompact objects is provided with complete information about sky position,\ndistance, extinction-corrected blue luminosity and error estimates. With our\ncurrent understanding of binary evolution, rates of formation and coalescence\nfor binary compact objects scale with massive-star formation and hence the\n(extinction-corrected) blue luminosity of host galaxies. Coalescence events in\nbinary compact objects are among the most promising gravitational-wave sources\nfor ground-based gravitational-wave detectors such as LIGO. Our catalog and\nassociated error estimates are important for the interpretation of analyses,\ncarried out for LIGO, to constrain the rates of compact binary coalescence,\ngiven an astrophysical population model for the sources considered. We discuss\nhow the notion of effective distance, created to account for the antenna\npattern of a gravitational-wave detector, must be used in conjunction with our\ncatalog. We note that the catalog provided can be used on other astronomical\nanalysis of populations that scale with galaxy blue luminosity.\n","label":0,"model":"human","source":"arxiv","id":3690}
{"text":"  We present a detailed analysis of the intergalactic metal-line absorption\nsystems in the archival HST\/STIS and FUSE ultraviolet spectra of the\nlow-redshift quasar PKS1302-102 (z_QSO = 0.2784). We supplement the archive\ndata with CLOUDY ionization models and a survey of galaxies in the quasar\nfield. There are 15 strong Lya absorbers with column densities logN_HI > 14. Of\nthese, six are associated with at least CIII 977 absorption (logN(C^++) > 13);\nthis implies a redshift density dN_CIII\/dz = 36+13\/-9 (68% confidence limits)\nfor the five detections with rest equivalent width W_r > 50 mA. Two systems\nshow OVI 1031,1037 absorption in addition to CIII (logN(O^+5) > 14). One is a\npartial Lyman limit system (logN_HI = 17) with associated CIII, OVI, and SiIII\n1206 absorption. There are three tentative OVI systems that do not have CIII\ndetected. For one OVI doublet with both lines detected at 3 sigma with W_r > 50\nmA, dN_OVI\/dz = 7+9\/-4. We also search for OVI doublets without Lya absorption\nbut identify none. From CLOUDY modeling, these metal-line systems have\nmetallicities spanning the range -4 < [M\/H] < -0.3. The two OVI systems with\nassociated CIII absorption cannot be single-phase, collisionally-ionized media\nbased on the relative abundances of the metals and kinematic arguments. From\nthe galaxy survey, we discover that the absorption systems are in a diverse set\nof galactic environments. Each metal-line system has at least one galaxy within\n500 km\/s and 600 h^-1 kpc with L > 0.1 L_*.\n","label":0,"model":"human","source":"arxiv","id":3691}
{"text":"  In some bicategories, the 1-cells are `morphisms' between the 0-cells, such\nas functors between categories, but in others they are `objects' over the\n0-cells, such as bimodules, spans, distributors, or parametrized spectra. Many\nbicategorical notions do not work well in these cases, because the `morphisms\nbetween 0-cells', such as ring homomorphisms, are missing. We can include them\nby using a pseudo double category, but usually these morphisms also induce base\nchange functors acting on the 1-cells. We avoid complicated coherence problems\nby describing base change `nonalgebraically', using categorical fibrations. The\nresulting `framed bicategories' assemble into 2-categories, with attendant\nnotions of equivalence, adjunction, and so on which are more appropriate for\nour examples than are the usual bicategorical ones.\n  We then describe two ways to construct framed bicategories. One is an\nanalogue of rings and bimodules which starts from one framed bicategory and\nbuilds another. The other starts from a `monoidal fibration', meaning a\nparametrized family of monoidal categories, and produces an analogue of the\nframed bicategory of spans. Combining the two, we obtain a construction which\nincludes both enriched and internal categories as special cases.\n","label":0,"model":"human","source":"arxiv","id":3692}
{"text":"  We present an experimental and theoretical study of the polarized\nphotoluminescence spectrum of single semiconductor quantum dots in various\ncharge states. We compare our high resolution polarization sensitive spectral\nmeasurements with a new many-carrier theoretical model, which was developed for\nthis purpose. The model considers both the isotropic and anisotropic exchange\ninteractions between all participating electron-hole pairs. With this addition,\nwe calculate both the energies and polarizations of all optical transitions\nbetween collective, quantum dot confined charge carrier states. We succeed in\nidentifying most of the measured spectral lines. In particular, the lines\nresulting from singly-, doubly- and triply- negatively charged excitons and\nbiexcitons. We demonstrate that lines emanating from evenly charged states are\nlinearly polarized. Their polarization direction does not necessarily coincide\nwith the traditional crystallographic direction. It depends on the shells of\nthe single carriers, which participate in the recombination process.\n","label":0,"model":"human","source":"arxiv","id":3693}
{"text":"  A 21 cm neutral hydrogen interferometric survey of the Large Magellanic Cloud\n(LMC) combined with the Parkes multi-beam HI single-dish survey clearly shows\nthat the HI gas is distributed in the form of clumps or clouds. The HI clouds\nand clumps have been identified using a thresholding method with three separate\nbrightness temperature thresholds ($T_b$). Each catalog of HI cloud candidates\nshows a power law relationship between the sizes and the velocity dispersions\nof the clouds roughly following the Larson Law scaling $\\sigma_v \\propto\nR^{0.5}$, with steeper indices associated with dynamically hot regions. The\nclouds in each catalog have roughly constant virial parameters as a function\nmass suggesting that that the clouds are all in roughly the same dynamical\nstate, but the values of the virial parameter are significantly larger than\nunity showing that turbulent motions dominate gravity in these clouds. The mass\ndistribution of the clouds is a power law with differential indices between\n-1.6 and -2.0 for the three catalogs. In contrast, the distribution of mean\nsurface densities is a log-normal distribution.\n","label":0,"model":"human","source":"arxiv","id":3694}
{"text":"  Anhedonia is one of the key symptoms of depression in humans. Consumption of\n1% sucrose solution supplemented with 0.2% vanillin was studied in two\nexperimental contexts in male mice living under chronic social stress induced\nby daily experience of defeats in agonistic interactions and leading to\ndevelopment of depression. In the first experiment, vanillin sucrose solution\nwas made available as an option of water during 10 days to mice living in group\nhome cages. Then the mice were subjected to social defeat stress and during\nstress exposure they were provided with both vanillin sucrose solution and\nwater using a free two bottles choice paradigm. In the other experiment,\nvanillin sucrose solution were first offered to mice after 8 days of exposure\nto social defeat stress. Males familiar with vanillin sucrose solution showed\nvanillin sucrose preference while experiencing defeat stress: consumption of\nvanillin sucrose solution was about 70% of total liquid consumption. However,\nthe consumption of vanillin sucrose solution per gram of body weight in mice\nimposed to social stress during 20 days was significantly lower than in control\nmales. In the second experiment, males after 8 days of social defeat stress\nwere found to consume significantly less vanillin sucrose solution as compared\nwith control males. On average during two weeks of measurements, vanillin\nsucrose solution intake was less than 20% of total liquid consumption in males\nwith symptoms of depression and anxiety. Consumption per gram of body weight\nalso appeared to be significantly lower than in control group. Influence of the\nexperimental context on the development of anhedonia, which was measured by the\nreduction in sucrose solution intake by chronically stressed male mice, has\nbeen discussed.\n","label":0,"model":"human","source":"arxiv","id":3695}
{"text":"  We present broadband (3-500 keV) INTEGRAL X-ray spectra and X-ray\/optical\nlight curves of the luminous black hole X-ray transient and relativistic jet\nsource GRO J1655-40. Our analysis covers four Target of Opportunity\nobservations of the outburst that started in February 2005. We find that the\nhigh energy emission of GRO J1655-40 can be modelled well with an unbroken\npower-law (with photon indices of 1.72+-0.03,2.21+-0.04 for the first and the\nsecond observations, respectively). These correspond to hard and thermal\ndominant states, respectively. In contrast to many other black hole spectra,\nhigh energy complexity in the form of a break or cut-off is not required for\nthe hard state, contrary to previous expectations for this state. We show for\nthe first time that Comptonization by non-thermal electrons is the dominant\nprocess for the high energy emission in the hard state. We discuss our results\nin terms of models for broad-band emission and accretion flows in stellar-mass\nblack holes.\n","label":0,"model":"human","source":"arxiv","id":3696}
{"text":"  We analyze numerical-relativity (NR) waveforms that cover nine orbits (18\ngravitational-wave cycles) before merger of an equal-mass system with low\neccentricity, with numerical uncertainties of 0.25 radians in the phase and\nless than 2% in the amplitude; such accuracy allows a direct comparison with\npost-Newtonian (PN) waveforms. We focus on one of the PN approximants that has\nbeen proposed for use in gravitational-wave data analysis, the restricted 3.5PN\n``TaylorT1'' waveforms, and compare these with a section of the numerical\nwaveform from the second to the eighth orbit, which is about one and a half\norbits before merger. This corresponds to a gravitational-wave frequency range\nof $M\\omega = 0.0455$ to 0.1. Depending on the method of matching PN and NR\nwaveforms, the accumulated phase disagreement over this frequency range can be\nwithin numerical uncertainty. Similar results are found in comparisons with an\nalternative PN approximant, 3PN ``TaylorT3''. The amplitude disagreement, on\nthe other hand, is around 6%, but roughly constant for all 13 cycles that are\ncompared, suggesting that only 4.5 orbits need be simulated to match PN and NR\nwaves with the same accuracy as is possible with nine orbits. If, however, we\nmodel the amplitude up to 2.5PN order, the amplitude disagreement is roughly\nwithin numerical uncertainty up to about 11 cycles before merger.\n","label":0,"model":"human","source":"arxiv","id":3697}
{"text":"  Cortical neurons are subject to sustained and irregular synaptic activity\nwhich causes important fluctuations of the membrane potential (Vm). We review\nhere different methods to characterize this activity and its impact on spike\ngeneration. The simplified, fluctuating point-conductance model of synaptic\nactivity provides the starting point of a variety of methods for the analysis\nof intracellular Vm recordings. In this model, the synaptic excitatory and\ninhibitory conductances are described by Gaussian-distributed stochastic\nvariables, or colored conductance noise. The matching of experimentally\nrecorded Vm distributions to an invertible theoretical expression derived from\nthe model allows the extraction of parameters characterizing the synaptic\nconductance distributions. This analysis can be complemented by the matching of\nexperimental Vm power spectral densities (PSDs) to a theoretical template, even\nthough the unexpected scaling properties of experimental PSDs limit the\nprecision of this latter approach. Building on this stochastic characterization\nof synaptic activity, we also propose methods to qualitatively and\nquantitatively evaluate spike-triggered averages of synaptic time-courses\npreceding spikes. This analysis points to an essential role for synaptic\nconductance variance in determining spike times. The presented methods are\nevaluated using controlled conductance injection in cortical neurons in vitro\nwith the dynamic-clamp technique. We review their applications to the analysis\nof in vivo intracellular recordings in cat association cortex, which suggest a\npredominant role for inhibition in determining both sub- and supra-threshold\ndynamics of cortical neurons embedded in active networks.\n","label":0,"model":"human","source":"arxiv","id":3698}
{"text":"  We study numerically finite-size corrections in scaling relations for\nroughness distributions of various interface growth models. The most common\nrelation, which considers the average roughness $<w_2>$ as scaling factor, is\nnot obeyed in the steady states of a group of ballistic-like models in 2+1\ndimensions, even when very large system sizes are considered. On the other\nhand, good collapse of the same data is obtained with a scaling relation that\ninvolves the root mean square fluctuation of the roughness, which can be\nexplained by finite-size effects on second moments of the scaling functions. We\nalso obtain data collapse with an alternative scaling relation that accounts\nfor the effect of the intrinsic width, which is a constant correction term\npreviously proposed for the scaling of $<w_2>$. This illustrates how\nfinite-size corrections can be obtained from roughness distributions scaling.\nHowever, we discard the usual interpretation that the intrinsic width is a\nconsequence of high surface steps by analyzing data of restricted\nsolid-on-solid models with various maximal height differences between\nneighboring columns. We also observe that large finite-size corrections in the\nroughness distributions are usually accompanied by huge corrections in height\ndistributions and average local slopes, as well as in estimates of scaling\nexponents. The molecular-beam epitaxy model of Das Sarma and Tamborenea in 1+1\ndimensions is a case example in which none of the proposed scaling relations\nworks properly, while the other measured quantities do not converge to the\nexpected asymptotic values. Thus, although roughness distributions are clearly\nbetter than other quantities to determine the universality class of a growing\nsystem, it is not the final solution for this task.\n","label":0,"model":"human","source":"arxiv","id":3699}
{"text":"  The results of ac and dc magnetic susceptibility isothermal magnetization and\nheat-capacity measurements as a function of temperature (T) are reported for\nSr3NiRhO6 and Sr3NiPtO6 containing magnetic chains arranged in a triangular\nfashion in the basal plane and crystallizing in K4CdCl6-derived rhombohedral\nstructure. The results establish that both the compounds are magnetically\nfrustrated, however in different ways. In the case of the Rh compound, the\nsusceptibility data reveal that there are two magnetic transitions, one in the\nrange 10 -15 K and the other appearing as a smooth crossover near 45 K, with a\nlarge frequency dependence of ac susceptibility in the range 10 to 40 K; in\naddition, the features in C(T) are smeared out at these temperatures. The\nmagnetic properties are comparable to those of previously known few compounds\nwith partially disordered antiferromagnetic structure. On the other hand, for\nSr3NiPtO6, there is no evidence for long-range magnetic ordering down to 1.8 K\ndespite large value of paramagnetic Curie temperature.\n","label":0,"model":"human","source":"arxiv","id":3700}
{"text":"  Full details are given for the definition and construction of the wreath\nproduct of two arbitrary Lie algebras, in the hope that it can lead to the\ndefinition of a suitable Lie group to be the wreath product of two given Lie\ngroups. In the process, quite a few new notions are needed, and introduced.\nSuch are, for example : Formal series with variables in a vector space and\ncoefficients in some other vector space. Derivation of a formal series relative\nto another formal series. The Lie algebra of a vector space. Formal actions of\nLie algebras over vector spaces. The basic formal action of a Lie algebra over\nitself (as a formal version of the analytic aspect of the infinitesimal\noperation law of a Lie groupuscule). More generally, the wreath product of two\nLie algebras is defined, relative to a formal action of the second onto an\narbitrary vector space. Main features are : A description of the triangular\nactions of wreath products over product vector spaces, and a Kaloujnine-Krasner\ntype theorem : In essence, it says that all Lie extensions of a given Lie\nalgebra by another Lie algebra are, indeed, subalgebras of their wreath\nproduct.\n","label":0,"model":"human","source":"arxiv","id":3701}
{"text":"  This paper proposes a novel learning method for a mixture of recurrent neural\nnetwork (RNN) experts model, which can acquire the ability to generate desired\nsequences by dynamically switching between experts. Our method is based on\nmaximum likelihood estimation, using a gradient descent algorithm. This\napproach is similar to that used in conventional methods; however, we modify\nthe likelihood function by adding a mechanism to alter the variance for each\nexpert. The proposed method is demonstrated to successfully learn Markov chain\nswitching among a set of 9 Lissajous curves, for which the conventional method\nfails. The learning performance, analyzed in terms of the generalization\ncapability, of the proposed method is also shown to be superior to that of the\nconventional method. With the addition of a gating network, the proposed method\nis successfully applied to the learning of sensory-motor flows for a small\nhumanoid robot as a realistic problem of time series prediction and generation.\n","label":0,"model":"human","source":"arxiv","id":3702}
{"text":"  LS I +61 303 is a puzzling Be\/X-ray binary with variable gamma-ray emission\nat up TeV energies. The nature of the compact object and the origin of the\nhigh-energy emission are unclear. One family of models invokes particle\nacceleration in shocks from the collision between the B-star wind and a\nrelativistic pulsar wind, while another centers on a relativistic jet powered\nby accretion. Recent high-resolution radio observations showing a putative\n\"cometary tail\" pointing away from the Be star near periastron have been cited\nas support for the pulsar-wind model. We wish here to carry out a quantitative\nassessment of these competing models for this extraordinary source. We apply a\n3D SPH code for dynamical simulations of both the pulsar-wind-interaction and\naccretion-jet models. The former yields a description of the shape of the\nwind-wind interaction surface. The latter provides an estimation of the\naccretion rate. The results allow critical evaluation of how the two distinct\nmodels confront the data in various wavebands under a range of conditions. When\none accounts for the 3D dynamical wind interaction under realistic constraints\nfor the relative strength of the B-star and pulsar winds, the resulting form of\nthe interaction front does not match the putative \"cometary tail\" claimed from\nradio observations. On the other hand, dynamical simulations of the\naccretion-jet model indicate that the orbital phase variation of accretion\npower includes a secondary broad peak well away from periastron, thus providing\na plausible way to explain the observed TeV gamma ray emission toward apastron.\nWe conclude that the colliding-wind model is not clearly established for LS I\n+61 303, while the accretion-jet model can reproduce many key characteristics\nof the observed TeV gamma-ray emission.\n","label":0,"model":"human","source":"arxiv","id":3703}
{"text":"  We explore the physics behind one of the brightest radio afterglows ever, GRB\n030329, at late times when the jet is non-relativistic. We determine the\nphysical parameters of the blast wave and its surroundings, in particular the\nindex of the electron energy distribution, the energy of the blast wave, and\nthe density (structure) of the circumburst medium. We then compare our results\nwith those from image size measurements. We observed the GRB 030329 radio\nafterglow with the Westerbork Synthesis Radio Telescope and the Giant Metrewave\nRadio Telescope at frequencies from 325 MHz to 8.4 GHz, spanning a time range\nof 268-1128 days after the burst. We modeled all the available radio data and\nderived the physical parameters. The index of the electron energy distribution\nis p=2.1, the circumburst medium is homogeneous, and the transition to the\nnon-relativistic phase happens at t_NR ~ 80 days. The energy of the blast wave\nand density of the surrounding medium are comparable to previous findings. Our\nfindings indicate that the blast wave is roughly spherical at t_NR, and they\nagree with the implications from the VLBI studies of image size evolution. It\nis not clear from the presented dataset whether we have seen emission from the\ncounter jet or not. We predict that the Low Frequency Array will be able to\nobserve the afterglow of GRB 030329 and many other radio afterglows,\nconstraining the physics of the blast wave during its non-relativistic phase\neven further.\n","label":0,"model":"human","source":"arxiv","id":3704}
{"text":"  We present the full thermodynamics of a fluid confined by an arbitrary\nexternal potential based on the virial expansion of the grand potential. The\nfluid may be classical or quantum and it is assumed that interatomic\ninteractions are pairwise additive. We indicate how the appropriate\n\"generalized\" volume and pressure variables, that replace the usual volume and\nhydrostatic pressure, emerge for a given confining potential in the\nthermodynamic limit. A discussion of the physical meaning and of the\nmeasurement of these variables is presented. We emphasize that this treatment\nyields the correct equation of state of the fluid and we give its virial\nexpansion. We propose an experiment to measure the heat capacity, so that with\nthis quantity and the equation of state, the complete thermodynamics of the\nsystem may be extracted. As a corollary, we find that the so-called {\\it local\ndensity approximation} for these systems follows in the thermodynamic limit,\nalthough we also point out that it cannot be used indiscriminately for all\nlocal variables. Along the text we discuss the relevance of these findings in\nthe description of the currently confined ultracold gases.\n","label":0,"model":"human","source":"arxiv","id":3705}
{"text":"  This paper is the fifth in a series exploring the physical consequences of\nthe solidity of glass-forming liquids. Paper IV proposed a model where the\ndensity field is described by a time-dependent Ginzburg-Landau equation of the\nnonconserved type with rates in $k$ space of the form $\\Gamma_0+Dk^2$. The\nmodel assumes that $D\\gg\\Gamma_0a^2$ where $a$ is the average intermolecular\ndistance; this inequality expresses a long-wavelength dominance of the dynamics\nwhich implies that the Hamiltonian (free energy) to a good approximation may be\ntaken to be ultralocal. In the present paper we argue that this is the simplest\nmodel consistent with the following three experimental facts: 1) Viscous\nliquids approaching the glass transition do not develop long-range order; 2)\nThe glass has lower compressibility than the liquid; 3) The alpha process\ninvolves several decades of relaxation times shorter than the mean relaxation\ntime. The paper proceeds to list six further experimental facts characterizing\nequilibrium viscous liquid dynamics and shows that these are readily understood\nin terms of the model; some are direct consequences, others are quite natural\nwhen viewed in light of the model.\n","label":0,"model":"human","source":"arxiv","id":3706}
{"text":"  We explore the initial conditions for cosmological N-body simulations\nsuitable for calculating the skewness and kurtosis of the density field. In\ngeneral, the initial conditions based on the perturbation theory (PT) provide\nincorrect second-order and higher-order growth. These errors implied by the use\nof the perturbation theory to set up the initial conditions in N-body\nsimulations are called transients. Unless these transients are completely\nsuppressed compared with the dominant growing mode, we can not reproduce the\ncorrect evolution of cumulants with orders higher than two, even though there\nis no problem with the numerical scheme. We investigate the impact of\ntransients on the observable statistical quantities by performing $N$-body\nsimulations with initial conditions based on Lagrangian perturbation theory\n(LPT). We show that the effects of transients on the kurtosis from the initial\nconditions, based on second-order Lagrangian perturbation theory (2LPT) have\nalmost disappeared by $z\\sim5$, as long as the initial conditions are set at $z\n> 30$. This means that for practical purposes, the initial conditions based on\n2LPT are accurate enough for numerical calculations of skewness and kurtosis.\n","label":0,"model":"human","source":"arxiv","id":3707}
{"text":"  In the standard Lambda CDM cosmological model with a Gaussian primordial\ndensity fluctuation field, the relatively low value of the mass variance\nparameter (sigma_8=0.74{+0.05}{-0.06}, obtained from the WMAP 3-year data)\nresults in a reduced likelihood that the measured level of CMB anisotropy on\nthe scales of clusters is due to the Sunyaev-Zeldovich (S-Z) effect. To assess\nthe feasibility of producing higher levels of S-Z power, we explore two\nalternative models which predict higher cluster abundance. In the first model\nthe primordial density field has a chi^2_1 distribution, whereas in the second\nan early dark energy component gives rise to the desired higher cluster\nabundance. We carry out the necessary detailed calculations of the levels of\nS-Z power spectra, cluster number counts, and angular 2-point correlation\nfunction of clusters, and compare (in a self-consistent way) their predicted\nredshift distributions. Our results provide a sufficient basis upon which the\nviability of the three models may be tested by future high quality\nmeasurements.\n","label":0,"model":"human","source":"arxiv","id":3708}
{"text":"  Claims have been made that f0(1370) does not exist. The five primary sets of\ndata requiring its existence are refitted. Major dispersive effects due to the\nopening of the 4pi threshold are included for the first time; the sigma -> 4pi\namplitude plays a strong role. Crystal Barrel data on pbar-p -> 3pizero at rest\nrequire f0(1370) signals of at least 32 and 33 standard deviations in 1S0 and\n3P1 annihilation respectively. Furthermore, they agree within 5 MeV for mass\nand width. Data on pbar-p -> eta-eta-pizero agree and require at least a 19\nstandard deviation contribution. This alone is sufficient to demonstrate the\nexistence of f0(1370). BES II data for J\/Psi -> phi-pi-pi contain a visible\nf0(1370) signal > 8 standard devations. In all cases, a resonant phase\nvariation is required. The possibility of a second pole in the sigma amplitude\ndue to the opening of the 4pi channel is excluded. Cern-Munich data for pi-pi\nelastic scattering are fitted well with the inclusion of some mixing between\nsigma, f0(1370) and f0(1500). The pi-pi widths for f2(1565), rho3(1690),\nrho3(1990) and f4(2040) are determined.\n","label":0,"model":"human","source":"arxiv","id":3709}
{"text":"  Ever since the first discoveries of the quantum-interference transport in\nmesoscopic systems, the electron dephasing times, $\\tau_\\phi$, in the\nconcentrated AuPd alloys have been extensively measured. The samples were made\nfrom different sources with different compositions, prepared by different\ndeposition methods, and various geometries (1D narrow wires, 2D thin films, and\n3D thickfilms) were studied. Surprisingly, the low-temperature behavior of\n$\\tau_\\phi$ inferred by different groups over two decades reveals a systematic\ncorrelation with the level of disorder of the sample. At low temperatures,\nwhere $\\tau_\\phi$ is (nearly) independent of temperature, a scaling\n$\\tau_\\phi^{\\rm max} \\propto D^{-\\alpha}$ is found, where $tau_\\phi^{\\rm max}$\nis the maximum value of $\\tau_\\phi$ measured in the experiment, $D$ is the\nelectron diffusion constant, and the exponent $\\alpha$ is close to or slightly\nlarger than 1. We address this nontrivial scaling behavior and suggest that the\nmost possible origin for this unusual dephasing is due to dynamical structure\ndefects, while other theoretical explanations may not be totally ruled out.\n","label":0,"model":"human","source":"arxiv","id":3710}
{"text":"  We present optical photometry and spectroscopy of the afterglow and host\ngalaxy of gamma-ray burst 040924. This GRB had a rather short duration of T90\n~2.4s, and a well sampled optical afterglow light curve. We aim to use this\ndataset to find further evidence that this burst is consistent with a massive\nstar core-collapse progenitor. We combine the afterglow data reported here with\ndata taken from the literature and compare the host properties with survey\ndata. We find that the global behaviour of the optical afterglow is well fit by\na broken power-law, with a break at ~0.03 days. We determine the redshift z =\n0.858 +\/- 0.001 from the detected emission lines in our spectrum. Using the\nspectrum and photometry we derive global properties of the host, showing it to\nhave similar properties to long GRB hosts. We detect the [Ne III] emission line\nin the spectrum, and compare the fluxes of this line of a sample of 15 long GRB\nhost galaxies with survey data, showing the long GRB hosts to be comparable to\nlocal metal-poor emission line galaxies in their [Ne III] emission. We fit the\nsupernova bump accompanying this burst, and find that it is similar to other\nlong GRB supernova bumps, but fainter. All properties of GRB 040924 are\nconsistent with an origin in the core-collapse of a massive star: the\nsupernova, the spectrum and SED of the host and the afterglow.\n","label":0,"model":"human","source":"arxiv","id":3711}
{"text":"  The results of the Koenigstuhl survey in the Southern Hemisphere are\npresented. I have searched for common-proper motion companions to 173 field\nvery low-mass stars and brown dwarfs with spectral types > M5.0V and magnitudes\nJ <= 14.5 mag. I have measured for the first time the common-proper motion of\ntwo new wide systems containing very low-mass components, Koenigstuhl 2 AB and\n3 A-BC. Together with Koenigstuhl 1 AB and 2M0126-50AB, they are among the\nwidest systems in their respective classes (r = 450-11900 AU). Koenigstuhl 3\nA-BC contains a well-known F8V star and a M8.0+L3.0V tight binary. I have\ndetermined the minimum frequency of field wide multiples (r > 100 AU) with\nlate-type components at 5.0+\/-1.8 % and the frequency of field wide late-type\nbinaries with mass ratios q > 0.5 at 1.2+\/-0.9 %. These values represent a key\ndiagnostic of evolution history and low-mass star and brown-dwarf formation\nscenarios. Additionally, the proper motions of 76 field very low-mass dwarfs\nare measured here for the first time.\n","label":0,"model":"human","source":"arxiv","id":3712}
{"text":"  We identify satellites of isolated galaxies in SDSS and examine their angular\ndistribution. Using mock catalogues generated from cosmological N-body\nsimulations, we demonstrate that the selection criteria used to select isolated\ngalaxies and their satellites must be very strict in order to correctly\nidentify systems in which the primary galaxy dominates its environment. The\ncriteria used in many previous studies instead select predominantly group\nmembers. We refine a set of selection criteria for which the group\ncontamination is estimated to be less than 7% and present a catalogue of the\nresulting sample. The angular distribution of satellites about their host is\nbiased towards the major axes for spheroidal galaxies and probably also for red\ndisc galaxies, but is isotropic for blue disc galaxies, i.e. it is the colour\nof the host that determines the distribution of its satellites rather than its\nmorphology. The similar anisotropy measured in this study as in studies that\nwere dominated by groups implies that group-specific processes are not\nresponsible for the angular distribution. Satellites that are most likely to\nhave been recently accreted show a tendancy to lie along the same axis as the\nsurrounding large scale structure. The orientations of isolated early and\nintermediate-type galaxies also align with the surrounding large scale\nstructures. We discuss the origin of the anisotropic satellite distribution and\nconsider the implications of our results, critically assessing the respective\nroles played by the orientation of the visible galaxy within its dark matter\nhalo; anisotropic accretion of satellites from the larger scale environment;\nand the biased nature of satellites as tracers of the underlying dark matter\nsubhalo population. (Abridged)\n","label":0,"model":"human","source":"arxiv","id":3713}
{"text":"  We have upgraded the 60-cm radio survey telescope located in Nobeyama, Japan.\nWe developed a new waveguide-type sideband-separating SIS mixer for the\ntelescope, which enables the simultaneous detection of distinct molecular\nemission lines both in the upper and lower sidebands. Over the RF frequency\nrange of 205-240 GHz, the single-sideband receiver noise temperatures of the\nnew mixer are 40-100 K for the 4.0-8.0 GHz IF frequency band. The image\nrejection ratios are greater than 10 dB over the same range. For the dual IF\nsignals obtained by the receiver, we have developed two sets of acousto-optical\nspectrometers and a telescope control system. Using the new telescope system,\nwe successfully detected the 12CO (J=2-1) and 13CO (J=2-1) emission lines\nsimultaneously toward Orion KL in 2005 March. Using the waveguide-type\nsideband-separating SIS mixer for the 200 GHz band, we have initiated the first\nsimultaneous 12CO (J=2-1) and 13CO (J=2-1) survey of the galactic plane as well\nas large-scale mapping observations of nearby molecular clouds.\n","label":0,"model":"human","source":"arxiv","id":3714}
{"text":"  Water is necessary both for the evolution of life and its continuance. It\npossesses particular properties that cannot be found in other materials and\nthat are required for life-giving processes. These properties are brought about\nby the hydrogen bonded environment particularly evident in liquid water. Each\nliquid water molecule is involved in about four hydrogen bonds with strengths\nconsiderably less than covalent bonds but considerably greater than the natural\nthermal energy. These hydrogen bonds are roughly tetrahedrally arranged such\nthat when strongly formed the local clustering expands, decreasing the density.\nSuch low density structuring naturally occurs at low and supercooled\ntemperatures and gives rise to many physical and chemical properties that\nevidence the particular uniqueness of liquid water. If aqueous hydrogen bonds\nwere actually somewhat stronger then water would behave similar to a glass,\nwhereas if they were weaker then water would be a gas and only exist as a\nliquid at sub-zero temperatures. The overall conclusion of this investigation\nis that water's hydrogen bond strength is poised centrally within a narrow\nwindow of its suitability for life.\n","label":0,"model":"human","source":"arxiv","id":3715}
{"text":"  We extend the definition of the Costas property to functions in the\ncontinuum, namely on intervals of the reals or the rationals, and argue that\nsuch functions can be used in the same applications as discrete Costas arrays.\nWe construct Costas bijections in the real continuum within the class of\npiecewise continuously differentiable functions, but our attempts to construct\na fractal-like Costas bijection there are successful only under slight but\nnecessary deviations from the usual arithmetic laws. Furthermore, we are able,\ncontingent on the validity of Artin's conjecture, to set up a limiting process\naccording to which sequences of Welch Costas arrays converge to smooth Costas\nbijections over the reals. The situation over the rationals is different:\nthere, we propose an algorithm of great generality and flexibility for the\nconstruction of a Costas fractal bijection. Its success, though, relies heavily\non the enumerability of the rationals, and therefore it cannot be generalized\nover the reals in an obvious way.\n","label":0,"model":"human","source":"arxiv","id":3716}
{"text":"  We examine the relationship between little Higgs and 5d composite models with\nidentical symmetry structures. By performing an \"extreme\" deconstruction, one\ncan reduce any warped composite model to a little Higgs theory on a handful of\nsites. This allows us to use 4d intuition and the powerful constraints of\nnonlinear sigma models to elucidate obscure points in the original setup. We\nfind that the finiteness of the Higgs potential in 5d is due to the same\ncollective symmetry breaking as in the little Higgs. We compare a 4d and a 5d\nmodel with the same symmetry to the data. Reviewing the constraints on models\nrelated to the Minimal Composite Higgs (hep-ph\/0412089), we see that it has\ndifficulty in producing acceptable values for S, T, and m_{top} simultaneously.\nBy contrast, in a global analysis, the Minimal Moose with custodial symmetry is\nviable in a large region of its parameter space and suffers from no numeric\ntunings. We conjecture that this result is generic for 4d and 5d models with\nidentical symmetries. The data will less strongly constrain the little theory.\n","label":0,"model":"human","source":"arxiv","id":3717}
{"text":"  We obtain the phase diagram and thermodynamic behavior of the Kondo necklace\nmodel for arbitrary dimensions $d$ using a representation for the localized and\nconduction electrons in terms of local Kondo singlet and triplet operators. A\ndecoupling scheme on the double time Green's functions yields the dispersion\nrelation for the excitations of the system. We show that in $d\\geq 3$ there is\nan antiferromagnetically ordered state at finite temperatures terminating at a\nquantum critical point (QCP). In 2-d, long range magnetic order occurs only at\nT=0. The line of Neel transitions for $d>2$ varies with the distance to the\nquantum critical point QCP $|g|$ as, $T_N \\propto |g|^{\\psi}$ where the shift\nexponent $\\psi=1\/(d-1)$. In the paramagnetic side of the phase diagram, the\nspin gap behaves as $\\Delta\\approx \\sqrt{|g|}$ for $d \\ge 3$ consistent with\nthe value $z=1$ found for the dynamical critical exponent. We also find in this\nregion a power law temperature dependence in the specific heat for\n$k_BT\\gg\\Delta$ and along the non-Fermi liquid trajectory. For $k_BT\n\\ll\\Delta$, in the so-called Kondo spin liquid phase, the thermodynamic\nbehavior is dominated by an exponential temperature dependence.\n","label":0,"model":"human","source":"arxiv","id":3718}
{"text":"  Simulations of liquid-gas systems with extended interfaces are observed to\nfail to give accurate results for two reasons: the interface can get ``stuck''\non the lattice or a density overshoot develops around the interface. In the\nfirst case the bulk densities can take a range of values, dependent on the\ninitial conditions. In the second case inaccurate bulk densities are found. In\nthis communication we derive the minimum interface width required for the\naccurate simulation of liquid gas systems with a diffuse interface. We\ndemonstrate this criterion for lattice Boltzmann simulations of a van der Waals\ngas. When combining this criterion with predictions for the bulk stability we\ncan predict the parameter range that leads to stable and accurate simulation\nresults. This allows us to identify parameter ranges leading to high density\nratios of over 1000. This is despite the fact that lattice Boltzmann\nsimulations of liquid-gas systems were believed to be restricted to modest\ndensity ratios of less than 20.\n","label":0,"model":"human","source":"arxiv","id":3719}
{"text":"  An optical cavity enhances the interaction between atoms and light, and the\nrate of coherent atom-photon coupling can be made larger than all decoherence\nrates of the system. For single atoms, this strong coupling regime of cavity\nquantum electrodynamics (cQED) has been the subject of spectacular experimental\nadvances, and great efforts have been made to control the coupling rate by\ntrapping and cooling the atom towards the motional ground state, which has been\nachieved in one dimension so far. For N atoms, the three-dimensional ground\nstate of motion is routinely achieved in atomic Bose-Einstein condensates\n(BECs), but although first experiments combining BECs and optical cavities have\nbeen reported recently, coupling BECs to strong-coupling cavities has remained\nan elusive goal. Here we report such an experiment, which is made possible by\ncombining a new type of fibre-based cavity with atom chip technology. This\nallows single-atom cQED experiments with a simplified setup and realizes the\nnew situation of N atoms in a cavity each of which is identically and strongly\ncoupled to the cavity mode. Moreover, the BEC can be positioned\ndeterministically anywhere within the cavity and localized entirely within a\nsingle antinode of the standing-wave cavity field. This gives rise to a\ncontrolled, tunable coupling rate, as we confirm experimentally. We study the\nheating rate caused by a cavity transmission measurement as a function of the\ncoupling rate and find no measurable heating for strongly coupled BECs. The\nspectrum of the coupled atoms-cavity system, which we map out over a wide range\nof atom numbers and cavity-atom detunings, shows vacuum Rabi splittings\nexceeding 20 gigahertz, as well as an unpredicted additional splitting which we\nattribute to the atomic hyperfine structure.\n","label":0,"model":"human","source":"arxiv","id":3720}
{"text":"  Electron properties of graphene are described in terms of Dirac fermions.\nHere we thoroughly outline the elastic scattering theory for the\ntwo-dimensional massive Dirac fermions in the presence of an axially symmetric\npotential. While the massless limit is relevant for pristine graphene, keeping\nfinite mass allows for generalizations onto situations with broken symmetry\nbetween the two sublattices, and provides a link to the scattering theory of\nelectrons in a parabolic band. We demonstrate that the Dirac theory requires\nshort-distance regularization for potentials which are more singular than 1\/r.\nThe formalism is then applied to scattering off a smooth short-ranged\npotential. Next we consider the Coulomb potential scattering, where the Dirac\ntheory is consistent for a point scatterer only for the effective impurity\nstrength below 1\/2. From the scattering phase shifts we obtain the exact\nCoulomb transport cross-section in terms of the impurity strength. The results\nare relevant for transport in graphene in the presence of impurities that do\nnot induce scattering between the Dirac points in the Brillouin zone.\n","label":0,"model":"human","source":"arxiv","id":3721}
{"text":"  We carry out preliminary numerical study of Sugino's lattice formulation\n\\cite{Sugino:2004qd,Sugino:2004qdf} of the two-dimensional $\\mathcal{N}=(2,2)$\nsuper Yang-Mills theory (2d $\\mathcal{N}=(2,2)$ SYM) with the gauge group\n$\\SU(2)$. The effect of dynamical fermions is included by re-weighting a\nquenched ensemble by the pfaffian factor. It appears that the complex phase of\nthe pfaffian due to lattice artifacts and flat directions of the classical\npotential are not problematic in Monte Carlo simulation. Various one-point\nsupersymmetric Ward-Takahashi (WT) identities are examined for lattice spacings\nup to $a=0.5\/g$ with the fixed physical lattice size $L=4.0\/g$, where $g$\ndenotes the gauge coupling constant in two dimensions. WT identities implied by\nan exact fermionic symmetry of the formulation are confirmed in fair accuracy\nand, for most of these identities, the quantum effect of dynamical fermions is\nclearly observed. For WT identities expected only in the continuum limit, the\nresults seem to be consistent with the behavior expected from supersymmetry,\nalthough we do not see clear distintion from the quenched simulation. We\nmeasure also the expectation values of renormalized gauge-invariant bi-linear\noperators of scalar fields.\n","label":0,"model":"human","source":"arxiv","id":3722}
{"text":"  Longitudinal data tracking repeated measurements on individuals are highly\nvalued for research because they offer controls for unmeasured individual\nheterogeneity that might otherwise bias results. Random effects or mixed models\napproaches, which treat individual heterogeneity as part of the model error\nterm and use generalized least squares to estimate model parameters, are often\ncriticized because correlation between unobserved individual effects and other\nmodel variables can lead to biased and inconsistent parameter estimates.\nStarting with an examination of the relationship between random effects and\nfixed effects estimators in the standard unobserved effects model, this article\ndemonstrates through analysis and simulation that the mixed model approach has\na ``bias compression'' property under a general model for individual\nheterogeneity that can mitigate bias due to uncontrolled differences among\nindividuals. The general model is motivated by the complexities of longitudinal\nstudent achievement measures, but the results have broad applicability to\nlongitudinal modeling.\n","label":0,"model":"human","source":"arxiv","id":3723}
{"text":"  In the field of tutoring systems, investigations have shown that there are\nmany tutoring systems specific to a specific domain that, because of their\nstatic architecture, cannot be adapted to other domains. As consequence, often\nneither methods nor knowledge can be reused. In addition, the knowledge\nengineer must have programming skills in order to enhance and evaluate the\nsystem. One particular challenge is to tackle these problems with the\ndevelopment of a generic tutoring system. AnITA, as a stand-alone application,\nhas been developed and implemented particularly for this purpose. However, in\nthe testing phase, we discovered that this architecture did not fully match the\nuser's intuitive understanding of the use of a learning tool. Therefore, AnITA\nhas been redesigned to exclusively work as a client\/server application and\nrenamed to AnITA2. This paper discusses the evolvements made on the AnITA\ntutoring system, the goal of which is to use generic principles for system\nre-use in any domain. Two experiments were conducted, and the results are\npresented in this paper.\n","label":0,"model":"human","source":"arxiv","id":3724}
{"text":"  Mesh numbering is a critical issue in Finite Element Methods, as the\ncomputational cost of one analysis is highly dependent on the order of the\nnodes of the mesh. This paper presents some preliminary investigations on the\nproblem of mesh numbering using Evolutionary Algorithms. Three conclusions can\nbe drawn from these experiments. First, the results of the up-to-date method\nused in all FEM softwares (Gibb's method) can be consistently improved; second,\nnone of the crossover operators tried so far (either general or problem\nspecific) proved useful; third, though the general tendency in Evolutionary\nComputation seems to be the hybridization with other methods (deterministic or\nheuristic), none of the presented attempt did encounter any success yet. The\ngood news, however, is that this algorithm allows an improvement over the\nstandard heuristic method between 12% and 20% for both the 1545 and 5453-nodes\nmeshes used as test-bed. Finally, some strange interaction between the\nselection scheme and the use of problem specific mutation operator was\nobserved, which appeals for further investigation.\n","label":0,"model":"human","source":"arxiv","id":3725}
{"text":"  We report a direct observational evidence for the existence of the galaxy\nspin alignments with the real space tidal field. We calculate the real space\ntidal field from the real space density field reconstructed recently from the\nTwo Mass Redshift Survey (2MRS) by Erdogdu et al. in 2006. Using a total of\n12122 nearby spiral galaxies from the Tully Galaxy Catalog, we calculate the\norientations of their spin axes relative to the 2MRS tidal field. We find a\nclear signal of the intrinsic correlations between the galaxy spins and the\nintermediate principal axes of the tidal shears. The null hypothesis of no\ncorrelation is rejected at 99.99 % confidence level. We also investigate the\ndependence of the intrinsic correlations on the galaxy morphological type and\nthe environment. It is found that (i) the intrinsic correlation depends weakly\non the morphological type of the spiral galaxies but tends to decrease slightly\nas the type increases; (ii) it is stronger in the high-density regions than in\nthe low-density regions. The observational result is quantitatively consistent\nwith analytic prediction based on the tidal torque theory. It is concluded that\nthe galaxy spin orientations may provide in principle a new complimentary probe\nof the dark matter distribution.\n","label":0,"model":"human","source":"arxiv","id":3726}
{"text":"  As well known, the b-boundaries of the closed Friedman world model and of\nSchwarzschild solution consist of a single point. We study this phenomenon in a\nbroader context of differential and structured spaces. We show that it is an\nequivalence relation $\\rho $, defined on the Cauchy completed total space\n$\\bar{E}$ of the frame bundle over a given space-time, that is responsible for\nthis pathology. A singularity is called malicious if the equivalence class\n$[p_0]$ related to the singularity remains in close contact with all other\nequivalence classes, i.e., if $p_0 \\in \\mathrm{cl}[p]$ for every $p \\in E$. We\nformulate conditions for which such a situation occurs. The differential\nstructure of any space-time with malicious singularities consists only of\nconstant functions which means that, from the topological point of view,\neverything collapses to a single point. It was noncommutative geometry that was\nespecially devised to deal with such situations. A noncommutative algebra on\n$\\bar{E}$, which turns out to be a von Neumann algebra of random operators,\nallows us to study probabilistic properties (in a generalized sense) of\nmalicious singularities. Our main result is that, in the noncommutative regime,\neven the strongest singularities are probabilistically irrelevant.\n","label":0,"model":"human","source":"arxiv","id":3727}
{"text":"  The free convolution is the binary operation on the set of probability\nmeasures on the real line which allows to deduce, from the individual spectral\ndistributions, the spectral distribution of a sum of independent unitarily\ninvariant square random matrices or of a sum of free operators in a non\ncommutative probability space. In the same way, the rectangular free\nconvolution allows to deduce, from the individual singular distributions, the\nsingular distribution of a sum of independent unitarily invariant rectangular\nrandom matrices. In this paper, we consider the regularization properties of\nthese free convolutions on the whole real line. More specifically, we try to\nfind continuous semigroups $(\\mu_t)$ of probability measures such that $\\mu_0$\nis the Dirac mass at zero and such that for all positive $t$ and all\nprobability measure $\\nu$, the free convolution of $\\mu_t$ with $\\nu$ (or, in\nthe rectangular context, the rectangular free convolution of $\\mu_t$ with\n$\\nu$) is absolutely continuous with respect to the Lebesgue measure, with a\npositive analytic density on the whole real line. In the square case, we prove\nthat in semigroups satisfying this property, no measure can have a finite\nsecond moment, and we give a sufficient condition on semigroups to satisfy this\nproperty, with examples. In the rectangular case, we prove that in most cases,\nfor $\\mu$ in a continuous rectangular-convolution-semigroup, the rectangular\nconvolution of $\\mu$ with $\\nu$ either has an atom at the origin or doesn't put\nany mass in a neighborhood of the origin, thus the expected property does not\nhold. However, we give sufficient conditions for analyticity of the density of\nthe rectangular convolution of $\\mu$ with $\\nu$ except on a negligible set of\npoints, as well as existence and continuity of a density everywhere.\n","label":0,"model":"human","source":"arxiv","id":3728}
{"text":"  We report the discovery of a high CO(J=3-2)\/CO(J=1-0) ratio gas with an\narc-like distribution (``high-ratio gas arc'') surrounding the central star\ncluster of the supergiant HII region NGC 604 in the nearby spiral galaxy M 33,\nbased on multi-J CO observations of a 5' $\\times$ 5' region of NGC 604\nconducted using the ASTE 10-m and NRO 45-m telescopes. The discovered\n``high-ratio gas arc'' extends to the south-east to north-west direction with a\nsize of $\\sim$ 200 pc. The western part of the high-ratio gas arc closely\ncoincides well with the shells of the HII regions traced by H$\\alpha$ and radio\ncontinuum peaks. The CO(J=3-2)\/CO(J=1-0) ratio, R_{3-2\/1-0}, ranges between 0.3\nand 1.2 in the observed region, and the R_{3-2\/1-0} values of the high-ratio\ngas arc are around or higher than unity, indicating very warm (T_kin > 60 K)\nand dense (n(H_2) > 10^{3-4} cm^{-3}) conditions of the high-ratio gas arc. We\nsuggest that the dense gas formation and second-generation star formation occur\nin the surrounding gas compressed by the stellar wind and\/or supernova of the\nfirst-generation stars of NGC 604, i.e., the central star cluster of NGC 604.\n","label":0,"model":"human","source":"arxiv","id":3729}
{"text":"  Following an extensive survey of the galactic plane by the INTEGRAL\nsatellite, new hard X-ray sources are discovered with a significant fraction of\nCataclysmic Variables (CVs) among them. We report here the identification of\none of these hard X-ray sources, IGR J00234+6141, as an accreting magnetic\nwhite dwarf of intermediate polar type. We analyse the high energy emission of\nthe INTEGRAL source using all available data and provide complementary optical\nphotometric and spectroscopic data obtained respectively in August and October\n2006. Based on a refined INTEGRAL position, we confirm the proposed optical\nidentification. We clearly detect the presence of a 564 s periodic optical\nmodulation that we identify as the rotation of the white dwarf. The analysis of\nthe optical spectrum also demonstrates that the emission lines show a\nmodulation in radial velocity with an orbital period of Porb = (4.033 +\/-\n0.005) hr. The two periodicities indicate that IGR00234+6141 is a magnetic CV\nof the intermediate polar type. This is one of the faintest and hardest sources\nof this type detected by INTEGRAL. This confirms earlier conclusions that IPs\ncontribute significantly to the population of galactic X-ray sources and\nrepresent a significant fraction of the high energy background.\n","label":0,"model":"human","source":"arxiv","id":3730}
{"text":"  In this paper, we model a nonlinear dynamics of infectious diseases transfer.\nParticularly, we study possible applications to tubercular infection in models\nwith different profiles (peak values) of the population density dependence on\nspatial coordinates. Our approach is based on the well known method of\ninstantons which has been used by the authors to describe kinetics of adiabatic\nchemical reactions as a function of the heat-bath temperature and other system\nparameters. In our approach, we use \"social temperature\" T as one of the\ncontrolling parameters. Increase of T leads to acceleration of the infectious\ndiseases transfer. The \"blockage\" effect for the infectious diseases transfer\nhas been demonstrated in the case when peak values (in the population density)\nare equal to one and under condition that the \"social temperature\" is low.\nExistence of such effect essentially depends from environment \"activity\"\n(social and prophylactic). Results of our modeling qualitatively meet the\ntuberculosis dynamic spread data in Penza region of Russia.\n","label":0,"model":"human","source":"arxiv","id":3731}
{"text":"  Inflationary models with a superheavy scale F-term hybrid inflation followed\nby an intermediate scale modular inflation are considered. The restrictions on\nthe power spectrum P_R of curvature perturbation and the spectral index n_s\nfrom the recent data within the power-law cosmological model with cold dark\nmatter and a cosmological constant can be met provided that the number of\ne-foldings N_HI* suffered by the pivot scale k_*=0.002\/Mpc during hybrid\ninflation is suitably restricted. The additional e-foldings needed for solving\nthe horizon and flatness problems are generated by modular inflation with a\nstring axion as inflaton. For central values of P_R and n_s, the grand\nunification scale comes out, in the case of standard hybrid inflation, close to\nits supersymmetric value M_GUT=2.86 x 10^16 GeV, the relevant coupling constant\nis relatively large (0.005-0.14), and N_HI* is between 10 and 21.7. In the\nshifted [smooth] hybrid inflation case, the grand unification scale can be\nidentified with M_GUT for N_HI*=21 [N_HI*=18].\n","label":0,"model":"human","source":"arxiv","id":3732}
{"text":"  An extension of the New Standard Model, by introducing a mixing of the low\nmass ``active'' neutrinos with heavy ones, or by any model with lepton flavor\nviolation, is considered. This leads to non-orthogonal neutrino production and\ndetection states and to modifications of neutrino oscillations in both, vacuum\nand matter. The possibility of the discovery of such effects in current and\nfuture neutrino oscillation experiments is discussed. First order approximation\nformulas for the flavor transition probabilities in constant density matter,\nfor all experimentally available channels, are given. Numerical calculations of\nflavor transition probabilities for two sets of New Physics parameters\ndescribing a single ``effective'' heavy neutrino state, both satisfying present\nexperimental constraints, have been performed. Two energy ranges and several\nbaselines, assuming both the current ($\\pm2\\sigma$) and the expected in future\n($\\pm3%$) errors of the neutrino oscillation parameters are considered, keeping\ntheir present central values. It appears that the biggest potential of the\ndiscovery of the possible presence of any New Physics is pronounced in\noscillation channels in which $\\nu_{e}$, $\\nu_{\\bar{e}}$ are not involved at\nall, especially for two baselines, $L=3000 km$ and $L=7500 km$, which for other\nreasons are also called ``magic'' for future $Neutrino Factory$ experiments.\n","label":0,"model":"human","source":"arxiv","id":3733}
{"text":"  An integral PBW-basis of type $A_1^{(1)}$ has been constructed by Zhang [Z]\nand Chen [C] using the Auslander-Reiten quiver of the Kronecker quiver. We\nassociate a geometric order to elements in this basis following an idea of\nLusztig [L1] in the case of finite type. This leads to an algebraic realization\nof a bar-invariant basis of $\\uq2$. For any affine symmetric type, we obtain an\nintegral PBW-basis of the generic composition algebra, by using an algebraic\nconstruction of the integral basis for a tube in [DDX], an embedding of the\nmodule category of the Kronecker quiver into the module category of the tame\nquiver, and a list of the root vectors of indecomposable modules according to\nthe preprojective, regular, and preinjective components of the Auslander-Reiten\nquiver of the tame quiver. When the basis elements are ordered to be compatible\nwith the geometric order given by the dimensions of the orbit varieties and the\nextension varieties, we can show that the transition matrix between the\nPBW-basis and a monomial basis is triangular with diagonal entries equal to 1.\nTherefore we obtain a bar-invariant basis. By a orthogonalization for the\nPBW-basis with the inner product, we finally give an algebraic way to realize\nthe canonical bases of the quantized enveloping algebras of all symmetric\naffine Kac-Moody Lie algebras.\n","label":0,"model":"human","source":"arxiv","id":3734}
{"text":"  We propose a method to reconstruct the vibrational quantum state of molecules\nexcited by a general excitation laser pulse. Unlike existing methods, we do not\nrequire the molecules before excitation to be in a pure state, allowing us to\ntreat the important case of initially thermally excited molecules. Even if only\na single initial level is appreciably populated, initial levels with small\npopulations can still give major contributions to the unknown vibrational\nstate, making it essential to take them into account. In addition to the\nexcitation pulse, the method uses two incident, short laser pulses in a\nnon-co-linear geometry to create four-wave mixing in the molecules. The\nmeasurements used in the reconstruction are spectra of the outgoing four-wave\nmixing pulse at different time delays of the excitation laser pulse. An\nimportant point is that the method does not require detailed knowledge of\nmolecular transition moments between excited states nor of any of the incoming\nlaser pulses, but circumvents this requirement by using one or more calibration\nlaser pulses in a separate experiment either before or after the main data are\nrecorded. The only requirements for the calibration laser pulses are that the\nconstant parts of their spectrums should together cover the spectral range of\nthe excitation laser pulse, and the constant part of each should have\nsufficient spectral overlap with one other calibration pulse to populate two of\nthe same levels. Finally, we discuss the extension of the reconstruction method\nin this paper to more general situations, hereby presenting the new idea of\nquantum state reconstruction through perturbations with calibration.\n","label":0,"model":"human","source":"arxiv","id":3735}
{"text":"  There are clear benefits associated with a particular consumer choice for\nmany current markets. For example, as we consider here, some products might\ncarry environmental or `green' benefits. Some consumers might value these\nbenefits while others do not. However, as evidenced by myriad failed attempts\nof environmental products to maintain even a niche market, such benefits do not\nnecessarily outweigh the extra purchasing cost. The question we pose is, how\ncan such an initially economically-disadvantaged green product evolve to hold\nthe greater share of the market? We present a simple mathematical model for the\ndynamics of product competition in a heterogeneous consumer population. Our\nmodel preassigns a hierarchy to the products, which designates the consumer\nchoice when prices are comparable, while prices are dynamically rescaled to\nreflect increasing returns to scale. Our approach allows us to model many\nscenarios of technology substitution and provides a method for generalizing\nmarket forces. With this model, we begin to forecast irreversible trends\nassociated with consumer dynamics as well as policies that could be made to\ninfluence transitions\n","label":0,"model":"human","source":"arxiv","id":3736}
{"text":"  The MiniBooNE and LSND experiments are compatible with each other when two\nsterile neutrinos are added to the three active ones. In this case there are\neight possible mass orderings. In two of them both sterile neutrinos are\nheavier than the three active ones. In the next two scenarios both sterile\nneutrinos are lighter than the three active ones. The remaining four scenarios\nhave one sterile neutrino heavier and another lighter than the three active\nones. We analyze all scenarios with respect to their predictions for\nmass-related observables. These are the sum of neutrino masses as constrained\nby cosmological observations, the kinematic mass parameter as measurable in the\nKATRIN experiment, and the effective mass governing neutrinoless double beta\ndecay. It is investigated how these non-oscillation probes can distinguish\nbetween the eight scenarios. Six of the eight possible mass orderings predict\npositive signals in the KATRIN and future neutrinoless double beta decay\nexperiments. We also remark on scenarios with three sterile neutrinos. In\naddition we make some comments on the possibility of using decays of high\nenergy astrophysical neutrinos to discriminate between the mass orderings in\npresence of two sterile neutrinos.\n","label":0,"model":"human","source":"arxiv","id":3737}
{"text":"  We show that the Galois group $Gal(\\bar{\\Q} \/\\Q)$ operates faithfully on the\nset of connected components of the moduli spaces of surfaces of general type,\nand also that for each element $\\sigma \\in Gal(\\bar{\\Q} \/\\Q)$ different from\nthe identity and from complex conjugation, there is a surface of general type\nsuch that $X$ and the Galois conjugate variety $X^{\\sigma}$ have nonisomorphic\nfundamental groups. The result was announced by the second author at the\nAlghero Conference 'Topology of algebraic varieties' in september 2006. Before\nthe present paper was actually written, we received a very interesting preprint\nby Robert Easton and Ravi Vakil (\\cite{e-v}), where it is proven, with a\ncompletely different type of examples, that the Galois group $Gal(\\bar{\\Q}\n\/\\Q)$ operates faithfully on the set of irreducible components of the moduli\nspaces of surfaces of general type. We also give other simpler examples of\nsurfaces with nonisomorphic fundamental groups which are Galois conjugate,\nhence have isomorphic algebraic fundamental groups.\n","label":0,"model":"human","source":"arxiv","id":3738}
{"text":"  For points in $d$ real dimensions, we introduce a geometry for general digit\nsets. We introduce a positional number system where the basis for our\nrepresentation is a fixed $d$ by $d$ matrix over $\\bz$. Our starting point is a\ngiven pair $(A, \\mathcal D)$ with the matrix $A$ assumed expansive, and\n$\\mathcal D$ a chosen complete digit set, i.e., in bijective correspondence\nwith the points in $\\bz^d\/A^T\\bz^d$. We give an explicit geometric\nrepresentation and encoding with infinite words in letters from $\\mathcal D$.\nWe show that the attractor $X(A^T,\\mathcal D)$ for an affine Iterated Function\nSystem (IFS) based on $(A,\\mathcal D)$ is a set of fractions for our digital\nrepresentation of points in $\\br^d$. Moreover our positional \"number\nrepresentation\" is spelled out in the form of an explicit IFS-encoding of a\ncompact solenoid $\\sa$ associated with the pair $(A,\\mathcal D)$. The intricate\npart (Theorem \\ref{thenccycl}) is played by the cycles in $\\bz^d$ for the\ninitial $(A,\\mathcal D)$-IFS. Using these cycles we are able to write down\nformulas for the two maps which do the encoding as well as the decoding in our\npositional $\\mathcal D$-representation.\n  We show how some wavelet representations can be realized on the solenoid, and\non symbolic spaces.\n","label":0,"model":"human","source":"arxiv","id":3739}
{"text":"  The influence of three well-known disaccharides, namely trehalose, maltose\nand sucrose, on some structural and dynamical properties of lysozyme has been\ninvestigated by means of molecular dynamics computer simulations in the 37-60\nwt % concentration range. The effects of sugars on the protein conformation are\nfound relatively weak, in agreement with the preferential hydration of\nlysozyme. Conversely, sugars seem to increase significantly the relaxation\ntimes of the protein. These effects are shown to be correlated to the\nfractional solvent accessibilities of lysozyme residues and further support the\nslaving of protein dynamics. Moreover, a significant increase in the relaxation\ntimes of lysozyme, sugars and water molecules is observed within the studied\nconcentration range and may result from the percolation of the hydrogen-bond\nnetwork of sugar molecules. This percolation appears to be of primary\nimportance to explain the influence of sugars on the dynamical properties of\nlysozyme and water.\n","label":0,"model":"human","source":"arxiv","id":3740}
{"text":"  Some statistical properties of finite-time stability exponents in the\nstandard map can be estimated analytically. The mean exponent averaged over the\nentire phase space behaves quite differently from all the other cumulants.\nWhereas the mean carries information about the strength of the interaction, and\nonly indirect information about dynamical correlations, the higher cumulants\ncarry information about dynamical correlations and essentially no information\nabout the interaction strength. In particular, the variance and higher\ncumulants of the exponent are very sensitive to dynamical correlations and\neasily detect the presence of very small islands of regular motion via their\nanomalous time-scalings. The average of the stability matrix' inverse trace is\neven more sensitive to the presence of small islands and has a seemingly\nfractal behavior in the standard map parameter. The usual accelerator modes and\nthe small islands created through double saddle node bifurcations, which come\nhalfway between the positions in interaction strength of the usual accelerator\nmodes, are clearly visible in the variance, whose time scaling is capable of\ndetecting the presence of islands as small as 0.01% of the phase space. We\nstudy these quantities with a local approximation to the trace of the stability\nmatrix which significantly simplifies the numerical calculations as well as\nallows for generalization of these methods to higher dimensions. We also\ndiscuss the nature of this local approximation in some detail.\n","label":0,"model":"human","source":"arxiv","id":3741}
{"text":"  Closed-loop or feedback control ratchets use information about the state of\nthe system to operate with the aim of maximizing the performance of the system.\nIn this paper we investigate the effects of a time delay in the feedback for a\nprotocol that performs an instantaneous maximization of the center-of-mass\nvelocity. For the one and the few particle cases the flux decreases with\nincreasing delay, as an effect of the decorrelation of the present state of the\nsystem with the information that the controller uses, but the delayed\nclosed-loop protocol succeeds to perform better than its open-loop counterpart\nprovided the delays are smaller than the characteristic times of the Brownian\nratchet. For the many particle case, we also show that for small delays the\ncenter-of-mass velocity decreases for increasing delays. However, for large\ndelays we find the surprising result that the presence of the delay can improve\nthe performance of the nondelayed feedback ratchet and the flux can attain the\nmaximum value obtained with the optimal periodic protocol. This phenomenon is\nthe result of the emergence of a dynamical regime where the presence of the\ndelayed feedback stabilizes one quasiperiodic solution or several\n(multistability), which resemble the solutions obtained in the so-called\nthreshold protocol. Our analytical and numerical results point towards the\nfeasibility of an experimental implementation of a feedback controlled ratchet\nthat performs equal or better than its optimal open-loop version.\n","label":0,"model":"human","source":"arxiv","id":3742}
{"text":"  Context. X-ray flares are common phenomena in pre-main sequence stars. Their\nanalysis gives insights into the physics at work in young stellar coronae. The\nOrion Nebula Cluster offers a unique opportunity to study large samples of\nyoung low mass stars. This work is part of the Chandra Orion Ultradeep project\n(COUP), an ~10 day long X-ray observation of the Orion Nebula Cluster (ONC).\nAims. Our main goal is to statistically characterize the flare-like variability\nof 165 low mass (0.1-0.3 M_sun) ONC members in order to test and constrain the\nphysical scenario in which flares explain all the observed emission. Methods.\nWe adopt a maximum likelihood piece-wise representation of the observed X-ray\nlight curves and detect flares by taking into account both the amplitude and\ntime derivative of the count-rate. We then derive the frequency and energy\ndistribution of the flares. Results. The high energy tail of the energy\ndistribution of flares is well described by a power-law with index 2.2. We test\nthe hypothesis that light curves are built entirely by overlapping flares with\na single power law energy distribution. We constrain the parameters of this\nsimple model for every single light curve. The analysis of synthetic light\ncurves obtained from the model indicates a good agreement with the observed\ndata. Comparing low mass stars with stars in the mass interval (0.9-1.2M_sun),\nwe establish that, at ~1 Myr, low mass and solar mass stars of similar X-ray\nluminosity have very similar flare frequencies. Conclusions. Our observational\nresults are consistent with the following model\/scenario: the light curves are\nentirely built by over- lapping flares with a power-law intensity distribution;\nthe intense flares are individually detected, while the weak ones merge and\nform a pseudo-quiescent level, which we indicate as the characteristic level.\n","label":0,"model":"human","source":"arxiv","id":3743}
{"text":"  Initial reaction rate data for lactic dehydrogenase \/ pyruvate, lactic\ndehydrogenase \/ lactate and malic dehydrogenase \/ malate enzyme reactions were\nanalyzed to obtain activation free energy changes of -329, -195 and -221\ncal\/mole, respectively, for rate increases associated with time-specific\nirradiation of the crystalline substrates prior to dissolution and\nincorporation in the reaction solutions. These energies, presumably, correspond\nto conformational or vibrational changes in the reactants or the activated\ncomplex. For the lactic dehydrogenase \/ pyruvate reaction, it is estimated that\non the order of 10% of the irradiation energy (546 nm, 400 footcandles for 5\nseconds) would be required to produce the observed reaction rate increase if a\npresumed photoproduct is consumed stoichiometrically with the pyruvate\nsubstrate. These findings are consistent with the proposition that the observed\nreaction rate enhancement involves photoproducts derived from oscillatory\natmospheric gas reactions at the crystalline enzyme substrate surfaces rather\nthan photo-excitations of the substrate molecules, per se.\n","label":0,"model":"human","source":"arxiv","id":3744}
{"text":"  We report on the results from the observations in very high energy band (VHE,\nE_gamma > 100 GeV) of the black hole X-ray binary (BHXB) Cygnus X-1. The\nobservations were performed with the MAGIC telescope, for a total of 40 hours\nduring 26 nights, spanning the period between June and November 2006. Searches\nfor steady gamma-ray signals yielded no positive result and upper limits to the\nintegral flux ranging between 1 and 2% of the Crab nebula flux, depending on\nthe energy, have been established. We also analyzed each observation night\nindependently, obtaining evidence of gamma-ray signals at the 4.0 standard\ndeviations (sigma) significance level (3.2 sigma after trial correction) for\n154 minutes effective on-time (EOT) on September 24 between 20h58 and 23h41\nUTC, coinciding with an X-ray flare seen by RXTE, Swift and INTEGRAL. A search\nfor faster-varying signals within a night resulted in an excess with a\nsignificance of 4.9 sigma (4.1 sigma after trial correction) for 79 minutes EOT\nbetween 22h17 and 23h41 UTC. The measured excess is compatible with a\npoint-like source at the position of Cygnus X-1, and excludes the nearby radio\nnebula powered by its relativistic jet. The differential energy spectrum is\nwell fitted by an unbroken power-law described by dN\/(dA dt dE) = (2.3+- 0.6) x\n10^{-12} (E\/1 TeV)^{-3.2 +- 0.6}. This is the first experimental evidence of\nVHE emission from a stellar-mass black hole, and therefore from a confirmed\naccreting X-ray binary.\n","label":0,"model":"human","source":"arxiv","id":3745}
{"text":"  We present a detailed study of soliton compression of ultra-short pulses\nbased on phase-mismatched second-harmonic generation (\\textit{i.e.}, the\ncascaded quadratic nonlinearity) in bulk quadratic nonlinear media. The\nsingle-cycle propagation equations in the temporal domain including\nhigher-order nonlinear terms are presented. The balance between the quadratic\n(SHG) and the cubic (Kerr) nonlinearity plays a crucial role: we define an\neffective soliton number -- related to the difference between the SHG and the\nKerr soliton numbers -- and show that it has to be larger than unity for\nsuccessful pulse compression to take place. This requires that the phase\nmismatch be below a critical level, which is high in a material where the\nquadratic nonlinearity dominates over the cubic Kerr nonlinearity. Through\nextensive numerical simulations we find dimensionless scaling laws, expressed\nthrough the effective soliton number, which control the behaviour of the\ncompressed pulses. These laws hold in the stationary regime, in which\ngroup-velocity mismatch effects are small, and they are similar to the ones\nobserved for fiber soliton compressors. The numerical simulations indicate that\nclean compressed pulses below two optical cycles can be achieved in a\n$\\beta$-barium borate crystal at appropriate wavelengths, even for picosecond\ninput pulses.\n","label":0,"model":"human","source":"arxiv","id":3746}
{"text":"  We study the evolution of the Aromatic Infrared Bands (AIBs) emitters across\nthe illuminated edge of the Horsehead nebula and especially their survival and\nproperties in the HII region. We present spectral mapping observations taken\nwith the Infrared Spectrograph (IRS) at wavelengths 5.2-38 microns. A strong\nAIB at 11.3 microns is detected in the HII region, relative to the other AIBs\nat 6.2, 7.7 and 8.6 microns. The intensity of this band appears to be\ncorrelated with the intensity of the [NeII] at 12.8 microns and of Halpha,\nwhich shows that the emitters of the 11.3 microns band are located in the\nionised gas. The survival of PAHs in the HII region could be due to the\nmoderate intensity of the radiation field (G0 about 100) and the lack of\nphotons with energy above about 25eV. The enhancement of the intensity of the\n11.3 microns band in the HII region, relative to the other AIBs can be\nexplained by the presence of neutral PAHs. Our observations highlight a\ntransition region between ionised and neutral PAHs observed with ideal\nconditions in our Galaxy. A scenario where PAHs can survive in HII regions and\nbe significantly neutral could explain the detection of a prominent 11.3\nmicrons band in other Spitzer observations.\n","label":0,"model":"human","source":"arxiv","id":3747}
{"text":"  Gravastars have been recently proposed as potential alternatives to explain\nthe astrophysical phenomenology traditionally associated to black holes,\nraising the question of whether the two objects can be distinguished at all.\nLeaving aside the debate about the processes that would lead to the formation\nof a gravastar and the astronomical evidence in their support, we here address\ntwo basic questions: Is a gravastar stable against generic perturbations? If\nstable, can an observer distinguish it from a black hole of the same mass? To\nanswer these questions we construct a general class of gravastars and determine\nthe conditions they must satisfy in order to exist as equilibrium solutions of\nthe Einstein equations. For such models we perform a systematic stability\nanalysis against axial-perturbations, computing the real and imaginary parts of\nthe eigenfrequencies. Overall, we find that gravastars are stable to axial\nperturbations, but also that their quasi-normal modes differ from those of a\nblack hole of the same mass and thus can be used to discern, beyond dispute, a\ngravastar from a black hole.\n","label":0,"model":"human","source":"arxiv","id":3748}
{"text":"  GRB051022 was undetected to deep limits in early optical observations, but\nprecise astrometry from radio and X-ray showed that it most likely originated\nin a galaxy at z~0.8. We report radio, optical, near infra-red and X-ray\nobservations of GRB051022. Using the available X-ray and radio data, we model\nthe afterglow and calculate the energetics of the afterglow, finding it to be\nan order of magnitude lower than that of the prompt emission. The broad-band\nmodeling also allows us to precisely define various other physical parameters\nand the minimum required amount of extinction, to explain the absence of an\noptical afterglow. Our observations suggest a high extinction, at least 2.3\nmagnitudes in the infrared (J) and at least 5.4 magnitudes in the optical (U)\nin the host-galaxy restframe. Such high extinctions are unusual for GRBs, and\nlikely indicate a geometry where our line of sight to the burst passes through\na dusty region in the host that is not directly co-located with the burst\nitself.\n","label":0,"model":"human","source":"arxiv","id":3749}
{"text":"  We introduce a novel method for calculating the size of the critical nucleus\nand the value of the surface tension in systems with first order phase\ntransition. The method is based on classical nucleation theory, and it consists\nin studying the thermodynamics of a sphere of given radius embedded in a frozen\nmetastable surrounding. The frozen configuration creates a pinning field on the\nsurface of the free sphere. The pinning field forces the sphere to stay in the\nmetastable phase as long as its size is smaller than the critical nucleus. We\ntest our method in two first-order systems, both on a two-dimensional lattice:\na system where the parameter tuning the transition is the magnetic field, and a\nsecond system where the tuning parameter is the temperature. In both cases the\nresults are satisfying. Unlike previous techniques, our method does not require\nan infinite volume limit to compute the surface tension, and it therefore gives\nreliable estimates even by using relatively small systems. However, our method\ncannot be used at, or close to, the critical point, i.e. at coexistence, where\nthe critical nucleus becomes infinitely large.\n","label":0,"model":"human","source":"arxiv","id":3750}
{"text":"  The dynamical properties of a dense horizontally vibrated bidisperse granular\nmonolayer are experimentally investigated. The quench protocol produces states\nwith a frozen structure of the assembly, but the remaining degrees of freedom\nassociated with contact dynamics control the appearance of macroscopic\nrigidity. We provide decisive experimental evidence that this transition is a\ncritical phenomenon, with increasingly collective and heterogeneous\nrearrangements occurring at length scales much smaller than the grains'\ndiameter, presumably reflecting the contact force network fluctuations.\nDynamical correlation time and length scales soar on both sides of the\ntransition, as the volume fraction varies over a remarkably tiny range ($\\delta\n\\phi\/\\phi \\sim 10^{-3}$). We characterize the motion of individual grains,\nwhich becomes super-diffusive at the jamming transition $\\phi_J$, signaling\nlong-ranged temporal correlations. Correspondingly, the system exhibits\nlong-ranged four-point dynamical correlations in space that obey critical\nscaling at the transition density.\n","label":0,"model":"human","source":"arxiv","id":3751}
{"text":"The authors propose an interesting idea which seems promising but lacks experimental validation.  The proposed method should be compared against other methods such as Adam (Kingma & Ba 2014) and RMSprop (Tieleman & Hinton 2012). It would also help if they provide some intuition about why their approach works better than standard SGD. \n\nIn addition, it might make sense to compare different variants of their proposal - e.g., one could try replacing the outer-loop momentum term with Nesterov's accelerated gradient technique.  \n\nFinally, I think it's important to mention how much more computationally expensive your approach will likely be since you need to perform additional computations per iteration. Overall, I'm looking forward to seeing future work from these researchers! \nI am giving 3 stars","label":1,"model":"bloomz","source":"peerread","id":4238}
{"text":"The authors propose an interesting idea which seems promising but lacks some details about how it works exactly.  The proposed method looks like it's trying to find flatter areas instead of peaks so I wonder if there are any connections between their approach and other methods such as AdaGrad? Also, they mention that the bias towards flatter areas helps avoid poor generalizations at the bottom of steep valleys - does this mean that these bad generalizations can be found anywhere else besides valley bottoms?  Finally, I'm not sure why the authors chose to call their method \"Entropy\"-SGD since there's no explicit connection made here between entropy (a measure) and the actual loss functions being optimized... \nOverall, I'd say this work could benefit from more explanation\/clarification regarding its core ideas","label":1,"model":"bloomz","source":"peerread","id":4239}
{"text":"The authors present an interesting approach to multi-modal learning, which is based on tensor factorization and deep neural networks.\nThey show that their method outperforms state-of-the-art approaches in several tasks such as image classification or video captioning.  The proposed model can be trained end-to-end using stochastic gradient descent with backpropagation through time (BPTT). This makes it possible to train very large models efficiently even though they are not differentiable wrt all parameters at once. \nI think this work has great potential but I have some concerns about how well the results generalize beyond the training data set. \n\nIn particular, there seems to be no explicit regularizer preventing overfitting of the learned representations onto specifics of the training dataset. It would also help if more details were provided regarding hyperparameters used during experiments.  \n\nOverall, it's good research!","label":1,"model":"bloomz","source":"peerread","id":4240}
{"text":"The main contribution of this work is to propose an end-to-end deep multi-task representation learning framework for tensor factorization problems.\nIn particular, it proposes two novel loss functions that can be used in combination with any existing neural network architectures (e.g., CNNs) as well as other state-of-the-art methods such as GANs.  The proposed method has been evaluated on several benchmark datasets including MNIST, CIFAR-10\/100, SVHN, STL-10 etc.. In general, experimental results show that our approach outperforms all competing baselines significantly across different tasks. \nOverall, I think it's very interesting research which makes important contributions towards solving real-world applications using deep multi-task representations. \n\nStrengths: \n1. It provides new insights into how we should design more effective models for multi-view data analysis; 2. It presents some useful ideas about designing better losses for training DNNs; 3. It shows promising performance compared against many strong baseline algorithms","label":1,"model":"bloomz","source":"peerread","id":4241}
{"text":"The authors have done an excellent job with their research work.  The results are very promising.\nI would like to see more details about how they deal with overfitting issues when training large scale neural nets (e.g., VGG-16). I think it will be helpful if you can provide some examples showing what happens during your experiments as well. Overall, it's great! Keep up the good work! \nYes  Yes  No  Yes  Yes  Yes  Yes   Yes    Yes  Yes  Yes  No  Yes   Yes     Yes  Yes  Yes  None of these comments apply. Yes  Yes  Yes  False  True  False  False  False  False False False False False False False True False False False False","label":1,"model":"bloomz","source":"peerread","id":4242}
{"text":"The authors propose an interesting idea for automatically discovering task relatedness across layers using tensor decomposition.  The proposed method can be applied to any type of neural network architecture (e.g., CNN) as long as it has multiple output tasks. \nOne drawback with their current implementation is that they only consider one hidden layer between input\/output pairs which may limit its effectiveness when dealing with complex problems such as image classification where there are several levels of abstraction involved. \n\nAnother issue I have noticed while reading through the manuscript is how do you deal with data imbalance? For example if your dataset contains more samples from class 1 than 2 but you want to train all classes equally well.  \n\nOverall it's good work! \nStrengths: \n\nWeaknesses: \n\nI would like to see some additional experiments conducted to address my concerns above before recommending publication","label":1,"model":"bloomz","source":"peerread","id":4243}
{"text":"The authors propose an ensemble method to combine multiple semi-supervised learning algorithms in order to improve their performance on unlabeled data sets.\nThey show that this approach can be used with different types of models, including deep neural networks and tree-based methods such as random forests or gradient boosting machines. \nIn addition they provide some insights into how these approaches work by visualizing what happens during training time using t-SNE plots.  The results are very promising but I think it would have been interesting if more details about hyperparameters tuning were provided (e.g., number of epochs). Overall it's a nice contribution which could benefit from additional experiments. \n\nI recommend publication after minor revisions. Authors should also consider adding references related to temporal ensembling techniques applied to other fields than computer vision","label":1,"model":"bloomz","source":"peerread","id":4244}
{"text":"The authors propose an ensemble method to combine multiple semi-supervised learning algorithms in order to improve their performance on unlabeled data.\nThis is very interesting work that has not been done before.  The proposed algorithm can be used as part of any semisupervised learning system where there are many different ways to learn from labeled examples (e.g., deep neural networks) but only one way to use unlabeled ones. \nI think this article will have significant impact if it works well because most real-world applications involve some amount of unsupervised training which could benefit greatly from using temporal ensembling. \n\nStrengths: \n1. This idea seems novel enough so I don't see why anyone would've thought about doing something like this already.  \n\n2. It looks promising based off my cursory reading","label":1,"model":"bloomz","source":"peerread","id":4245}
{"text":"The authors propose an interesting approach that combines several models trained over time into one final model which performs well even when there are few or no labelled examples available.\nI think it would make sense if they also compared their results against other state-of-the-art methods such as self-paced learning since these approaches have been shown to perform very similarly but may require less hyperparameter tuning effort.  The proposed method seems like it's easy enough to implement so I don't see why not? It might help them get more citations too! \nOverall I'm positive about the work presented here although some minor improvements could still be made. Good job! Positive Reviews. Yes  No  Yes  Yes  Yes  Yes   Yes    Yes  Yes  Yes  No  Yes  No","label":1,"model":"bloomz","source":"peerread","id":4246}
{"text":"The authors propose an interesting approach that combines several ideas into one model which they call temporal ensembling.  The idea behind their proposal seems sound but it would have been nice if there was more discussion about how exactly these methods are combined together - e.g., whether all models use the same architecture? How many models do you train simultaneously?  What happens when some of them disagree?\nOverall I think it's a very promising direction so I'm happy to recommend publication after minor revisions. In particular I'd like to:  1)  Explain why your proposed solution works 2)  Showcase additional experiments 3)  Improve clarity 4)  Add references Strengths: \n1) Simple yet effective algorithm. \n2) State-of-the-art results across multiple datasets. \n\n3) Good explanation of the underlying concepts","label":1,"model":"bloomz","source":"peerread","id":4247}
{"text":"The authors propose to use an additional noise adaptation layer in order to improve training of DNN's on noisy data.\nThey show that this approach improves performance over standard approaches, especially when there is limited amount of clean data available and it also helps with generalization across different types of noises. \nI think it's interesting work but I have some questions about how they evaluate their method. \n\nFirst question - How do you measure improvement? Do you compare against other methods or just report accuracy numbers?\nSecond question - What are your results if you don't add any noise adaptation layers at all (i.e., only train from scratch)? This would be useful as baseline comparison since adding more parameters can sometimes hurt performance instead of improving it. Third question - Are these improvements statistically significant?  You should perform statistical tests such as t-test between each pair of models you're comparing","label":1,"model":"bloomz","source":"peerread","id":4248}
{"text":"The authors propose to use an additional noise adaptation layer in order to improve training of Deep Neural Networks (DNN). The proposed method is evaluated on several datasets for image classification tasks.\nStrengths: \n1) It improves performance over state-of-the-art methods. \n2) Noise Adaptation Layer can be used with any DNN architecture. \n\n3) Training time increases only slightly compared to standard backpropagation algorithms. \n\n\nWeaknesses: \n1) Does not provide theoretical analysis about why it works better than other approaches.  \n\n2) How does this approach compare against more recent techniques such as Batch Normalization? This work proposes adding another layer between input data and hidden layers during network training that adapts its parameters based on the statistics of the output from previous layers. In particular, they show how their technique outperforms existing ones when applied to various benchmark problems including MNIST handwritten digits recognition task.","label":1,"model":"bloomz","source":"peerread","id":4249}
{"text":"This work presents a new method to deal with noisy data when training DNN's (Deep Neural Networks). It uses the Expectation-Maximization (EM) algorithm which iteratively estimates the true labels from their corrupted counterparts. \n \n I think it would have been interesting if they had compared against other state-of-the-art approaches such as: \n\n1 - Label Smoothing \n2 - Dropout Regularisation \n \n \n \n Overall though it's still very good research! Congratulations! :) Keep up the great job! :) Good luck! :) Positive feedback. Very nice work!!!!! You should submit your manuscript soon!!!! \n\n\nBest wishes, \n\n\n\nSincerely yours,\nDr. Yi-Chun Chen Department of Electrical Engineering National Taiwan University","label":1,"model":"bloomz","source":"peerread","id":4250}
{"text":"This article presents a new method for training DNNs when there exists incorrect information about their output classes (i.e., noisy labels). It models these errors via a \"communication channel\" between true and false outputs which allows it to use Expectation-Maximization algorithms to optimize its objective functions. \n \n I think it's interesting how they introduce dependence among the \"noise variables\"; i.e., if one instance gets mislabeled due to some feature value being wrongly labeled too, other instances sharing those values may get similarly affected. They show experimentally that doing so improves accuracy over not modeling such dependencies at all; however, I'm curious whether there's any theoretical justification behind why this should work better? \n \n \n \n Overall, I'd say this is a very nice contribution!","label":1,"model":"bloomz","source":"peerread","id":4251}
{"text":"The authors present an interesting approach that combines reinforcement learning and neural networks in order to teach computers how to compose sentences from individual words.  The results are promising, but there is room for improvement.\nFirst of all, it would be helpful if they could provide more details on their training procedure (e.g., what was used as input data?).  Second, I think it's important to note whether or not this method can scale up beyond toy examples like those shown here.   Finally,  I'd encourage them to try other types of sentence composition tasks such as paraphrasing instead of just generating new sentences based off existing ones. Overall though, I'm excited about these findings! \nI look forward to seeing future work by the authors. Positive Reviews. This research has potential applications across many fields including natural language processing","label":1,"model":"bloomz","source":"peerread","id":4252}
{"text":"The authors present an interesting approach for learning how to compose sentences from text using reinforcement learning (RL). The main idea is that RL agents learn sentence composition in two steps: First they are trained on individual word embeddings of context-free phrases such as \"the cat eats\" which can be extracted directly from unstructured data sources like Wikipedia articles.\nSecond these phrase-level models are used together with a language model based on n-grams to generate full sentences. This way it becomes possible to train the system end-to-end without any manual annotation effort. \nI think this work has several advantages over previous approaches. \n\nFirst, unlike most other works where only one agent was learned per task there seems no limit here regarding number of tasks\/agents you could add at once - so if needed more complex compositions than just single-word sequences might also become feasible.  \n\nSecondly I find their use of pre-trained word embedding very promising since it allows them to avoid expensive training steps when creating new datasets while still being able to produce reasonable results. \n \n However, some questions remain open about the proposed method itself: \n\n1) How do we know whether our generated sentences actually make sense?","label":1,"model":"bloomz","source":"peerread","id":4253}
{"text":"This is an interesting approach but I am not sure how much it contributes beyond what has been done before.  The authors do mention their results differ somewhat from standard syntax analysis techniques so perhaps there will be more interest if these differences can be explained better.\nThe experiments seem well designed although I'm not convinced by all the details presented here - e.g., why was one experiment run only once? It would also help to have a comparison between training times since it's clear that the proposed method takes longer due to the need to train multiple compositions at each step rather than just one sequence model. \nOverall though, I'd say there's nothing really new about this research except maybe the idea of optimizing sentence structure instead of relying solely on hand-coded rules\/annotations. This may make sense when you consider that humans don't always follow strict grammar conventions anyway!","label":1,"model":"bloomz","source":"peerread","id":4254}
{"text":"The authors present an interesting approach towards sentence representation via RL-based training of Tree-Structured Neural Networks.  The proposed method is shown to be effective compared against other state-of-the-art methods including Recursive Encoder Models trained over Treebanks.   However,  I have several questions regarding how their model works.    First,    it would help if you could provide more details about your experimental setup such as:    - How many epochs were used?   - What was the batch size?  - Which optimizer did you use?\nSecond,   please clarify whether you performed any hyperparameter tuning during experimentation.   Thirdly,    can you explain why you chose to train only one layer at each level instead of multiple layers? Fourthly,    do you think there will be much difference between having 1-layer vs 2-layers per node when applying these learned compositions onto new data sets? Finally,    since you're not providing any information related to computational complexity\/time consumption issues associated with your algorithm implementation,...","label":1,"model":"bloomz","source":"peerread","id":4255}
{"text":"The authors present an interesting survey of adversarial examples, which are inputs to machine learning algorithms that cause them to produce incorrect outputs.  They also discuss transferability between different models trained on similar data sets.\nI think this is useful reading if you want to understand how these attacks work or what they can do in practice.   The book does not go as far as discussing defenses against such attacks but it provides enough information so I would recommend it over other books I've read about the topic (e.g., \"Adversarial Machine Learning\" by Moosavi-Dezfooli et al.) . \nIf you're interested in understanding more advanced topics like black-box attacks then you'll probably need another resource though.    This book covers some basic concepts well without getting too technical.  \n\nOverall I'd give 4 stars","label":1,"model":"bloomz","source":"peerread","id":4256}
{"text":"The authors present an interesting approach to transfer adversarial examples from one model to another in order to improve black box attacks on deep learning models.\nStrengths: The main contribution of this work is that it presents a novel way for generating adversaries which are more robust against defensive distillation techniques than previous approaches.  This makes their attack much harder to defend against as defenders will have less time to react before being attacked again with new types of adversaries generated using TAE. \nWeaknesses: It would be good if they could provide some experimental results showing how well these methods perform compared to other state-of-the-art defenses such as those proposed by Papernot et al. (2016) and Samanta & Mehta (2017). In addition, there seems to be no discussion about whether their method can also generate adversaries when only part of the training data has been seen during the initial phase. \n\nOverall, I think it's worth publishing but perhaps after minor revisions","label":1,"model":"bloomz","source":"peerread","id":4257}
{"text":"The authors have done good research on transferable adversarial attacks against DNNs in general as well as specifically targeting Clarifai's model.  The results presented here will be useful not only from academic point-of-view but also practical perspective since they provide insights about how one should defend themselves when facing these types of attacks.\nI would like to see more details regarding the proposed approach (e.g., what does it mean by \"ensemble\"?).  Also I think there could be better explanation why the proposed method performs so much worse than other methods at attacking Clarifai compared to MNIST or CIFAR-10 data sets. Overall though it's a very interesting piece of work! \nRecommendation: 4 stars  - Very nice job! \n\nAuthor Response","label":1,"model":"bloomz","source":"peerread","id":4258}
{"text":"The authors have done very good job in conducting experiments on large-scale data sets as well as proposing new methods to generate transferable adversarial examples. The results presented here will be useful not only for researchers working on machine learning but also those who use DNNs based systems like Clarifai.com. \n \n Strengths: \n1) Extensive experimental evaluation. \n2) Proposing new ways to generate transferable adversarial examples: Ensemble-based approach (EBE), Targeted EBE(T-EBE).\n3) Studying the properties of TAEEs from geometrical perspective.\n4) Successfully attacking Clarifai.com through proposed attacks Weaknesses: \nNo discussion about how these attacks could be defended against? Future work: \nHow do you defend your model against these attacks?","label":1,"model":"bloomz","source":"peerread","id":4259}
{"text":"The author of this article is an expert on identity and deep learning, which makes it interesting to read his views about how these two fields are related.  The main idea that he presents here is that there should be more research into what kind of identities we can build with machine-learning algorithms (e.g., neural networks).  He also discusses some possible applications where such identities could play important roles.\nI think it's very useful to have someone who has both academic backgrounds present their ideas like this because they provide us readers with new insights from different perspectives.   I would recommend reading this article if you want to learn something about how artificial intelligence works or just enjoy thinking deeply about human behavior! \nReviewer: Yi-Chun Chen, University at Buffalo Reviewed by: David Hsu","label":1,"model":"bloomz","source":"peerread","id":4260}
{"text":"The authors of this article are interested in how deep learning algorithms can be used to identify objects within images.  They argue that these systems should not only focus on accuracy but also take into account social identity issues such as race\/ethnicity.\nThey present an algorithm called Identity-Aware Convolutional Networks (IAC-Nets) which they claim is able to learn features from data sets with different racial compositions without biasing its predictions towards one particular group. \nStrengths: \nThis research has potential applications for many areas including medical diagnosis where it could help reduce health disparities between ethnic groups. \n\nWeaknesses: \n\nIt would have been helpful if there was some discussion about why IAC-Net outperforms other methods like Adversarial Training when applied to their datasets.  \n\nIn addition, while the results presented here show promise, more work needs to done before we know whether IAC-Net will perform well across all types of image recognition tasks","label":1,"model":"bloomz","source":"peerread","id":4261}
{"text":"The authors present an interesting new approach to understanding how to build better models using deeper architectures.  They provide some very nice results along these lines including showing that there are no spurious local minima when training residual nets (which I think most people would agree makes them easier to train) and proving universality properties which may help explain why they perform so well.\nI am curious about whether it might make sense to try out other activation functions besides relu's since they seem like something you could get stuck at if your net gets too large? Also, what happens without batch norm?  It seems like it's hard to imagine getting good performance from just residuals alone... Overall though, great job! \nThis article will definitely interest researchers working in machine learning who want to understand what's going on under-the-hood","label":1,"model":"bloomz","source":"peerread","id":4262}
{"text":"The authors present an interesting topic which they explore thoroughly throughout the article.  They provide both mathematical proofs and experimental results supporting their claims.   I think it would benefit from some additional references at times (e.g., page 4) so readers know where these ideas came from originally.    Overall though it's well written and easy to follow. It provides new insights into how to build better models using deeper architectures while still being computationally efficient. Strongest points:  Well-written and clear presentation of research findings. Provides plenty of examples\/experiments along with explanations. Weaknesses:   Some minor typos here and there could use fixing. For example,  \"...the number of samples\" instead of \"the number of parameters\" when discussing universality. (page 5).","label":1,"model":"bloomz","source":"peerread","id":4263}
{"text":"The authors propose an adversarially learned feature representation that is robust to label noise and outliers in data sets with limited training samples.\nThey show how this method can be used as part of a deep learning pipeline, where it improves performance on several benchmark datasets.  The proposed approach has some interesting connections to other recent work such as Dropout (Srivastava et al., 2014) and Batch Normalization (Ioffe & Szegedy,  2015). I think it's worthwhile to mention these works when discussing related literature. \nOverall, I'm very positive about this manuscript because it presents novel ideas which are relevant both theoretically and practically. \n\nHowever there were two minor issues I'd like to point out: \n1)  In Figure 1 , the caption should say \"(a),\" not \"a\"","label":1,"model":"bloomz","source":"peerread","id":4264}
{"text":"The authors propose an adversarial feature learning method for unsupervised domain adaptation.\nThey show that their approach outperforms state-of-the-art methods on several benchmark datasets in terms of accuracy. \nStrengths: \n1) The proposed model is simple yet effective. \n\n2) It can be easily implemented using existing deep neural network frameworks such as TensorFlow. \n \n3) They provide detailed experiments to demonstrate its effectiveness compared with other approaches.  \n\nWeaknesses: \n1) There are no theoretical analysis about why this works well.   \n\n2) No discussion about how it performs when there exists large discrepancy between source data distribution and target data distribution (e.g., different modalities). \n\nIn summary, we find this work very interesting because they have developed a new way to learn features from unlabeled data which could potentially benefit many applications including computer vision, speech recognition etc..","label":1,"model":"bloomz","source":"peerread","id":4265}
{"text":"This work presents an interesting idea on how adversarially trained networks can be used to generate features which could then be applied to other classification or regression related tasks.  The authors present some promising experimental evidence supporting these claims but I feel there needs to be more discussion about why BiGANs should perform better than standard GANs when it comes to generating good quality features.   It would also help if they provided further details regarding what exactly was being generated by each network during training - were images produced? text?  This information will allow readers who aren't familiar with GANs frameworks to understand what's going on under the hood.  \n\nOverall though it's a very well written article and I'm looking forward to seeing future research based off of this! 4 stars  \u2605\u2605\u2605\u2605","label":1,"model":"bloomz","source":"peerread","id":4266}
{"text":"This article presents an interesting idea on how we can use adversarially trained networks to generate features which could be used later when solving other classification related problems.  The authors present some experimental evidence supporting their claims but I think they should provide more details about these experiments so it would make sense why BiGANs perform better than competing methods. \nI also feel like there needs to be additional discussion regarding the computational complexity involved since training BiGANs involves optimizing two different objective functions at once. \n\nOverall though it's still worth reading because its novelty makes me believe there's potential here! Strengths: \nWeaknesses: \n\nRecommendation: Yes  - if you want to read further check out my comments below. No  - otherwise. Yes  - if you want","label":1,"model":"bloomz","source":"peerread","id":4267}
{"text":"The authors have presented an interesting approach to network quantization, which is one of the most important problems in wireless communications and networking.\nIn this work they propose a novel scheme that can achieve higher data rates than existing schemes by exploiting both spatial diversity gain as well as multiuser interference cancellation gains.  The proposed algorithm also has low complexity compared with other algorithms such as V-BLAST or MIMO-MRC because it does not require any matrix inversion operation at each receiver node unlike those approaches do. \nI think their results are very promising but I would like them to consider some issues before publication. \n\n1) They should provide more simulation results on how much performance improvement over conventional methods (e.g., V-BLAST\/MIMO-MRC)  they get under different channel conditions including fading channels","label":1,"model":"bloomz","source":"peerread","id":4268}
{"text":"The authors present an interesting approach to network quantization in this work.  The main idea is that they use multiple layers for different types of data (e.g., audio vs video) which are transmitted over separate channels.\nThis allows them to achieve higher compression rates than previous approaches while maintaining low distortion levels on all streams simultaneously. \nI think it would be useful if there were some more discussion about how their scheme compares with other schemes such as MPEG-2\/4 AVC\/H264 since these have been widely deployed already. \n\nStrengths: \n1. They provide experimental results showing good performance compared against existing techniques.  \n\n2. Their technique can potentially support many applications at once without requiring any changes to either client devices nor servers.  \n3. It has potential advantages when used together with emerging technologies like Software Defined Networks Weaknesses:","label":1,"model":"bloomz","source":"peerread","id":4269}
{"text":"The authors have done some interesting work on designing network quantization algorithms with theoretical analysis. The results seem promising but there still exist several issues which need more investigation.\n1)\nIn Section 3.2, it seems not necessary to use the term \"quantized\" when talking about the output of the cluster centers obtained through K-Means Clustering since they already represent discrete numbers after being clustered into different groups. \n2)\nIt would also help if you could provide examples showing how your method works compared against other methods such as BinaryConnect or XNOR-Network. \n\n\n3)\nFinally,  I think it's important to mention what happens during training? Does the model learns new clusters automatically?  How does the learning process look like? \n\n4) \nI am wondering why do you choose to compress only fully-connected layers instead of all layers including convolutional ones?\n\n5)  What kind of hardware will benefit most from these kinds of approaches?","label":1,"model":"bloomz","source":"peerread","id":4270}
{"text":"The authors present their work on designing network quantization scheme with minimum performance loss under certain constraints such as bit budget etc.. The idea behind it seems quite reasonable but I am not sure whether they have done enough experimentation to support their claims.  For example,  how do you know which bits should go into each cluster? How many clusters does your method produce?  What happens if there isn't any data point within some distance of another? These questions need more investigation before claiming superiority over other methods like VGGnet.   Also  why don't you compare against state-of-the-art results obtained recently at Google Brain?\nI think it's also important to mention here about the computational complexity involved when applying these algorithms since most of the time people will use GPUs instead of CPUs so speed becomes very crucial factor especially during training phase. \nOverall though I'm happy to see new ideas being presented","label":1,"model":"bloomz","source":"peerread","id":4271}
{"text":"The authors present an interesting approach to speed up convolutional neural networks (CNN) by using direct sparse convolution, which is similar in spirit but different from group sparsity used previously.  The main idea of this method is that it allows one to use fewer parameters than traditional methods while still maintaining accuracy.\nIn addition they introduce guided pruning as another way to reduce model size without sacrificing performance. \nI think their results are very promising especially since there has been little work done on reducing computational complexity of deep learning models so far. \n\nHowever I have some questions about how exactly these two techniques interact together.  \n\nFirst off what does \"guided\" mean? Doesn't say here... \nSecond why do you need both approaches at all?  Why not just go straight into direct sparse convolution?\n\nThirdly if we want to keep our network small shouldn't we be able to get rid of most layers anyway? \n\nFinally what's going on when training your network? How many epochs did you train each time before doing any pruning or fine-tuning?","label":1,"model":"bloomz","source":"peerread","id":4272}
{"text":"The authors propose to use direct sparse convolutional layers in combination with guided pruning for faster training of deep neural networks.\nThey show that their approach is able to train deeper models than previous methods while maintaining accuracy on ImageNet classification task. \nThis work has several interesting aspects. \n\nFirst, it shows how sparsity can be used as an additional regularizer during network optimization which leads to better generalization performance.  Second, they introduce new techniques such as weight sharing between different filters within one layer (which reduces number of parameters) and batch normalization applied directly after convolution operation instead of before activation function.  \nThirdly, this method allows us to build very efficient model architectures without sacrificing too much accuracy.  \n\nHowever there are some issues I would like to point out.    First,  although the proposed architecture achieves state-of-the-art results  its computational complexity remains high compared to other approaches.   Secondly,   the authors do not provide any information about hardware resources required to run these experiments so it's hard to judge whether the speedup achieved over baseline Resnet-50 implementation will translate into real-world applications.     Third,    the authors report only single value of top-1 error rate obtained from validation set but don't mention anything about standard deviation across multiple runs.    Finally,    the authors should try to explain why their technique works well when combined together rather than separately","label":1,"model":"bloomz","source":"peerread","id":4273}
{"text":"The authors propose direct sparse convolutions which can be applied directly after training without any additional fine-tuning or retraining steps.  They also introduce guided pruning where they use their new algorithm to guide how much each layer should be pruned based off of predicted execution time savings.\nThey show significant improvements across several hardware platforms including CPUs and GPUs using both synthetic benchmarks and real-world applications such as image classification tasks.   This work has broad applicability since many deep learning frameworks already support standard convolutions so only minor changes would need to be made before applying their proposed approach. I think it's very interesting because there hasn't really been anything like this published yet.  \n\nI am impressed by all aspects of this research but especially the fact that they were able to achieve similar accuracy results compared to regular convolutions when running at higher speeds. \n\nThis article will definitely get my vote!","label":1,"model":"bloomz","source":"peerread","id":4274}
{"text":"The authors propose direct sparse convolutions which can be applied directly during training without any additional overhead compared to standard convolutions.  They also introduce guided pruning where they use their new approach to guide how much should each layer be pruned based on its importance.   This work shows significant improvements both in terms of reducing network sizes but more importantly improving run times when using state-of-the-art hardware such as GPUs and CPU clusters.    In addition, the proposed approaches do not require special structure within the weights making them very flexible and easy to implement across many existing models. Overall I think it's a great contribution! Strengths:  1)  Very well written 2)  Extensive experiments showing results on multiple platforms Weaknesses:   No discussion about why there isn't better support for sparse matrices","label":1,"model":"bloomz","source":"peerread","id":4275}
{"text":"The authors present an interesting approach to variational autoencoder (VAE) training, which is based on breaking sticks in order to encourage exploration of latent space and improve sample quality.\nI think that this work could be improved by providing more details about how stick-breaking works exactly.  For example, what are the exact probabilities used? How does it affect the VAE objective function?  What happens if you break multiple sticks at once?\nIn addition I would like to see some experiments with different numbers of sticks broken per step or even varying number of steps taken between samples from the prior distribution. Finally, there should also be comparisons against other state-of-the-art methods such as SNAIL. Overall though it's still very promising research! \nGood luck! Yes","label":1,"model":"bloomz","source":"peerread","id":4276}
{"text":"The authors propose an interesting approach to variational autoencoder (VAE) training that is based on breaking sticks into pieces in order to learn latent representations of data.\nIn particular they show how this can be used for unsupervised learning from images as well as text documents. \nI think their work has several important contributions including: \n\n1.  They provide theoretical insights about why stick-breaking VAEs are able to perform better than standard VAE models. \n\n2.   Their method allows them to train very deep networks with many layers without suffering from vanishing gradients.  \n\n3.   The proposed model also provides interpretable results which makes it easier to understand what's going on inside the network during training time.    I would like to see more experiments done using different datasets such as: MNIST digits,  CIFAR-10\/100  and ImageNet   so we could get some idea if these methods generalize across tasks\/datasets","label":1,"model":"bloomz","source":"peerread","id":4277}
{"text":"The authors present an interesting extension on their previous work in this article.  The results are promising but I have some concerns about how they were obtained.  \n\nFirst off,  it is not clear what kind of data was used or whether any preprocessing steps where performed before training.   It would be helpful if these details could be included.    Second,   there seems like only one model was trained per experiment so  no comparison between different models can really be made.     Finally,    while the experiments show good performance   it does seem as though the number of samples available may limit the conclusions we can draw from them. \n\nOverall    however     ,the manuscript presents new ideas which should interest readers interested in VAEs and stick breaking processes. Yes \n\nYes\n\n\n\nNo \nYes \n \n \n \n No","label":1,"model":"bloomz","source":"peerread","id":4278}
{"text":"The authors present an interesting extension on their previous work in this article.  The idea is very novel but it would be good if they can provide more details about how exactly stick breaking process works as well as its connection to VAEs. Also I think there are some typos here and there which should also be fixed before publication. Overall it's still a great contribution! \nStrengths: \n\nIt presents new ideas. \n\nThere may exist typo errors.  \n\nWeaknesses: \n\nMore information needed regarding stick-breaking process. Typo error correction required. Yes  No  Yes  Yes  Yes  Yes   Yes    Yes     Yes      Yes        Yes       Yes         Yes          Yes             Yes              Yes               Yes                 Yes                Yes                   Yes                  Yes                      Yes","label":1,"model":"bloomz","source":"peerread","id":4279}
{"text":"The authors have done an excellent job in this work, which is very relevant to my research interests.  I would like to see more discussion on how their method compares with other state-of-the-art methods such as dictionary learning (e.g., K-SVD) or sparse coding using overcomplete dictionaries learned from data sets of natural images.\nI also think that it will be interesting if they can show some results where there are multiple classes present simultaneously so we could better understand what happens when one class has many similar samples compared to another class having only few examples available. \nOverall, it's great work! \nThanks,  Prof. David Hadsell  Imperial College London UK \n\nProf. David Hadsell's comments about Support Regularized Sparse coding: Excellent","label":1,"model":"bloomz","source":"peerread","id":4280}
{"text":"The authors present an algorithm for sparse coding that is based on regularization theory.\nThey show how to use this method in combination with fast encoding algorithms such as OMP (orthogonal matching pursuit) and SPAMS (sparsity promoting adaptive matching pursuit).\nThis work has several advantages over previous approaches including faster training time and better generalization performance.  The results are very promising but there still remains some room of improvement especially when it comes to computational efficiency. \nStrengths: \n1. Better generalization performance than other methods 2. Faster training 3. More efficient computation Weaknesses: \n1. Computational complexity could be reduced further  In summary,  we believe that this research will have significant impact both theoretically and practically if these issues can be addressed properly. This article presents a novel approach towards solving the sparsely coded signal reconstruction problems using regularization techniques.","label":1,"model":"bloomz","source":"peerread","id":4281}
{"text":"The authors have done an excellent job on proposing new methods based on sparse representation. The proposed method can be applied into many fields such as computer vision or speech recognition where there are large amounts of noisy data points lying close together along some manifolds. \n \n I think it would make sense if they could provide more details about how their algorithms work when dealing with high dimensional datasets like images etc.. \nI am looking forward to seeing future works from them! Yes  - we will try our best to do so. Thank you very much again for your comments. No doubt these suggestions will help us improve our research further. Yes  - thank you very much indeed. Yes  - yes definitely. Yes  - no problem at all. Yes  - thanks once again","label":1,"model":"bloomz","source":"peerread","id":4282}
{"text":"The authors have done an excellent job on proposing new methods based on sparse representation. The proposed method can be applied into many fields such as computer vision, speech recognition etc., where it has been shown very promising. \n \n This work makes several contributions including 1). A novel approach called \"Support Regularized Sparse Code\"(SRSC), 2). An efficient deep architecture named \"Deep Supported Regularized Sparse Code\"encoder 3). Several experiments are conducted to showcase its superiority compared to state-of-the-art approaches \n \n Strengths: \n1). It provides better generalization ability than other existing algorithms. \n2). It shows great potential in real-world applications due to its efficiency and accuracy. \n\nWeaknesses: \nNo obvious drawbacks were found from our reading","label":1,"model":"bloomz","source":"peerread","id":4283}
{"text":"The authors present an interesting approach to language modeling by combining word vectors with classifiers, which is shown in experiments on several datasets.\nI think that this work could be improved if it were more clear how exactly the loss function works (e.g., what are the parameters of the model?). Also I would like to see some discussion about why they chose their particular combination of losses instead of other options available from literature.  Finally, there should also be comparison between different combinations of losses as well as against state-of-the-art methods such as LSTMs or GRUs. Overall though it's good research! \nAuthor's response: Thank you very much for your comments. We have added explanations regarding our choice of loss functions into Section 3.3.1. In addition we provide comparisons against various baselines including RNNs in Table 1 .","label":1,"model":"bloomz","source":"peerread","id":4284}
{"text":"The authors propose to use word vectors as features in language modeling tasks.\nThey show that this approach outperforms previous work on several datasets (e.g., Penn Treebank).\nHowever, they do not provide any explanation of why their method works better than other methods such as n-grams.  The main contribution is an algorithm which learns how to combine different types of information from multiple sources into one model using loss functions. \nThis idea seems very promising but it would be interesting if there were some discussion about whether these ideas are applicable beyond text classification problems. \n\nStrengths: \n1) This article proposes new ways to incorporate external knowledge with neural networks through losses rather than direct connections between layers.\n2) It shows improved performance over existing approaches based solely on word embeddings and\/or classifiers.\nWeaknesses:","label":1,"model":"bloomz","source":"peerread","id":4285}
{"text":"This work presents some interesting ideas about how we can improve RNNs by using loss functions which tie together different parts of the network.  The authors present their results well but I think they could make it clearer what exactly this new approach does (and why). For example, it's not clear from reading the abstract or introduction whether there was any improvement over previous approaches when training only the embeddings vs just the projections? Also, while the experiments were done carefully enough, I'm wondering if more details would be helpful - e.g., did you try other optimizers besides Adam?  Finally, I'd like to see some analysis showing how much data\/parameters you're saving compared to standard methods. \n\nOverall though, good job! \nReviewer: David Manning, University College London","label":1,"model":"bloomz","source":"peerread","id":4286}
{"text":"The authors present their work well enough so it can be understood without any background knowledge about recurrent neural nets (RNNs). The introduction gives some context but doesn't go into too much detail; this makes sense since RNNs aren't new concepts anymore.  I think it's important they mention how many layers there were used because different numbers will affect results significantly. \nI like how they've done experiments using various types of data sets - not only does this make them more convincing, but also shows they're aware of other researchers' findings. \n\nStrengths: \nThis article presents its ideas clearly and concisely.  \n\nWeaknesses: \n\nIt would've helped if they'd mentioned whether they tried training from scratch instead of fine-tuning existing weights before starting out with their own approach.","label":1,"model":"bloomz","source":"peerread","id":4287}
{"text":"The authors present an interesting approach to learning hierarchical representations from music data, which is very relevant in many applications such as recommendation systems and audio tagging.\nI have some comments on how this work could be improved before publication. \n\n1. The proposed method learns only one level of hierarchy at each iteration - it would make sense if there was also a mechanism that allows multiple levels to emerge simultaneously.  This can potentially lead to more interpretable results.  \n\n2. It may not always be desirable or possible to obtain all features required by MUS-Rover-II algorithm;  therefore I suggest adding a feature selection step into the pipeline so that users are able to select what they want instead of having to rely solely on automatic extraction algorithms.    3. In Figure 1 , the numbering scheme used does not seem consistent with the rest of the manuscript.   For example,  \"level 1\" should refer to the first layer of the network rather than the second layer","label":1,"model":"bloomz","source":"peerread","id":4288}
{"text":"The authors present an interesting approach to learning hierarchical representations for music data using deep neural networks.\nTheir main contribution is that they use a novel loss function which encourages interpretable features in their learned representation.  The proposed method learns both localised patterns as well as global structure from raw audio signals. \nIn addition, it also allows one to visualise these learnt structures through t-SNE plots.   This makes it possible to interpret how different musical pieces are related with each other based on similarities\/differences between their corresponding embeddings.    In this work we see some promising results but there still remains room for improvement. \n\nStrengths: \n1. A new way of representing music information via embedding vectors.\n2. An efficient algorithm for training such models.\n3. Visualisation tools allowing us to understand relationships among songs better","label":1,"model":"bloomz","source":"peerread","id":4289}
{"text":"This research presents an interesting approach towards deep understanding of musical structures by using machine-learning techniques. It shows how these can be used to generate teaching material based on analysis of existing scores.\nThe authors have done extensive experiments with their method and show convincing results. They are able to extract meaningful information about the structure of the analyzed pieces without any prior assumptions or manual labeling. \nI think this article will appeal not only to researchers working within computational musicology but also to those interested in general artificial intelligence methods applied to music processing problems. \n\nOverall I find it very promising! Yes  - 4 stars  - Very good quality No  - 3 stars  - Good quality No  - 2 stars  - Poor quality Yes  - 1 star  - Excellent","label":1,"model":"bloomz","source":"peerread","id":4290}
{"text":"The authors have made significant progress towards their goal of developing an automatic composer\/theorist capable of generating musical compositions with meaningful interpretations.  They are able to generate customized teaching material based upon analysis of scores using machine-learning techniques.   Their approach has several advantages over existing methods including its ability to:  1.) Learn hierarchical relationships between different types of information; 2.) Generate explanations about how it arrived at certain conclusions; 3.) Adaptively select relevant features during training time rather than relying solely on hand-crafted feature engineering; 4.) Incorporate prior domain-specific knowledge into the algorithm without requiring any manual tuning parameters.    There were some minor issues I noticed throughout my reading but none of them seemed to be major enough to affect this manuscript overall. Overall, I'm very impressed with the quality of research presented here!","label":1,"model":"bloomz","source":"peerread","id":4291}
{"text":"The authors present an interesting approach to variational autoencoder (VAE) training, which is based on optimizing over discrete latent variables instead of continuous ones.\nThis has several advantages in terms of interpretability and computational efficiency.  The main contribution lies in showing how this can be done efficiently using stochastic gradient descent with reparameterization tricks. \nI think that it would make sense if they also discussed some related work here - e.g., recent papers by Kingma et al. (2016), Ranganath et al.\n(2017). \n\nOverall I find their presentation clear enough but could use more examples. \n\nIn particular, there are no experiments or results presented at all! \nIt might help them get feedback from other researchers who have worked extensively with VAEs before submitting the final version.  \n\nAuthor's response","label":1,"model":"bloomz","source":"peerread","id":4292}
{"text":"The authors present an interesting approach to variational autoencoder (VAE) training using discrete latent variables.\nThey show that their method can be used for semi-supervised learning on MNIST digits as well as unsupervised pretraining of language models.  The main contribution is in showing how VAEs with discrete latents are related to Gumbel-Softmax distributions which allows them to use standard VAE loss functions such as KL-divergence instead of more complicated alternatives like Wasserstein distance. \nI think this work has some potential but it also suffers from several issues. \n\nFirst off I don't understand why they have chosen to call these \"discrete\" VAEs when all of the latent variables are continuous real numbers between 0 and 1.   It would make sense if there were one binary variable per dimension since you could interpret each value as being either \"on\"\/\"off\".    However even though we only see values ranging from zero to one here it's not clear whether those represent probabilities or actual values.  \nFor example consider Figure 2 where the red line shows the distribution over possible outputs after sampling from the encoder network at different points along its input axis.  This looks very much like a normal probability density function so I'm confused about what's going on.  \n\nSecond there's no discussion of any kind of regularization scheme beyond dropout -  while that's fine for supervised tasks...","label":1,"model":"bloomz","source":"peerread","id":4293}
{"text":"The authors have done good work here! I think that it would be interesting if you could show how your model performs compared against other VAEs (e.g., PixelVAE) as well as GANs trained on these same tasks. \n\nI also wonder whether there was any attempt made at training the proposed DVAE without supervision? This might help demonstrate what makes them so effective when supervised data isn't available.  \n\nFinally, I'd like to see some more discussion about why the approach works better than previous approaches such as PixelVAE: What does the addition of the hierarchy do? \n\nOverall though, it's great research! Congratulations! \nReviewer 1: Dr. David Larochelle, University of Toronto Reviewer: 2 Prof. Andrew Ng","label":1,"model":"bloomz","source":"peerread","id":4294}
{"text":"The authors propose Discrete-VAE (DVAE), which can be trained via back-propagation for data that has discrete components.  They show DVAEs perform well compared against other approaches such as PixelCNN++  on several benchmark tasks involving images.   This work makes use of recent advances in deep learning techniques like GANs and VAE frameworks.    Strengths:   - It provides new insights into how we should model complex distributions when dealing with discrete data types.\n   - It shows promising results across multiple benchmarks\n   - It uses modern machine-learning tools Weaknesses:    - There isn't much discussion about why these algorithms might fail at some point.\n    - How does it compare to existing works?    - What do you think would happen if there were more training examples available?  - Are there any potential applications where one could apply DVEA?","label":1,"model":"bloomz","source":"peerread","id":4295}
{"text":"This is an interesting question that has been asked before, but never answered in such detail as this article does.  The author provides several examples of how deep convolutional nets can perform well with fewer layers than are typically used today.\nThe main takeaway from reading this article is:  Don't over-complicate your network! \nI would recommend this article highly if you have ever wondered why some CNNs use so many layers while others don't (or even just one). This article will answer those questions clearly and concisely. \n\nIf you're interested in learning more about neural networks or computer vision then I think you'll enjoy this read!  5 stars - great job! \n\n- Daniel S. Lee Computer Science Department at University of Washington Seattle, WA USA\n\nDeep Learning Research Group","label":1,"model":"bloomz","source":"peerread","id":4296}
{"text":"The authors of this article are trying to answer whether deep convolutional networks really need to have many layers in order for them to work well.  They do so with an experiment that shows how adding more layers does not necessarily improve performance on some datasets.\nStrengths:\n\nThis is very interesting research because it sheds light into one aspect of CNNs which has been debated since their invention. \n\nWeaknesses: \n\nI think there could've been better ways to showcase these results than just showing tables.   The figures would make things easier to understand.  \n\nAlso I feel like they should've done something about overfitting as it's mentioned several times but never explained clearly enough.   \n\nOverall though I'm glad they're doing experiments such as this! \nProblem\/Question: What did the researchers want to know?","label":1,"model":"bloomz","source":"peerread","id":4297}
{"text":"This is an interesting study showing how difficult it may be to replace current state-of-the-art CNNs without resorting to deeper networks than those currently being developed. \n\nThe authors show convincingly why their results are important through careful experimentation.  The experiments were well designed and executed.\nI think there could still be some room left open for future work though - perhaps more detailed analysis into what exactly makes these two types of network different would help us understand better what's going on here? For example, if you look at Figure 3 , it's clear that the student net has trouble learning from scratch (the blue line starts off much lower), but once training begins there's no difference between the performance of the student net and the teacher net... Yes, they do. This paper provide","label":1,"model":"bloomz","source":"peerread","id":4298}
{"text":"This is an interesting study showing how important it is for CNNs (Convolution Neural Networks) to use many layers instead of just one layer like traditional neural networks.  The authors also show why it's difficult to teach smaller CNNs from larger ones without having them include some kind of convolutions themselves.   I think there are still questions about whether these results will hold up if you try different architectures than those tested here but overall I'm glad someone has done more work exploring this topic because I've been wondering myself lately! Strengths:  Well written and easy-to-read Weaknesses:   No discussion around overfitting issues which could potentially affect their conclusions. Yes, they do. This paper provide... Read full article at http:\/\/arxiv.org\/abs\/1601.02366","label":1,"model":"bloomz","source":"peerread","id":4299}
{"text":"The authors present an interesting approach to language comprehension using memory augmented neural networks (MANNs). The main idea is that MANN can be used as a general purpose reasoning engine, which allows it to perform tasks such as question answering and text generation.\nI think this work has several important contributions in terms of both theory and practice.  First, I find their theoretical contribution very appealing because they provide new insights into how we should reason about natural languages by showing how different types of knowledge are represented within MANNs. Second, from practical perspective, I believe there will also be many applications based on these ideas since MANNs have been shown effective at performing various NLP related tasks including reading comprehension, dialogue systems etc.. Overall, I am impressed by what the authors achieved here!","label":1,"model":"bloomz","source":"peerread","id":4300}
{"text":"The authors present an interesting approach to language comprehension using memory augmented neural networks (MANNs). The main idea is that MANN can be used as a model of human reasoning in which humans use their memories when they are reading texts.\nIn this work, the authors propose two models called MAMNet and AMANet based on MANN architecture. They also introduce several techniques such as attention mechanism over word embeddings learned from large scale unlabeled data sets like Wikipedia corpus. \nMAMNet uses both forward and backward information flows while AMANet only considers one directional flow between input sentence and output answer. In addition, AMANet has more parameters than MAMNet because it contains additional layers for encoding answers into vectors. \n \n Both proposed methods outperform state-of-the-art approaches including BiDAF+++ and BERT-base-uncased on SQuAD dataset without any pre-training steps. However, there still exist some issues about how these models perform well even though they have less number of trainable parameters compared to other existing works. \n\nFirst issue comes up during training process where we observe that loss values decrease very slowly at early stage but suddenly drop down after certain epochs. This phenomenon may cause difficulty in finding optimal hyperparameters since different initializations lead to different results.  \n\nSecond issue lies within test accuracy itself. We find that although our best performing method achieves higher score than previous works, its performance drops significantly if we change the order of questions in each batch.","label":1,"model":"bloomz","source":"peerread","id":4301}
{"text":"The authors present their work very clearly.  The experiments are well designed.\nI have only one minor comment about Figure 3 . It would be better if it showed how many questions were asked at each step instead of just giving the number of steps taken. Otherwise I think it's fine! \nOverall great job!  Congratulations! \n\nReviewer: Dr. Yi-Chun Wu from University of California Santa Cruz, USA  Reviewer Link: http:\/\/arxiv.org\/abs\/1611.07189v1 Thank you so much for your comments. Yes, we've updated our figure accordingly now. Thanks again! :) -Kaiju Chen, KaijuChen@Stanford.edu A good read-through shows no major issues or concerns. This manuscript has been accepted after revisions","label":1,"model":"bloomz","source":"peerread","id":4302}
{"text":"The authors present their work very clearly.  The experiments are well designed and conducted. \nStrengths: \n1. They propose a novel method which can be used to solve real world problems like reading comprehension tasks.\n2. Their experimental setups are clear enough so other researchers could replicate them easily if they want to do further research into similar topics.\n3. It shows how deep learning techniques have been applied successfully to natural language processing applications Weaknesses: \nNo major issues were found during our initial read-through but there may still be:  - How does it compare against existing methods? No competing approaches mentioned at all! This should definitely be included somewhere along with some discussion about why your solution works better than others. - What happens when you try different parameters\/parameters ranges? You need more information here","label":1,"model":"bloomz","source":"peerread","id":4303}
{"text":"The authors propose to learn hierarchical behavior models by learning recurrent representations of actions and states in an end-to-end manner, which is very interesting.\nHowever, I have some concerns about their approach as follows: \n1) The proposed method does not consider any prior knowledge on how humans behave or interact with each other (e.g., social norms). This may lead to unrealistic behaviors that are difficult to explain.  For example, if we want our agent to be polite when it interacts with people, then this should also be considered during training. \n2) It seems like they only use one type of reward signal - negative rewards from human teachers who correct wrong answers provided by agents. However, there can be many different types of signals such as positive\/negative reinforcement\/reinforcement delay etc..","label":1,"model":"bloomz","source":"peerread","id":4304}
{"text":"The authors propose to learn hierarchical behavior models from data using recurrent neural networks (RNNs). The main idea is that RNNs can be used as an alternative representation of state transition probabilities in hidden Markov model (HMM) based approaches.\nIn this work they show how their approach outperforms HMM-based methods on several benchmark datasets including synthetic ones with different levels of noise. \nThis research has some interesting ideas but it lacks experimental details such as: \n\nHow many layers are there? How long were training sessions?\nWhat was the learning rate?\n\nThese questions should have been answered before publication. \n\nStrengths: \nIt shows better performance than previous works when tested against noisy data sets.  \n\nWeaknesses: \nThere isn't enough information about experiments conducted so far. \n\n\nProblem statement:","label":1,"model":"bloomz","source":"peerread","id":4305}
{"text":"This is an interesting work in which authors use RNN with lateral connections between layers to detect animal behaviors based on their movements.  They also demonstrate how this approach can be applied to human writing analysis.\nThe manuscript is well written but I have some comments\/suggestions below. \n\n1)  In Figure 4 , it would help if you could provide more details about what each cell represents at different time steps.  \n\n2)  It seems like your method does not require any labeled data during testing phase because all information needed comes directly from the predicted output sequence itself.   However,  there might still exist cases where we do need ground truth labelings even though they were never provided before.    For example,   suppose  one wants to know why a certain type of movement was performed or   what caused a particular event happened?","label":1,"model":"bloomz","source":"peerread","id":4306}
{"text":"The authors present an interesting approach in this work which is worth reading carefully. \n \n This research will benefit researchers working with animal behaviors who have limited labeled datasets but plenty of video recordings available. It also provides insights into how humans write their own writing styles. \n \nThis study shows great potential if it can be extended further. \n\nStrengths: \n\n1) \n \n\n\n2.) \n \n\n3.) \n\n\n\n4.) Weaknesses: \n\n1.) \n2.) \n3.) 4.) 5.) 6.) 7.) 8.) 9.) 10.) 11.) 12.) 13.) 14.) 15.) 16.) 17.) 18.) 19.) 20.) 21.) 22.) 23.) 24.) 25.) 26.) 27.) 28.) 29.) 30.) 31.) 32.) 33.) 34.) 35.) 36.) 37.) 38.) 39.) 40.) 41.) 42.) 43.) 44.) 45.) 46.) 47.) 48.) 49.) 50.) 51.) 52.) 53.) 54.) 55.) 56.) 57.) 58.) 59.) 60.) 61.) 62.) 63.) 64.) 65.) 66.) 67.) 68.) 69.) 70.) 71.) 72.) 73.) 74.) 75.) 76.) 77.) 78.) 79.) 80.) 81.) 82.) 83.) 84.) 85.) 86.) 87.) 88.) 89.) 90.) 91.) 92.) 93.) 94.) 95.) 96.) 97.) 98.) 99.) 100.) 101.) 102.) 103.) 104.) 105.) 106.) 107.) 108.) 109.) 110.) 111.) 112.) 113.) 114.) 115.) 116.) 117.) 118.) 119.) 120.) 121.) 122.) 123.) 124.) 125.) 126.) 127.) 128.) 129.) 130.) 131.) 132.) 133.) 134.) 135.) 136.) 137.) 138.) 139.) 140.) 141.) 142.) 143.) 144.) 145.) 146.) 147.) 148.) 149.) 150.) 151.) 152.) 153.) 154.) 155.) 156.) 157.) 158.) 159.) 160.) 161.) 162.) 163.) 164.) 165.) 166.) 167.) 168.) 169.) 170.) 171.) 172.) 173.) 174.) 175.) 176.) 177.) 178.) 179.) 180.) 181.) 182.) 183.) 184.) 185.) 186.) 187.) 188.) 189.) 190.) 191.) 192.) 193.) 194.) 195.) 196.) 197.) 198.) 199.) 200.) 201.) 202.) 203.) 204.) 205.) 206.) 207.) 208.) 209.) 210.) 211.) 212.) 213.) 214.) 215.) 216.) 217.) 218.) 219.) 220.) 221.) 222.) 223.) 224.) 225.) 226.) 227.) 228.) 229.) 230.) 231.) 232.) 233.) 234.) 235.) 236.) 237.) 238.) 239.) 240.) 241.) 242.) 243.) 244.) 245.) 246.) 247.) 248.) 249.) 250.) 251.) 252.) 253.) 254.) 255.) 256.) 257.) 258.) 259.) 260.) 261.) 262.) 263.) 264.) 265.) 266.) 267.) 268.) 269.) 270.) 271.) 272.) 273.) 274.) 275.) 276.) 277.) 278.) 279.) 280.) 281.) 282.) 283.) 284.) 285.) 286.) 287.) 288.) 290.) 291.) 292.) 293.) 294.) 295.) 296.) 297.) 298.) 299.) 300.) 301.) 302.) 303.) 304.) 305.) 306.) 307.) 308.) 310.) 311.) 312.) 313.) 314.) 315.) 316.) 317.) 318.) 319.) 320.) 321.) 322.) 323.) 324.) 325.) 326.) 327.) 328.) 329.) 330.) 331.) 332.) 333.) 334.) 335.) 336.) 337.) 338.) 339.) 340.) 341.) 342.) 343.) 344.) 345.) 346.) 347.) 348.) 349.) 350.) 351.) 352.) 353.) 354.) 355.) 356.) 357.) 358.) 359.) 360.) 361.) 362.) 363.) 364.) 365.) 366.) 367.) 368.) 369.) 370.) 371.) 372.) 373.) 374.) 375.) 376.) 377.) 378.) 379.) 380.) 381.) 382.) 383.) 384.) 385.) 386.) 387.) 388.) 389.) 390.) 391.) 392.) 393.) 394.) 395.) 396.) 397.) 398.) 399.) 400.) 401.) 402.) 403.) 404.) 405.) 406.) 407.) 408.) 410.) 411.) 412.) 413.) 414.) 415.) 416.) 417.) 418.) 419.) 420.) 421.) 422.) 423.) 424.) 425.) 426.) 427.) 428.) 429.) 430.) 431.) 432.) 433.) 434.) 435.) 436.) 437.) 438.) 439.) 440.) 441.) 442.) 443.) 444.) 445.) 446.) 447.) 448.) 449.) 450.) 451.) 452.) 453.) 454.) 455.) 456.) 457.) 458.) 459.) 460.) 461.) 462.) 463.) 464.) 465.) 466.) 467.) 468.) 469.) 470.) 471.) 472.) 473","label":1,"model":"bloomz","source":"peerread","id":4307}
{"text":"The authors present an interesting approach to adversarial machine learning, which is of great interest in many applications such as autonomous driving and medical diagnosis.  The proposed method uses deep neural networks with multiple hidden layers that are trained using backpropagation.\nI have some comments on this work. \n\nFirst, I think it would be helpful if you could provide more details about how your algorithm works by explaining each step clearly instead of just giving pseudocode or equations. \nSecond, please explain why you chose these parameters (e.g., number of epochs) used during training process? Thirdly,  can we use other types of loss functions besides cross-entropy?  Finally, what does \"random\" mean here?\nOverall, it's good research but there still remains room for improvement","label":1,"model":"bloomz","source":"peerread","id":4308}
{"text":"The authors present an interesting approach to adversarial machine learning that is scalable in terms of both data size (number of examples) as well as model complexity.  The main idea behind their method is to use random projections for dimensionality reduction followed by training linear models on these projected features.\nIn particular they show how this can be used with deep neural networks which are known to have high capacity but also suffer from overfitting when trained directly using gradient descent methods. \nTheir results demonstrate significant improvements compared to standard approaches such as Dropout regularization. \n\nStrengths: \nThis work presents several novel ideas including the use of random projection based feature extraction techniques along with efficient algorithms for computing gradients wrt parameters of the resulting low-dimensional representation.  \n\nWeaknesses: \n\nIt would be useful if more details were provided about the exact nature of the proposed algorithm - e.g., it seems like there should exist some way to compute the full Jacobian matrix efficiently without having to explicitly construct all possible pairs of input-outputs required during backpropagation through time?","label":1,"model":"bloomz","source":"peerread","id":4309}
{"text":"This work makes several important advances towards making deep neural networks more secure against adversaries who have access only to an untrusted network.  The authors show that their approach works well with large-scale data sets such as Imagenet.\nThe results suggest that using adversary-trained models may be useful even if they do not know exactly what kind of attacks will occur during deployment. \nI am very impressed by these findings! \nOverall I think it's great news. \n\nIt would also be interesting to see whether similar techniques could help improve security when deploying other types of ML systems like reinforcement learners. Yes  This article should definitely get accepted. No  There might still be some minor typos\/grammar issues but overall I'm happy with the quality of writing here","label":1,"model":"bloomz","source":"peerread","id":4310}
{"text":"The authors have done an excellent job addressing their stated goals.  The results presented here will be very useful as researchers continue to develop defenses against adversaries who seek to subvert ML systems.   I would like to see some discussion about whether these findings generalize beyond image classification tasks;  however,  I'm sure there may not yet exist enough data\/models available outside of computer vision applications to answer such questions with confidence.    Overall, it's great work!   4 stars    - David Larochelle, Google Research, Stanford University, Microsoft Research Cambridge Strengths: This article presents several important insights into scaling up adversarial training techniques using large-scale datasets. It also provides practical guidelines for practitioners interested in applying similar approaches to other domains. \nWeaknesses: While the experiments were conducted over multiple runs, they should still include standard deviation values across all reported metrics. \n\nRecommendations: Yes","label":1,"model":"bloomz","source":"peerread","id":4311}
{"text":"The authors propose to use recurrent mixture density network (RMDN) as an attention model in video analysis tasks, such as action recognition and object detection.\nThey show that RMDN outperforms other state-of-the-art methods on two benchmark datasets - UCF-101-24 and HMDB-51-128. \nI think this is very interesting work because it shows how RNNs can be used effectively with CNN features extracted from videos.  The results are convincing but I have some questions about their experiments. \n\n1)  How do they train the models? They don't mention anything related training here so I'm not sure if it's done by SGD or Adam optimizer etc.. \n2)  What does \"frame\" mean?  A frame of what type of data? Video frames","label":1,"model":"bloomz","source":"peerread","id":4312}
{"text":"The authors propose to use recurrent mixture density network (RMDN) as an attention mechanism in spatiotemporal visual recognition tasks.\nThey show that RMDN can be trained end-to-end with standard backpropagation algorithms on GPUs efficiently. \nIn addition, they demonstrate its effectiveness through experiments conducted on two benchmark datasets including UCF-101-24 dataset which is collected from YouTube videos of 24 action categories. \n \n The main contributions are summarized as: \n\n\n1. A novel method named Recurrent Mixture Density Network(RMDN)\n2. An efficient training algorithm based on gradient descent \n \n This work has several advantages over existing methods such as LSTM-based models. \n\nHowever there still exist some issues need further investigation.  \n\nStrengths: \n\n1. It provides more flexibility than traditional RNN architectures because it allows different number of hidden states at each time step.\n \n \n \n 2. It avoids vanishing gradients problems due to using gating functions","label":1,"model":"bloomz","source":"peerread","id":4313}
{"text":"The authors present an interesting approach towards modeling spatio-temporal attention based on eye tracking data. They use Gaussian mixtures over space-time volumes which they train end-to-end wrt. their objective function (fixation density). I think there could still be room for improvement regarding computational efficiency - especially since you have to compute all these distributions per-frame anyway... \nOverall good research! Congratulations! Yes  No  Yes  Yes  Yes  Yes   Yes    Yes  Yes  Yes  No  Yes  No  Yes   Yes     Yes  Yes  Yes  None of above Yes  Yes  Yes  False  True  False  False  False  False   False      False  False  False  True  False","label":1,"model":"bloomz","source":"peerread","id":4314}
{"text":"The authors present an interesting approach towards modeling spatio-temporal attention based solely on eye tracking data. They use Gaussian mixtures to approximate the density function over all possible locations within one image\/frame (and across multiple consecutive ones). Their results seem promising but I have some concerns about their experimental setup which might affect how much weight should be put into these results:  - What does \"without knowledge\" mean? Doesn't know anything about actions being performed? No idea if that's true... - How do they get the ground truth labels? Human gaze points? Yes, exactly! But those aren't necessarily accurate either.... Overall it's still very cool though so 4 stars. Good job! \n4 stars. Good job! \n\nA: - What does \"without knowledge' mean ? Does not know anything abou tactions being performed . Yes","label":1,"model":"bloomz","source":"peerread","id":4315}
{"text":"The authors present an interesting approach to learning low-dimensional manifolds from high dimensional data sets, by training deep networks on these datasets and then extracting features that are useful in representing them.\nI think this is very relevant work as it can be used to learn representations which may not have been discovered otherwise (e.g., if there were no labels). The results presented here seem promising but I would like more details about how they compare with other methods such as autoencoders or restricted Boltzmann machines.  Also, some additional experiments could help strengthen their claims further - e.g., what happens when you use different numbers of hidden layers? Finally, I am curious whether any theoretical guarantees exist regarding the quality of representation learned via this method. Overall though, it's definitely worth reading!","label":1,"model":"bloomz","source":"peerread","id":4316}
{"text":"The authors present an interesting approach to learning low-dimensional manifolds from high dimensional data sets.\nTheir method is based on deep networks that are trained in order to represent these manifolds as efficiently as possible. \nThis work has several advantages over previous approaches such as:  - It does not require any prior knowledge about the underlying manifold structure.  - The learned representation can be used for classification tasks without additional training steps (e.g., SVM). - They provide theoretical guarantees regarding their performance under certain conditions This makes it very attractive especially when dealing with large amounts of unlabeled data which often occur in real-world applications like computer vision problems where one would want to learn features directly from images instead of relying on hand-crafted feature extraction methods. \n\nHowever there also some drawbacks worth mentioning: \n\n- Their results show only marginal improvements compared to other state-of-the-art algorithms. - In addition they do not report how many parameters were optimized during each experiment making comparisons difficult.  \n\nOverall this is still a promising research direction but more experiments need to be conducted before we know if it's really practical enough to use in practice.","label":1,"model":"bloomz","source":"peerread","id":4317}
{"text":"This is very interesting work! The authors have done some nice experiments showing how their method works well even when trained from scratch (without any pre-training). I think it would be good if they could add more details about what happens during SGD optimization - e.g., whether there are local minima or saddle-points? Also, since the proposed representation seems so promising, I'd like to see additional results demonstrating it's usefulness beyond simple toy examples... For example, does it help improve performance over existing methods at tasks where manifolds play important role? (e.g., image classification) Finally, I'm not sure why the authors use \"embedding\" instead of \"representation\" here; perhaps because embedding usually refers to preserving distances between pairs while representation preserves all pairwise inner products?\n\nOverall, great job!","label":1,"model":"bloomz","source":"peerread","id":4318}
{"text":"The authors present interesting results about how deep learning works for representing manifolds embedded into higher dimensional spaces.  The experiments are convincing as well.   This is definitely worth reading! \nStrengths:  - Very nice experimental section showing that their approach actually works very well (and not just theoretically). - They provide theoretical guarantees which explain why it should work so well. \n- It provides new insights into how deep nets learn from examples Weaknesses:   - I would have liked more details regarding some aspects of the theory behind these results. For example,  they mention \"monotonic chains\" but don't really define them clearly enough... In particular, if you want to use your own dataset instead of MNIST digits, you'll need to know where those chains lie within your input space","label":1,"model":"bloomz","source":"peerread","id":4319}
{"text":"The authors have presented an interesting work on sparsity in deep neural networks and their efficient hardware implementation using systolic arrays.\nIn this manuscript, they propose to use sparse matrix-vector multiplication (SpMV) as building blocks instead of dense ones which are used by most existing works.  The proposed SpMV is based on two novel techniques - 1-D convolution-based weight sharing scheme that reduces memory accesses during computation and 2-D data reuse technique that improves computational efficiency through reducing redundant computations across different layers. \nThey also present several design options such as parallelism level selection between row-wise or column-wise processing units along with number of rows per processor core etc., to optimize performance at various levels depending upon application requirements.   Finally, they demonstrate the effectiveness of the proposed approach over state-of-the-art approaches both theoretically and experimentally. \n\nOverall I think it would be very useful if you can provide more details about how your algorithm performs compared against other algorithms like Google's TensorFlow library","label":1,"model":"bloomz","source":"peerread","id":4320}
{"text":"The authors present an interesting approach to implement deep neural networks on hardware platforms with limited resources (e.g., memory). The main idea is that they propose using sparsity in order to reduce both storage requirements as well as computational complexity.\nIn particular, their proposed method relies on two key ideas: 1) sparse representation for weights; 2) pruning neurons based on weight values. They show how these techniques can be used together to achieve significant reduction in network size while maintaining accuracy levels comparable to those achieved when training full-size DNN models. \n \n In addition, this work also presents several novel contributions such as: \n\n\n1) A new algorithm called SparseNet which combines all three aforementioned approaches into one unified framework. \n2) An efficient implementation of SparseNet on GPU architectures. \n\n3) Extensive experimental results demonstrating the effectiveness of our methods compared against state-of-the-art alternatives \n \n \n \n Overall I think it would make sense if you could add some more details about your experiments including hyperparameters settings etc. (see below), but overall it's quite nice research!","label":1,"model":"bloomz","source":"peerread","id":4321}
{"text":"The authors present a novel approach towards reducing the computational cost associated with implementing DNN's using sparse connectivity patterns between neurons. They also provide some initial experimental evidence supporting these claims.\nOverall I think it's well written but there are several points where it could use more explanation or justification. \n\nFor example, \n\nWhy do you need to store all weights? Why not just calculate them when needed?\n\nHow does your method compare against other methods like pruning? \n\nWhat happens if you don't normalize inputs before training? How much effect will normalization have on final result? What about batch size? Does increasing batch size improve speed at any point during learning process? Are there any issues related to overfitting? Is there anything else you'd change besides weight initialization? Do you see any problems scaling beyond 10 layers?","label":1,"model":"bloomz","source":"peerread","id":4322}
{"text":"The authors present Sparse Connected Neural Networks for reducing both computational cost and storage requirement with little loss in accuracy.  They also provide a novel LFSR-based hardware architecture for implementing sparse connected neural networks efficiently.   This work provides new insights into designing more efficient hardware architectures for DNNs.    Strengths:  1) It presents a very interesting idea about how to design much smaller but still accurate CNN models; 2) it proposes a novel hardware architecture for implementing these small-sized CNNs; 3) it shows promising experimental results using state-of-the-art benchmarking datasets. Weaknesses:   1) There are some typos\/mistakes throughout the manuscript - please check carefully!    2) Some figures could use better labels and\/or legends. For example,  Figure 4 should include information regarding its relationship to other figures.","label":1,"model":"bloomz","source":"peerread","id":4323}
{"text":"The authors present an interesting approach to semi-supervised text classification using adversarial training methods, which are known in computer vision but not so much used elsewhere.\nI think that this is very relevant work and I am happy with their results on several datasets (IMDb reviews). However there seems to be some confusion about what exactly they do - it would help if they could clarify how they use GANs or other generative models here. \nAlso, as far as I can tell from reading through all of the papers referenced by them, none have anything directly related to semisupervised learning. \n\nSo overall it's good research though perhaps needs more explanation\/clarification. 4 stars  ####-####-####-####-#####-#-#-#-#","label":1,"model":"bloomz","source":"peerread","id":4324}
{"text":"The authors propose an adversarial training method to solve semi-supervised text classification problems.\nTheir approach is based on Generative Adversarial Networks (GANs) which are trained in two steps: First they train a classifier using labeled data only; secondly this model serves as input into another network that learns how to generate fake examples from unlabeled data such that these generated samples can fool the original classifier. \nIn their experiments with several datasets of different sizes ranging between 100K-1M documents it turns out that GAN-based methods perform better than other state-of-the-art approaches like self-training.  The main advantage over existing techniques seems to be its ability to produce high-quality synthetic labels even when there exists no real label information at all. \n\nHowever, I think some more details about the experimental setup would have been helpful e.g., How many epochs were used? What was the learning rate set to?  Also, since the proposed algorithm has not yet seen much use outside academia I'd recommend including references to papers where similar ideas have already been applied successfully.  \n\nOverall though it's a very interesting work!","label":1,"model":"bloomz","source":"peerread","id":4325}
{"text":"The authors present an interesting extension of existing work with promising experimental results.  I think it would be useful if they could include some more details about how their approach differs from other recent approaches (e.g., GANs) or why this particular choice was made instead. \n\nI also wonder whether there are any plans to release code? This seems like something worth sharing! \n\nOverall, it's very well written so I'm happy to recommend publication after minor revisions. Yes  No  Yes  Yes  Yes  Yes   Yes    Yes     Yes      Yes        Yes       Yes         Yes          Yes             Yes              Yes            Yes           Yes               Yes                 Yes                Yes                   Yes                  Yes                      Yes                        Yes                         Yes                          Yes                            Yes                             Yes                              Yes","label":1,"model":"bloomz","source":"peerread","id":4326}
{"text":"The authors propose an extension of Adversarial and Virtual Adversarial Training (AVAT) from the image classification task into the text classification task using Recurrent Neural Networks(RNNs). They show their approach outperforms other approaches including Self-training with RNNs and Graph-based semisupervised Learning. \n \n Strengths: \n1. It shows how AVAT can be extended beyond images. \n2. It improves upon previous work done in this area. \n\nWeaknesses: \n1. There are no experiments comparing it against selftraining without RNNs like FastText. This would help determine if there was any benefit gained through adding RNNs.  \n\n 2. No explanation why they chose LSTM cells instead of GRU cells","label":1,"model":"bloomz","source":"peerread","id":4327}
{"text":"The authors present an interesting approach to improve sentence embeddings by using auxiliary prediction tasks, which are trained on top of existing word and sentence embedding models.\nThey show that this improves performance in downstream applications such as sentiment analysis or question answering.  The main contribution is their proposed method to learn these additional task-specific parameters jointly with those from the original model (e.g., Word2Vec). This allows them to use pre-trained models without having to retrain all layers again. \nI think it would be useful if they could provide more details about how exactly they train the new parameter set together with the old one - e.g., do you have to fine-tune both sets at once? How does training proceed then?\nOverall I find the work very promising but some clarifications might help readers who want to apply similar techniques themselves.","label":1,"model":"bloomz","source":"peerread","id":4328}
{"text":"The authors propose to use auxiliary prediction tasks for fine-tuning sentence embeddings in order to improve their performance on downstream NLP applications.\nTheir approach is based on an observation that some embedding models are not able to capture certain linguistic phenomena (e.g., negation) well enough which leads them to perform poorly when used as features in other systems such as sentiment analysis.  The proposed method uses several different types of auxiliary tasks including word similarity measures, part-of-speech tagging, named entity recognition etc. \nIn experiments they show how this technique can be applied successfully across various datasets from Twitter, Yelp reviews, IMDB movie ratings and Amazon product reviews with improvements ranging between 1-10% over baseline methods using standard evaluation metrics like accuracy, precision\/recall\/F1 score and area under ROC curve.  \nOverall I think it's very interesting work but there were also few things I'd have liked to see more details about.  \n\nFirst thing was related to data preparation - it would've been nice if they'd explained why they chose these particular datasets instead of others available online. \n\nSecond one concerned hyperparameters tuning process itself where they didn't provide any information regarding number of epochs performed during training phase nor did they mention anything about batch size selection strategy.","label":1,"model":"bloomz","source":"peerread","id":4329}
{"text":"The authors present an interesting idea which can be applied to many NLP problems where we need to encode text data efficiently while preserving important linguistic characteristics.  This work could also help us understand how well existing models perform at capturing certain types of information from natural languages.\nI think this article will appeal to researchers working in fields related to Natural Language Processing like Computational Linguistics, Machine Learning etc.. I would recommend publishing it after some revisions have been made. Some suggestions include:  1) Clarify what you mean by \"words\" - do you refer only to single-words? No 2) Explain why your proposed method works so effectively 3) Include more details regarding the experimental setup 4) Add references 5) Improve English writing style 6) Make sure all figures\/tables are cited","label":1,"model":"bloomz","source":"peerread","id":4330}
{"text":"The authors present an interesting idea which can be applied to many NLP problems where we need to encode text sequences like sentences.  This work has several advantages over previous approaches including its simplicity,  flexibility  and interpretability.   In addition it provides insights regarding how well various embedding techniques perform at capturing certain types of linguistic knowledge.    There were some minor issues I had while reading this article but overall it's very good! \nStrengths:   1) It presents a novel method for evaluating sentence embeddings; 2)  It shows promising results compared against other state-of-the-art models 3) It's simple yet effective 4) Interpretable 5) Flexible Weaknesses:    1) No comparison between their model and existing ones was provided. 2) They didn't provide any experimental details","label":1,"model":"bloomz","source":"peerread","id":4331}
{"text":"The authors have developed an interesting mixture model that can be used to detect outliers in data sets with missing values.  The method is based on using mixtures of multivariate normal distributions and has been applied successfully by the authors to several real world problems.\nI think this work will find wide application, especially since it does not require any special assumptions about how many or which variables are missing from each observation (unlike other methods such as EM).  I would like to see some more discussion regarding computational issues associated with fitting these models - specifically whether there exist efficient algorithms for finding maximum likelihood estimates when the number of components grows large? This could also include references to software packages implementing your approach. Finally,  you might consider discussing extensions where observations may contain multiple types of missingness patterns","label":1,"model":"bloomz","source":"peerread","id":4332}
{"text":"The authors present an interesting approach to mixture models for detecting anomalies in network traffic data.\nStrengths: \n1) The model is very flexible with respect to how it can be used (e.g., as a classifier).\n2) It has been shown that this method performs well on real world datasets. \n3) This work provides some theoretical insights into why their algorithm works so well Weaknesses: \n1) There are no experiments showing how sensitive the results of the algorithms are when there are changes made to parameters such as number of clusters.  2) No discussion about computational complexity 3) No comparison between different methods 4) No explanation regarding how they handle missing values 5) No description of any visualization tools 6) No mention if these techniques have ever been deployed","label":1,"model":"bloomz","source":"peerread","id":4333}
{"text":"The authors present an interesting new approach that is able to combine both approaches in one single system, i.e., it uses pointers as sentinels when necessary but falls back onto traditional softmax classification otherwise.\nI think this work could be improved by providing some additional details about what exactly happens during training - especially since there are several different components involved here.  For example, I would like to know whether the pointer component always tries to copy previous tokens before producing output? Or does it sometimes just use the softmax distribution instead?  Also, do you need special tricks such as scheduled sampling to train your model efficiently?\nFinally, I'd love to see some analysis showing why this method works so much better compared to other methods... Yes  No  Yes  Yes  Yes","label":1,"model":"bloomz","source":"peerread","id":4334}
{"text":"The authors present an interesting new approach that is able to use both local information as well as global knowledge in its predictions.  The results are impressive but it would be good to see some analysis about why this works better compared to other approaches such as attention mechanisms. \nAnother point worth mentioning here might be: How does the proposed method compare against existing methods when used together? For example, could you combine your approach with an attention mechanism?\nFinally I think it's important to mention whether there were any problems during training\/validation\/testing phase - especially since these kind of architectures tend to overfit quite easily. \n\nOverall I'm positive towards publication! \nStrengths: \n\nWeaknesses: \n\nProblem statement:  State-of-the-art neural networks achieve high accuracy at predicting sequences like sentences or paragraphs, however, they require huge amounts of data and\/or computational resources.   This work presents Pointer-Sentinel Mixture Model (PSMM), a novel deep learning framework designed specifically to address this issue.    PSMM combines two different types of components into one single model:    1.) A \"pointer\" component that allows the model to copy-paste text directly from previous tokens within the same sentence;   2.) An \"sentinel\" component that uses traditional SoftMax classification to generate out-of-vocabulary words based on contextual cues.     Results:     On PTB dataset our PSMM-LSTM model outperforms all previously published results without requiring additional data sources or pre-training steps.        To demonstrate applicability beyond toy datasets,    we propose WikiText Corpus, a large-scale benchmark consisting of Wikipedia articles written in English.      Strengths & Weaknesses:","label":1,"model":"bloomz","source":"peerread","id":4335}
{"text":"The authors present an interesting approach that is based on using tighter bounds in order to improve classification performance of SVM and boosting algorithms.\nThey show how this can be done by introducing new loss functions, which are then used within existing frameworks such as SVMs or AdaBoost. \nThis work has been presented at several conferences but not yet published elsewhere. \n \n The results seem very promising although it would have helped if they had shown some more experimental evidence (e.g., plots) instead of just presenting numbers from their experiments. \n\nOverall I think it's worth publishing since there seems little overlap with other papers already accepted here. However, before publication I'd like them to: \n\n- Show additional experimental evidence - e.g., plots \n- Improve clarity","label":1,"model":"bloomz","source":"peerread","id":4336}
{"text":"The authors present an algorithm for learning linear separators in high dimensions with provable guarantees on its performance.\nStrengths: The main contribution of this work is that it provides tighter bounds than previous works.  This leads to better classification algorithms (in terms of accuracy) when applied to real world data sets. \nWeaknesses: It would be interesting if they could provide some experimental results showing how their method performs compared against other methods such as SVM. \n\nProblem statement: \nWhat does the author want you to know about his\/her research? What are the advantages\/disadvantages?\n\nSummary: In this article we will discuss the following topics:  - Introduction - Previous Work - Our Results - Proofs - Discussion - Future Directions - References - Acknowledgements","label":1,"model":"bloomz","source":"peerread","id":4337}
{"text":"This work presents some interesting ideas but I am not sure if they are really new or novel enough to warrant publication at ICML. The authors should try to make their contribution more clear by providing additional details about how exactly updating the loss function improves results (e.g., what happens when you update the loss? How does this affect training?). They also need to provide experimental evidence showing why using tighter bounds can improve accuracy -the current experiments only show improvements with respect to convergence speed-while making comparisons against other state-of-the-art methods such as SVM-light. \n\nIn summary, there seems to be room for improvement before this manuscript would qualify for submission to ICML. However, since the topic looks promising we encourage the authors to revise their article accordingly so that it may become suitable for presentation at ICML 2015.","label":1,"model":"bloomz","source":"peerread","id":4338}
{"text":"This work presents some interesting ideas for improving the accuracy of machine-learning algorithms.  The authors show how they can use their method with logistic regression (a linear model) but also mention extensions to other models such as SVMs which are more complex than simple linear ones.   They provide experimental results showing improvements using both synthetic data sets and real-world datasets.    This article should be published after minor revisions have been made. Some issues need clarifying before publication. \n\nStrengths: \n1)  A nice idea - updating the loss function used within the training process so that misclassified points further away from the decision boundary get less weighting. \n2)  Extensions beyond Logistic Regression seem promising  3)  Experimental evidence shows improvement compared against state-of-the-art methods 4)  Good explanation","label":1,"model":"bloomz","source":"peerread","id":4339}
{"text":"The dataset is very useful and I am looking forward to using it in my research on theorem proving with machine learning algorithms.\nI have one question about how HolStep was created, which might be of interest also to other researchers who are interested in this topic.  How did you decide what theorems should go into your database? Did you use some kind of automated process or were they all hand-coded by humans?  It would be interesting if there could be an explanation here as well that describes exactly how the data set has been constructed so far (e.g., number of examples per concept). Thank you! Yes we used human coding only at present time but plan to automate most parts soon. We will add more information later when our work gets published.","label":1,"model":"bloomz","source":"peerread","id":4340}
{"text":"The authors present HolStep as an open source dataset of higher order logic theorem proving problems that can be used to train machine learning models.\nThey provide some examples on how they have already applied their data set in practice using neural networks trained with TensorFlow. \nI think this is very interesting work because it provides researchers working on automated reasoning algorithms (e.g., SAT solvers) access to large amounts of training data which will hopefully lead them towards more efficient solutions. \n \n The main strength I see here lies in the fact that these datasets are publicly available online so anyone interested may use them without any restrictions whatsoever.  \n \n However, there seems to me one major drawback when compared against other similar resources such as TPTP or SMT-LIB. While those two libraries contain many different types of theories from various domains ranging from mathematics over computer science up to artificial intelligence theory, HolStep only contains theorems related to computational complexity theory. This means that while you might find useful information about certain aspects of your algorithm within its scope, if you're looking at something outside of computational complexity theory you'll probably not get much out of it.","label":1,"model":"bloomz","source":"peerread","id":4341}
{"text":"The authors present an interesting idea in using ML techniques to improve automated theorem provers.  They provide some preliminary experiments with several different types of ML algorithms but do not yet have any significant conclusions about which ones are most promising.\nI think it would help if they could explain how exactly HolStep is useful beyond just being another data source for researchers interested in improving automated theorem proving systems.   What does having access to large amounts of human-written proof fragments allow you to learn? How will this knowledge benefit other areas besides theorem proving?  For example, what insights might one gain by studying why humans choose certain tactics over others when trying to prove theorems? This information may even lead to better ways of teaching logic courses! I am curious as well whether there were any unexpected difficulties encountered while collecting the data...","label":1,"model":"bloomz","source":"peerread","id":4342}
{"text":"The authors present an interesting idea in using ML techniques to improve automated theorem proving.  They provide some initial experiments with several different types of ML algorithms which are promising but need further investigation.   I think it would help if they provided more details about how exactly each algorithm works as well as its performance compared to other approaches.    Also there is no discussion whether any of those methods could scale up beyond toy examples like the ones presented here. Overall it's still too early to judge the potential impact of this work so I'm giving them 3 stars at this point. Strengths:  This research presents a novel approach towards improving automated theorem proving through machine learning. \nWeaknesses: \nThere needs to be additional experimentation done before one can determine the true value of this project. \n\nRecommendations: \n\nMore information should be included regarding specifics of the proposed methods\/algorithms\/models","label":1,"model":"bloomz","source":"peerread","id":4343}
{"text":"The author has done an excellent job in explaining how deep neural networks are used to approximate functions and why they work so well.  The book is very easy-to-read, with plenty of examples that make it clear what each concept means.\nI highly recommend this book! It will be useful not only as a reference guide but also as a textbook on function approximation using DNNs. I have already recommended it to my students at Georgia Tech who study machine learning algorithms. 5 stars! -Dr. David M. Blei  Georgia Institute of Technology This book provides a comprehensive introduction into the field of deep neural network (DNN) based function approximators from both theoretical and practical perspectives. In particular, the authors present several state-of-the-art techniques along with their applications ranging from computer vision problems such as image classification\/segmentation\/detection, video analysis, face recognition etc., speech processing including voice conversion\/synthesis\/morphing, audio tagging\/classification\/retrieval, natural language processing like text generation\/inference\/deep translation, bioinformatics tasks involving protein folding prediction\/protein-protein interaction detection\/prediction of drug-target interactions, recommender systems, computational biology among others. \nThis book covers all major topics related to DNN-based function approximator design which include various types of feed-forward architectures, recurrent architectures, convolutional architectures, autoencoders, variational auto-encoder models, generative adversarial nets, attention mechanisms, reinforcement learning methods, unsupervised feature extraction approaches, semi-supervised learning strategies, transfer learning schemes, model compression frameworks, data augmentation techniques, regularization methods, optimization procedures, hyperparameter tuning tools, benchmark datasets, experimental results & comparisons between different competing solutions.","label":1,"model":"bloomz","source":"peerread","id":4344}
{"text":"The main idea of this article is to use deep neural networks (DNNs) in function approximation problems.\nIn order to do so they introduce two new algorithms called DQN and DDQN which are based on Q-learning with experience replay memory. \nThey show that these methods can be used successfully as an alternative approach compared to other state-of-the-art techniques such as LSTM-RNNs. \n\nStrengths:\n\n- The authors present their work clearly - They provide experimental results showing how well their proposed method works \n\nWeaknesses: \n- It would have been nice if there were more experiments done using different datasets\/functions - There isn't much discussion about why it might not perform better than RNN-LSTMs? What could go wrong? \n\nOverall I think it's interesting research but some additional experimentation should be performed before publication","label":1,"model":"bloomz","source":"peerread","id":4345}
{"text":"The authors present some interesting ideas but I think they need more work before publication.  The first part seems too easy compared to what follows later; it would have helped if this had been explained better or omitted altogether as it's only tangential to their main point anyway.\nIn addition, the second half needs further explanation - how do you prove something like \"there exists a constant $c > 0$ such that...\"? It should also be made clear whether you're talking about one-dimensional or multi-dimensional problems here. Finally, I'd suggest rewriting parts where you've used terms like \"uniformly approximates\"; that's confusing at best! \nOverall though, I'm impressed enough by your idea to give you my vote so long as you address all those points above. Good luck!","label":1,"model":"bloomz","source":"peerread","id":4346}
{"text":"The authors present some interesting ideas about how many neurons one needs when approximating differentiable functions using feed-forward neural nets.  The idea behind their approach seems quite natural but I am unsure whether it will lead to any practical insights into this topic as they do not provide numerical examples.   In addition, the proof techniques used seem rather complicated compared to other approaches presented recently.   \n\nStrengths: \nThis work provides new theoretical insight into the advantages of deeper architectures. \n\nWeaknesses: \n\nI would like more details from the author regarding his\/her motivation for writing this article.  \n\nIn particular,  if you have read recent papers  related to your research area,   please explain where yours fits within them?    Also,    could you give me references   to previous works?  Finally,    describe briefly the contributions made by your own study","label":1,"model":"bloomz","source":"peerread","id":4347}
{"text":"The authors have done an excellent job of presenting their ideas in this article.  The concepts are well explained and easy to understand, which is important when trying to explain something new like Hypernetworking.\nI think that it would be helpful if they could provide more examples or case studies so people can get some idea how these networks work on real world problems.   Also I feel there should be at least one example where hypernetworks were used instead of traditional network models (e.g., social networking).   Finally,  since the concept has been around only recently  it's hard to find any information about existing implementations.    Overall though, I'm very impressed with what they've accomplished here! Yes - we need more concrete examples\/case-studies as you suggest. We will try our best to add them into future versions of the manuscript.","label":1,"model":"bloomz","source":"peerread","id":4348}
{"text":"The authors present an interesting idea of using hypernetwork to represent social networks in this article.\nHowever there are some issues that need further discussion before it can be accepted for publication.  The main issue is how do you define \"hyperedges\"? In fact I think it's not necessary at all because we don't have any examples on how they work with real data sets yet. \nAnother thing worth mentioning here is about the concept of \"social networks\" itself which has been discussed extensively over years but still lacks consensus among researchers as well as practitioners.   It would also help if more details were provided regarding how these concepts could be applied into practice such as identifying communities within organizations etc... \nOverall though I'm quite positive towards their ideas so I'd like them to revise again after addressing my comments above","label":1,"model":"bloomz","source":"peerread","id":4349}
{"text":"The authors have done some interesting research here but I think they need more explanation about how their method works.  For example,  what does it mean by \"relaxed form\"? How do you train both models together?  What happens if there isn't enough information from your training data set? The abstract doesn't really explain these things clearly so it's hard to understand exactly what's going on without reading through all the text first. \n\nI would suggest adding a few sentences at the beginning explaining why you're interested in doing this type of research (why did you choose to use hypernetworks?) and then briefly describe each step along the way before moving onto something else.  \n\nAlso try not to repeat yourself too much throughout - sometimes you'll mention something once or twice only to say again later down the page which makes it harder to read than necessary!","label":1,"model":"bloomz","source":"peerread","id":4350}
{"text":"The authors present their idea very clearly throughout the text.  The experiments show promising results but there seems no discussion about how they compare against other methods such as attention mechanism which has been widely used recently. \nStrengths: \n1) It presents new ideas related to deep learning architectures 2) It shows good experimental results  3) It provides references Weaknesses: 1) No comparison between different approaches (attention mechanisms etc.) 2) There seem some typos\/errors in the manuscript. For example,  \"hypernetworks\" should probably read \"hypernetwork\" at several places; similarly \"machine translation\" may need to change into \"neural machine translation\"; finally \"they\" instead of \"he\/she\" appears twice near the beginning of page 4.3.2. These errors do not affect understanding though","label":1,"model":"bloomz","source":"peerread","id":4351}
{"text":"The authors present their work on evaluating chatbot conversations in this article, which is very relevant and timely as we are seeing more and more conversational agents being deployed online.\nI think that it would be interesting if they could also discuss how these results can help improve existing systems (e.g., by providing feedback) or even create new ones.  For example, one of my research interests has been developing methods for improving user experience with intelligent assistants such as Siri\/Google Now etc.. I am curious whether there might be any connections between what you have done here and some of your previous works? Yes  No  No  Yes   No    No  Yes  Yes  Yes  Yes   Yes  Yes  Yes  No  Yes  Yes  No","label":1,"model":"bloomz","source":"peerread","id":4352}
{"text":"The authors present their work on evaluating chatbot conversations in this article.\nThey propose using machine learning techniques for automatically assessing whether human users are satisfied with bot-generated answers during conversation. \nThis is done through training models that learn from user feedback (evaluated via surveys) as well as features extracted directly from the text of the dialogues themselves. \n \n The proposed approach has been evaluated against two datasets collected over different domains - travel planning and restaurant reservations respectively. \n\nOverall I think it's very interesting research which could be useful both academically but also practically if implemented into real-world applications such as customer service bots etc.  \n\nI have some minor comments\/questions about it though: \n\n1) \n \n\nYes \n \n\n\n\nNo \n\n\n2) Yes \n3) No","label":1,"model":"bloomz","source":"peerread","id":4353}
{"text":"The authors have done some good work in their study on automatically evaluating dialogues by formulating it into machine-learning problems. The results seem promising but I am not sure whether they will be able to solve all issues related to biases or other factors affecting current automatic evaluation methods. It would be interesting if more details about how these biases were handled could be provided so readers know what challenges remain ahead before fully-automatic evaluation becomes possible. \n \n Overall, it's worth reading! \n\nAuthor Response: Yes, there remains significant room for improvement - our goal was simply to demonstrate feasibility rather than perfection. \n\nWe agree that providing additional information may help clarify things further; however, due to space constraints, many aspects had already been discussed elsewhere. For example, see Liu et al. (2016), which describes how we handle biases; and Li et al. (2017a), where we discuss generalization across different datasets.","label":1,"model":"bloomz","source":"peerread","id":4354}
{"text":"The authors propose a novel approach towards automatically assessing dialogues in open-domain settings. The proposed method uses machine-learning techniques on large datasets containing user ratings from humans who have evaluated different types of answers generated by various systems. This work has several advantages over previous approaches which relied solely on text similarity measures between candidate answer texts and reference texts provided alongside questions. \nIn particular, their results suggest that:  - Their method correlates better with human judgement compared to standard text-similarity based methods.  - It works well even when applied to previously-unseen dialogue systems without any additional tuning required. However there are some issues worth noting about how they conducted experiments. \n\nFirst off all, it's not clear whether the data used was collected specifically for the purpose of developing automated scoring algorithms like those presented here; if so, one would expect more consistent performance across different test sets since these were designed to be similar enough to each other. Second, while the authors report high correlations between predicted values produced by their algorithm and actual human-judged ones, I wonder why they didn't use statistical significance tests to confirm that differences observed weren't due simply to chance fluctuations?","label":1,"model":"bloomz","source":"peerread","id":4355}
{"text":"The authors have presented an interesting and novel approach to information fusion in this work, which is based on gating multimodal units (GMUs). The GMU concept has been introduced by the same group of researchers as well before. \n \n In their previous works they showed that it can be used successfully with different types of data sources such as audio-visual speech recognition or human motion tracking systems.\n \nIn contrast to other approaches like Dempster-Shafer theory or fuzzy logic methods there are no additional parameters needed here. \nThis makes the method very easy applicable especially if only little knowledge about the underlying system exists. This might also explain why the proposed algorithm was able to outperform state-of-the-art algorithms using similar input features but more complex models during experiments conducted within the scope of this publication.  \n\n \n\n\nHowever I would recommend some minor changes regarding the presentation style. \n\nFirst off all please use English language throughout your manuscript instead of German abbreviations wherever possible.","label":1,"model":"bloomz","source":"peerread","id":4356}
{"text":"The authors present an interesting approach to information fusion using gating units in multimodal networks.\nTheir work is based on previous research that has shown how such gates can be used effectively with recurrent neural network architectures.  The main contribution of this article lies in showing how these gate mechanisms are useful when applied across multiple modalities (e.g., audio-visual speech recognition).\nThis study also shows some promising results from applying their method to real-world data sets. \nStrengths: \n1) This is one of few papers addressing multi-modal fusion problems within deep learning frameworks. \n\n2) It presents novel ideas about combining different types of features into single representations via gating functions.  \n\n3) They show good performance improvements over baseline methods on several benchmark datasets including AMI Meeting Corpus and TIMIT Speech Database Weaknesses:","label":1,"model":"bloomz","source":"peerread","id":4357}
{"text":"The authors present their new approach called \"Gated Multimodal Unit\" that can fuse information coming from multiple sources into one common feature space.\nThey evaluate it against several baselines on movie genres by combining text features extracted from plots or posters along with visual features obtained via deep convolutional nets trained over images representing those same films. \nTheir results show improvements compared to previous works but also highlight some issues such as: \n-The use of only two types of inputs: textual and visual -It would have been interesting if they had included audio inputs too  -No comparison between the proposed method and state-of-the-art methods like DenseNet121 They should address these points before publishing again. Overall I think it's worthwhile publication though! Yes  This article has potential interest because there are few papers dealing specifically with multi-source\/multimodality problems","label":1,"model":"bloomz","source":"peerread","id":4358}
{"text":"The authors present their new approach called \"Gated Multimodal Unit\" that combines information coming from multiple sources into one common feature space. They show promising results when applied it to movie genre recognition task but also provide insights about its limitations. \nStrengths: \n1.) A very interesting idea 2.) Very good experimental evaluation 3.) Large scale publically available dataset Weaknesses: \n1.) No theoretical analysis yet 2.) Limited number of experiments performed  We believe there are still many open questions regarding the proposed method such as: 1.)  How does the gate mechanism affect the final decision? What happens if you remove some input channels completely?  2.)  Does adding more layers improve accuracy furthermore? 3.)  Is there any way we can use transfer learning techniques here?\nWe hope these comments will help the author(s).","label":1,"model":"bloomz","source":"peerread","id":4359}
{"text":"The authors propose an unsupervised reward learning method to learn from demonstrations in imitation learning problems, where no explicit rewards are available.\nThey show that their approach can be used with deep neural networks and is able to outperform supervised methods on several benchmark tasks.  The proposed algorithm has two main components - one which estimates the value of each demonstration using Q-learning (with experience replay); another component which uses this estimated value as input into a policy gradient based optimisation scheme. \nI think it would make sense if they also compared against other state-of-the-art approaches such as DAgger or PPO. \n\nOverall I find the work interesting but there seems little novelty here beyond what was already known about combining RL algorithms like these together.  \n\nIn particular, the use of Q-Learning does not seem very original since it's been around for decades now...","label":1,"model":"bloomz","source":"peerread","id":4360}
{"text":"The authors propose an unsupervised reward mechanism to encourage imitative behavior in robots.\nThis is very interesting work that has potential applications beyond robotics.  The proposed method can be used as part of any learning algorithm where there are multiple agents interacting with each other (e.g., multi-agent reinforcement learning). I think this could also have some impact on human-robot interaction research. \nI am not sure if it would make sense to use such rewards when training autonomous vehicles since they do not interact directly with humans but rather follow traffic rules. Strengths: \n1) This approach does not require explicit supervision from external sources like teachers\/experts which makes it more practical than supervised approaches. \n\n2) It works well even without knowing how many different behaviors exist among all possible actions taken by the agent(s).\n\n3) There may be several ways to extend their current framework so that it can handle continuous action spaces instead of discrete ones.  \n\nWeaknesses: \n\n1) They only tested their model using simulated environments; therefore, it's unclear whether similar results will hold true under real-world conditions.    2) Their experiments were conducted over relatively short time periods compared to typical robotic tasks","label":1,"model":"bloomz","source":"peerread","id":4361}
{"text":"This work presents a novel approach towards unsupervised imitation learning with applications to autonomous robots operating in unstructured environments.  It addresses some important challenges such as how to define rewards based solely on observations rather than expert annotations.   I think this research will have significant impact if successful because it could lead to more robust systems capable of generalizing beyond what was observed during training.  \n\nThe authors should consider addressing several questions raised below before publication. \n\n1)  How do you handle situations where there may exist different ways to achieve each step? For example, one way might involve pushing while another involves pulling?  What happens if your system observes something unexpected like someone walking into frame instead of standing still?   2)  Are all possible outcomes considered equally likely at every state?","label":1,"model":"bloomz","source":"peerread","id":4362}
{"text":"The authors have done some good work here.  They show how they can train robots with little data inputted into them.   However,  I feel like there could've been more information about this topic presented throughout the article instead of having to read between lines.    For example,    if you were trying to teach your dog new tricks,   wouldn't you want him\/her to know exactly why he\/she was being praised?    It would make sense to praise certain behaviors over others so as to encourage repetition of those specific actions\/behaviors... \nIn addition, while the idea behind training a robot through imitating humans seems great,... ...the way it's explained isn't very clear....   How do you get around the fact that people don't always follow the same pattern?  What happens during times where no one else is available to help out? ...","label":1,"model":"bloomz","source":"peerread","id":4363}
{"text":"The authors present an interesting approach to evaluate distributed representations by clustering them into language specific clusters and then evaluating these clusters using standard metrics such as purity, entropy etc.\nI think that this is a very good idea but I have some concerns about how it was implemented in practice. \n\nFirst off there are several issues with the evaluation procedure itself which should be addressed before publication.  The first issue regards the fact that the number of languages used (5) seems too small to draw any conclusions from the results presented here. \nSecond, the choice of datasets also needs more explanation - why did you choose only 5 out of all available datasets? What makes those particular ones so special?\nThirdly, what does \"clustering\" mean exactly?  How do we know if two vectors belong together or not?\n\nFinally,  I'm wondering whether your method would work on other types of data than word embeddings like e.g., sentence embeddings","label":1,"model":"bloomz","source":"peerread","id":4364}
{"text":"The authors present an interesting approach to evaluate distributed representations in multilingual settings using clustering algorithms.\nStrengths: The proposed method is simple yet effective as it can be easily implemented on any existing model without requiring additional data collection efforts from users. \nWeaknesses: \n1) It does not consider word order information which may lead to incorrect cluster assignments when dealing with languages that have different ordering conventions (e.g., English vs Chinese). 2) No evaluation has been conducted against other state-of-the-art methods such as BERT. 3) There are no experiments showing how well this algorithm performs compared to human-generated clusters.  4) In addition, there should also be some discussion about possible applications where these techniques could potentially benefit. Overall, I think it's worth publishing but more work needs to be done before publication","label":1,"model":"bloomz","source":"peerread","id":4365}
{"text":"The authors present their work very clearly.  They have done extensive research into how they can generate data that is useful when evaluating distributed representations (word vectors). I think it would make sense if there was some discussion about why these types of evaluations are important - what do you hope will come from them? What does your proposed method add over existing methods?  How could other researchers use or build upon your approach?\nI also wonder whether it's possible to get more information about the Wikipedia pages being clustered together by looking at the cluster labels themselves... For example, one might expect that all articles under the label \"Music\" should share certain properties such as genre etc., but perhaps not so much those labeled \"Science fiction novels\".... \nFinally,  I'm curious about how many different languages were included in the clustering process. \n\nAnswer Yes","label":1,"model":"bloomz","source":"peerread","id":4366}
{"text":"The authors present their work well but I think they should have included more details about how exactly they generated these datasets (e.g., number of documents that were crawled). Also it would help if there was some discussion as to why they chose 500 items per cluster instead of 1000 - is there any reason? Finally, it's not clear from reading the abstract whether all of the data presented here has been made publicly available yet.  Strengths: This article presents interesting ideas related to evaluating distributed representations using clustering algorithms Weaknesses: It could use additional explanation regarding its methods. For example, how many documents did you crawl? How do you know your method isn't biased towards certain topics\/domains?  Questions: What are the advantages over other approaches like WordSim353?","label":1,"model":"bloomz","source":"peerread","id":4367}
{"text":"The authors present an interesting approach to extract networks from time series data, which is very relevant in many fields of science and engineering.\nI have some comments on this manuscript as follows: \n1) The proposed method can be applied only when there are no missing values (NA). However, it may happen that we do not know all possible links between nodes at each timestamp due to incomplete information or measurement errors. \n\n\n2) In Figure 1 , the number of extracted edges seems too large compared with other figures. \n\n3) It would also help if you could provide more details about how your algorithm works by explaining what parameters were used such as: \n\n4) How did you choose the thresholds? 5) What does \"shift\" mean exactly?  6) Why was the shift parameter chosen so small?\n\n7) Is there any way to visualize the results using network visualization software like Gephi? \n\n8) Finally I think it's important to mention whether these methods will scale up well enough to handle larger datasets","label":1,"model":"bloomz","source":"peerread","id":4368}
{"text":"The authors present an interesting approach to extracting information from text documents.\nStrengths:\n1) The method is simple yet effective in practice; 2) It can be easily implemented on GPUs for fast processing of large data sets; 3) It has been tested with real-world datasets (e.g., news articles). Weaknesses: 1) There are no experiments that compare their proposed model against other state-of-the-art models such as LSTM-based ones.  In addition,  it would also be helpful if they could provide some analysis about how well this new model performs compared to existing methods when applied to different types of tasks\/datasets.   For example,   do  these two approaches perform similarly   across all kinds of applications? Yes2No3 No4Yes","label":1,"model":"bloomz","source":"peerread","id":4369}
{"text":"This work presents a novel approach towards representation learning from graph structured data that has several advantages over existing methods.\nIt introduces new concepts such as shift-aggregate-extract networks (SAENs) or hierarchy decomposition trees with their associated algorithms. The authors show how these can be used effectively within a deep neural net framework using efficient training techniques based on backpropagation through structure. \nIn addition they introduce a concept called \"domain compression\" where similar substructures across different nodes at one level of abstraction are identified and represented only once thereby reducing both storage requirements and computational complexity significantly.  This results in substantial improvements compared to previous approaches especially when dealing with large scale problems like those arising e.g., in recommendation systems. \n\nOverall this research makes important contributions not just to machine learning but also to computer science more generally since it provides insights about how we might represent knowledge efficiently in future intelligent agents.","label":1,"model":"bloomz","source":"peerread","id":4370}
{"text":"Strengths: \n1) The proposed model can be applied directly to large scale networks with millions of nodes. \n2) It has been shown that it performs better than other models such as DeepWalk and node2vec when dealing with large-scale networks. \n \n Weaknesses: \n1) There isn't much information about how this works compared to existing methods like Deepwalk and Node2Vec. \n\n2) How does one know if their dataset fits well within these parameters? 3) What happens if there aren't any patterns present? \n\n4) Doesn't seem very scalable since you have to do all sorts of calculations at once (e.g., calculating eigenvectors). 5) No mention was made regarding privacy issues related to using your system. 6) I would love more details\/information","label":1,"model":"bloomz","source":"peerread","id":4371}
{"text":"This is an interesting and well-written article that describes how to use differentiable programming techniques in order to implement a compiler from a subset of FORTH into Python, which can then be used as part of a reinforcement learning algorithm.\nThe authors describe their approach by first introducing some background on differentiable programming (DP) using TensorFlow's graph optimizer API; they also provide examples showing how DP works with this library.  They then explain how to apply these ideas when implementing a compiler between two languages - specifically, one language being a subset of FORTH called Lisp-like Interactive Language System or LILE, while the other is Python. \nFinally, they showcase several experiments where they demonstrate how such a system could potentially be useful within machine learning applications. \n\nOverall I think it's very nice work!","label":1,"model":"bloomz","source":"peerread","id":4372}
{"text":"The author describes an interpreter for differentiable programs written in a dialect of FORTH called D-FORTH.\nDifferential programming is a technique that allows one to compute derivatives symbolically with respect to program inputs.  The author's goal was to create a system where symbolic differentiation could be performed on arbitrary computer code without requiring any changes to existing software libraries (e.g., BLAS\/LAPACK).\nThis work has several interesting features including:  1) It uses automatic differentiation techniques from numerical analysis; 2) it supports both forward-mode and reverse-mode differentiation; 3) it can differentiate between multiple versions of the same function at once using operator overloading mechanisms similar to those found in modern object-oriented languages such as Java and C++. \nHowever there are some issues worth noting about this work: \n1)  There appears to have been little effort made towards optimizing performance beyond ensuring correctness.   This may limit its usefulness when applied to large problems involving many functions and\/or variables.   ...","label":1,"model":"bloomz","source":"peerread","id":4373}
{"text":"This work presents an interesting approach towards incorporating structured information about programs when learning their behavior using deep reinforcement learning (DRL). The authors propose a novel way of implementing DRL by leveraging existing software infrastructure instead of building it from scratch which makes sense especially if one wants to focus more on algorithm development than engineering details.\nThe proposed method has several advantages compared to other approaches including faster experimentation time due to reduced overheads associated with developing new algorithms\/software frameworks etc., easier integration within existing systems since they already exist, higher flexibility because you don't have to change your system architecture everytime there is some modification required in order to implement something new. \nHowever I think it's important to mention here that while these benefits make the presented idea very appealing at first glance, it might not always be possible depending upon what kind of applications you're trying to build. \n\nIn my opinion, the most exciting part of the article was the introduction of Neural Program Optimizations...","label":1,"model":"bloomz","source":"peerread","id":4374}
{"text":"The authors propose an interesting approach towards incorporating prior knowledge about programs when learning them using deep nets. \n \n The idea behind their work is to provide users with a way to specify parts of a program they want to train while leaving other parts fixed (e.g., constants). This allows one to focus computational resources where it matters most without having to manually design features beforehand. \n \n I think there could have been more discussion around why exactly Forth was chosen instead of another language like Lisp which has similar properties? Also, some explanation would've helped me understand whether the proposed method scales well beyond toy examples? \n \n Finally, I'd love if you had included a few references at least pointing out related works along these lines so readers know what's going on here! \nOverall though it's definitely worth reading because it provides new insights into combining structured priors with deep net models. \n\nStrengths:","label":1,"model":"bloomz","source":"peerread","id":4375}
{"text":"The authors present an interesting approach to solving Hamilton-Jacobi-Isaacs (HJI) partial differential equations using neural networks and recursive regression.  The results are promising, but there is room for improvement in several areas.\nFirst of all, it would be helpful if they could provide more details on how their method works by explaining what happens at each step along the way.   For example,  when do you use your first-order approximation?  What does this mean? How did you choose these parameters?\nAlso,   I think that some explanation should be provided as to why the second order term was chosen over other terms such as $(x-y)^2$ or $x^y$.    It seems like the choice may have been made based solely upon numerical experiments rather than any theoretical justification.     Finally,    while the authors claim that their algorithm can solve problems involving multiple agents simultaneously, no examples were presented where two or three agents interacted together.","label":1,"model":"bloomz","source":"peerread","id":4376}
{"text":"The authors present an interesting approach to solving Hamilton-Jacobi-Isaacs (HJI) partial differential equations using neural networks trained on data generated from recursive regression.\nThis is not exactly new research but it does provide some insight into how this type of equation can be solved in practice.  The main contribution here seems to lie more so in the training process than anything else - that being said there are still several issues worth noting.  \n\nFirst off I think it's important for readers who aren't familiar with these types of problems to understand why they need to solve them at all.   In particular I'd like to see something about how one would use such solutions as part of their overall strategy when playing games where optimal control theory plays a role.    This could include things like:  How do you know if your solution actually solves the original problem?","label":1,"model":"bloomz","source":"peerread","id":4377}
{"text":"The authors propose a novel method based on recursive least squares combined with deep recurrent neural network architectures to solve a challenging nonlinear optimization problem arising from optimal control theory.  The proposed methodology leverages recent advances in both areas - namely, the use of RNNs within RL algorithms and the application of DRL techniques to solving PDEs.\nIn addition, they provide extensive numerical experiments demonstrating their effectiveness over existing methods. \n\nOverall I think it's very interesting work! \nI would like to see more details about how exactly does the proposed technique perform compared against other approaches such as policy gradient or value iteration. \nAlso, since the focus here seems to be on approximation rather than exact solutions, I'd also recommend including additional results showing convergence rates and\/or bounds on error between the true solution and your estimate.  \n\nFinally,","label":1,"model":"bloomz","source":"peerread","id":4378}
{"text":"The authors propose a novel method for solving PDEs through recursive regression on artificial neural networks; they show how their proposed technique compares favorably against existing methods.  The article presents several examples illustrating its effectiveness; however, I would like more details about why certain parameters were chosen over others when comparing results across different algorithms. \nStrengths & Weaknesses: Strengths include clear presentation of ideas and comparisons between competing techniques. \n\nWeaknesses include: lack of explanation behind parameter choices during comparison experiments. Overall, it's well written but could use additional explanations\/clarifications throughout. It also lacks references to other work done along these lines so readers may not know whether they're reading something new or just rediscovering old ground. 4 stars overall. Author Response: Thank you very much! We will try our best to improve upon your suggestions.","label":1,"model":"bloomz","source":"peerread","id":4379}
{"text":"The authors propose an algorithm to learn activation functions of deep neural nets nonparametrically, which is very interesting and important work.\nHowever there are some issues that need further clarifications or improvements as follows: \n1) The proposed method can be applied only when we have enough data points (i.e., samples). How does it perform on small datasets? 2) What do you mean by \"nonparametric\" learning here ? 3) In Figure 1 , what is the meaning of \"Non-Parametric\"? 4) Why did they use two different methods to train their networks with same dataset? 5) Is this approach scalable?  6) Can I get more information about how many layers were used in each network? 7) Are these results statistically significant?\n8) Does your model generalize well across multiple tasks\/datasets? 9) Do you think this could lead to new insights into DNNs?","label":1,"model":"bloomz","source":"peerread","id":4380}
{"text":"The authors propose an algorithm to learn activation functions of deep neural nets nonparametrically.\nThey show that their method can achieve comparable performance with state-of-the-art methods on several benchmark datasets. \nHowever, there are some issues which need further investigation. \n\nFirst is how they choose the number of layers for each network? The current approach seems ad hoc.  Second,  it would be interesting if  they could provide more insights into why their proposed model works well compared to other models such as RNNs (Recurrent Neural Networks). \n\nStrengths: \n1) It achieves competitive results against existing algorithms without requiring any parameter tuning during training time.  \n\n2) They use simple networks so it's easy to implement this idea using GPUs.    Weaknesses: \n1) How do we know when our learning process has converged?  2) What happens if you have multiple classes instead of binary classification problems? \n\n3) Why does their method work better than others?\n\n4) Isn't it possible to train deeper networks like ResNet-50?","label":1,"model":"bloomz","source":"peerread","id":4381}
{"text":"This is an interesting work which provides new insights into how one should design DNNs.  The authors use some nice tricks such as using kernel density estimators (KDE) instead of simple histograms when estimating the distribution over activations at hidden nodes.\nThe results are very promising but I have two concerns about this work:  1)  How do you choose your bandwidth? 2)  What happens if there exists no data point close enough to any particular value?  For example, consider the case where all values lie between 0 and 10 except for one outlier lying outside these bounds - what will happen then?\nOverall though it seems like a good piece of research! \nI would recommend publishing after addressing my comments above. Yes \n\nAuthors: Thank you so much for reviewing our manuscript. Your feedback has been extremely helpful.","label":1,"model":"bloomz","source":"peerread","id":4382}
{"text":"The authors propose an interesting approach to improve generalization ability of DNNs using non-parametric activation functions (NPAFs). The proposed method is theoretically justified and experimentally validated through extensive experiments conducted over several benchmark data sets including MNIST, CIFAR-10\/100, SVHN etc.. In particular, they show how NPAF-based models outperform their parametric counterparts such as ReLU\/LeakyReLU\/PReLU\/Sigmoid etc., especially when trained from scratch without any pre-training stage. \nOverall this work provides new insights into improving the current practice of designing DNN architectures which will benefit both researchers working towards building more powerful machine learning systems and practitioners who are interested in deploying these technologies.  Strengths:  1) A comprehensive experimental study showing superiority of NPAs against existing methods 2) Novelty -the idea behind NPAF has not been explored previously 3) Easy-to-implement algorithm Weaknesses: \n1) No comparison between different types of NPAFs","label":1,"model":"bloomz","source":"peerread","id":4383}
{"text":"This is an interesting and well-written article that describes how to use unsupervised learning techniques in order to find generalizable features from data sets.  The authors describe several different algorithms, including clustering methods such as k-means or hierarchical cluster analysis (HCA), dimensionality reduction approaches like principal component analysis (PCA) or multidimensional scaling (MDS),  and manifold learning models such as Isomap.   They also provide examples of these techniques applied on real-world datasets.\nThe main contribution of this work lies in showing how one can combine multiple unsupervised learning techniques into a single model by using ensemble learning strategies.    This approach allows us not only to extract useful information but it may be used to improve prediction accuracy when compared with traditional supervised machine-learning techniques. \nI think it's important to mention here some limitations associated with this type of research which are related mainly to:  1)  lack of interpretability; 2)  computational complexity","label":1,"model":"bloomz","source":"peerread","id":4384}
{"text":"The authors present an interesting approach to unsupervised learning that is based on finding generalizable features in data sets.  The main idea of their method is to find clusters within each class (clustering) which are similar across classes.\nStrengths:  This work has several advantages over other clustering methods such as k-means.   First it does not require any prior knowledge about number of clusters for different datasets; secondly,  unlike most existing algorithms this algorithm can be applied directly without requiring preprocessing steps like feature selection etc., thirdly its computational complexity scales linearly with respect to dataset size rather than quadratic scaling associated with many other clustering techniques.    Finally they show how these clusterings can be used effectively to perform classification tasks using only unlabeled examples from target domain. \nWeaknesses: \nThere seems no obvious way to extend this technique beyond binary problems i.e. when there are two possible outcomes per example e.g. positive\/negative sentiment prediction task.  \nAlso I am curious if one could use some kind of regularization term here so that we don't get too much overlap between clusters?","label":1,"model":"bloomz","source":"peerread","id":4385}
{"text":"The authors present interesting results on how they used unsupervised learning techniques to improve their predictions.  The experiments are well designed with clear explanations.\nI have only one minor comment regarding Figure 3 . It would be helpful if you could add some explanation as to why your method works so much better than other methods (e.g. LSTM) when it comes to out-of-distribution data points. I think it's because those models tend not to perform very well at all out there while yours does quite well even though you're using fewer parameters overall? Yes  This was my thought too! Thank you for pointing me towards Lake's work - I'll definitely read up more on his ideas now :)  No  I'm glad our findings align!  Thanks again! Yes  Great question!","label":1,"model":"bloomz","source":"peerread","id":4386}
{"text":"The authors present their approach for finding \"generalizability\" from data using unsupervised methods on a toy example (stability of towering blocks) with promising results.\nStrengths: \n1. The idea behind the method seems sound. \n2. They show some nice visualizations which help explain how it works. \n\nWeaknesses: \n1. It would be good if they could provide more details regarding the exact implementation used here - e.g. What was the architecture? How many layers were there?\n2. There are no quantitative comparisons between different approaches so it's hard to assess whether this one really does outperform others. 3. It's not clear why exactly this particular choice of objective function worked well enough to produce useful features but other choices didn't","label":1,"model":"bloomz","source":"peerread","id":4387}
{"text":"The book is very well written, easy to read but also contains many details that are not available in other books on deep learning.\nI would recommend this book as an excellent reference guide if you want to learn about deep learning using sets or point clouds.  The author has done extensive research into these topics which makes it clear why he chose them over others such as images etc. \nThere were some typos\/grammatical errors throughout (which I think can be attributed to the fact that it's still under development) so please correct those before publication! \nOverall 4 stars  - good job! \n\nThanks again! Yes, there will always be mistakes when writing something like this from scratch... But we have fixed most of them now :)  Thanks for your feedback","label":1,"model":"bloomz","source":"peerread","id":4388}
{"text":"The main contribution of this work is to provide an end-to-end deep learning framework for point cloud processing that can be used in many applications such as autonomous driving.\nIn particular, they propose a novel network architecture called DeepSets which combines set-processing operations (union\/intersection) into standard convolutional layers.  The authors show how their approach outperforms state-of-the-art methods on several benchmark datasets including KITTI object detection dataset. \nStrengths: \n1. They introduce new techniques combining sets and points clouds together using CNNs.\n2. Their method achieves better results than previous works based only on 3D data without any additional information like RGB images from cameras.\n3. It also shows good performance when applied directly to raw LiDAR scans instead of preprocessed ones Weaknesses:","label":1,"model":"bloomz","source":"peerread","id":4389}
{"text":"The authors present an interesting approach that can be used on sets or clouds (i.e., points). The idea behind it seems very promising but I am not sure how much work there still needs to do before one could apply these ideas successfully outside toy problems such as those presented here.\nI think more experiments are needed to showcase their method's strengths over other approaches like graph convolutions which have been shown to outperform them so far. \n\nIn particular, they should try to compare against GraphSAGE [?] , which was recently introduced at NeurIPS 2018.  It would also help if they provide some intuition about why their model works better than others when dealing with sets instead of graphs.  \n\nOverall though, it's good to see new research being done into this area!","label":1,"model":"bloomz","source":"peerread","id":4390}
{"text":"The authors present an interesting approach that can be used on sets (e.g., images) which are not necessarily ordered but have some internal order.  The proposed method seems very promising since it does not require any additional data structures such as trees etc.. However there seem to exist several open questions regarding how exactly one should define \"order\" between two elements from different sets. \nStrengths: \n1. Simple concept 2. Linear time 3. No extra storage Weaknesses: 1. How do you compare two elements? 2. What if your dataset contains multiple instances per class? 3. Doesn't work when all points belong to same cluster 4. Not clear whether it's possible to learn parameters 5. Can only handle binary labels 6. Can't deal with missing values 7.","label":1,"model":"bloomz","source":"peerread","id":4391}
{"text":"The book is very well written and easy to read, but it lacks depth in some areas.  The author does not go into detail on how he implemented his algorithms or what software was used.   He also doesn't explain why certain approaches are better than others (e.g., neural networks vs decision trees).  I would have liked more examples of real-world applications as opposed to just academic ones.\nOverall though this is an excellent introduction that will get you started with exponential machines if you're interested in learning about them! 4 stars  - Excellent Introduction!   Highly Recommended!   ...more details needed....but still good overall..4 stars ...more details needed....but still ggood overall..4 stars ..more details needed.....still good overall","label":1,"model":"bloomz","source":"peerread","id":4392}
{"text":"The author of this article is trying to describe how exponential machines work in order for readers who are not familiar with them to understand their use.\nStrengths: The author's writing style makes it easy to follow along as he describes each step involved.  He also provides examples that help explain his points clearly.   This helps make understanding easier than if there were no examples at all.\nWeaknesses: There isn't much information about these types of machines on other websites besides Wikipedia so I don't know where else people can find more detailed explanations beyond those provided here. If you want to learn more you'll have to do some searching yourself. Overall though it's an informative piece that's worth reading even without knowing anything beforehand because it'll give you enough background knowledge to get started using such machines effectively once you've found out more from elsewhere online.","label":1,"model":"bloomz","source":"peerread","id":4393}
{"text":"The authors present their new method \"Exponential Machine\" as well as some experiments showing good results compared to other methods.\nI think it's interesting but I have no experience working with such high dimensional datasets so I'm not sure if my opinion would be helpful here... \nOverall I'd say they did a great job presenting their work! It seems very promising though I'll probably need more time before trying out these techniques myself :) Good luck! Yes  - Very nice presentation. No doubt about the quality of your research.  You should consider adding one sentence at the end explaining how much better TTs are than Tucker3's when dealing with extremely sparse matrices like those produced by word2vec embeddings. This will help convince people who don't know anything about TTs why they're useful","label":1,"model":"bloomz","source":"peerread","id":4394}
{"text":"Strengths: \nThe authors present their new method for modeling higher-order feature interactions using TT-format Tensors. \nWeaknesses: \nNo experiments are presented where they compare against other methods such as Factorization Machine or Random Forests. This would be useful information when trying to decide if one should use these techniques over others.  Also no explanation was provided about how the algorithm scales up beyond small datasets? How does computational complexity scale?  Finally there were some typos throughout the manuscript but nothing major enough to distract from reading the article overall. Overall I think it's a good start towards addressing problems involving high dimensionality and complex relationships among variables. It could benefit greatly from more thorough testing across different types of datasets though! Yes  - very interesting work. No","label":1,"model":"bloomz","source":"peerread","id":4395}
{"text":"The authors present an interesting approach to training recurrent neural networks (RNNs) that is based on normalizing their activations at each time step, rather than using gradient descent or backpropagation through time.  The method has been shown in practice to be effective and efficient.\nI think this work would benefit from some additional explanation of how it differs from other approaches such as LSTM cells which are also used successfully with RNNs.   I am not sure if there was any attempt made by the authors to compare these two methods directly? Yes  No  No  Yes   No    No  Yes  Yes  Yes  Yes   No  Yes  Yes  No  Yes  Yes  None  None  None  None   None  None","label":1,"model":"bloomz","source":"peerread","id":4396}
{"text":"The authors propose an algorithm for training recurrent neural networks (RNNs) that is based on backpropagating errors through time.\nThis approach has been shown to be effective in practice but suffers from exploding gradients when applied directly.  The proposed method solves this issue using gradient clipping which limits how much each weight can change at any one step of learning.   This technique also allows RNNs with long sequences to learn effectively without requiring many more parameters than standard feed-forward nets. \nStrengths: \n1)  It works well even if there are very large changes between successive inputs.    2)  No extra parameters needed compared to FFNs    3)  Doesn't require expensive optimization techniques such as LSTM  4)  Can easily incorporate other types of layers 5)  Works well across different datasets Weaknesses:","label":1,"model":"bloomz","source":"peerread","id":4397}
{"text":"The authors have done an excellent job in presenting their work clearly with appropriate references.  The experiments are well designed and executed.\nI would like to see more discussion about how this approach could be used as part of larger models such as transformer networks which use attention mechanisms between layers rather than just within them (as is assumed here). I think there may still be some benefit from using RNP even if you don't know exactly what layer you're going to attend too but it's worth investigating further. \n\nOverall great research! \nYes  Yes  No  Yes \n\nNo  Yes  Yes  Yes  Yes   Yes    Yes  Yes  Yes  No  Yes  Yes  Yes  \n\nNo  Yes  Yes  No Yes Yes Yes Yes Yes Yes Yes No","label":1,"model":"bloomz","source":"peerread","id":4398}
{"text":"The authors have done an excellent job in presenting their work clearly with appropriate examples.  The proposed method is very interesting as they are able to reduce computational complexity without sacrificing performance compared to existing methods such as RNN-BatchNorm (Ioffe & Szegedy 2015) which requires estimating batch statistics for every timestep during backpropagation through time. \nHowever I think there should be more discussion about how this approach can scale up when dealing with large datasets like ImageNet where we may want to use larger number of layers and\/or higher dimensionality per layer. \n\nIn addition, since the mean\/variance preservation property only holds true if all inputs are normalized into zero-mean unit-variance space before feeding them into network, one might wonder whether it's possible to extend\/improve upon this idea so that input data doesn't necessarily require preprocessing?","label":1,"model":"bloomz","source":"peerread","id":4399}
{"text":"The authors present an interesting approach to generating images that are interpretable by humans, but also have some controllable structure.  The main idea is to use deep learning techniques in combination with genetic algorithms (GA) and evolutionary strategies (ES).  This allows them to generate new image structures from existing ones using GA or ES.\nI think this work has potential applications beyond just creating artwork; it could be used as part of data augmentation when training machine learning models on medical imaging datasets where there may not always be enough labeled examples available.   I would like to see more details about how they plan to evaluate their method's performance against other state-of-the-art approaches such as GANs which can produce similar results without requiring any human intervention during generation time.    Overall though, it's good research!","label":1,"model":"bloomz","source":"peerread","id":4400}
{"text":"The authors present an interesting approach to generating images that are interpretable in terms of their structure.  The main idea is to use deep learning techniques for image generation but also incorporate some prior knowledge about how objects should be structured into the model.\nI think this work has several important contributions including (1) introducing new ways to generate more realistic looking images, (2) showing how incorporating structural priors can help improve results on real-world datasets such as CelebA, \n(3)  demonstrating how these models could potentially be used to create photo-realistic renderings from 3D CAD data,\nHowever I feel like there may still be room for improvement.   For example it would have been nice if they had shown examples where the generated images were not only structurally correct but also semantically meaningful","label":1,"model":"bloomz","source":"peerread","id":4401}
{"text":"The authors present their work well but I have some comments that may help improve it further. \n\n1) It is not clear what exactly does \"conditioning on segmentation\" mean? Does this refer only to generating from mask or also from other information like location?\n\n2) How do you handle occlusion when synthesizing images based on segmentation? For example if there were two birds sitting next to each other then one bird would be covered by another during training so how will your system know which bird should appear at certain place? \n\n3) What happens if input sentence has more than 1 word per line? Will all these lines get concatenated together before being fed into the neural net? Yes  No  No  Yes   Yes    Yes     Yes      Yes        Yes","label":1,"model":"bloomz","source":"peerread","id":4402}
{"text":"The authors present their work as follows: \"We propose...\" This is not enough information about your approach.  You need to describe it more fully so that readers know exactly why you think this method will be useful.\nStrengths & Weaknesses: \nThis article describes some interesting ideas but does little to explain them clearly. It would benefit from additional explanation throughout. For example,  - What do they mean when they say \"jointly trained\"? How did they accomplish joint training?  - Why was MLE used instead of GANs?\n- Where were these models applied? These questions should all have been answered before moving onto other topics. If there isn't time\/space available here, please provide references where people could find answers if interested. Also note that many journals require supplemental material such as code and\/or datasets which may answer any lingering questions.","label":1,"model":"bloomz","source":"peerread","id":4403}
{"text":"The authors present an interesting approach of using chirplets in machine listening, which is very relevant as it can be used with speech recognition systems.\nHowever there are some issues that need further clarifications before publication. \n\n1) The proposed method does not seem to have been tested against other methods such as MFCCs or PLPs (which were developed earlier). It would also help if they could compare their results with those obtained by these two popular techniques. \n2) In addition, I think it's important to show how this new technique performs compared to existing state-of-the-art approaches like deep learning based ones.  3) Finally, since the main motivation behind developing this algorithm was to improve performance over noisy data sets, then testing should include both clean and noisy datasets.  \n\nI hope my comments will prove useful!","label":1,"model":"bloomz","source":"peerread","id":4404}
{"text":"The authors propose an algorithm for detecting animal sounds in noisy environments using Fast Chirplets transform (FCT). The proposed method is validated with two datasets of real-world recordings from different types of animals.\nStrengths: \n1) This work provides insights into how FCT can be used as feature extraction technique for machine listening applications. \n2) It also shows that this approach outperforms other state-of-the-art techniques such as Mel-frequency cepstral coefficients(MFCCs), linear prediction coding (LPCC).\n3) Finally it demonstrates its effectiveness when applied to detect speech signals mixed up with background noise.  4) In addition, they provide some interesting results about chirplets properties which are useful information for future research works. \n\nWeaknesses: \n1) There should have been more discussion regarding why MFCC was not chosen over LPCC","label":1,"model":"bloomz","source":"peerread","id":4405}
{"text":"This is very interesting work which has potential applications in many areas including speech recognition.  I have only one minor comment about Figure 1 . It would help if you could explain what each line represents. For example \"Orca\" should probably read \"Ouroboros\" or something similar since this word was used earlier in your text. Otherwise it's great! \nI look forward to seeing more papers like yours published soon. \n\nThanks again! Yes  No  Yes  Yes   Yes  Yes  Yes  Yes  No  Yes  No  Yes    Yes  Yes  Yes  YES  Yes  Yes  Yes Yes  Yes  Yes  NO  Yes  Yes  Yes   Yes   Yes  Yes  No  No  Yes  Yes  Yes","label":1,"model":"bloomz","source":"peerread","id":4406}
{"text":"This is very interesting work in which the authors have developed fast chirplet transform method based on wavelets theory.  This new technique has been applied successfully to enhance performance of CNNs trained using bird sounds and speech signals. In addition this study provides some insights into how different layers contribute differently towards improving recognition rates depending upon whether they were learned directly from acoustic features extracted via FFT-based methods or indirectly through feature extraction performed at lower levels within the network itself. \nOverall I think it's a good research article but there could still be room for improvement if possible. \n\nStrengths: \n1. Authors provide detailed description about their proposed methodology alongwith experimental validation 2. They use real world datasets like Bird sound database 3. Results obtained clearly indicate effectiveness of proposed scheme 4. Paper written well","label":1,"model":"bloomz","source":"peerread","id":4407}
{"text":"The authors present an interesting convergence analysis on two layer neural networks, which is based upon their recent work in [1] . The main contribution lies in that they show how to use symmetry breaking techniques from convex optimization theory and combine them with tools from functional analysis (e.g., Sobolev spaces) to obtain improved bounds.\nI think this article will be useful not only as it provides new insights into the behavior of certain types of deep learning models but also because it can serve as a reference point when studying related problems such as generalization error or robustness properties.  I would like to see more discussion about possible extensions beyond the current setting. \n\nOverall, it's well written and easy-to-read so I'd recommend publication after minor revisions are made. Below you find some suggestions regarding what could improve your manuscript further.  \n\n1. In Section 3.2, please add references where readers may learn more about the concept of \"functional space\"","label":1,"model":"bloomz","source":"peerread","id":4408}
{"text":"The main contribution is to provide an analysis on convergence for two layer neural networks using rectified linear units (ReLUs). The authors show that under certain conditions there exists at least one solution which converges in finite time.\nStrengths: \n1) This work provides some theoretical guarantees about how well these models will perform. \n2) It also shows that it can be done efficiently without having to use gradient descent methods Weaknesses: \n1) There are no experiments showing this method actually works better than other approaches such as SGD 2) No discussion if this approach could scale up beyond small datasets 3) No comparison between different types of activation functions 4) No explanation why they chose ReLUs over others 5) No mention of any potential applications 6) No proof of theorem 1","label":1,"model":"bloomz","source":"peerread","id":4409}
{"text":"The authors have done an excellent job proving their claims by applying some powerful tools from mathematics such as matrix theory and differential equations. This work will definitely benefit researchers working in machine learning area who are interested in understanding how deep neural networks learn. I would like to see more discussion about possible extensions or applications of these results. \n\nI am very pleased to recommend publication after minor revisions. Minor revision suggestions include: \n- Clarify what you mean by \"orthonormal basis\" - It seems that your definition differs slightly from other sources' definitions.\n- Explain why the assumption of orthogonality between teachers weights makes sense intuitively. - Figure out whether there exists any practical way to enforce orthogonality among teachers weights? - Add references to support statements made throughout the manuscript","label":1,"model":"bloomz","source":"peerread","id":4410}
{"text":"The authors present an interesting work which provides new insights into training deep learning models under some conditions. However there are several issues I would like them to address before publication. \n\nFirst, they should provide more details about their proposed method such as how to choose the parameter $\\delta$.  Second,  they need to explain why the assumption that the teacher weights form orthogonal basis holds true even though the number of hidden units $K$ may not equal one. Thirdly, since the results hold only asymptotically, i.e., when the dimension $d \\to \\infty$, they have to justify whether these results still hold good  for small values of d. Finally, they could add numerical experiments to support their claims. Overall, it's a promising research direction but needs further improvements","label":1,"model":"bloomz","source":"peerread","id":4411}
{"text":"The authors present an efficient softmax approximation method on graphics processing units (GPU). The proposed algorithm is based on Taylor expansion and can be implemented in parallel with little overhead, which makes it suitable to accelerate large-scale deep learning applications.\nIn addition, they provide theoretical analysis of their approach by proving that the error bound between the exact value and approximate one decreases exponentially as more terms are added into the Taylor series expansions. \nFinally, extensive experiments have been conducted using several benchmark datasets from different domains including computer vision, natural language processing, speech recognition etc., showing significant speedups over state-of-the-art methods while maintaining comparable accuracy. \n \n This work has potential impact both theoretically and practically since it provides new insights about how to efficiently implement neural network models on GPU platforms. \n\nI would like to see some additional details regarding the implementation aspects such as: \n\n1) How does this work?","label":1,"model":"bloomz","source":"peerread","id":4412}
{"text":"The authors present an efficient softmax approximation method that can be used to accelerate deep learning on graphics processing units (GPU). The proposed algorithm is based on Taylor expansion of exponential functions in order to reduce computational complexity.\nThis work has several advantages over previous works such as faster convergence speed with less memory consumption.  However there are some issues which need further investigation. \n\nStrengths: \n1) It provides better performance than other methods. \n2) It requires fewer computations compared to existing algorithms.\n3) It reduces storage requirements significantly Weaknesses: \n1) There may exist numerical instability problems when using this approach because it involves approximating exponentials through polynomials.  \n2) This technique does not provide any theoretical guarantees about its accuracy.  \n\nIn conclusion, we believe that this research will have significant impact if these limitations could be addressed properly","label":1,"model":"bloomz","source":"peerread","id":4413}
{"text":"The authors present their work with clarity.  The results are convincing.\nI have only one minor comment about Figure 1 . It would be helpful if they could add some information (e.g., number) next to each bar indicating how many times more efficient is AdaptiveSoftMax compared to FullSoftMax or FastText. This will help readers understand better what these bars represent. Otherwise I think this article makes a good contribution to NLP research community. Yes  No  Yes  Yes  Yes  Yes   Yes    Yes  Yes  Yes  No  Yes  No  Yes   Yes  Yes  Yes  None  Yes  Yes  Yes Yes  Yes  Yes  True  False  False  False  False   False  False","label":1,"model":"bloomz","source":"peerread","id":4414}
{"text":"The authors present their work with clarity.  The results are convincing:  they demonstrate significant speedups without sacrificing much accuracy compared to other approaches (e.g., hierarchical softmax). I think this is quite impressive! \nI have only one minor comment about Figure 1 . It would be helpful if there were some indication how many examples each curve represents. (This could also help readers understand why the curves look so smooth.) Otherwise it's fine. Overall, great job!  This article should make a good contribution to NIPS 2016. Thanks again! \n\nMinor comments: \nIn Figure 1 , please indicate how many samples\/words per epoch you used? Great work! Very interesting idea which can lead to huge gains when training large-scale LMs using GPGPUs","label":1,"model":"bloomz","source":"peerread","id":4415}
{"text":"The authors present an interesting approach to data augmentation, which is one of my favorite topics when it comes to deep learning and computer vision.\nI think that this work has some good ideas but I am not sure if they are implemented correctly or efficiently enough.  For example,  how do you know what features should be augmented? How does your method scale with more complex datasets?  What happens during training time?\nOverall though, it's worth reading because there may still be something useful here! \nGood luck on publication! \n\nSincerely,\nJens Hagenauer  University of Hamburg Germany\n\n\n\n- Introduction - The main idea behind feature space augmentation (FSA) is to generate new samples by applying transformations to existing ones. This can help improve generalization performance as well as reduce overfitting. \n\nIn contrast to other methods such as dropout regularization [Srivastava et al., 2014] , FSA operates directly at the level of individual examples rather than entire batches. As shown below, we demonstrate significant improvements using our proposed technique across several benchmark tasks including image classification [Krizhevsky et al., 2012] .","label":1,"model":"bloomz","source":"peerread","id":4416}
{"text":"- It is not fair to use a character 5-gram language model and compare to [1] which doesn't use any language modeling, the authors should also report their results without using any language modeling (IMHO using a language model with a limited vocabulary corpus like GRID which has only 51 words and 64000 possible sentence makes the results misleading, which, I think, is the reason why no language modeling was used in [1])\n\n- It is not fair to augment the training data to 15x, then compare to [1] which doesn't use any data augmentation\n\n- It should be mentioned and cited that a CNN+RNN+CTC architecture is not novel and has been widely used in literature for sequence recognition tasks (e.g. [2],[3],[4])\n\n- I encourage the authors to also try a ConvLSTM [5], which have recently shown very promising performance in a number of video-related tasks\n\n---------------------------------------------------\n[1] M. Wand, J. Koutnik, and J. Schmidhuber. Lipreading with long short-term memory.\n[2] B.  Shi,  X.  Bai,  and  C.  Yao. An  end-to-end  trainable  neural  network for  image-based  sequence  recognition  and  its  application  to  scene  text recognition.\n[3]  Z.  Xie,  Z.  Sun,  L.  Jin,  Z.  Feng,  and  S.  Zhang. Fully  convolutional recurrent network for handwritten chinese text recognition.\n[4] Li, H., Shen, C.: Reading car license plates using deep convolutional neural networks and lstms\n[5] Xingjian Shi, Zhourong Chen, Hao Wang, Dit-Yan Yeung, Wai-Kin Wong, and Wang-chun Woo. Convolutional LSTM network: A machine learning approach for precipitation nowcasting.","label":0,"model":"human","source":"peerread","id":4632}
{"text":"This corpus is a small data set created 10 years ago by colleagues and friends (Martin Cooke, Jon Barker, Stuart Cunningham and Xu Shao) at the Department of Computer Science. I recall that Martin gave me a bottle of Spanish wine for my trouble.\nAs far as I remember the corpus, it was designed to remove higher order language structure. That structure that (I believe) is used by humans to cue on when reading lips.\n\nThe corpus has a limited vocabulary and a single syntax grammar. So while it's promising to perform well on this data, it's not really ground breaking, particularly if you are interested in sentence models: the corpus sentence structure is super simple.\nSo while the model may be able to read my lips better than a human, it can only do so when I say a meaningless list of words from a highly constrained vocabulary in a specific order. That may be an advance, but it's not one worthy of disturbing me on a Sunday (serves me right for reading Twitter on a Sunday).\n\nI'm not making a comment about whether the paper should be accepted or not, but merely reacting to the large number of claims for the paper we are seeing on social media. The particular result for this data set may well be state of the art.\n","label":0,"model":"human","source":"peerread","id":4633}
{"text":"all reviewers agree that the paper is not convincing enough at this stage but needs more work to be ready for ICLR (e.g. missing comparisons to other existing methods).","label":0,"model":"human","source":"peerread","id":4634}
{"text":"This paper misses discussion of 'Diversity Networks' published in ICLR 2016 (","label":0,"model":"human","source":"peerread","id":4635}
{"text":"Summary:\nIn this paper, the authors introduce NoiseOut, a way to reduce parameters by pruning neurons from a network. \nThey do this by identifying pairs of neurons produce the most correlated outputs, and replacing the pair by one neuron, and then appropriately adjusting weights.\nThis technique relies on neurons having high correlations however, so they introduce an additional output neuron -- a noise output, which results in the network trying to predict the mean of the noise distribution.\nAs this is a constant, it increases correlation between neurons.\nExperiments test this out on MNIST and SVHN\n\nComments:\nThis is an interesting suggestion on how to prune neurons, but more experiments (on larger datasets) are probably need to be convincing that this is an approach that is guaranteed to work well. \n\nEquation (5) seems to be very straightforwards?\n\nIt seems like that for larger datasets, more noise outputs might have to be added to ensure higher correlations? Is there a downside to this in terms of the overall accuracy?\n\nThe paper is presented clearly, and was definitely interesting to read, so I encourage the authors to continue this line of work.\n","label":0,"model":"human","source":"peerread","id":4636}
{"text":"This paper proposes and tests two ideas. (1) a method of pruning networks by identifying highly correlated neuron pairs, pruning one of the pair, and then modifying downstream weights to compensate for the removal (which works well if the removed neurons were highly correlated). (2) a method, dubbed NoiseOut, for increasing neuron correlation by adding auxiliary noise target outputs to the network during training.\n\n\nThe first idea (1) is fairly straightforward, and it is not clear if it has been tried before. It does seem to work.\n\n\nThe second idea (2) is of unclear value and seems to this reviewer that it may merely add a regularizing effect. Comments in this direction:\n - In Fig 4 (right), the constant and Gaussian treatments seem to produce the same effect in both networks, right? And the Binomial effect seems the same as No_Noise. If this is true, can we conclude that the NoiseOut targets are simply serving to regularize the network, that is, to reduce its capacity slightly?\n - To show whether this effect is true, one would need to compare to other methods of reducing the network capacity, for example: by reducing the number of neurons, by applying L2 regularization of various values, or by applying Dropout of various strengths. Fig 7 makes an attempt at this direction, but critically misses several comparison treatments: \u201cPruned without any regularization\u201d, \u201cPruned with only L2\u201d, and \u201cPruned with only DropOut\u201d. Have these experiments been run? Can their results be included and used to produce plots like Fig 5 and Fig 7?\n\nWithout these comparisons, it seems impossible to conclude that NoiseOut does anything but provide similar regularization to DropOut or L2.\n\n\nThe combined ideas (1) + (2) DO produce a considerable reduction in parameters, but sadly the experiments and exposition are somewhat too lacking to really understand what is going on. With a little more work the paper could be quite interesting, but as is it should probably not be accepted.\n\n\nAdditional comments:\n - Section 4 states: \u201cIn all of these experiments, the only stop criteria is the accuracy decay of the model. We set the threshold for this criteria to match the original accuracy; therefore all the compressed network have the same accuracy as the original network.\u201d Is this accuracy the train accuracy or test accuracy? If train, then test accuracy needs to be shown (how much test performance is lost when pruning?). If test, then this would typically be referred to as \u201ccheating\u201d and so the choice needs to be very clearly stated and then defended.\n - Lowercase rho is used to indicate correlation but this is never actually specified, which is confusing for. Just state once that it indicates correlation.\n - How do these results compare to other pruning methods? No numerical comparison is attempted.","label":0,"model":"human","source":"peerread","id":4637}
{"text":"The paper proposes to prune a neural network by removing neurons whose operation is highly correlated with other neurons. The idea is nice and somewhat novel - most pruning methods concentrate on removal of individual weights, however I haven't done a through research on this topic. However, the experimental and theoretical justification of this method need to be improved before publication:\n\n1. Experiments. The authors do not report accuracy degradation while pruning in the tables, laconically stating that the networks did not degrade. This is not convincing. The only details are given in Figure 5, however this Figure disagrees with Table 2: in the Table, the number of parameters ranges from 40k-600k, while the Figure pictures the range 12k-24k. Unless more details are provided, simply claiming that a network can remove 50% neurons with no number on the degradation of accuracy is not convincing.\n\n2. Theory. The proofs do not match the experimental conditions and make unreasonable assumptions. The proofs show that in the absence of biases a network with a constant output will have two correlated neurons that generate the output offset. However, this is exactly why networks have biases and doesn't explain why noise injection helps (the proof suggests that all should be fine with deterministic auxiliary neuron). My interpretation is that the noisy output injects gradient noise (see e.g. the concurrent ICLR submission ","label":0,"model":"human","source":"peerread","id":4638}
{"text":"The paper proposed to analyze several recently developed machine readers and found that some machine readers could potentially take advantages of the entity marker (given that the same marker points out to the same entity). I usually like analysis papers, but I found the argument proposed in this paper not very clear.\n\nI like the experiments on the Stanford reader, which shows that the entity marker in fact helps the Stanford reader on WDW. I found that results rather interesting.\n\nHowever, I found the organization and the overall message of this paper quite confusing. First of all, it feels that the authors want to explain the above behavior with some definition of the \u201cstructures\u201d. However, I am not sure that how successful the attempt is. For me, it is still not clear what the structures are. This makes reading section 4 a bit frustrating. \n\nI am also not sure what is the take home message of this paper. Does it mean that the entity marking should be used in the MR models? Should we design models that can also model the entity reference at the same time? What are the roles of the linguistic features here? Should we use linguistic structure to overcome the reference issue?\n\nOverall, I feel that the analysis is interesting, but I feel that the paper can benefit from having a more focused argument.","label":0,"model":"human","source":"peerread","id":4639}
{"text":"The reviewers mostly agree that this paper offers valuable insights about a family of problems in automatic \"reading\" of text, as well as current solutions. The paper seems to fail to generate excitement because it doesn't really point the way forward. I disagree with some reviewers about the work's suitability for ICLR (as opposed to an ACL venue) since ML researchers are also thinking about these tasks now. The consensus is that the paper will have greater impact (wherever it is published) with a clearer message.","label":0,"model":"human","source":"peerread","id":4640}
{"text":"The paper aims to consolidate some recent literature in simple types of \"reading comprehension\" tasks involving matching questions to answers to be found in a passage, and then to explore the types of structure learned by these models and propose modifications. These reading comprehension datasets such as CNN\/Daily Mail are on the simpler side because they do not generally involve chains of reasoning over multiple pieces of supporting evidence as can be found in datasets like MCTest. Many models have been proposed for this task, and the paper breaks down these models into \"aggregation readers\" and \"explicit reference readers.\" The authors show that the aggregation readers organize their hidden states into a predicate structure which allows them to mimic the explicit reference readers. The authors then experiment with adding linguistic features, including reference features, to the existing models to improve performance.\n\nI appreciate the re-naming and re-writing of the paper to make it more clear that the aggregation readers are specifically learning a predicate structure, as well as the inclusion of results about dimensionality of the symbol space. Further, I think the effort to organize and categorize several different reading comprehension models into broader classes is useful, as the field has been producing many such models and the landscape is unclear. \n\nThe concerns with this paper are that the predicate structure demonstrated is fairly simple, and it is not clear that it provides insight towards the development of better models in the future, since the \"explicit reference readers\" need not learn it, and the CNN\/Daily Mail dataset has very little headroom left as demonstrated by Chen et al. 2016. The desire for \"dramatic improvements in performance\" mentioned in the discussion section probably cannot be achieved on these datasets. More complex datasets would probably involve multi-hop inference which this paper does not discuss. Further, the message of the paper is a bit scattered and hard to parse, and could benefit from a bit more focus.\n\nI think that with the explosion of various competing neural network models for NLP tasks, contributions like this one which attempt to organize and analyze the landscape are valuable, but that this paper might be better suited for an NLP conference or journal such as TACL.\n","label":0,"model":"human","source":"peerread","id":4641}
{"text":"This paper aims to provide an insightful and analytic survey over the recent literature on reading comprehension with the distinct goal of investigating whether logical structure (or predication, as the authors rephrased in their response) arises in many of the recent models. I really like the spirit of the paper and appreciate the efforts to organize rather chaotic recent literature into two unified themes: \"aggregation readers\" and \"explicit reference models\u201d. Overall the quality of writing is great and section 3 was especially nice to read. I\u2019m also happy with the proposed rewording from \"logical structure\" to \u201cpredication\", and the clarification by the authors was detailed and helpful.\n\nI think I still have slight mixed feelings about the contribution of the work. First, I wonder whether the choice of the dataset was ideal in the first place to accomplish the desired goal of the paper. There have been concerns about CNN\/DailyMail dataset (Chen et al. ACL\u201916) and it is not clear to me whether the dataset supports investigation on logical structure of interesting kinds. Maybe it is bound to be rather about lack of logical structure.\n\nSecond, I wish the discussion on predication sheds more practical insights into dataset design or model design to better tackle reading comprehension challenges. In that sense, it may have been more helpful if the authors could make more precise analysis on different types of reading comprehension challenges, what types of logical structure are lacking in various existing models and datasets, and point to specific directions where the community needs to focus more.\n\n","label":0,"model":"human","source":"peerread","id":4642}
{"text":"Hi, Dear reviewers,\n     \n   Thanks for the valuable suggestions on our paper and sorry to make you confused about the notations, we have updated the paper to make it more clear. Please see the latest version.","label":0,"model":"human","source":"peerread","id":4643}
{"text":"The paper proposed to analyze several recently developed machine readers and found that some machine readers could potentially take advantages of the entity marker (given that the same marker points out to the same entity). I usually like analysis papers, but I found the argument proposed in this paper not very clear.\n\nI like the experiments on the Stanford reader, which shows that the entity marker in fact helps the Stanford reader on WDW. I found that results rather interesting.\n\nHowever, I found the organization and the overall message of this paper quite confusing. First of all, it feels that the authors want to explain the above behavior with some definition of the \u201cstructures\u201d. However, I am not sure that how successful the attempt is. For me, it is still not clear what the structures are. This makes reading section 4 a bit frustrating. \n\nI am also not sure what is the take home message of this paper. Does it mean that the entity marking should be used in the MR models? Should we design models that can also model the entity reference at the same time? What are the roles of the linguistic features here? Should we use linguistic structure to overcome the reference issue?\n\nOverall, I feel that the analysis is interesting, but I feel that the paper can benefit from having a more focused argument.\n","label":0,"model":"human","source":"peerread","id":4644}
{"text":"This paper proposes to initialize the weights of a deep neural network layer-wise with a marginal Fisher analysis model, making use of potentially the similarity metric.\n \nPros: \nThere are a lot of experiments, albeit small datasets, that the authors tested their proposed method on.\n\nCons:\nlacking baseline such as discriminatively trained convolutional network on standard dataset such as CIFAR-10.\nIt is also unclear how costly in computation to compute the association matrix A in equation 4.\n\nThis is an OK paper, where a new idea is proposed, and combined with other existing ideas such as greedy-layerwise stacking, dropout, and denoising auto-encoders.\nHowever, there have been many papers with similar ideas perhaps 3-5 years ago, e.g. SPCANet. \n\nTherefore, the main novelty is the use of marginal Fisher Analysis as a new layer. This would be ok, but the baselines to demonstrate that this approach works better is missing. In particular, I'd like to see a conv net or fully connected net trained from scratch with good initialization would do at these problems.\n\nTo improve the paper, the authors should try to demonstrate without doubt that initializing layers with MFA is better than just random weight matrices.","label":0,"model":"human","source":"peerread","id":4645}
{"text":"The proposed approach consists in a greedy layer wise initialization strategy for a deep MLP model, which is followed by global gradient-descent with dropout for fine-tuning. The initialization strategy uses a first randomly initialized sigmoid layer for dimensionality expansion followed by 2 sigmoid layers whose weights are initialized by Marginal Fisher Analysis (MFA) which learns a linear dimensionality reduction based on a neighborhood graph constructed using class label information (i.e. supervised dimensionality reduction). Output layer is a standard softmax layer.\n\nThe approach is thus to be added to a growing list of heuristic layer-wise initialization schemes.\nThe particular choice of initialization strategy, while reasonable, is not sufficiently well motivated in the paper relative to alternatives, and thus feels rather arbitrary.\nThe paper lacks clarity in the description of the approach:  MFA is poorly explained with undefined notations (in Eq. 4, what is A? It has not been properly defined); the precise use of alluded denoising in the model is also unclear (is there really training of an additional denoting objective, or just input corruption?).\n\nThe question of the (arguably mild) inconsistency of applying a linear dimensionality reduction algorithm, that is trained without any sigmoid, and then passing its learned representation through a sigmoid is not even raised. This, in addition to the fact that sigmoid hidden layers are no longer commonly used (why did you not also consider using RELUs?).\n\nMore importantly I suspect methodological problems with the experimental comparisons: the paper mentions using *default* values for learning-rate and momentum, and having (arbitrarily?) fixed epoch to 400 (no early stopping?) and L2 regularization to 1e-4 for some models. \n*All* hyper parameters should always be properly hyper-optimized using a validation set (or cross-validation) including early-stopping, and this separately for each model under comparison (ideally also including layer sizes). This is all the more important since you are considering smallish datasets, so that the various initialization strategies act mainly as different indirect regularization schemes: they thus need to be carefully tuned. This casts serious doubts as to the amount of hyper-parameter tuning (close to none?) that went into training the alternative models used for comparison. \n\nThe Marginal Fisher Analysis dimensionality reduction initialization strategy may well offer advantages, but as it currently stands this paper doesn\u2019t yet make a sufficiently convincing case for it, nor provide useful insights into the nature of the expected advantages.\n\nI would also suggest, for image inputs such as CIFAR10, to use the qualitative tool of showing the filters (back projected to input space) learned by the different initialization schemes under consideration, as this could help visually gain insight as to what sets methods apart. \n\n","label":0,"model":"human","source":"peerread","id":4646}
{"text":"The authors pointed out some limitations of existing deep architectures, in particular hard to optimize on small or mid size datasets, and proposed to stack marginal fisher analysis (MFA) to build deep models. The proposed method is tested on several small to mid size datasets and compared with several feature learning methods. The authors also applied some existing techniques in deep learning, such as backprop, denoising and dropout to improve performance. \n\nThe new contribution of the paper is limited. MFA has long been proposed. The authors fail to theoretically or empirically justify the stacking of MFAs. The authors did not include any deep architectures that requires backprop over multiple layers in the comparison, which the authors set out to address, instead all the methods compared were learned layer by layer. Will a randomly initialized deep model such as DBN or CNN perform poorly on these datasets? It is also not clear how the authors came up with each particular model architecture and hyper-parameters used in the different datasets. The writing of the paper needs to be significantly improved. A lot of details were omitted, for example, how is dropout applied in the MFA. ","label":0,"model":"human","source":"peerread","id":4647}
{"text":"This paper proposes to initialize the weights of a deep neural network layer-wise with a marginal Fisher analysis model, making use of potentially the similarity metric.\n \nPros: \nThere are a lot of experiments, albeit small datasets, that the authors tested their proposed method on.\n\nCons:\nlacking baseline such as discriminatively trained convolutional network on standard dataset such as CIFAR-10.\nIt is also unclear how costly in computation to compute the association matrix A in equation 4.\n\nThis is an OK paper, where a new idea is proposed, and combined with other existing ideas such as greedy-layerwise stacking, dropout, and denoising auto-encoders.\nHowever, there have been many papers with similar ideas perhaps 3-5 years ago, e.g. SPCANet. \n\nTherefore, the main novelty is the use of marginal Fisher Analysis as a new layer. This would be ok, but the baselines to demonstrate that this approach works better is missing. In particular, I'd like to see a conv net or fully connected net trained from scratch with good initialization would do at these problems.\n\nTo improve the paper, the authors should try to demonstrate without doubt that initializing layers with MFA is better than just random weight matrices.\n","label":0,"model":"human","source":"peerread","id":4648}
{"text":"This paper presents a new technique for adapting a neural network to a new task for which there is not a lot of training data. The most widely used current technique is that of fine-tuning. The idea in this paper is to instead learn a network that learns features that are complementary to the fixed network. Additionally, the authors consider the setting where the new network\/features are \u201cstitched\u201d to the old one at various levels in the hieararchy, rather that it just being a parallel \u201ctower\u201d. \n\nThis work is similar in spirit (if not in some details) to the Progressive Nets paper by Rusu et al, as already discussed. The motivations and experiments are certainly different so this submission has merit on its own.\n\nThe idea of learning a \u201cresidual\u201d with the stitched connnections is very similar in spirit to the ResNet work. It would be nice to compare and contrast those approaches.\n\nI\u2019ve never seen a batch being used 5 times in a row during training, does this work better than just regular SGD?\n\nIn Figure 5 it\u2019d be nice to label the y-axis. That Figure would also benefit from not being a bar chart, but simply emulating Figure 4, which is much more readable!\n\nFigure 5 again: what is an untrained model? It\u2019s not immediately obvious why this is a good idea at all. Is TFT-1 simply fine-tuning one more layer than \u201cRetrain Softmax\u201d?\n\nI think that the results at the end of section 3 are a bit weak because of usage of a big network. I would definitely like to see how the results change if using a smaller net.\n\nThe authors claim throughout the paper that the purpose of the added connections and layers is to learn *complementary* features and they show this with some figures. The latter are a convinving evidence, but not proof or guarantee that this is what is actually happening. I suggest the authors consider adding an explicit constraint in their loss that encourages that, e.g. by having a soft orthogonality constraing (assuming one can project intermediate features to some common feature dimensionality). The usage of very small L2 regularization maybe achieves the same thing, but there\u2019s no evidence for that in the paper (in that we don\u2019t have any visualizations of what happens if there\u2019s no L2 reg.).\n\nOne of the big questions for me while reading the paper was how would an ensemble of 2 pre-trained nets would do on the tasks that the authors consider. This is especially relevant in the cars classification example, where I suspect that a strong baseline is that of fine-tuning VGG on this task, fine-tuning resnet on this task, and possibly training a linear combination of the two outputs or just averaging them naively.\n\nDisappointing that there are no results in figure 4, 5 and 8 except the ones from this paper. It\u2019s really hard to situate this paper if we don\u2019t actually know how it compares to previously published results.\n\n\nIn general, this was an interesting and potentially useful piece of work. The problem of efficiently reusing the previously trained classifier for retraining on a small set is certainly interesting to the community. While I think that this paper takes a good step in the right direction, it falls a bit short in some dimensions (comparisons with more serious baselines, more understanding etc).","label":0,"model":"human","source":"peerread","id":4649}
{"text":"The method was developed to provide an alternative for fine-tuning by augmenting a pre-trained network with new capacity. The differential from other related methods is low, and the evaluated baselines were not well-chosen, so this is not a strong submission.","label":0,"model":"human","source":"peerread","id":4650}
{"text":"This paper proposes a method of augmenting pre-trained networks for one task with an additional inference path specific to an additional task, as a replacement for the standard \u201cfine-tuning\u201d approach.\n\nPros:\n-The method is simple and clearly explained.\n-Standard fine-tuning is used widely, so improvements to and analysis of it should be of general interest.\n-Experiments are performed in multiple domains -- vision and NLP.\n\nCons:\n-The additional modules incur a rather large cost, resulting in 2x the parameters and roughly 3x the computation of the original network (for the \u201cstiched\u201d network).  These costs are not addressed in the paper text, and make the method significantly less practical for real-world use where performance is very often important.\n\n-Given these large additional costs, the core of the idea is not sufficiently validated, to me.  In order to verify that the improved performance is actually coming from some unique aspects of the proposed technique, rather than simply the fact that a higher-capacity network is being used, some additional baselines are needed:\n(1) Allowing the original network weights to be learned for the target task, as well as the additional module.  Outperforming this baseline on the validation set would verify that freezing the original weights provides an interesting form of regularization for the network.\n(2) Training the full module\/stitched network from scratch on the *source* task, then fine-tuning it for the target task.  Outperforming this baseline would verify that having a set of weights which never \u201csees\u201d the source dataset is useful.\n\n-The method is not evaluated on ImageNet, which is far and away the most common domain in which pre-trained networks are used and fine-tuned for other tasks.  I\u2019ve never seen networks pre-trained on CIFAR deployed anywhere, and it\u2019s hard to know whether the method will be practically useful for computer vision applications based on CIFAR results -- often improved performance on CIFAR does not translate to ImageNet.  (In other contexts, such as more theoretical contributions, having results only on small datasets is acceptable to me, but network fine-tuning is far enough on the \u201cpractical\u201d end of the spectrum that claiming an improvement to it should necessitate an ImageNet evaluation.)\n\nOverall I think the proposed idea is interesting and potentially promising, but in its current form is not sufficiently evaluated to convince me that the performance boosts don\u2019t simply come from the use of a larger network, and the lack of ImageNet evaluation calls into question its real-world application.\n\n===============\n\nEdit (1\/23\/17): I had indeed missed the fact that the Stanford Cars does do transfer learning from ImageNet -- thanks for the correction.  However, the experiment in this case is only showing late fusion ensembling, which is a conventional approach compared with the \"stitched network\" idea which is the real novelty of the paper.  Furthermore the results in this case are particularly weak, showing only that an ensemble of ResNet+VGG outperforms VGG alone, which is completely expected given that ResNet alone is a stronger base network than VGG (\"ResNet+VGG > ResNet\" would be a stronger result, but still not surprising). Demonstrating the stitched network idea on ImageNet, comparing with the corresponding VGG-only or ResNet-only finetuning, could be enough to push this paper over the bar for me, but the current version of the experiments here don't sufficiently validate the stitched network idea, in my opinion.","label":0,"model":"human","source":"peerread","id":4651}
{"text":"This paper proposed to perform finetuning in an augmentation fashion by freezing the original network and adding a new model aside it. The idea itself is interesting and complements existing training and finetuning approaches, although I think there are a few baseline approaches that can be compared against, such as:\n\n(1) Ensemble: in principle, the idea is similar to an ensembling approach where multiple networks are ensembled together to get a final prediction. The approach in Figure 1 should be compared with such ensemble baselines - taking multiple source domain predictors, possibly with the same modular setting as the proposed method, and compare the performance.\n\n(2) comparison with late fusion: if we combine the pretrained network and a network finetuned from the pretrained one, and do a late fusion?\n\nBasically, I think it is a valuable argument in section 3.2 (and Figure 4) that finetuning with a small amount of data may hurt the performance in general. This builds the ground for freezing a pretrained network and only augmenting it, not changing it. I agree with the authors on this argument, although currently other than Figure 4 there seem to be little empirical study that justifies it.\n\nIt is worth noting that Figure 3 seems to suggest that some of the module filters are either not converging or are learning unuseful features - like the first two filters in 3(a).\n\nOverall I think it is an interesting idea and I would love to see it better developed, thus I am giving a weak accept recommendation, but with a low confidence as the experiments section is not very convincing.","label":0,"model":"human","source":"peerread","id":4652}
{"text":"This paper presents a new technique for adapting a neural network to a new task for which there is not a lot of training data. The most widely used current technique is that of fine-tuning. The idea in this paper is to instead learn a network that learns features that are complementary to the fixed network. Additionally, the authors consider the setting where the new network\/features are \u201cstitched\u201d to the old one at various levels in the hieararchy, rather that it just being a parallel \u201ctower\u201d. \n\nThis work is similar in spirit (if not in some details) to the Progressive Nets paper by Rusu et al, as already discussed. The motivations and experiments are certainly different so this submission has merit on its own.\n\nThe idea of learning a \u201cresidual\u201d with the stitched connnections is very similar in spirit to the ResNet work. It would be nice to compare and contrast those approaches.\n\nI\u2019ve never seen a batch being used 5 times in a row during training, does this work better than just regular SGD?\n\nIn Figure 5 it\u2019d be nice to label the y-axis. That Figure would also benefit from not being a bar chart, but simply emulating Figure 4, which is much more readable!\n\nFigure 5 again: what is an untrained model? It\u2019s not immediately obvious why this is a good idea at all. Is TFT-1 simply fine-tuning one more layer than \u201cRetrain Softmax\u201d?\n\nI think that the results at the end of section 3 are a bit weak because of usage of a big network. I would definitely like to see how the results change if using a smaller net.\n\nThe authors claim throughout the paper that the purpose of the added connections and layers is to learn *complementary* features and they show this with some figures. The latter are a convinving evidence, but not proof or guarantee that this is what is actually happening. I suggest the authors consider adding an explicit constraint in their loss that encourages that, e.g. by having a soft orthogonality constraing (assuming one can project intermediate features to some common feature dimensionality). The usage of very small L2 regularization maybe achieves the same thing, but there\u2019s no evidence for that in the paper (in that we don\u2019t have any visualizations of what happens if there\u2019s no L2 reg.).\n\nOne of the big questions for me while reading the paper was how would an ensemble of 2 pre-trained nets would do on the tasks that the authors consider. This is especially relevant in the cars classification example, where I suspect that a strong baseline is that of fine-tuning VGG on this task, fine-tuning resnet on this task, and possibly training a linear combination of the two outputs or just averaging them naively.\n\nDisappointing that there are no results in figure 4, 5 and 8 except the ones from this paper. It\u2019s really hard to situate this paper if we don\u2019t actually know how it compares to previously published results.\n\n\nIn general, this was an interesting and potentially useful piece of work. The problem of efficiently reusing the previously trained classifier for retraining on a small set is certainly interesting to the community. While I think that this paper takes a good step in the right direction, it falls a bit short in some dimensions (comparisons with more serious baselines, more understanding etc).\n\n\n","label":0,"model":"human","source":"peerread","id":4653}
{"text":"This paper provides two RNN-based architectures for extractive document summarization. The first, \"Classify\", reads in the whole document and traverses the sentences a second time to decide whether to include them or not (0\/1 decisions). The second, \"Select\",  reads in the whole document and picks the most relevant sentence one at the time. The models assume that oracle extractive summaries exist, and a pseudo ground-truth generation procedure is used, which mimics Svore et al. (2007) among others. \n\nOverall, this paper seems a small increment over Cheng & Lapata (2016) and performance is similar or worse to that paper. The problem of single document extractive summarization is not particularly exciting since in DUC 2002 (14 years ago) existing models could not beat the lead baseline (which selects the first sentences of the document). It's a pity that this paper doesn't address the most interesting problems of abstractive summarization or apply the proposed approach to multi-document summarization. It's also a little disappointing that the maximum sentence length had to be capped to 50, which suggests the model has some trouble to scale.","label":0,"model":"human","source":"peerread","id":4654}
{"text":"Reviewers found this paper clear to read, but leaned negative on in terms of impact and originality of the work. Main complaint is that the paper is neither significantly novel in terms of modeling (pointing to Cheng & Lapata), nor significantly more performative on this task (\"only slightly better\"). One reviewer also has a side complaint that the task itself is also somewhat simplistic and simplified, and suggests other tasks. This comment is perhaps harsh, but reflects a mandate for revisiting \"old\" problems to provide significant improvements in accuracy or novel modeling.","label":0,"model":"human","source":"peerread","id":4655}
{"text":"This paper presents two models for extractive document summarization: the classifier architecture and the selector architecture. These two models basically use either classification or ranking in a sequential order to pick the candidate sentences for summarization. Experiments in this paper show the results are either better or close to the SOTA.\n\nTechnical comments:\n\n- In equation (1), there is a position-relevant component call \"positional importance\". I am wondering how important this component is? Is it possible to show the performance without this component? Especially, for the discussion on impact of document structure, when the model is trained on the shuffled order but tested on the original order.\n- A similar question about equation (1), is the content-richness component really necessary? Since the score function already has salience part, which could measure how important of $h_j$ with respect to the whole document.\n- For the dynamic summary representation in equation (3), why not use the same updating equation for both training and test procedures? During test time, the model actually knows the decisions that have been made so far by the decoder. In this way, the model will be more consistent during training and test. \n- I think section 5 is the most interesting part of this paper, and it is also convincing on the difference between the two architectures.\n- It is a little disappointing that the decoding algorithm used in this paper is too simple. In a minimal case, both of them could use beam search and the results could be better.","label":0,"model":"human","source":"peerread","id":4656}
{"text":"This paper presents two RNN architectures for extractive document summarization. The first one, Classifier, takes into account the order in which sentences appear in the original document, whereas the second one, Selector, chooses sentences in an arbitrary order. For each architecture, the concatenated RNN hidden state from a sentence forward and backward pass  is used as features to compute a score that captures content richness, salience, positional importance, and redundancy. Both models are trained in a supervised manner, so the authors used \"pseudo-ground truth generation\" to create training data from abstractive summaries. Experiments show that the Classifier model performs better, and it achieves near state-of-the-art performance for some evaluation metrics.\n\nThe proposed model is in general an extension of Cheng and Lapata, 2016. Unfortunately, the performance is only slightly better or sometimes even worse. The authors mentioned that one key difference how they transform abstractive summaries to become gold labels for the supervised method. However, in the experiment results, the authors described that one potential reason their models do not consistently outperform the extractive model of Cheng & Lapata, 2016 is that the unsupervised greedy approximation may generate noisier ground truth labels than Cheng & Lapata. Is there a reason to construct the training data similar to Cheng & Lapata, if that turns out to be a better method?\nIn order for the proposed models to be convincing, they need to outperform this baseline that's very similar to the proposed methods more consistently, since the main contribution is improved neural architectures for extractive document summarization.\n","label":0,"model":"human","source":"peerread","id":4657}
{"text":"Very interesting paper.\nI have one clarifying question.\nHow are the embeddings for the forward and backward position indices of the sentence in the document computed? Basically, I want to understand how the positional embedding for the sentences (Pj) calculated. \nThank you.\n","label":0,"model":"human","source":"peerread","id":4658}
{"text":"This paper proposed a hardware accelerator for DNN. It utilized the fact that DNN are very tolerant to low precision inference and outperforms a state-of-the-art bit-parallel accelerator by 1.90x without any loss in accuracy while it is 1.17x more energy efficient. TRT requires no network retraining. It achieved super linear scales of performance with area.\n\nThe first concern is that this paper doesn't seem very well-suited to ICLR. The circuit diagrams makes it more interesting for the hardware or circuit design community. \n\nThe second concern is the \"take-away for machine learning community\", seeing from the response, the take-away is using low-precision to make inference cheaper. This is not novel enough. In last year's ICLR, there were at least 4 papers discussing using low precision to make DNN more efficient. These ideas have also been explored in the authors' previous papers.","label":0,"model":"human","source":"peerread","id":4659}
{"text":"This metareview is written based on the reviews of the two expert reviewers (scores of 5(5), 5(5)),\n given the wide disparity of reviewer expertise, and the relevance of hardware expertise to this particular paper.\n \n Quality, Clarity: The paper is clearly written. It does require hardware expertise to read and appreciate the ideas, methodology, and the results. There was a request to post a clear \"take home\" message for ML research, and the authors did post a clear statement in this regard.\n \n Originality: The size of the contribution is the major point of contention for this paper. The expert reviewers believe it to be an incremental\/small idea, particularly in relation to the prior work of the authors. The authors provide rebut that while the implementation builds on their previous work, it can be seen as a general concept that could be applied to other architectures, and that this work targets the important case of fully-connected architectures, vs the case of convolutional architectures that were the case where the prior work did well.\n \n Overall: I have asked the expert reviewers for further input. In the absence of further information, updated scores, or some reasonably strong indication of excitement about the ideas from the expert reviewers, there is little choice but to currently lean towards \"reject\" in terms of this being an inspirational paper for ICLR.","label":0,"model":"human","source":"peerread","id":4660}
{"text":"Here are our post-layout results with actual data-driven activity factors. The 2-bit configuration, detailed in the updated paper, is a design that processes two bits at once using half as many SIPs, increasing efficiency and reducing area overhead. It requires that precision is an even number.\n\nWe report results for the typical design case. Pre-layout results showed that the worst case design corner results in a larger advantage for TRT. The results show increased energy efficiency compared to the pre-layout results. This is expected as the pre-layout results used 50% activity factors whereas here we use actual activity factors measured on a typical layer. This is consistent with the behavior observed during the STRIPES work.\n\nFully-connected layers, whole chip (TSMC 65nm typical case):\n\nBaseline (DaDN) area - 80.41 mm2\n\nTRT area - 120.04 mm2\nEfficiency (TRT vs baseline):\nAlexNet  1.062\nVGG_S    1.059\nVGG_M    1.063\nVGG_19   1.059\ngeomean  1.061\n\nTRT 2-bit area - 100.43 mm2\nEfficiency (TRT 2-bit vs baseline):\nAlexNet  1.228\nVGG_S    1.235\nVGG_M    1.268\nVGG_19   1.237\ngeomean  1.242\n\nNumbers greater than 1 mean less energy used overall by TRT.\n","label":0,"model":"human","source":"peerread","id":4661}
{"text":"We feel uncomfortable completing the reviewer ratings as provided except for one case where a review misrepresents the facts and which we have addressed. This is because the reviews primarily state that the paper does not fit into ICLR. This is a question for the organizers and an interpretation of the CFP which clearly states \"hardware\".\n\nIn summary, the take-away for the ML community is this:\n\n1. Adjusting the precision used per layer or even at a finer granularity of groups of 256 or activations and weights can lead to faster processing and higher energy efficiency. This adjustment can be done at runtime and at a single-bit granularity. Performance can be had without sacrificing accuracy, but if one is willing to sacrifice accuracy more performance and more energy efficiency can be had. We envision follow up work on runtime adjustment of precisions (e.g,, incremental adjustment) to achieve better response times or higher throughput.\n\nIn more detail:\n\n2. There is an energy efficient high-performance hardware design that can offer performance inversely proportional to the precision being used per layer or even at a finer granularity (we do not present any results on this but the design obviously support it as-is). \n\n2. This design does not hardwire the precisions at manufacturing time but instead allows programmatic control at runtime.\n\n3. This capability opens up an additional design know that network designers can use to tradeoff execution time and accuracy.\n\n4. Earlier we had proposed a method to choose per layer precisions  for convolutional layers here we extend this method to fully-connected layers. This method was published on arxiv and has never been accepted to any peer reviewed publication. The only related publication to the work here is STRIPES (MICRO) (12 pages) and a pre-print at iEEE Computer Architecture Letters (4 pages) that explained the basic idea behind Stripes. This work extends STRIPES for Fully-connected layers --- Stripes did not improve performance for FC layers and its energy efficiency was worse than DaDianNao for those layers.\n\nConcern summary:\n\n1. Some concerns were raised about the energy and area measurements. We have posted an update and we will deliver the final results post-layout in a day or so. We will also deliver results on an optimized configuration that drastically reduces area overhead and improves energy efficiency as well. The delay in response was due to having received the reviews during the Xmas break when the author that can perform these measurements was unreachable due to travel. \n\n2. Incremental over DaDianNao: Tartan is a general concept which can be integrated to many different architectures. We chose DaDN as the baseline architecture as it widely known and often compared against and offers the additional challenge of being a very wide vector-like architecture (doing bit-serial computation for a single lane -- product -- independently is easy but doing it for 4K terms in parallel without extremely wide memories is hard). So, we disagree that this is an incremental improvement of DaDianNao. To draw an analogy, from hardware the seminal work on pattern based branch prediction was not an incremental improvement over out-of-order execution even though previous techniques included branch predictors. Not that we feel that TARTAN is at the same level as pattern based branch prediction but we use this example to illustrate what such an argument can lead to. Also, STRIPES is receiving a honorable mention in the upcoming IEEE MICRO Topic PIcs in Computer Architecture researcher, which is akin to a best paper award in the field of computer architecture (each accepted paper in the last year in a top-tier conference receives 10 more peer reviews that state whether they feel the paper has the potential for high impact). So, at least some people that are credible enough to be invited to that panel in the comp arch community think it's not incremental.\n\n3. FC layers are not important: this is not true. They are still in use and more so in different applications. Moreover, last years best paper award in ICLR rightfully went to an excellent work that addressed both pruning the model and proposing an optimized hardware architecture solely for FC layers.\n\n\n4. TARTAN takes too much area: The units are larger but this is not where most of the area cost is. The area cost is in the surrounding memory so in the big picture the overall area cost is much lower. also, in modern technology area is not the concern.\n\n5. Energy efficiency is not that much better than DaDN and within the error margin of the tools. We used industry standard tools and the results are positive. moreover, TARTAN is faster and has we can use frequency and voltage scaling to improve energy efficiency (which depends on voltage square) while still performing better. We show results assuming same voltage and frequency.\n\n6. You did not use power gating for DaDN. Did we not use it for TARTAN either. It is not straightforward to do for DaDN as it is a bit parlallel engine and there are not that many zero values to necessarily justify the logic needed to enable power gating. Which is to say that this ia a non-trivial task. Moreover, the same technique can be applied to TATRAN. We have results reported in another submission that show a heavy bias toward the zero bit value which suggestes that power gating will most likely be a lot more effective for TARTAN than DaDN.\n\nIn summary, combined this work, with STRIPES and the earlier arxiv report present a comprehensive hardware\/software approach to exploiting precision to improve performance and energy efficiency for CNNs. Stripes performed very well for convolutional layers but did poorly for fully-connected layers. TARTAN fixes this.\n","label":0,"model":"human","source":"peerread","id":4662}
{"text":"Our apologies for the long delay. Our co-author that has the expertise to do this work was overseas for the Christmas break and she returned this Tuesday.\n\nShe has synthesized the designs for three cases: bc, tc, and wc for best, typical and worst case respectively. The previous results were for the bc. The detailed results are below. In summary, efficiency improves for tc and wc. We have layout being synthesized and we expect to have the results in a day or so. The results below are pre-layout and use 50% activity factors. The layout results that we will post as soon as possible will be a testbench for a typical layer. As with STRIPES we expect that energy efficiency will be better with real inputs as the inputs exhibit many more zero bits. Please keep in mind that the results in the STRIPES publication in MICRO are post-layout.\n\nMore importantly, we are also synthesizing a different configuration cuts down area costs and improves energy efficiency even further. We will post the post-layout results shortly.\n\nWe hope that we will be given the opportunity to post the updated results before a decision is made.\n\nThank you.\n\nHere are the detailed pre-layout results for all three cases and for fully-connected layers only. Best case is the configuration used in the original submission. We will update the writing with post-layout and actual activity-based results shortly. Effieciency numbers above 1.0 mean that TRT is better than DaDN.\n\nArea & Efficiency, Fully-connected layers, whole chip TRT vs DaDN:\n\nBest case\n\nBaseline area - 77.91 mm2\nTRT area - 108.61 mm2 (+39.4%)\nEfficiency:\nAlexNet  0.935\nVGG_S    0.932\nVGG_M    0.935\nVGG_19   0.932\ngeomean  0.933\n\nTypical case\n\nBaseline area - 79.28 mm2\nTRT area - 111.29 mm2 (+40.4%)\nEfficiency:\nAlexNet  1.014\nVGG_S    1.011\nVGG_M    1.014\nVGG_19   1.010\ngeomean  1.012\n\nWorst case\n\nBaseline area - 83.5 mm2\nTRT area - 121.39 mm2 (+45.3%)\nEfficiency:\nAlexNet  1.048\nVGG_S    1.046\nVGG_M    1.049\nVGG_19   1.045\ngeomean  1.047\n\n\n\n\n\n","label":0,"model":"human","source":"peerread","id":4663}
{"text":"The authors present TARTAN, a derivative of the previously published DNN accelerator architecture: \u201cDaDianNao\u201d. The key difference is that TARTAN\u2019s compute units are bit-serial and unroll MAC operation over several cycles. This enables the units to better exploit any reduction in precision of the input activations for improvement in performance and energy efficiency.\n\nComments:\n\n1. I second the earlier review requesting the authors to be present more details on the methodology used for estimating energy numbers for TARTAN. It is claimed that TARTAN gives only a 17% improvement in energy efficiency. However, I suspect that this small improvement is clearly within the margin of error ij energy estimation.  \n\n2. TARTAN is a derivative of DaDianNao, and it heavily relies the overall architecture of DaDianNao. The only novel aspect of this contribution is the introduction of the bit-serial compute unit, which (unfortunately) turns out to incur a severe area overhead (of nearly 3x over DaDianNao's compute units).\n\n3. Nonetheless, the idea of bit-serial computation is certainly quite interesting. I am of the opinion that it would be better appreciated (and perhaps be even more relevant) in a circuit design \/ architecture focused venue.","label":0,"model":"human","source":"peerread","id":4664}
{"text":"Summary:\n\nThe paper describes how the DaDianNao (DaDN) DNN accelerator can be improved by employing bit serial arithmetic.  They replace the bit-parallel multipliers in DaDN with multipliers that accept the weights in parallel but the activations serially (serial x parallel multipliers).  They increase the number of units keeping the total number of adders constant.  This enables them to tailor the time and energy consumed to the number of bits used to represent activations.  They show how their configuration can be used to process both fully-connected and convolutional layers of DNNs.\n\nStrengths:\n\nUsing variable precision for each layer of the network is useful - but was previously reported in Judd (2015)\n\nGood evaluation including synthesis - but not place and route - of the units.  Also this evaluation is identical to that in Judd (2016b)\n\nWeaknesses:\n\nThe idea of combining bit-serial arithmetic with the DaDN architecture is a small one.\n\nThe authors have already published almost everything that is in this paper at Micro 2016 in Judd (2016b).  The increment here is the analysis of the architecture on fully-connected layers.  Everything else is in the previous publication.\n\nThe energy gains are small - because the additional flip-flop energy of shifting the activations in almost offsets the energy saved on reducing the precision of the arithmetic.\n\nThe authors don\u2019t compare to more conventional approaches to variable precision - using bit-parallel arithmetic units but data gating the LSBs so that only the relevant portion of the arithmetic units toggle.  This would not provide any speedup, but would likely provide better energy gains than the bit-serial x bit-parallel approach.\n\nOverall:\n\nThe Tartan and Stripes architectures are interesting but the incremental contribution of this paper (adding support for fully-connected layers) over the three previous publications on this topic, and in particular Judd (2016b) is very small.  This idea is worth one good paper, not four.","label":0,"model":"human","source":"peerread","id":4665}
{"text":"This seems like a reasonable study, though it's not my area of expertise. I found no fault with the work or presentation, but did not follow the details or know the comparable literature. \n\nThere seem to be real gains to be had through this technique, though they are only in terms of efficiency in hardware, not changing accuracy on a task. The tasks chosen (Alexnet \/ VGG) seem reasonable. The results are in simulation rather than in actual hardware.\n\nThe topic seems a little specialized for ICLR, since it does not describe any new advances in learning or representations, albeit that the CFP includes \"hardware\".  I think the appeal among attendees will be rather limited. \n\nPlease learn to use parenthetical references correctly. As is your references make reading harder. ","label":0,"model":"human","source":"peerread","id":4666}
{"text":"Hardware is listed on the call-for-papers as relevant topic for ICLR 2017, and so the paper is on-topic.\n\nWe worked hard to improve on the initial reviewer assignment for this paper in ensure that we would get hardware-knowledgeable reviewers on board for this paper, although we only partly succeeded (due to conflicts, tight review deadlines, and more).\n\nWe are also still missing questions and comments from one reviewer.\n\nICLR papers should succeed in communicating with the ICLR audience, i.e., introducing concepts for this audience, and using a language that is accessible for this community.  But it need not connect with all of the ICLR community, which is dominated by algorithmic concerns; many may not have much background or interest in hardware and that is fine.  This situation is not unique, e.g., hardware-related papers in computer graphics and computer vision conferences are in the same situation.  It is also true, I think, that the audience and reviewers of hardware-related conferences may not be well-placed to fully understand and comment on all relevant algorithmic considerations that a hardware implementation targets. \n\nThere remains disagreement as to the utility of achieving improvements related to fully connected layers, which would be good to resolve. And there also remains disagreement on the improvements (conceptual and performance-related) with respect to the state of the art, as recently rebutted by the authors. \n\n\n","label":0,"model":"human","source":"peerread","id":4667}
{"text":"I do not feel very qualified to review this paper. I studied digital logic back in university, that was it. I think the work deserves a reviewer with far more sophisticated background in this area. It certainly seems useful. My advice is also to submit it another venue.","label":0,"model":"human","source":"peerread","id":4668}
{"text":"This paper proposed a hardware accelerator for DNN. It utilized the fact that DNN are very tolerant to low precision inference and outperforms a state-of-the-art bit-parallel accelerator by 1.90x without any loss in accuracy while it is 1.17x more energy efficient. TRT requires no network retraining. It achieved super linear scales of performance with area.\n\nThe first concern is that this paper doesn't seem very well-suited to ICLR. The circuit diagrams makes it more interesting for the hardware or circuit design community. \n\nThe second concern is the \"take-away for machine learning community\", seeing from the response, the take-away is using low-precision to make inference cheaper. This is not novel enough. In last year's ICLR, there were at least 4 papers discussing using low precision to make DNN more efficient. These ideas have also been explored in the authors' previous papers. \n","label":0,"model":"human","source":"peerread","id":4669}
{"text":"CONTRIBUTIONS \nThis paper introduces a method for learning semantic \"word-like\" units jointly from audio and visual data. The authors use a multimodal neural network architecture which accepts both image and audio (as spectrograms) inputs. Joint training allows one to embed both image and spoken language captions into a shared representation space. Audio-visual groundings are generated by measuring affinity between image patches and audio clips. This allows the model to relate specific visual regions to specific audio segments. Experiments cover image search (audio to image) and annotation (image to audio) tasks and acoustic word discovery.\n\n\nNOVELTY+SIGNIFICANCE\nAs correctly mentioned in Section 1.2, the computer vision and natural language communities have studied multimodal learning for use in image captioning and retrieval. With regards to multimodal learning, this paper offers incremental advancements since it primarily uses a novel combination of input modalities (audio and images).\n\nHowever, bidirectional image\/audio retrieval has already been explored by the authors in prior work (Harwath et al, NIPS 2016). Apart from minor differences in data and CNN architecture, the training procedure in this submission is identical to this prior work. The novelty in this submission is therefore the procedure for using the trained model for associating image regions with audio subsequences.\n\nThe methods employed for this association are relatively straightforward combination of standard techniques with limited novelty. The trained model is used to compute alignment scores between densely sampled image regions and audio subsequences; from these alignment scores a number of heuristics are applied to associate clusters of image regions with clusters of audio subsequences.\n\n\nMISSING CITATION\nThere is a lot of work in this area spanning computer vision, natural language, and speech recognition. One key missing reference:\n\nNgiam, et al. \"Multimodal deep learning.\" ICML 2011\n\n\nPOSITIVE POINTS\n- Using more data and an improved CNN architecture, this paper improves on prior work for bidirectional image\/audio retrieval\n- The presented method performs efficient acoustic pattern discovery\n- The audio-visual grounding combined with the image and acoustic cluster analysis is successful at discovering audio-visual cluster pairs\n\nNEGATIVE POINTS\n- Limited novelty, especially compared with Harwath et al, NIPS 2016\n- Although it gives good results, the clustering method has limited novelty and feels heuristic\n- The proposed method includes many hyperparameters (patch size, acoustic duration, VAD threshold, IoU threshold, number of k-means clusters, etc) and there is no discussion of how these were set or the sensitivity of the method to these choices","label":0,"model":"human","source":"peerread","id":4670}
{"text":"This paper received borderline reviews. All reviewers as well as AC agree that the authors pursue a very interesting and less explored direction. The paper essentially addresses the problem of double grounding; visual information helping to group acoustic signal into words, and words helping to localize object-like regions in images. While somewhat hidden under the rug, this is what makes the paper different from the authors' previous work. The reviewers mentioned this to be a minor contribution. The AC agrees with the authors that this is an interesting and novel problem worth studying. However, the AC also agrees with the reviewers that this major novelty over the previous work is missing technical depth. The AC strongly encourages the authors to improve on this aspect of the paper. A simple intuition would be to look at ","label":0,"model":"human","source":"peerread","id":4671}
{"text":"We'd like to first thank the reviewers for taking the time to read our submission and offer their thoughtful opinions and encouragement. Since the three reviewers raise some similar questions and concerns, I'll try to address them in one post instead of multiple replies.\n\nI'll first address the issue of novelty, which is the main criticism raised by all three reviewers. When thinking about research, I think it's important to consider the problem to be solved separately from the tool(s) used to solve it. While I agree that the deep neural network architecture used in this submission is not a significant departure from the one used in our NIPS 2016 paper, the way we use it is very different. I believe that the problem we address in this submission - that is, joint localization\/isolation, clustering, and association of word-like speech patterns and object-like visual patterns - is significantly novel and wasn't studied in our NIPS paper. I don't believe that this submission qualifies as just an analysis paper, because it actually attempts to solve a specific and novel problem, and does so surprisingly well.\n\nThere exists an active sub-field of the speech\/linguistics\/cogsci communities which studies the problem of acquiring language from untranscribed speech audio alone (see relevant citations in the submission), and most of the techniques used in that problem space rely on segmentation and clustering of the speech signal into linguistically meaningful units (often called unsupervised term discovery or UTD when the desired granularity of the units is at the word or phrase level). The most widely-used and successful techniques for UTD are based on segmental dynamic time warping, which is inherently O(N^2) complexity. I believe that one of the most significant contributions of this submission is the demonstration that the addition of contextually relevant visual information is sufficient to reduce the computational complexity of UTD to O(N), allowing it to scale to much larger datasets.\n\nI'd also like to respond to a few specific points:\n\nFrom reviewer 2:\n\"As correctly mentioned in Section 1.2, the computer vision and natural language communities have studied multimodal learning for use in image captioning and retrieval. With regards to multimodal learning, this paper offers incremental advancements since it primarily uses a novel combination of input modalities (audio and images).\"\n\nI respectfully disagree that the move from text to speech audio constitutes an incremental step. The field of automatic speech recognition research has been grappling with the problem of mapping speech audio to symbolic strings for over 65 years, so it's not a trivial problem. By marrying language and vision at the raw signal level as we are here, we're not just creating models that learn the associations between words and images, but actually forcing the models to simultaneously learn how to perform speech recognition in their own, non-symbolic way.\n\nFrom reviewer 1:\n\"Clustering and grouping in section 4, is hacky. Instead of gridding the image, the authors could actually use an object detector (SSD, Yolo, FasterRCNN, etc.) to estimate accurate object proposals; rather than using k-means, a spectral clustering approach would alleviate the gaussian assumption of the distributions. In assigning visual hypotheses with acoustic segments, some form of bi-partite matching should be used.\"\n\nI think that these optimizations, while good suggestions if one's goal is to squeeze every last bit of performance out of the system, are not crucial to the central theme of the paper. We chose not to use an off-the-shelf object detection system for the same reason that we chose not to give the system oracle word boundaries derived from a speech recognizer. We wanted the model to learn to localize, identify, and associate spoken words and visual objects without being explicitly trained to do so. Spectral clustering is an O(N^3) complexity algorithm, and our problem deals with clustering on the order of a million points; a full similarity matrix at floating point precision would require on the order of 4 terabytes of memory, and that's just an O(N^2) subroutine in the spectral clustering algorithm. I realize that various approximate algorithms could possibly be made to work, but in our case I think that the fact that a classic algorithm like k-means works so well is actually a testament to the separability of the embeddings that are being learned by the multimodal CNN.","label":0,"model":"human","source":"peerread","id":4672}
{"text":"This paper is a follow-up on the NIPS 2016 paper \"Unsupervised learning of spoken language with visual context\", and does exactly what that paper proposes in its future work section: \"to perform acoustic segmentation and clustering, effectively learning a lexicon of word-like units\" using the embeddings that their system learns. The analysis is very interesting and I really like where the authors are going with this.\n\nMy main concern is novelty. It feels like this work is a rather trivial follow-up on an existing model, which is fine, but then the analysis should be more satisfying: currently, it feels like the authors are just illustrating some of the things that the NIPS model (with some minor improvements) learns. For a more interesting analysis, I would have liked things like a comparison of different segmentation approaches (both in audio and in images), i.e., suppose we have access to the perfect segmentation in both modalities, what happens? It would also be interesting to look at what is learned with the grounded representation, and evaluate e.g. on multi-modal semantics tasks.\n\nApart from that, the paper is well written and I really like this research direction. It is very important to analyze what models learn, and this is a good example of the types of questions one should ask. I am afraid, however, that the model is not novel enough, nor the questions deep enough, to make this paper better than borderline for ICLR.","label":0,"model":"human","source":"peerread","id":4673}
{"text":"This work proposes a joint classification of images and audio captions for the task of word like discovery of acoustic units that correlate to semantically visual objects. The general this is a very interesting direction of research as it allows for a richer representation of data: regularizing visual signal with audio and visa versa. This allows for training of visual models from video, etc. \n\nA major concern is the amount of novelty between this work and the author's previous publication at NIPs 2016. The authors claim a more sophisticated architecture and indeed show an improvement in recall. However, the improvements are marginal, and the added complexity to the architecture is a bit ad hoc. Clustering and grouping in section 4, is hacky. Instead of gridding the image, the authors could actually use an object detector (SSD, Yolo, FasterRCNN, etc.) to estimate accurate object proposals; rather than using k-means, a spectral clustering approach would alleviate the gaussian assumption of the distributions. In assigning visual hypotheses with acoustic segments, some form of bi-partite matching should be used.\n\nOverall, I really like this direction of research, and encourage the authors to continue developing algorithms that can train from such multimodal datasets. However, the work isn't quite novel enough from NIPs 2016.","label":0,"model":"human","source":"peerread","id":4674}
{"text":"CONTRIBUTIONS \nThis paper introduces a method for learning semantic \"word-like\" units jointly from audio and visual data. The authors use a multimodal neural network architecture which accepts both image and audio (as spectrograms) inputs. Joint training allows one to embed both image and spoken language captions into a shared representation space. Audio-visual groundings are generated by measuring affinity between image patches and audio clips. This allows the model to relate specific visual regions to specific audio segments. Experiments cover image search (audio to image) and annotation (image to audio) tasks and acoustic word discovery.\n\n\nNOVELTY+SIGNIFICANCE\nAs correctly mentioned in Section 1.2, the computer vision and natural language communities have studied multimodal learning for use in image captioning and retrieval. With regards to multimodal learning, this paper offers incremental advancements since it primarily uses a novel combination of input modalities (audio and images).\n\nHowever, bidirectional image\/audio retrieval has already been explored by the authors in prior work (Harwath et al, NIPS 2016). Apart from minor differences in data and CNN architecture, the training procedure in this submission is identical to this prior work. The novelty in this submission is therefore the procedure for using the trained model for associating image regions with audio subsequences.\n\nThe methods employed for this association are relatively straightforward combination of standard techniques with limited novelty. The trained model is used to compute alignment scores between densely sampled image regions and audio subsequences; from these alignment scores a number of heuristics are applied to associate clusters of image regions with clusters of audio subsequences.\n\n\nMISSING CITATION\nThere is a lot of work in this area spanning computer vision, natural language, and speech recognition. One key missing reference:\n\nNgiam, et al. \"Multimodal deep learning.\" ICML 2011\n\n\nPOSITIVE POINTS\n- Using more data and an improved CNN architecture, this paper improves on prior work for bidirectional image\/audio retrieval\n- The presented method performs efficient acoustic pattern discovery\n- The audio-visual grounding combined with the image and acoustic cluster analysis is successful at discovering audio-visual cluster pairs\n\nNEGATIVE POINTS\n- Limited novelty, especially compared with Harwath et al, NIPS 2016\n- Although it gives good results, the clustering method has limited novelty and feels heuristic\n- The proposed method includes many hyperparameters (patch size, acoustic duration, VAD threshold, IoU threshold, number of k-means clusters, etc) and there is no discussion of how these were set or the sensitivity of the method to these choices\n","label":0,"model":"human","source":"peerread","id":4675}
{"text":"UPDATE: I have read the authors' rebuttal and also the other comments in this paper's thread. My thoughts have not changed.\n\nThe authors propose using a mixture prior rather than a uni-modal\nprior for variational auto-encoders. They argue that the simple\nuni-modal prior \"hinders the overall expressivity of the learned model\nas it cannot possibly capture more complex aspects of the data\ndistribution.\"\n\nI find the motivation of the paper suspicious because while the prior\nmay be uni-modal, the posterior distribution is certainly not.\nFurthermore, a uni-modal distribution on the latent variable space can\ncertainly still lead to the capturing of complex, multi-modal data\ndistributions. (As the most trivial case, take the latent variable\nspace to be a uniform distribution; take the likelihood to be a\npoint mass given by applying the true data distribution's inverse CDF\nto the uniform. Such a model can capture any distribution.)\n\nIn addition, multi-modality is arguably an overfocused concept in the\nliterature, where the (latent variable) space is hardly anymore worth\ncapturing from a mixture of simple distributions when it is often a\ncomplex nonlinear space. It is unclear from the experiments how much\nthe influence of the prior's multimodality influences the posterior to\ncapture more complex phenomena, and whether this is any better than\nconsidering a more complex (but still reparameterizable) distribution\non the latent space.\n\nI recommend that this paper be rejected, and encourage the authors to\nmore extensively study the effect of different priors.\n\nI'd also like to make two additional comments:\n\nWhile there is no length restriction at ICLR, the 14 page document can\nbe significantly condensed without loss of describing their innovation\nor clarity. I recommend the authors do so.\n\nFinally, I think it's important to note the controversy in this paper.\nIt was submitted with many significant incomplete details (e.g., no experiments,\nmany missing citations, a figure placed inside that was pencilled in\nby hand, and several missing paragraphs). These details were not\ncompleted until roughly a week(?) later. I recommend the chairs discuss\nthis in light of what should be allowed next year.","label":0,"model":"human","source":"peerread","id":4676}
{"text":"This paper explores a variational autoencoder variant.\n \n ICLR gives authors some respect that other conferences don't. It is flexible about the length of the paper, and allows revisions to be submitted. The understanding should be that authors should in turn treat reviewers with respect. The paper should still be finished. Reviewers can't be expected to read a churn of large revisions. The final paper should be roughly the right length, unless with very good reason.\n \n This paper was clearly not finished, and now is too long, with issues remaining. I hope that it will be submitted again, but not until it is actually ready.","label":0,"model":"human","source":"peerread","id":4677}
{"text":"The authors introduce some new prior and approximate posterior families for variational autoencoders, which are compatible with the reparameterization trick, as well as being capable of expressing multiple modes. They also introduce a gating mechanism between prior and posterior. They show improvements on bag of words document modeling, and dialogue response generation. The original abstract is overly strong in its assertion that a unimodal latent prior p(z) cannot fit a multimodal marginal int_z p(x|z)p(x)dz with a DNN response model p(x|z) (\"it cannot possibly capture more complex aspects of the data distribution\", \"critical restriction\", etc).\n\nWhile the assertion that a unimodal latent prior is necessary to model multimodal observations is false, there are sensible motivations for the piecewise constant prior and posterior. For example, if we think of a VAE as a sort of regularized autoencoder where codes are constrained to \"fill up\" parts of the prior latent space, then there is a sphere-packing argument to be made that filling a Gaussian prior with Gaussian posteriors is a bad use of code space. Although the authors don't explore this much, a hypercube-based tiling of latent code space is a sensible idea.\n\nAs stated, I found the message of the paper to be quite sloppy with respect to the concept of \"multi-modality.\" There are 3 types of multimodality at play here: multimodality in the observed marginal distribution p(x), which can be captured by any deep latent Gaussian model, multimodality in the prior p(z), which makes sense in some situations (e.g. a model of MNIST digits could have 10 prior modes corresponding to latent codes for each digit class), and multimodality in the posterior z for a given observation x_i, q(z_i|x_i). The final type of multimodality is harder to argue for, except in so far as it allows the expression of flexibly shaped distributions without highly separated modes. I believe flexible posterior approximations are important to enable fine-grained and efficient tiling of latent space, but I don't think these need to have multiple strong modes. I would be interested to see experiments demonstrating otherwise for real world data.\n\nI think this paper should be more clear about the different types of multi-modality and which parts of their analysis demonstrate which ones. I also found it unsatisfactory that the piecewise variable analysis did not show different components of the multi-modal prior corresponding to different words, but rather just a separation between the Gaussian and the piecewise variables.\n\nAs I mention in my earlier questions, I found it surprising that the learned variance and mean for the Gaussian prior helps so dramatically with G-NVDM likelihood when the powerful networks transforming to and from latent space should make it scale-invariant. Explicitly separating out the contributions of a reimplemented base model, prior-posterior interpolation and the learned prior parameters would strengthen these experiments. Overall, the very strong improvements on the text modeling task over NVDM seem hard to understand, and I would like to see an ablation analysis of all the differences between that model and the proposed one.\n\nThe fact that adding more constant components helps for document modeling is interesting, and it would be nice to see more qualitative analysis of what the prior modes represent. I also would be surprised if posterior modes were highly separated, and if they were it would be interesting to explore if they corresponded to e.g. ambiguous word-senses.\n\nThe experiments on dialog modeling are mostly negative results, quantitatively. The observation that the the piecewise constant variables encode time-related words and the Gaussian variables encode sentiment is interesting, especially since it occurs in both sets of experiments. This is actually quite interesting, and I would be interested in seeing analysis of why this is the case. As above, I would like to see an analysis of the sorts of words that are encoded in the different prior modes and whether they correspond to e.g. groups of similar holidays or days.\n\nIn conclusion, I think the piecewise constant variational family is a good idea, although it is not well-motivated by the paper. The experimental results are very good for document modeling, but without ablation analysis against the baseline it is hard to see why they should be with such a small modification in G-NVDM. The fact that H-NVDM performs better is interesting, though. This paper should better motivate the need for different types of multi-modality, and demonstrate that those sorts of things are actually being captured by the model. As it is, the paper introduces an interesting variational family and shows that it performs better for some tasks, but the motivation and analysis is not clearly focused. To demonstrate that this is a broadly applicable family, it would also be good to do experiments on a more standard datasets like MNIST. Even without an absolute log-likelihood improvement, if the method yielded interpretable multiple modes this would be a valuable contribution.","label":0,"model":"human","source":"peerread","id":4678}
{"text":"This paper proposes a piecewise constant parameterisation for neural variational models so that it could explore the multi-modality of the latent variables and develop more powerful neural models. \nThe experiments of neural variational document models and variational hierarchical recurrent encoder-decoder models show that the introduction of the piecewise constant distribution helps achieve better perplexity on modelling documents and seemly better performance on modelling dialogues.\nThe idea of having a piecewise constant prior for latent variables is interesting, but the paper is not well-written (even 14 pages long) and the design of the experiments fails to demonstrate the most of the claims.  \n\nThe detailed comments are as follows:\n\n--The author explains the limitations of the VAEs with standard Gaussian prior in the last paragraph of 3.1 and the last paragraph of 5.1. Hence, a multimodal prior would help the VAEs overcome the issues of optimisation. However, there is a lack of evidence showing the multimodality of the prior helps break the bottleneck. \n\n--In the last paragraph of 6.1, the author claimed the decoder parameter matrix is directly affected by the latent variables. But what the connects the decoder is a combination of a piecewise constant and Gaussian latent variables. No matter what is discovered in the experiments, it only shows z=","label":0,"model":"human","source":"peerread","id":4679}
{"text":"The reparameterization does not seem to be differentiable? I.e. eq. (8) consists of indicator functions of the parameters you are optimizing with respect to.","label":0,"model":"human","source":"peerread","id":4680}
{"text":"This paper is incomplete. Most of results are blank.  What is the meaning of \"table XXXX\"? \nSuch strategy seems unfair....\n\nHowever, I agree that methods are good.  This paper should be submitted to ICML or workshop in ICLR.\n\nIf this type of method is allowed, I would wonder the credibility of papers in this conference. \n\n","label":0,"model":"human","source":"peerread","id":4681}
{"text":"This paper introduces pointer-network neural networks, which are applied to referring expressions in three small-scale language modeling tasks: dialogue modeling, recipe modeling and news article modeling. When conditioned on the co-reference chain, the proposed models outperform standard sequence-to-sequence models with attention.\n\nThe proposed models are essentially variants of pointer networks with copy mechanisms (Gulcehre et al., 2016; Gu et al., 2016; Ling et al., 2016), which have been modified to take into account reference chains. As such, the main architectural novelty lies in 1) restricting the pointer mechanism to focus on co-referenced entities, 2) applying pointer mechanism to 2D arrays (tables), and 3) training with supervised alignments. Although useful in practice, these are minor contributions from an architectural perspective.\n\nThe empirical contributions are centred around measuring perplexity on the three language modeling tasks. Measuring perplexity is typical for standard language modeling tasks, but is really an unreliable proxy for dialogue modeling and recipe generation performance. In addition to this, both the dialogue and recipe tasks are tiny compared to standard language modeling tasks. This makes it difficult to evaluate the impact of the dialogue and recipe modeling results. For example, if one was to bootstrap from a larger corpus, it seems likely that a standard sequence-to-sequence model with attention would yield performance comparable to the proposed models (with enough data, the attention mechanism could learn to align referring entities by itself). The language modeling task on news article (Gigaword) seems to yield the most conclusive results. However, the dataset for this task is non-standard and results are provided for only a single baseline. Overall, this limits the conclusions we can draw from the empirical experiments.\n\n\nFinally, the paper itself contains many errors, including mathematical errors, grammatical errors and typos:\n- Eq. (1) is missing a sum over $z_i$.\n- \"into the a decoder LSTM\" -> \"into the decoder LSTM\"\n- \"denoted as his\" -> \"denoted as\"\n- \"Surprising,\" -> \"Surprisingly,\"\n- \"torkens\" -> \"tokens\"\n- \"if follows that the next token\" -> \"the next token\"\n- In the \"COREFERENCE BASED LANGUAGE MODEL\" sub-section, what does $M$ denote?\n- In the sentence: \"The attribute of each column is denoted as $s_c, where $c$ is the c-th attribute\". For these definitions to be make sense, $s_c$ has to be a one-hot vector. If yes, please clarify this in the text.\n- \"the weighted sum is performed\" -> \"the weighted sum is computed\"\n- \"a attribute\" -> \"an attribute\"\n- In the paragraph on Pointer Switch, change $p(z_{i,v} |s_{i,v}) = 1$ -> $p(z_{i,v} |s_{i,v}) = 0$.\n- In the \"Table Pointer\" paragraph, I assume you mean outer-product instead of cross-product? Otherwise, I don't see how the equations add up.\n\n\nOther comments:\n- For the \"Attention based decoder\", is the attention computed using the word embeddings themselves or the hidden states of the sentence encoder? Also, it applied only to the previous turn of the dialogue or to the entire dialogue history? Please clarify this.\n- What's the advantage of using an \"Entity state update\" rule, compared to a pointer network or copy network, which you used in the dialogue and recipe tasks? Please elaborate on this.\n- In the Related Work section, the following sentence is not quite accurate: \"For the task oriented dialogues, most of them embed the seq2seq model in traditional dialogue systems while our model queries the database directly.\". There are task-oriented dialogue models which do query databases during natural language generation. See, for example, \"A Network-based End-to-End Trainable Task-oriented Dialogue System\" by Wen et al.","label":0,"model":"human","source":"peerread","id":4682}
{"text":"All of the reviewers point out clarity problems; while these may have been resolved in an updated version, the reviewers have not expressed that the matter is resolved. There are several questions raised about the use of perplexity, both whether the comparison is fair, and whether it is a valid proxy for more standard measures in NLP. The former seems to be more of an issue for this area chair, and the discussion did not convince me that it was adequately resolved.","label":0,"model":"human","source":"peerread","id":4683}
{"text":"This paper presents a new type of language model that treats entity references as latent variables. The paper is structured as three specialized models for three applications: dialog generation with references to database entries, recipe generation with references to ingredients, and text generation with coreference mentions.\n\nDespite some opaqueness in details that I will discuss later, the paper does a great job making the main idea coming through, which I think is quite interesting and definitely worth pursuing further. But it seems the paper was rushed into the deadline, as there are a few major weaknesses.\n\nThe first major weakness is that the claimed latent variables are hardly latent in the actual empirical evaluation. As clarified by the authors via pre-review QAs, all mentions were assumed to be given to all model variants, and so, it would seem like an over-claim to call these variables as latent when they are in fact treated as observed variables. Is it because the models with latent variables were too difficult to train right?\n\nA related problem is the use of perplexity as an evaluation measure when comparing reference-aware language models to vanilla language models. Essentially the authors are comparing two language models defined over different event space, which is not a fair comparison. Because mentions were assumed to be given for the reference-aware language models, and because of the fact that mention generators are designed similar to a pointer network, the probability scores over mentions will naturally be higher, compared to the regular language model that needs to consider a much bigger vocabulary set. The effect is analogous to comparing language models with aggressive UNK (and a small vocabulary set) to a language models with no UNK (and a much larger vocabulary set).\n\nTo mitigate this problem, the authors need to perform one of the following additional evaluations: either assuming no mention boundaries and marginalizing over all possibilities (treating latent variables as truly latent), or showing other types of evaluation beyond perplexity, for example, BLEU, METEOR, human evaluation etc on the corresponding generation task.\n\nThe other major weakness is writing in terms of technical accuracy and completeness. I found many details opaque and confusing even after QAs. I wonder if the main challenge that hinders the quality of writing has something to do with having three very specialized models in one paper, each having a lot of details to be worked out, which may have not been extremely important for the main story of the paper, but nonetheless not negligible in order to understand what is going on with the paper.    Perhaps the authors can restructure the paper so that the most important details are clearly worked out in the main body of the paper, especially in terms of latent variable handling \u2014 how to make mention detection and conference resolution truly latent, and if and when entity update helps, which in the current version is not elaborated at all, as it is mentioned only very briefly for the third application (coreference resolution) without any empirical comparisons to motivate the update operation.\n\n","label":0,"model":"human","source":"peerread","id":4684}
{"text":"This paper explores 3 language modeling applications with an explicit modeling of reference expressions: dialog, receipt generation and coreferences. While these are important tasks for NLP and the authors have done a number of experiments, the paper is limited for a few reasons:\n\n1. This paper is not clearly written and is pretty hard to follow some details. In particular,  there are many obvious math errors, such as missing the marginalization sum in Eq (1), and P(z_{i,v}...) = 1 (should be 0 here) on page 5, pointer switch section.\n\n2. The major novelty seems to be the 2-dimensional attention from the table and the pointer to the 2-D table. These are more of a customization of existing work to a particular task with 2-D tables as a part of the input to seq2seq model with both attentions and pointer networks.\n\n3. The empirical results are not very conclusive yet, limited by either the relatively small data size, or the lack of well-established baseline for some new applications (e.g., the recipe generation task).\n\nOverall, this paper, as it is for now, is more suitable for a workshop rather than for the main conference.","label":0,"model":"human","source":"peerread","id":4685}
{"text":"Hi, can you tell me the reference-chain is obtained by manual annotation or by some automatical tools such stanford corenlp ? ","label":0,"model":"human","source":"peerread","id":4686}
{"text":"The authors propose a method to generate adversarial examples w\/o relying on knowledge of the network architecture or network gradients.\n\nThe idea has some merit, however, as mentioned by one of the reviewers, the field has been studied widely, including black box setups.\n\nMy main concern is that the first set of experiments allows images that are not in image space. The authors acknowledge this fact on page 7 in the first paragraph. In my opinion, this renders these experiments completely meaningless. At the very least, the outcome is not surprising to me at all.\n\nThe greedy search procedure remedies this issue. The description of the proposed method is somewhat convoluted. AFAICT, first a candidate set of pixels is generated by using PERT. Then the pixels are perturbed using CYCLIC.\nIt is not clear why this approach results in good\/minimal perturbations as the candidate pixels are found using a large \"p\" that can result in images outside the image space. The choice of this method does not seem to be motivated by the authors.\n\nIn conclusion, while the authors to an interesting investigation and propose a method to generate adversarial images from a black-box network, the overall approach and conclusions seem relatively straight forward. The paper is verbosely written and I feel like the findings could be summarized much more succinctly.","label":0,"model":"human","source":"peerread","id":4687}
{"text":"While this is an interesting topic, both the method description and experimental setup could be improved.","label":0,"model":"human","source":"peerread","id":4688}
{"text":"\n\nPaper summary:\nThis work proposes a new algorithm to generate k-adversarial images by modifying a small fraction of the image pixels and without requiring access to the classification network weight.\n\n\nReview summary:\nThe topic of adversarial images generation is of both practical and theoretical interest. This work proposes a new approach to the problem, however the paper suffers from multiple issues. It is too verbose (spending long time on experiments of limited interest); disorganized (detailed description of the main algorithm in sections 4 and 5, yet a key piece is added in the experimental section 6); and more importantly the resulting experiments are of limited interest to the reader, and the main conclusions are left unclear.\nThis looks like an interesting line of work that has yet to materialize in a good document, it would need significant re-writing to be in good shape for ICLR.\n\n\nPros:\n* Interesting topic\n* Black-box setup is most relevant\n* Multiple experiments\n* Shows that with flipping only 1~5% of pixels, adversarial images can be created\n\n\nCons:\n* Too long, yet key details are not well addressed\n* Some of the experiments are of little interest\n* Main experiments lack key measures or additional baselines\n* Limited technical novelty\n\n\n\n\nQuality: the method description and experimental setup leave to be desired. \n\n\nClarity: the text is verbose, somewhat formal, and mostly clear; but could be improved by being more concise.\n\n\nOriginality: I am not aware of another work doing this exact same type of experiments. However the approach and results are not very surprising.\n\n\nSignificance: the work is incremental, the issues in the experiments limit potential impact of this paper.\n\n\nSpecific comments:\n* I would suggest to start by making the paper 30%~40% shorter. Reducing the text length, will force to make the argumentation and descriptions more direct, and select only the important experiments.\n* Section 4 seems flawed. If the modified single pixel can have values far outside of the [LB, UB] range; then this test sample is clearly outside of the training distribution; and thus it is not surprising that the classifier misbehaves (this would be true for most classifiers, e.g. decision forests or non-linear SVMs). These results would be interesting only if the modified pixel is clamped to the range [LB, UB].\n* [LB, UB] is never specified, is it ? How does p = 100, compares to [LB, UB] ? To be of any use, p should be reported in proportion to [LB, UB]\n* The modification is done after normalization, is this realistic ? \n* Alg 2, why not clamping to [LB, UB] ?\n* Section 6, \u201cimplementing algorithm LocSearchAdv\u201d, the text is unclear on how p is adjusted; new variables are added. This is confusion.\n* Section 6, what happens if p is _not_ adjusted ? What happens if a simple greedy random search is used (e.g. try 100 times a set of 5 random pixels with value 255) ?\n* Section 6, PTB is computed over all pixels ? including the ones not modified ? why is that ? Thus LocSearchAdv PTB value is not directly comparable to FGSM, since it intermingles with #PTBPixels (e.g. \u201cin many cases far less average perturbation\u201d claim).\n* Section 6, there is no discussion on the average number of model evaluations. This would be equivalent to the number of requests made to a system that one would try to fool. This number is important to claim the \u201ceffectiveness\u201d of such black box attacks. Right now the text only mentions the upper bound of 750 network evaluations. \n* How does the number of network evaluations changes when adjusting or not adjusting p during the optimization ?\n* Top-k is claimed as a main point of the paper, yet only one experiment is provided. Please develop more, or tune-down the claims.\n* Why is FGSM not effective for batch normalized networks ? Has this been reported before ? Are there other already published techniques that are effective for this scenario ? Comparing to more methods would be interesting.\n* If there is little to note from section 4 results, what should be concluded from section 6 ? That is possible to obtain good results by modifying only few pixels ? What about selecting the \u201ctop N\u201d largest modified pixels from FGSM ? Would these be enough ? Please develop more the baselines, and the specific conclusions of interest.\n\n\nMinor comments:\n* The is an abuse of footnotes, most of them should be inserted in the main text.\n* I would suggest to repeat twice or thrice the meaning of the main variables used (e.g. p, r, LB, UB)\n* Table 1,2,3 should be figures\n* Last line of first paragraph of section 6 is uninformative.\n* Very tiny -> small","label":0,"model":"human","source":"peerread","id":4689}
{"text":"The paper presents a method for generating adversarial input images for a convolutional neural network given only black box access (ability to obtain outputs for chosen inputs, but no access to the network parameters).  However, the notion of adversarial example is somewhat weakened in this setting: it is k-misclassification (ensuring the true label is not a top-k output), instead of misclassification to any desired target label.\n\nA similar black-box setting is examined in Papernot et al. (2016c).  There, black-box access is used to train a substitute for the network, which is then attacked.  Here, black-box access in instead exploited via local search.  The input is perturbed, the resulting change in output scores is examined, and perturbations that push the scores towards k-misclassification are kept.\n\nA major concern with regard to novelty is that this greedy local search procedure is analogous to gradient descent; a numeric approximation (observe change in output for corresponding change in input) is used instead of backpropagation, since one does not have access to the network parameters.  As such, the greedy local search algorithm itself, to which the paper devotes a large amount of discussion, is not surprising and the paper is fairly incremental in terms of technical novelty.\n","label":0,"model":"human","source":"peerread","id":4690}
{"text":"Authors propose the use of layer-wise language model-like pretraining for encoder-decoder models. This allows to leverage separate source and target corpora (in unsupervised manner) without necessity of large amounts of parallel training corpora. The idea is in principle fairly simple, and rely on initial optimising both encoder and decoder with LSTM tasked to perform language modelling. \n\nThe ideas are not new, and the paper is more like a successful compilation of several approaches that have been around for some time. The experimental validation, though, offers some interesting insights into importance of initialization, and the effectiveness of different initialisations approaches in enc-dec setting.\n\nThe regulariser you propose to use on page 3, looks like typical multi-task objective function, especially it is used in an alternating manner would be interesting to see whether similar performance might have been obtained starting with this objective, from random initialisation.\n\nYou should probably give credit for encoder-decoder like-RNN models published in 1990s.\n\nMinors:\nPg. 2, Sec 2.1 2nd paragraph: can be different sizes -> can be of different sizes","label":0,"model":"human","source":"peerread","id":4691}
{"text":"This paper effectively demonstrates that the use of pretraining can improve the performance of seq2seq models for MT and summarization tasks. However, despite these empirical gains, the reviewers were not convinced enough by the novelty of the work itself and did not feel like there were technical contributions to make this a fit for ICLR.\n \n Pros:\n - All reviewers agree that the empirical gains in this paper are convincing and lead to BLEU improvements on a large scale translation and translation like tasks. Reviewer 4 also praises the detailed analysis that demonstrates that these gains come from the pretraining process itself. \n - From an impact perspective, the reviewers found the approach clear and implementable. \n \n Cons:\n - Novelty criticisms are that the method is a \"compilation\" of past approaches (although at a larger scale) and therefore primarily experimental, and that the objectives given are \"highly empirical\" and not yet motivated by theory. The authors did respond, but the reviewer did not change their score.\n - There are suggestions that this type of work would perhaps be more widely impactful in an NLP venue, where a BLEU improvement of this regard is a strong supporting piece on its own.","label":0,"model":"human","source":"peerread","id":4692}
{"text":"In this paper, the authors propose to pretrain the encoder\/decoder of seq2seq models on a large amount of unlabeled data using a LM objective. They obtain improvements using this technique on machine translation and abstractive summarization.\n\nWhile the effectiveness of pretraining seq2seq models has been known among researchers and explored in a few papers (e.g. Zoph et al. 2016,  Dai and Le 2015), I believe this is the first paper to pretrain using a LM for both the encoder\/decoder. The technique is simple, but the gains are large (e.g. +2.7 BLEU on NMT). In addition, the authors perform extensive ablation studies to analyze where the performance is coming from. Hence, I think this paper should be accepted.\n","label":0,"model":"human","source":"peerread","id":4693}
{"text":"strengths:\n\nA method is proposed in this paper to initialize the encoder and decoder of the seq2seq model using the trained weights of language models with no parallel data. After such pretraining, all weights are jointly fine-tuned with parallel labeled data with an additional language modeling loss.\n\nIt is shown that pretraining accelerates training and improves generalization of seq2seq models.\n\nThe main value of the proposed method is to leverage separate source and target corpora, contrasting the common methods of using large amounts of parallel training corpora.\n\n\nweaknesses:\n\nThe objective function shown in the middle of pg 3 is highly empirical, not directly linked to how non-parallel data helps to improve the final prediction results. The paper should compare with and discuss the objective function based on expectation of cross entropy which is directly linked to improving prediction results as proposed in arXiv:1606.04646, Chen et al.:  Unsupervised Learning of Predictors from Unpaired Input-Output Samples, 2016.\n\nThe pre-training procedure proposed in this paper is also closely connected with the DNN pretraining method presented in Dahl et al. 2011, 2012. Comparisons should be made in the paper, highlighting why the proposed one is conceptually superior if the authors believe so. \n","label":0,"model":"human","source":"peerread","id":4694}
{"text":"what's your own baseline for NMT without pre-training? Jean et al. (2015) uses a more shallow architecture than this paper, so I presume your baseline would be higher (this is also corrobated in figure 3).\n\nyou should make it clear how much of your improvement over related work comes from pretraining, and how much from having a deeper architecture.","label":0,"model":"human","source":"peerread","id":4695}
{"text":"The paper is a novel application for the sticky HDP-HMM, focused on correctly identifying the number of components in bird and whale song across a variety of datasets. It's nice to see the model applied to an interesting dataset. My main issues with the paper have to do with structure and the choice of representation used in the model. Namely:\n\nThe organization of the paper could be significantly improved. There is a lot of repetitive introduction that adds little to the paper. The first and last two sentences of the abstract could be cut. Many other parts of the abstract basically repeat the introduction. The second paragraph of section 2.3 also repeats your introduction - by now we know what you're doing. I think most people reading this will have no idea what Kershenbaum (2014) is. The description of the data should go in the experiments section. \"Different hypotheses for the songs were emitted\" in the introduction is odd phrasing. Figure 4 should be the first figure and go in the introduction. Figure 5 should be in the methods section. A summary of Table 1 should be in the experiments section. Generally the writing could be tightened quite a bit, which would make space for these figures. The description of the HDP-HMM, which mostly follows the existing literature, is well done.\n\nSome general questions about the methods used:\n\nIf you're interested in scalable inference, why use Gibbs sampling? Why not the beam sampler (van Gael 2008), which at least recently was the state of the art for MCMC inference in the HDP-HMM? More generally, why use MCMC at all? For very large datasets, most of the Bayesian ML community has converged on stochastic variational inference as the most practical method (eg Wang, Paisley and Blei 2011).\n\nIf your interest is mainly in the number of clusters, how would you address the fact that DP mixture models are known not to be consistent for estimating the true number of clusters (Miller and Harrison 2013)?\n\nMFCC features are calibrated to the human auditory system, not bird or whale auditory systems. In your data, do you calibrate the MFCC scale to be closer to the auditory systems of the animals that generated the song?\n\nAnd a final suggestion for future work, which could use the results presented here as a baseline:\n\nGiven the success of LSTMs in speech recognition in recent years, it may be the case that deep learned representations are superior to linear features (like the means of each cluster in an HDP-HMM) for animal song as well. Have you considered a hybrid model, similar to recent work combining autoencoders and graphical models (Johnson, Duvenaud, Wiltschko, Datta and Adams 2016)?","label":0,"model":"human","source":"peerread","id":4696}
{"text":"The paper studies an unusual and (apparently) challenging application -- segmentation of whale and bird songs. Though the authors claim that the applications is much more challenging than previous applications of the proposed techniques (in speech processing), the evaluation is very questionable (as there is no gold standard), there is no convincing comparison with other (potentially simpler techniques). Overall, the reviewers believe the work is not mature enough to be accepted at ICLR.\n \n + interesting dataset \/ task \n \n - novelty is limited\n - evaluation is weak\n - writing is poor","label":0,"model":"human","source":"peerread","id":4697}
{"text":"This paper applies HDP-HMM to challenging bioacoustics segmentation problems including humpback whole sound and bird sound segmentation. Although the technique itself is not novel, the application of this data-driven method to bioacoustics segmentation is quite challenging, and may yield some scientific findings, and this is a valuable contribution to the bioacoustics field. My concern for this paper is that it does not have fair comparison of the other simple methods including BIC and AIC, and it is better to provide such comparisons. Especially, as the authors pointed out, the computational cost of HDP-HMM is a big issue, and the other simple methods may solve this issue.","label":0,"model":"human","source":"peerread","id":4698}
{"text":"This paper presented an unsupervised approach for the automatic segmentation of bioacoustic data. The authors applied an existing approach (Hierarchical Dirichlet Process Hidden Markov Models) to their task. The originality of their work is the investigation of this approach on a new task, which they argue is more difficult, namely bioacoustic segmentation. They provide evidence that this is a difficult task by explaining that there doesn't exist a consensus among human experts on how this should be done. However, they do not provide convincing results that their approach is successful, as it fails in many cases to replicate the correct segmentations as defined by their baseline: human experts. In addition, the clarity of the writing is extremely poor, including many grammatical errors and awkward sentences. ","label":0,"model":"human","source":"peerread","id":4699}
{"text":"The authors proposed RASOR to address the problem of finding the best answer span according to a given question. The focus of the paper is mainly on how to model the relationship between question and the answer spans. The idea proposed by this paper is reasonable, but not ground breaking. The analysis is interesting and potentially useful. I would hope the authors can go extra miles to analyze different choices of boundary prediction models and make a more convincing case for the necessity of modeling the score of the span globally.\n\nThe main idea behind RASOR is to globally normalize and rank the scores of the possible answer spans. RASOR is able to achieve this by first modeling the hidden vectors of all words with LSTMs. Then, the representation of a text span is formed by concatenating the corresponding hidden vectors of the start and the end word of the corresponding chunk. The approach is reasonable, but not earth shattering. Also, the table 6 shows that the improvement over end-prediction point is not very large.\n\nI appreciate the fact that authors conduct several analysis experiments as some of them are quite interesting. For example, it seems that question independent representation is also very import to the performance. In addition to the current analysis, I also want to get a clear idea on what makes the current model be better than the Match-LSTM. Is it hyper-parameter tuning? Or it is due to the use of the question independent representation?\n\nAnother good thing about the proposed model is that it is relatively simple, so there is a chance that the proposed techniques can be combined with other newly proposed ones.","label":0,"model":"human","source":"peerread","id":4700}
{"text":"The program committee appreciates the authors' response to concerns raised in the reviews. Unfortunately, most reviewers are not leaning sufficiently towards acceptance. In particular, it is unfortunate that authors can not evaluate their model on the leaderboard due to copyright issues. The role of standard datasets and benchmarks is to allow for meaningful comparisons. Evaluation on non-standard splits defeats this purpose. Fortunately, sounds like authors are working on getting their model evaluated on the leaderboard. Resolving that and incorporating reviewers' feedback will help make the paper stronger.","label":0,"model":"human","source":"peerread","id":4701}
{"text":"We thank all three reviewers for the valuable comments and suggestions. \n\nWe agree with reviewer 1 that the lack of test results is not ideal and sadly we do not yet have a manner in which we can run on the hidden test set, as not all of our code is open-sourced. However, we hope that the significant dev set size of 10k items, along with the cross-validation results add some reassurance that our hyperparameter tuning scheme has not overfit the data.\n\nReviewer 3 points out that the results in this paper are no longer state of the art. It is true that there are other papers on the leaderboard that have now surpassed our results, largely through ensembling. However, we believe that our paper is the only work to specifically study the impact of different span representations and we agree with Reviewer 3 that our findings should be complementary to other recent work on this dataset. We have added some extra quantitative and qualitative analysis of the differences between the span classifier and the endpoints predictor to illustrate the manner in which the quality of endpoint predictions degrade for longer sentences, in particular showing the tendency of endpoint models to pick out endpoints from separate answer candidates.\n\nReviewer 2 points out that the difference in performance between our model and the Match-LSTM cannot be accounted for by the difference in label type alone, and asks for the other most salient differences between the two approaches. While there are many small differences between the two implementations, the ablations in Table 2.a. suggest that most of this gap is accounted for by the passage independent question representation that is missing in the Match-LSTM. We have added an analysis of this representation in a new Table 3 and we have updated our discussion of the Match LSTM to clarify the basis of our comparison.","label":0,"model":"human","source":"peerread","id":4702}
{"text":"This paper proposes RaSoR, a method to efficiently representing and scoring all possible spans in an extractive QA task. While the test set results on SQuAD have not been released, it looks likely that they are not going to be state-of-the-art; with that said, the idea of enumerating all possible spans proposed in this paper could potentially improve many architectures. The paper is very well-written and the analysis\/ablations in the final sections are mostly interesting (especially Figure 2, which confirms what we would intuitively believe). Based on its potential to positively impact other researchers working on SQuAD, I recommend that the paper is accepted. ","label":0,"model":"human","source":"peerread","id":4703}
{"text":"This paper presents an architecture for answer extraction task and evaluates on the SQUAD dataset. The proposed model builds fixed length representations of all spans in the answer document based on recurrent neural network. It outperforms a few baselines in exact match and F1 on SQUAD.\n\nIt is unfortunate that the blind test results are not obtained yet due to the copyright issue. There are quite a few other systems\/submissions on the SQUAD leader board that were available for comparison.\n\nGiven that there's no result on the test set reported, the grid search for hyperparameters on the dev set directly is also a concern, even though the authors did cross validation experiments.\n","label":0,"model":"human","source":"peerread","id":4704}
{"text":"The authors proposed RASOR to address the problem of finding the best answer span according to a given question. The focus of the paper is mainly on how to model the relationship between question and the answer spans. The idea proposed by this paper is reasonable, but not ground breaking. The analysis is interesting and potentially useful. I would hope the authors can go extra miles to analyze different choices of boundary prediction models and make a more convincing case for the necessity of modeling the score of the span globally.\n\nThe main idea behind RASOR is to globally normalize and rank the scores of the possible answer spans. RASOR is able to achieve this by first modeling the hidden vectors of all words with LSTMs. Then, the representation of a text span is formed by concatenating the corresponding hidden vectors of the start and the end word of the corresponding chunk. The approach is reasonable, but not earth shattering. Also, the table 6 shows that the improvement over end-prediction point is not very large.\n\nI appreciate the fact that authors conduct several analysis experiments as some of them are quite interesting. For example, it seems that question independent representation is also very import to the performance. In addition to the current analysis, I also want to get a clear idea on what makes the current model be better than the Match-LSTM. Is it hyper-parameter tuning? Or it is due to the use of the question independent representation?\n\nAnother good thing about the proposed model is that it is relatively simple, so there is a chance that the proposed techniques can be combined with other newly proposed ones. \n\n","label":0,"model":"human","source":"peerread","id":4705}
{"text":"Thank you for an interesting read.\n\nI found the application of VRNN type generative model to financial data very promising. But since I don't have enough background knowledge to judge whether the performance gap is significant or not, I wouldn't recommend acceptance at this stage. \n\nTo me, the biggest issue for this paper is that I'm not sure if the paper contains significant novelty. The RNN-VAE combination has been around for more than a year and this paper does not propose significant changes to it. Maybe this paper fits better to an application targeting conference, rather than ICLR. But I'm not exactly sure about ICLR's acceptance criteria, and maybe the committee actually prefer great performances and interesting applications?","label":0,"model":"human","source":"peerread","id":4706}
{"text":"This paper presents an interesting application of variational methods for time series, in particular VRNN-like approaches, for stochastic volatility. Applications such as these are clearly in the scope of the conference, which was a point that one of the reviewer brought up. That said, questions of context, especially with regards to relation to different approaches, especially smoothing are relevant. Also, the number of methods GRACH and stochastic volatility is immense and this makes assessing the impact of this very hard to do and is more is needed to address this point. This paper is certainly interesting, but given these concerns, the paper is not yet rady for acceptance at the conference.","label":0,"model":"human","source":"peerread","id":4707}
{"text":"The authors propose a recurrent variational neural network approach to modelling volatility in financial time series. This model consists of an application of Chung et al.\u2019s (2015) VRNN model to volatility forecasting, wherein a Variational Autoencoder (VAE) structure is repeated at each time step of the series. \n\nThe paper is well written and easy to follow (although this reviewer suggests applying a spelling checking, since the paper contains a number of harmless typos). \n\nThe paper\u2019s main technical contribution is to stack two levels of recurrence, one for the latent process and one for the observables. This appears to be a novel, if minor contribution. The larger contribution is methodological, in areas of time series modelling that are both of great practical importance and have hitherto been dominated by rigid functional forms. The demonstration of the applicability and usefulness of general-purpose non-linear models for volatility forecasting would be extremely impactful.\n\nI have a few comments and reservations with the paper:\n1) Although not  mentioned explicitly, the authors\u2019 framework are couched in terms of carrying out one-timestep-ahead forecasts of volatility. However, many applications of volatility models, for instance for derivative pricing, require longer-horizon forecasts. It would be interesting to discuss how this model could be extended to forecast at longer horizons.\n \n2) In Section 4.4, there\u2019s a mention that a GARCH(1,1) is conditionally deterministic. This is true only when forecasting 1 time-step in the future. At longer horizons, the GARCH(1,1) volatility forecast is not deterministic. \n\n3) I was initially unhappy with the limitations of the experimental validation, limited to comparison with a baseline GARCH model. However, the authors provided more comparisons in the revision, which adds to the quality of the results, although the models compared against cannot be considered state of the art. It would be well advised to look into R packages such as `stochvol\u2019 and \u2018fGarch\u2019 to get implementations of a variety of models that can serve as useful baselines, and provide convincing evidence that the modelled volatility is indeed substantially better than approaches currently entertained by the finance literature.\n\n4) In Section 5.2, more details should be given on the network, e.g. number of hidden units, as well as the embedding dimension D_E (section 5.4)\n\n5) In Section 5.3, more details should be given on the data generating process for the synthetic data experiments. \n\n6) Some results in the appendix are very puzzling: around jumps in the price series, which are places where the volatility should spike, the model reacts instead by huge drops in the volatility (Figure 4(b) and (c), respectively around time steps 1300 and 1600). This should be explained and discussed.\n\nAll in all, I think that the paper provides a nice contribution to the art of volatility modelling. In spite of some flaws, it provides a starting point for the broader impact of neural time series processing in the financial community.\n","label":0,"model":"human","source":"peerread","id":4708}
{"text":"\nThe authors propose a recurrent neural network approach for constructing a\nstochastic volatility model for financial time series. They introduce an\ninference network based on a recurrent neural network that computes the\napproximation to the posterior distribution for the latent variables given the\npast data. This variational approximation is used to maximize the marginal\nlikelihood in order to learn the parameters of the model. The proposed method\nis validated in experiments with synthetic and real-world time series, showing\nto outperform parametric GARCH models and a Gaussian process volatility model.\n\nQuality:\n\nThe method proposed seems technically correct, with the exception that in\nequation (19) the inference model is doing filtering and not smoothing, in the\nsense that the posterior for z_t' only depends on those other z_t and x_t\nvalues with t","label":0,"model":"human","source":"peerread","id":4709}
{"text":"Authors present a parameterized variant of ELU and show that the proposed function helps to deal with vanishing gradients in deep networks in a way better than existing non-linearities. They present both a theoretical analysis and practical validation for presented approach. \n\nInteresting observations on statistics of the PELU parameters are reported. Perhaps explanation for the observed evolution of parameters can help better understand the non-linearity. It is hard to evaluate the experimental validation presented given the difference in number of parameters compared to other approaches.","label":0,"model":"human","source":"peerread","id":4710}
{"text":"The paper describes a parametric version of the exponential linear unit (ELU) activation function. The novelty of the contribution is limited, and the experimental evaluation in its current form is not convincing.","label":0,"model":"human","source":"peerread","id":4711}
{"text":"The paper deals with a very important issue of vanishing gradients and the quest for a perfect activation function. Proposed is an approach of learning the activation functions during the training process. I find this research very interesting, but I am concerned that the paper is a bit premature.\n\nThere is a long experimental section, but I am not sure what the conclusion is. The authors appear to be somewhat confused themselves. The amount of \"maybe\" \"could mean\", \"perhaps\" etc. statements in the paper is exceptionally high. For this paper to be accepted it needs a bold statement about the performance, with a solid evidence. In my opinion, that is lacking as of now. This approach is either a breakthrough or a dud, and after reading the paper I am not convinced which case it is.\n\nThe theoretical section could be made a little clearer.\n\nFinally, how is the performance affected. The huge advantage if ReLU is in the fact that the formula is so simple and thus not costly to evaluate. How do PELU-s compare.","label":0,"model":"human","source":"peerread","id":4712}
{"text":"This paper presents a new non-linear function for CNN and deep neural networks. \nThe new non-linearity reports some gains on most datasets of interest, and can be used in production networks with minimal increase in computation.","label":0,"model":"human","source":"peerread","id":4713}
{"text":"This paper proposes a modification of the ELU activation function for neural networks, by parameterizing it with 2 trainable parameters per layer. This parameter is proposed to more effectively counter vanishing gradients. \n\nMy main concern regarding this paper is related to the authors' claims about the effectiveness of PELU. The analysis in Sections 2 and 3 discusses how PELU might improve training by combating gradient propagation issues. This by itself does not imply that improved generalization will result, only that models may be easier to train. However, the experiments all seek to demonstrate improved generalization performance.\nBut this could in principle be due to a better inductive bias, and have nothing to do with the optimization analysis. None of the experiments are designed to directly support the stated theoretical advantage of PELU compared to ELU in optimizing models.\n\nIn the response to the pre-review question, the authors state that the claims in Section 2 and 3.3 are meant to apply to generalization performance. I fail to see how this is true for most claims, except the flexibility claim. As the authors agree, better training may or may not lead to better out-of-sample performance. I can only agree that having flexibility can sometimes help the network adapt its inductive bias to the problem (instead of overfitting), but this is a much weaker claim compared to the mathematical justifications for improved optimization.\n\nOn selection of learning hyperparameters:\nThe authors state in the discussion on OpenReview that the learning rates selected were favorable to ReLU, and not PELU. However, this does not guarantee that they were not unfavorable to ELU. It raises the question: can a regime be constructed where ELU has better performance than PELU? If so, how can we draw the conclusion that PELU is better?\n\nOverall, I am not yet convinced by the experimental setup and the match between theory and experiments in this paper.","label":0,"model":"human","source":"peerread","id":4714}
{"text":"Authors present a parameterized variant of ELU and show that the proposed function helps to deal with vanishing gradients in deep networks in a way better than existing non-linearities. They present both a theoretical analysis and practical validation for presented approach. \n\nInteresting observations on statistics of the PELU parameters are reported. Perhaps explanation for the observed evolution of parameters can help better understand the non-linearity. It is hard to evaluate the experimental validation presented given the difference in number of parameters compared to other approaches.\n ","label":0,"model":"human","source":"peerread","id":4715}
{"text":"This paper proposed to use unsupervised learning to learn features in a reinforcement learning setting. It is unclear what \"unsupervised\" means here since the \"causality prior\" uses reward signals for training. This is reinforcement learning, not unsupervised learning.\n\nThe experiments are also very premature. The task is as simple as moving the head of the robot left or right. There is also no comparison to baselines.\n\nIn conclusions section, the authors claim the proposed method can be used for transfer learning without experiments to backup the claim.\n\nOverall this paper is confusing and premature.","label":0,"model":"human","source":"peerread","id":4716}
{"text":"The authors apply an already-published method for state representation learning in a very simple experimental scenario. They give no additional contribution or comparison, nor do they offer any empirical or analytical study.","label":0,"model":"human","source":"peerread","id":4717}
{"text":"Dear reviewers,\n\nwe agreed that comparisons with other works need to be done and that other tasks have to be tested to improve evaluation and validation.\nIt will be done in future experiments.","label":0,"model":"human","source":"peerread","id":4718}
{"text":"This paper implements the method of Jonschkowski & Brock to learn a low-dimensional state representation represented as the last layer of a neural network. The experiments apply the method for learning a one-dimensional state representation of a simulated robot\u2019s head position from synthetic images.\n\nLearning state representations is an active and useful area of research for learning representations in interactive domains such as robotics. However, there seems to be no novelty in the method, over Jonschkowki & Brock. The primary contribution is the experimental evaluation performed on one task, where the paper evaluates the correlation between the learned state representation and the ideal state representation for the task (which is the robot\u2019s head position).\n\nAs acknowledged by the authors, the experiments are very preliminary, only showing one simple task with a one-dimensional learned representation and a two-dimensional discrete action space. To make the experiments compelling, there need to be comparisons to prior methods such as Lange et al. \u201912, Watter et al. NIPS \u201915, and Finn et al. ICRA \u201916 which also learn state representations from raw images. PCA on the images would also be a useful comparison, especially for simple tasks. Without these comparisons, it is impossible to evaluate the effectiveness of the method.\n\nLastly, as mentioned in the pre-review questions, the related work should include a discussion of other state representation learning methods such as Watter et al. NIPS \u201915, Finn et al. ICRA \u201916, and van Hoof et al. IROS \u201916.\n\nIn summary, this paper lacks novelty and significance, as the paper implements an existing method and demonstrates results on only one simple task. Without comparisons, the results are impossible to interpret. More challenging tasks and experimental comparisons would significantly improve the paper. Additionally, this paper does not introduce any novel contributions to state representation learning for solving challenges in this domain. One pro is that the paper is generally written clearly.","label":0,"model":"human","source":"peerread","id":4719}
{"text":"The paper proposes to use the representation learning approach of [Jonschkowski & Brock, 2015] with a deep network as function approximator. The general task and approach are interesting, but contribution of this work is limited, and experimental evaluation is absolutely unsatisfactory, so the paper cannot be accepted for publications. \n\nThe approach is tested on a simple synthetic task with very small training and test sets and very little variation in the data. The authors admitted themselves that the results are preliminary. The proposed method is not compared with existing approaches or simple hand-crafted baselines. It is impossible to judge if the proposed method is useful and\/or performs well compared to existing approaches. This makes the paper unfit for publication. \n\nWith proper experiments, and if the method works in interesting realistic scenarios, this could become a good paper.\n","label":0,"model":"human","source":"peerread","id":4720}
{"text":"Summary: There are many different pruning techniques to reduce memory footprint of CNN models, and those techniques have different granularities (layer, maps, kernel or intra kernel), pruning ratio and sparsity of representation. The work proposes a method to choose the best pruning masks out to many trials. Tested on CIFAR-10, SVHN and MNIST.\n\nPros:\nProposes a method to choose pruning mask out of N trials. \nAnalysis on different pruning methods.\n\nCons & Questions:\n\u201cThe proposed strategy selects the best pruned network through N random pruning trials. This approach enables one to select pruning mask in one shot and is simpler than the multi-step technique.\u201d How can one get the best pruning mask in one shot if you ran N random pruning trials? (answered)\nMissing tests of the approach with bigger CNN: like AlexNet, VGG, GoogLeNet or ResNet. (extended to VGG ok)\nSince reducing model size for embedded systems is the final goal, then showing how much memory space in MB is saved with the proposed technique compared with other approaches like Han et al. (2015) would be good.\n\nMisc:\nTypo in figure 6 a) caption: \u201cFeatuer\u201d (corrected)","label":0,"model":"human","source":"peerread","id":4721}
{"text":"Unfortunately this paper is not competitive enough for ICLR. The paper is focusing on efficiency where for the results to be credible it is of utmost importance to present experiments on large scale data and state-of-the-art models.\n I am afraid the reviewers were not able to take into account the latest drafts of the paper that were submitted in January, but those submissions came very late.","label":0,"model":"human","source":"peerread","id":4722}
{"text":"Dear reviewers,\n\ncan you please take a look at the responses by the authors and add a comment indicating that you have taken them into consideration?\n\nThanks!\n","label":0,"model":"human","source":"peerread","id":4723}
{"text":"This paper proposes a simple randomized algorithm for selecting which weights in a ConvNet to prune in order to reduce theoretical FLOPs when evaluating a deep neural network. The paper provides a nice taxonomy or pruning granularity from coarse (layer-wise) to fine (intra-kernel). The pruning strategy is empirically driven and uses a validation set to select the best model from N randomly pruned models. Makes claims in the intro about this being \"one shot\" and \"near optimal\" that cannot be supported: it is \"N-shot\" in the sense that N networks are generated and tested and there is no evidence or theory that the found solution is \"near optimal.\"\n\nPros:\n- Nice taxonomy of pruning levels\n- Comparison to the recent weight-sum pruning method\n\nCons:\n- Experimental evaluation does not touch upon recent models (ResNets) and large scale datasets (ImageNet)\n- Paper is somewhat hard to follow\n- Feature map pruning can obviously accelerate computation without specialized sparse implementations of convolution, but this is not the case for finer grained sparsity; since this paper considers fine-grained sparsity it should provide some evidence that introducing that sparsity can yield performance improvements\n\nAnother experimental downside is that the paper does not evaluate the impact of filter pruning on transfer learning. For example, there is not much direct interest in the tasks of MNIST, CIFAR10, or even ImageNet. Instead, a main interest in both academia and industry is the value of the learned representation for transferring to other tasks. One might expect pruning to harm transfer learning. It's possible that the while the main task has about the same performance, transfer learning is strongly hurt. This paper has missed an opportunity to explore that direction.\n\nIn summary, the proposed method is simple, which is good, but the experimental evaluation is somewhat incomplete and does not cover recent models and larger scale datasets.","label":0,"model":"human","source":"peerread","id":4724}
{"text":"This paper proposes two pruning methods to reduce the computation of deep neural network. In particular, whole feature maps and the kernel connections can be removed with not much decrease of classification accuracy. \nHowever, this paper also has the following problems. \n1)\tThe method is somehow trivial, since the pruning masks are mainly chosen by simple random sampling. The novelty and scalability are both limited. \n2)\tExperiment results are mainly focused on the classification rate and the ideal complexity. As a paper on improving computation efficiency, it should include results on practical time consumption. It is very common that reducing numbers of operations may not lead to reduced computational time on a highly parallel platform (e.g., GPU). \n3)\tIt is more important to improve the computational efficiency on large-scale models (e.g., ImageNet classification network) than on small models (e.g., MNIST, CIFAR network). However, results on large-scale network is missing.\n4)\t(*Logical validity of the proposed method*) For feature map pruning, what if just to train reduced-size network is trained from scratch without transfer any knowledge from the pretrained large network? Is it possible to get the same accuracy? If so, it will simply indicate the hyper-parameter is not optimal for the original network. Experimental results are necessary to clarify the necessity of feature map pruning. \nNote that I agree with that a smaller network may be more generalizable than a larger network. \n\n----------------------------------------------\n\nComments to the authors's response:\n\nThanks for replying to my comments. \n\n1) I still believe that the proposed methods are trivial.\n2) It is nice to show GPU implementation. Compared to existing toolboxes (e.g., Torch, Caffe, TensorFlow), is the implementation of convolution efficient enough?\n3) Experiments on Cifar-100 are helpful (better than cifar-10), but it is not really large-scale, where speed-up is not so critical. ImageNet and Places datasets are examples of large-scale datasets.\n4) The author did not reply to the question wrt the validity of the proposed methods. This question is critical.   \n\n","label":0,"model":"human","source":"peerread","id":4725}
{"text":"Summary: There are many different pruning techniques to reduce memory footprint of CNN models, and those techniques have different granularities (layer, maps, kernel or intra kernel), pruning ratio and sparsity of representation. The work proposes a method to choose the best pruning masks out to many trials. Tested on CIFAR-10, SVHN and MNIST.\n\nPros:\nProposes a method to choose pruning mask out of N trials. \nAnalysis on different pruning methods.\n\nCons & Questions:\n\u201cThe proposed strategy selects the best pruned network through N random pruning trials. This approach enables one to select pruning mask in one shot and is simpler than the multi-step technique.\u201d How can one get the best pruning mask in one shot if you ran N random pruning trials? (answered)\nMissing tests of the approach with bigger CNN: like AlexNet, VGG, GoogLeNet or ResNet. (extended to VGG ok)\nSince reducing model size for embedded systems is the final goal, then showing how much memory space in MB is saved with the proposed technique compared with other approaches like Han et al. (2015) would be good.\n\nMisc:\nTypo in figure 6 a) caption: \u201cFeatuer\u201d (corrected)\n","label":0,"model":"human","source":"peerread","id":4726}
{"text":"The paper introduces a lightweight network for semantic segmentation that combines several acceleration ideas.\nAs indicated in my preliminary question, the authors do not make the case about why any of the techniques they propose is beyond what we know already: factorizing filters into alternating 1-D convolutions, using low-rank kernels, or any of the newer inception network architectures.\n\nI have had a hard time figuring out what is the take-home message of this paper. All of these ideas are known, and have proven their worth for detection. If a paper is going to be accepted for applying them to semantic segmentation, then in the next conference another paper should be accepted for applying them to normal estimation, another to saliency estimation and so on. \n\nAs the authors mention in their preliminary review:\n\"I agree that most improvements from classification architectures are straightforward to apply to object segmentation, and that's exactly what we've done - our network is based on current state of the art models. Instead of repeating most of the discussion on factorizing filters, etc., that has been discussed in a lot of papers already, we have decided that it's much more valuable to describe in depth the choices that are related to segmentation only - these are the most important contributions of our paper.\"\n\nI do not see however any in-depth discussion of certain choices - e.g. an analysis of how certain choices influence performance or speed. Instead all one gets are some statements \"these gave a significant accuracy boost\" \"this helped a lot\", \"that did not help\", \"this turned out to work much better than that\" . This is not informative - and is more like an informal chat rather than an in-depth discussion. \n\nIf novelty is not that important, and it is only performance or speed that matter, I am still not convinced.\nThe authors only compare to [1,2] (SegNet) in terms of both accuracy and speed. I cannot see the reason why they do so, and they do not really justify it. According to the authors' evaluation, [1] requires ~1 sec. per frame,  while Deeplab v2, without the DenseCRF, runs at 5-8fps. \n(","label":0,"model":"human","source":"peerread","id":4727}
{"text":"Three knowledgable reviewers recommend rejection and there was no rebuttal. The AC agrees with the reviewers.","label":0,"model":"human","source":"peerread","id":4728}
{"text":"\nPaper summary: this work presents ENet, a new convnet architecture for semantic labeling which obtains comparable performance to the previously existing SegNet while being ~10x faster and using ~10x less memory. \n\n\nReview summary: Albeit the results seem interesting, the paper lacks detailed experimental results, and is of limited interest for the ICLR audience.\n\n\nPros:\n* 10x faster\n* 10x smaller\n* Design rationale described in detail\n\n\nCons:\n* The quality of the reference baseline is low. For instance, cityscapes results are 58.3 IoU while state of the art is ~80 IoU. Thus the results are of limited interest.\n* The results that support the design rationale are not provided. It is important to provide the experimental evidence to support each claim.\n\n\nQuality: the work is interesting but feels incomplete. If your model is 10x faster and smaller, why not try build a model 10x longer to obtain improved results ? The paper focuses only on  nimbleness at the cost of quality (using a weak baseline). This limits the interest for the ICLR audience.\n\n\nClarity: the overall text is somewhat clear, but the model description (section 3) could be more clear. \n\n\nOriginality: the work is a compendium of \u201cpractitioners wisdom\u201d applied to a specific task. It has thus limited originality.\n\n\nSignificance: I find the work that establishes a new \u201cbest practices all in one\u201d quite interesting, but however these must shine in all aspects. Being fast at the cost of quality, will limit the impact of this work.\n\n\nMinor comments:\n* Overall the text is proper english but the sentences constructions is often unsound, specific examples below. \n* To improve the chances of acceptance, I invite the authors to also explore bigger models and show that the same \u201ccollected wisdom\u201d can be used both to reach high speed and high quality (with the proper trade-off curve being shown). Aiming for only one end of the quality versus speed curve limits too much the paper.\n* Section 1: \u201cmobile or battery powered \u2026 require rates > 10 fps\u201c. 10 fps with which energy budget ? Should not this be  > 10 fps && < X Watt.\n* \u201cRules and ideas\u201d -> rules seem too strong of a word, \u201cguidelines\u201d ?\n* \u201cIs of utmost importance\u201d -> \u201cis of importance\u201d (important is already important)\n* \u201cPresents a trainable network \u2026 therefore we compare to \u2026 the large majority of inference the same way\u201d; the sentence makes no sense to me, I do not see the logical link between before and after \u201ctherefore\u201d\n* Scen-parsing -> scene-parsing\n* It is arguable if encoder and decoder can be called \u201cseparate\u201d\n* \u201cUnlike in Noh\u201d why is that relevant ? Make explicit or remove\n* \u201cReal-time\u201d is vague, you mean X fps @ Y W ?\n* Other existing architectures -> Other architectures\n* Section 3, does not the BN layer include a bias term ? Can you get good results without any bias term ?\n* Table 1: why is the initial layer a downsampling one, since the results has half the size of the input ?\n* Section 4, non linear operations. What do you mean by \u201csettle to recurring pattern\u201d ?\n* Section 4, dimensionality changes. \u201cComputationally expensive\u201d, relative to what ?\n* Section 4, dimensionality changes. \u201cThis technique ... speeds-up ten times\u201d, but does not provide the same results. Without an experimental validation changing an apple for an orange does not make the orange better than the apple.\n* Section 4, dimensionality changes. \u201cFound one problem\u201d, problem would imply something conceptually wrong. This is more an \u201cissue\u201d or an \u201cmiss-match\u201d when using ResNet for semantic labelling.\n* Section 4, factorizing filters. I am unsure of why you call nx1 filter asymmetric. A filter could be 1xn yet be symmetric (e.g. -2 -1 0 1 2). Why not simply call them rectangular filters ?\n* Section 4, factorizing filters. Why would this change increase the variety ? I would have expected the opposite.\n* Section 4, regularization. Define \u201cmuch better\u201d.\n* Section 5.1; \u201c640x360 is adequate for practical applications\u201d; for _some_ applications.\n* Section 5.2, \u201cvery quickly\u201d is vague and depends on the reader expectations, please be quantitative.\n* Section 5.2, Haver -> have\n* Section 5.2, in this work -> In this work\n* Section 5.2, unclear what you use the class weighting for. Is this for class balancing ?\n* Section 5.2, Cityscapes was -> Cityscapes is\n* Section 5.2, weighted by the average -> is each instance weighted relative the average object size.\n* Section 5.2, fastest model in the Cityscapes -> fastest model in the public Cityscapes","label":0,"model":"human","source":"peerread","id":4729}
{"text":"This paper aims at designing a real-time semantic segmentation network. The proposed approach has an encoder-decoder architecture with many pre-existing techniques to improvement the performance and speed. \n\nMy concern is that the most of design choices are pretty ad-hoc and there is a lack of ablation study to validate each choice. \n\nMoreover, most of the components are not new to the community (indexed pooling, dilated convolution, PReLu, steerable convolution, spatial dropout). The so-called 'early down-sampling' or 'decoder size' are also just very straightforward trade-off between speed and performance through reducing the size\/depth of the layers. \n\nThe performance and inference comparison is only conducted against a rather weak baseline, SegNet, which also makes the paper less convincing. On the public benchmark the proposed model does not achieve comparable results against state-of-the-art. As some other reviewer raised, there are some stronger model that has similar efficiency compared with SegNet.\n\nThe speed-up improvement is good yet reasonable given all the components used. However, we also did see a big sacrifice in performance on some benchmarks, which makes all these tricks less promising. \n\nThe only fact I found impressive is that the model size is 0.7MB, which is of good practical use and helpful to dump on mobile devices. However, there is NO analysis over how is the trade-off between the model size and the performance, and what design would result how much reduction in model size. I did not find the memory consumption report for the inference stage, which are perhaps even more crucial for embedded systems. \n\nPerhaps this paper does have a practical value for practical segmentation network design on embedding systems. But I do not believe the paper brings insightful ideas that are worthy to be discussed in ICLR, either from the perspective of model compression or semantic segmentation. ","label":0,"model":"human","source":"peerread","id":4730}
{"text":"This paper describes a fast image semantic segmentation network.  Many different techniques are combined to create a system much faster than the baseline SegNet approach, with accuracy comparable or somewhat worse in most of three datasets evaluated.\n\nThe choices and techniques used to achieve these speed optimizations are enumerated and described along with intuitions behind them.  However, this section lacks measurements and experimental results showing the effects of these choices.  To me, that would have been a key component to the paper.  As it stands now, we only get to see final evaluation numbers, which appear to describe a speed\/accuracy tradeoff with little insight into the pieces sum to get there.\n\nIn addition, I feel there could be a more thorough comparison with different existing systems.  Only SegNet is shown in comparison tables, even though many current systems are outlined in the related work.  Additional datasets such as Pascal or COCO may be interesting here as well, perhaps with a larger version of the ENet model.\n\nThe system looks to be fast, with decent accuracy on the majority of benchmarks described.  However, as a practical implementation paper, I feel it needs to more thoroughly demonstrate the effects of each component, as well as possibly some of the sizing\/tuning, in order to provide a more robust picture.\n","label":0,"model":"human","source":"peerread","id":4731}
{"text":"I have tried the open source implementation for different tasks these several months. It's fast, reasonably accurate and useful for prototyping.\n\nHowever I see no technical quality improvements against its NIPS submission version.\n\nI am really wondering which design choice is the most dominant one. \n\nWondering what I should do if I want to design a network as efficient as this one.","label":0,"model":"human","source":"peerread","id":4732}
{"text":"Interesting work and especially relevant going forward with the plethora of mobile devices. It would be especially interesting to see the time comparisons\non a mobile device using any of the currently available mobile frameworks. For example iOS has CNN support using Metal. Here is sample code [bit.ly\/2fQQcrX]\nthat specifically runs VGG on an iPhone (not in fully convolutional format) but which you could probably modify easily to match your architecture.\nAlso, have you tried training the same architecture on the Pascal VOC segmentation challenge? SegNet has a mean IU of 59.9 according to the VOC benchmark [bit.ly\/2g9Mpa3]. What does ENet achieve? ","label":0,"model":"human","source":"peerread","id":4733}
{"text":"This paper makes three main methodological contributions:\n - definition of Neural Feature (NF) as the pixel average of the top N images that highly activation a neuron\n - ranking of neurons based on color selectivity\n - ranking of neurons based on class selectivity\n\nThe main weaknesses of the paper are that none of the methodological contributions are very significant, and no singularly significant result arises from the application of the methods.\n\nHowever, the main strengths of the paper are its assortment of moderately-sized interesting conclusions about the basic behavior of neural nets. For example, a few are:\n - \u201cIndexing on class selectivity neurons we found highly class selective neurons like digital-clock at conv2, cardoon at conv3 and ladybug at conv5, much before the fully connected layers.\u201d As far as I know, this had not been previously reported.\n - Color selective neurons are found even in higher layers. (25% color selectivity in conv5)\n - \u201cour main color axis emerge (black-white, blue-yellow, orange-cyan and cyan- magenta). Curiously, these two observations correlate with evidences in the human visual system (Shapley & Hawken (2011)).\u201d Great observation!\n\nOverall, I\u2019d recommend the paper be accepted, because although it\u2019s difficult to predict at this time, there\u2019s a fair chance that one of the \u201csmaller conclusions\u201d would turn out to be important in hindsight a few years hence.\n\n\nOther small comments:\n - The cite for \u201cLearning to generate chairs\u2026\u201d is wrong (first two authors combined resulting in a confusing cite)\n\n - What exactly is the Color Selectivity Index computing? The Opponent Color Space isn\u2019t well defined and it wasn\u2019t previously familiar to me. Intuitively it seems to be selecting for units that respond to a constant color, but the highest color selectivity NF in Fig 5 i for a unit with two colors, not one. Finally, the very last unit (lowest color selectivity) is almost the same edge pattern, but with white -> black instead of blue -> orange. Why are these considered to be so drastically different? This should probably be more clearly described.\n\n - For the sake of argument, imagine a mushroom sensitive neuron in conv5 that fires highly for mushrooms of *any* color but not for anything else. If the dataset contains only red-capped mushrooms, would the color selectivity index for this neuron be high or low? If it is high, it\u2019s somewhat misleading because the unit itself actually isn\u2019t color selective; the dataset just happens only to have red mushrooms in it. (It\u2019s a subtle point but worth considering and probably discussing in the paper)","label":0,"model":"human","source":"peerread","id":4734}
{"text":"While this is interesting work, one major concern comes from Reviewer 2 regarding the attempt of characterizing the tuning properties, which has proven useless both in neuroscience and in machine learning. Currently, no attempt so far has lived up to the promises this line of research is aiming for. In summary, this work is explorative and incremental but worthwhile. We encourage the authors to further refine their research effort and resubmit.","label":0,"model":"human","source":"peerread","id":4735}
{"text":"Hi authors,\n\nCongrats for an interesting submission! I wonder what other selectivity indices you could use beside color and class.\n\nI just noticed that your definition of Neuron Feature (weighted average image) is very close to what we use in this paper (an average image):\n\n","label":0,"model":"human","source":"peerread","id":4736}
{"text":"This paper makes three main methodological contributions:\n - definition of Neural Feature (NF) as the pixel average of the top N images that highly activation a neuron\n - ranking of neurons based on color selectivity\n - ranking of neurons based on class selectivity\n\nThe main weaknesses of the paper are that none of the methodological contributions are very significant, and no singularly significant result arises from the application of the methods.\n\nHowever, the main strengths of the paper are its assortment of moderately-sized interesting conclusions about the basic behavior of neural nets. For example, a few are:\n - \u201cIndexing on class selectivity neurons we found highly class selective neurons like digital-clock at conv2, cardoon at conv3 and ladybug at conv5, much before the fully connected layers.\u201d As far as I know, this had not been previously reported.\n - Color selective neurons are found even in higher layers. (25% color selectivity in conv5)\n - \u201cour main color axis emerge (black-white, blue-yellow, orange-cyan and cyan- magenta). Curiously, these two observations correlate with evidences in the human visual system (Shapley & Hawken (2011)).\u201d Great observation!\n\nOverall, I\u2019d recommend the paper be accepted, because although it\u2019s difficult to predict at this time, there\u2019s a fair chance that one of the \u201csmaller conclusions\u201d would turn out to be important in hindsight a few years hence.\n\n\nOther small comments:\n - The cite for \u201cLearning to generate chairs\u2026\u201d is wrong (first two authors combined resulting in a confusing cite)\n\n - What exactly is the Color Selectivity Index computing? The Opponent Color Space isn\u2019t well defined and it wasn\u2019t previously familiar to me. Intuitively it seems to be selecting for units that respond to a constant color, but the highest color selectivity NF in Fig 5 i for a unit with two colors, not one. Finally, the very last unit (lowest color selectivity) is almost the same edge pattern, but with white -> black instead of blue -> orange. Why are these considered to be so drastically different? This should probably be more clearly described.\n\n - For the sake of argument, imagine a mushroom sensitive neuron in conv5 that fires highly for mushrooms of *any* color but not for anything else. If the dataset contains only red-capped mushrooms, would the color selectivity index for this neuron be high or low? If it is high, it\u2019s somewhat misleading because the unit itself actually isn\u2019t color selective; the dataset just happens only to have red mushrooms in it. (It\u2019s a subtle point but worth considering and probably discussing in the paper)\n","label":0,"model":"human","source":"peerread","id":4737}
{"text":"The authors analyze trained neural networks by quantifying the selectivity of individual neurons in the network for a variety of specific features, including color and category.   \n\nPros:\n   * The paper is clearly written and has good figures. \n   * I think they executed their specific stated goal reasonably well technically.   E.g. the various indexes they use seem well-chosen for their purposes. \n\nCons:\n   * I must admit that I am biased against the whole enterprise of this paper.   I do not think it is well-motivated or provides any useful insight whatever.   What I view their having done is produced, and then summarized anecdotally, a catalog of piecemeal facts about a neural network without any larger reason to think these particular facts are important.  In a way, I feel like this paper suffers from the same problem that plagues a typical line of research in neurophysiology, in which a catalog of selectivity distributions of various neurons for various properties is produced -- full stop.  As if that were in and of itself important or useful information.   I do not feel that either the original neural version of that project, or this current model-based virtual electrophysiology, is that useful.   Why should we care about the distribution of color selectivities?   Why does knowing distribution as such constitute \"understanding\"?    To my mind it doesn't, at least not directly.   \n\nHere's what they could have done to make a more useful investigation:\n  \n     (a) From a neuroscience point of view, they could have compared the properties that they measure in models to the same properties as measured in neurons the real brain.   If they could show that some models are better matches on these properties to the actual neural data than others, that would be a really interesting result.   That is is to say, the two isolated catalogs of selectivities (from model neurons and real neurons)  alone seem pretty pointless.  But if the correspondence between the two catalogs was made -- both in terms of where the model neurons and the real neurons were similar, and (especially importantly) where they were different --- that would be the beginning of nontrivial understanding.   Such results would also complement a growing body of literature that attempts to link CNNs to visual brain areas.  Finding good neural data is challenging, but whatever the result, the comparison would be interesting. \n\nand\/or \n\n    (b) From an artificial intelligence point of view, they could have shown that their metrics are *prescriptive* constraints.   That is, suppose they had shown that the specific color and class selectivity indices that they compute, when imposed as a loss-function criterion on an untrained neural network, cause the network to develop useful filters and achieve significantly above-chance performance on the original task the networks were trained on.     This would be a really great result, because it would not only give us a priori reason to care about the specific property metrics they chose, but it would also help contribute to efforts to find unsupervised (or semi-supervised) learning procedures, since the metrics they compute can be estimated from comparatively small numbers of stimuli and\/or high-level semantic labels.    To put this in perspective, imagine that they had actually tested the above hypothesis and found it to be false:  that is, that their metrics, when used as loss function constraints, do not improve performance noticeably above chance performance.  What would we then make of this whole investigation?  It would then be reasonable to think that the measured properties were essentially epiphenomenal and didn't contribute at all to the power of neural networks in solving perceptual tasks.  (The same could be said about neurophysiology experiments doing the same thing.)  \n     [--> NB: I've actually tried things just like this myself over the years, and have found exactly this disappointing result.  Specifically,  I've found a number of high-level generic statistical property of DNNs that seem like they might potentially \"interesting\", e.g. because they apparently correlate with complexity or appear to illustrate difference between low, intermediate and high layers of DNNs.  Every single one of these, when imposed as optimization constraints, has basically lead nowhere on the challenging tasks (like ImageNet) that cause the DNNs to be interesting in the first place.  Basically, there is to my mind no evidence at this point that highly-summarized generic statistical distributions of selectivities, like those illustrated here, place any interesting constraints on filter weights at all.   Of course, I haven't tried the specific properties the authors highlight in these papers, so maybe there's something important there.]\n\nI know that both of these asks are pretty hard, but I just don't know what else to say -- this work otherwise seems like a step backwards for what the community ought to be spending its time on. \n ","label":0,"model":"human","source":"peerread","id":4738}
{"text":"This paper attempts to understand and visualize what deep nets are representing as one ascends from low levels to high levels of the network.  As has been shown previously, lower levels are more local image feature based, whereas higher levels correspond to abstract properties such as object identity.  In semantic space, we find higher level nodes to be more semantically selective, whereas low level nodes are more diffuse.\n\nThis seems like a good attempt to tease apart deep net representations.  Perhaps the most important finding is that color figures prominently into all levels of the network, and that performance on gray scale images is significantly diminished.  The new NF measure proposed here is sensible, but still based on the images shown to the network.  What one really wants to know is what function these nodes are computing - i.e., out of the space of *all* possible images, which most activate a unit?  Of course this is a difficult problem, but it would be nice to see us getting closer to understanding the answer.  The color analysis here I think brings us a bit closer.  The semantic analysis is nice but I'm not sure what new insight we gain from this.  \n","label":0,"model":"human","source":"peerread","id":4739}
{"text":"We have updated the pdf of the paper after the reviewer comments, in this way:\n  - Giving more details about similarities and differences between the proposed approach and previous.\n  - Adding the clarifications required by the reviewers on Figure1\n  - Improving the global understanding by joining section 1 and 2 into a single one. ","label":0,"model":"human","source":"peerread","id":4740}
{"text":"Unfortunately, the paper is not clear enough for me to understand what is being proposed. At a high-level the authors seem to propose a generalization of the standard layered neural architecture (of which MLPs are a special case), based on arbitrary nodes which communicate via messages. The paper then goes on to show that their layer-free architecture can perform the same computation as a standard MLP. This logic appears circular. The low level details of the method are also confusing: while the authors seem to be wanting to move away from layers based on matrix-vector products, Algorithm 4 nevertheless resorts to matrix-vector products for the forward and backwards pass. Although the implementation relies on asynchronously communicating nodes, the \u201clocking\u201d nature of the computation makes the two entirely equivalent.","label":0,"model":"human","source":"peerread","id":4741}
{"text":"The reviewers were consistent in their review that they thought this was a strong rejection.\n Two of the reviewers expressed strong confidence in their reviews.\n The main arguments made by the reviewers against acceptance were:\n Lack of novelty (R2, R3)\n Lack of knowledge of literature and development history; particularly with respect to biological inspiration of ANNs (R3)\n Inappropriate baseline comparison (R2)\n Not clear (R1)\n \n The authors did not provide a response to the official reviews. Therefore I have decided to follow the consensus towards rejection.","label":0,"model":"human","source":"peerread","id":4742}
{"text":"The multiagent system is proposed as a generalization of neural network. The proposed system can be used with less restrictive network structures more efficiently by computing only those necessary computations in the graph. Unfortunately, I don't find the proposed system different from the framework of artificial neural network. Although for today's neural network structures are designed to have a lot of matrix-matrix multiplications, but it is not limited to have such architecture. In other words, the proposed multiagent system can be framed in the artificial neural network with more complicated layer\/connectivity structures while considering each neuron as layer. The computation efficiency is argued among different sparsely connected denoising autoencoder in multiagent system framework only but the baseline comparison should be against the fully-connected neural network that employs matrix-matrix multiplication.","label":0,"model":"human","source":"peerread","id":4743}
{"text":"The paper reframes feed forward neural networks as a multi-agent system.\n\nIt seems to start from the wrong premise that multi-layer neural networks were created expressed as full matrix multiplications. This ignores the decades-long history of development of artificial neural networks, inspired by biological neurons, which thus started from units with arbitrarily sparse connectivity envisioned as computing in parallel. The matrix formulation is primarily a notational convenience; note also that when working with sparse matrix operations (or convolutions) zeros are neither stored not multiplied by.\n\nBesides the change in terminology, essentially renaming neurons agents, I find the paper brings nothing new and interesting to the table.\n\nPulling in useful insights from a different communitiy such as multi-agent systems would be most welcome. But for this to be compelling, it would have to be largely unheard-of elements in neural net research, with clear supporting empirical evidence that they significantly improve accuracy or efficiency. This is not achieved in the present paper.","label":0,"model":"human","source":"peerread","id":4744}
{"text":"In this paper, the author proposed an approach for feature combination of two embeddings v1 and v2. This is done by first computing the pairwise combinations of the elements of v1 and v2 (with complicated nonlinearity), and then pick the K-Max as the output vector. For triple (or higher-order) combinations, two (or more) consecutive pairwise combinations are performed to yield the final representations. It seems that the approach is not directly related to categorical data, and can be applied to any embeddings (even if they are not one-hot). So is there any motivation that brings about this particular approach? What is the connection? \n\nThere are many papers with similar ideas. CCPM (A convolutional click prediction model) that the authors have compared against, also proposes very similar network structure (conv + K-max + conv + K-max). In the paper, the author does not mention their conceptual similarity and difference versus CCPM. Compact Bilinear Pooling,","label":0,"model":"human","source":"peerread","id":4745}
{"text":"There is consensus among the three reviewers that (1) the originality of the proposed approach is limited and (2) the experimental evaluation is too limited in that it lacks strong baseline models as well as an ablation study that explores the different aspects of the proposed model.","label":0,"model":"human","source":"peerread","id":4746}
{"text":"A method for click prediction is presented. Inputs are a categorical variables and output is the click-through-rate. The categorical input data is embedded into a feature vector using a discriminative scheme that tries to predict whether a sample is fake or not. The embedding vector is passed through a series of SUM\/MULT gates and K-most important interactions are identified (K-max pooling). This process is repeated multiple times (i.e. multiple layers) and the final feature is passed into a fully connected layer to output the click prediction rate. \n\nAuthors claim:\n(1)\tUse of gates and K-max pooling allow modeling of interactions that lead to state of art results. \n(2)\tIt is not straightforward to apply ideas in papers like word2vec to obtain feature embeddings and consequently they use the idea of discriminating between fake and true samples for feature learning. \n\nTheoretically convolutions can act as \u201csum\u201d gates between pairs of input dimensions. Authors make these interactions explicit (i.e. imposed structure) by using gates. Now, the merit of the proposed method can be tested if a network using gates outperforms a network without gates. This baseline is critically missing \u2013 i.e. Embedding Vector followed by a series of convolution\/pooling layers. \n\nAnother related issue is that I am not sure if the number of parameters in the proposed model and the baseline models is similar or not. For instance \u2013 what is the total number of parameters in the CCPM model v\/s the proposed model? \n\nOverall, there is no new idea in the paper. This by itself is not grounds for rejection if the paper outperforms established baselines.  However, such comparison is weak and I encourage authors to perform these comparisons. \n\n\n","label":0,"model":"human","source":"peerread","id":4747}
{"text":"The paper proposes a way to learn continuous features for input data which consists of multiple categorical data. The idea is to embed each category in a learnable low dimensional continuous space, explicitly compute the pair-wise interaction among different categories in a given input sample (which is achieved by either taking a component-wise dot product or component-wise addition), perform k-max pooling to select a subset of the most informative interactions, and repeat the process some number of times, until you get the final feature vector of the given input. This feature vector is then used as input to a classifier\/regressor to accomplish the final task. The embeddings of the categories are learnt in the usual way. In the experiment section, the authors show on a synthetic dataset that their procedure is indeed able to select the relevant interactions in the data. On one real world dataset (iPinYou) the model seems to outperform a couple of simple baselines. \n\nMy major concern with this paper is that their's nothing new in it. The idea of embedding the categorical data having mixed categories has already been handled in the past literature, where essentially one learns a separate lookup table for each class of categories: an input is represented by concatenation of the embeddings from these lookup table, and a non-linear function (a deep network) is plugged on top to get the features of the input. The only rather marginal contribution is the explicit modeling of the interactions among categories in equations 2\/3\/4\/5. Other than that there's nothing else in the paper. \n\nNot only that, I feel that these interactions can (and should) automatically be learned by plugging in a deep convolutional network on top of the embeddings of the input. So I'm not sure how useful the contribution is. \n\nThe experimental section is rather weak. They authors test their method on a single real world data set against a couple of rather weak baselines. I would have much preferred for them to evaluate against numerous models proposed in the literature which handle similar problems, including wsabie. \n\nWhile the authors argued in their response that wsabie was not suited for their problem, i strongly disagree with that claim. While the original wsabie paper showed experiments using images as inputs, their training methodology can easily be extended to other types of data sets, including categorical data. For instance, I conjecture that the model i proposed above (embed all the categorical inputs, concatenate the embeddings, plug a deep conv-net on top and train using some margin loss) will perform as well if not better than the hand coded interaction model proposed in this paper. Of course I could be wrong, but it would be far more convincing if their model was tested against such baselines. ","label":0,"model":"human","source":"peerread","id":4748}
{"text":"In this paper, the author proposed an approach for feature combination of two embeddings v1 and v2. This is done by first computing the pairwise combinations of the elements of v1 and v2 (with complicated nonlinearity), and then pick the K-Max as the output vector. For triple (or higher-order) combinations, two (or more) consecutive pairwise combinations are performed to yield the final representations. It seems that the approach is not directly related to categorical data, and can be applied to any embeddings (even if they are not one-hot). So is there any motivation that brings about this particular approach? What is the connection? \n\nThere are many papers with similar ideas. CCPM (A convolutional click prediction model) that the authors have compared against, also proposes very similar network structure (conv + K-max + conv + K-max). In the paper, the author does not mention their conceptual similarity and difference versus CCPM. Compact Bilinear Pooling, ","label":0,"model":"human","source":"peerread","id":4749}
{"text":"A new sparse coding model is introduced that learns features jointly with their transformations. It is found that inference over per-image transformation variables is hard, so the authors suggest tying these variables across all data points, turning them into global parameters, and using multiple transformations for each feature. Furthermore, it is suggested to use a tree of transformations, where each path down the tree generates a feature by multiplying the root feature by the transformations associated with the edges. The one-layer tree model achieves similar reconstruction error as traditional sparse coding, while using fewer parameters.\n\nThis is a nice addition to the literature on sparse coding and the literature on learning transformation models. The authors identify and deal with a difficult inference problem that can occur in transformation models. That said, I am skeptical about the usefulness of the general approach. The authors take it as a given that \u201clearning sparse features and transformations jointly\u201d is an important goal in itself, but this is never really argued or demonstrated with experiments. It doesn\u2019t seem like this method enables new applications, extends our understanding of learning what\/where pathways in the brain, or improve our ability to model natural images.\n\nThe authors claim that the model extracts pose information, but although the model explicitly captures the transformation that relates different features in a tree, at test time, inference is only performed over the (sparse) coefficient associated with each (feature, transformation) combination, just like in sparse coding. It is not clear what we gain by knowing that each coefficient is associated with a transformation, especially since there are many models that do this general \u201cwhat \/ where\u201d split.\n\nIt would be good to check that the x_{v->b} actually change significantly from their initialization values. The loss surface still looks pretty bad even for tied transformations, so they may actually not move much. Does the proposed model work better according to some measure, compared to a model where x_{v->b} are fixed and chosen from some reasonable range of parameter values (either randomly or spaced evenly)?\n\nOne of the conceptually interesting aspects of the paper is the idea of a tree of transformations, but the advantage of deeper trees is never demonstrated convincingly. It looks like the authors have only just gotten this approach to work on toy data with vertical and horizontal bars.\n\nFinally, it is not clear how the method could be extended to have multiple layers. The transformation operators T can be defined in the first layer because they act on the input space, but the same cannot be done in the learned feature space. It is also not clear how the pose information should be further processed in a hierarchical manner, or how learning in a deep version should work.\n\nIn summary, I do not recommend this paper for publication, because it is not clear what problem is being solved, the method is only moderately novel and the novel aspects are not convincingly shown to be beneficial.","label":0,"model":"human","source":"peerread","id":4750}
{"text":"This paper learns affine transformations from images jointly with object features. The motivation is interesting and sound, but the experiments fail to deliver and demonstrate the validity of the claims advanced -- they are restricted to toy settings. What is presented as logical next steps for this work (extending to higher scale multilayer convolutional frameworks, beyond toy settings) seems necessary for the paper to hold its own and deliver the promised insights.","label":0,"model":"human","source":"peerread","id":4751}
{"text":"This paper trains a generative model of image patches, where dictionary elements undergo gated linear transformations before being combined. The transformations are motivated in terms of Lie group operators, though in practice they are a set of fixed linear transformations. This is motivated strongly in terms of learning a hierarchy of transformations, though only one layer is used in the experiments (except for a toy case in the appendix).\n\nI like the motivation for this algorithm. The realization seems very similar to a group or block sparse coding implementation. I was disappointed by the restriction to linear transformations. The experiments were all toy cases, demonstrating that the algorithm can learn groups of Gabor- or center surround-like features. They would have been somewhat underpowered five years ago, and seemed extremely small by today's standards.\n\nSpecific comments:\n\nBased on common practices in ML literature, I have a strong bias to think of $x$ as inputs and $w$ as network weights. Latent variables are often $z$ or $a$. Depending on your target audience, I would suggest permuting your choice of symbols so the reader can more quickly interpret your model.\n\nnit: number all equations for easier reference\n\nsec 2.2 -- It's weird that the transformation is fixed, but is still written as a function of x.\n\nsec 2.3 -- The updated text here confuses me actually. I had thought that you were using a fixed set of linear transformations, and were motivating in terms of Lie groups, but were not actually taking matrix exponentials in your algorithm. The equations in the second half of this section suggest you are working with matrix exponentials though. I'm not sure which direction I'm confused in, but probably good to clarify the text either way.\n\nBTW -- there's another possible solution to the local minima difficulty, which is the one used in Sohl-Dickstein, 2010. There, they introduce blurring operators matched to each transformation operator, and gradient descent can escape local minima by detouring through coarser (more blurred) scales.\n\nsec 3.2 -- I believe by degrees of freedom you mean the number of model parameters, not the number of latent coefficients that must be inferred? Should make this more clear. Is it more appropriate to compare reconstruction error while matching number of model parameters, or number of latent variables?\n\nI wonder if a convolutional version of this algorithm would be practical \/ would make it more suited as a generative model of whole images.\n\n====\npost rebuttal update\n\nThank you for taking the time to write the rebuttal! I have read it, but it did not significantly effect my rating.","label":0,"model":"human","source":"peerread","id":4752}
{"text":"This paper proposes an approach to unsupervised learning based on a modification to sparse coding that allows for explicit modeling of transformations (such as shift, rotation, etc.), as opposed to simple pooling as is typically done in convnets.  Results are shown for training on natural images, demonstrating that the algorithm learns about features and their transformations in the data.  A comparison to traditional sparse coding shows that it represents images with fewer degrees of freedom.\n\nThis seems like a good and interesting approach, but the work seems like its still in its early formative stages rather than a complete work with a compelling punch line.  For example one of the motivations is that you'd like to represent pose along with the identity of an object.  While this work seems well on its way to that goal, it doesn't quite get there - it leaves a lot of dots still to be connected.  \n\nAlso there are a number of things that aren't clear in the paper:\n\no The central idea of the paper it seems is the use of a transformational sparse coding tree to make tractable the inference of the Lie group parameters x_k.  But how exactly this is done is not at all clear.  For example, the sentence: \"The main idea is to gradually marginalize over an increasing range of transformations,\" is suggestive but not clear.  This needs to be much better defined.  What do you mean by marginalization in this context?  \n\n o The connection between the Lie group operator and the tree leaves and weights w_b is not at all clear.   The learning rule spells out the gradient for the Lie group operator, but how this is used to learn the leaves of the tree is not clear.  A lot is left to the imagination here.  This is especially confusing because although the Lie group operator is introduced earlier, it is then stated that its not tractable for inference because there are too many local minima, and this motivates the tree approach instead.  So its not clear why you are learning the Lie group operator.\n\n o It is stated that \"Averaging over many data points, smoothens the surface of the error function.\"  I don't understand why you would average over many data points.  It seems each would have its own transformation, no?\n\n o What data do you train on?  How is it generated?  Do you generate patches with known transformations and then show that you can recover them?  Please explain.\n\nThe results shown in Figure 4 look very interesting, but given the lack of clarity in the above, difficult to interpret and understand what this means, and its significance.\n\nI would encourage the authors to rewrite the paper more clearly and also to put more work into further developing these ideas, which seem very promising.\n\n\n\n","label":0,"model":"human","source":"peerread","id":4753}
{"text":"A new sparse coding model is introduced that learns features jointly with their transformations. It is found that inference over per-image transformation variables is hard, so the authors suggest tying these variables across all data points, turning them into global parameters, and using multiple transformations for each feature. Furthermore, it is suggested to use a tree of transformations, where each path down the tree generates a feature by multiplying the root feature by the transformations associated with the edges. The one-layer tree model achieves similar reconstruction error as traditional sparse coding, while using fewer parameters.\n\nThis is a nice addition to the literature on sparse coding and the literature on learning transformation models. The authors identify and deal with a difficult inference problem that can occur in transformation models. That said, I am skeptical about the usefulness of the general approach. The authors take it as a given that \u201clearning sparse features and transformations jointly\u201d is an important goal in itself, but this is never really argued or demonstrated with experiments. It doesn\u2019t seem like this method enables new applications, extends our understanding of learning what\/where pathways in the brain, or improve our ability to model natural images.\n\nThe authors claim that the model extracts pose information, but although the model explicitly captures the transformation that relates different features in a tree, at test time, inference is only performed over the (sparse) coefficient associated with each (feature, transformation) combination, just like in sparse coding. It is not clear what we gain by knowing that each coefficient is associated with a transformation, especially since there are many models that do this general \u201cwhat \/ where\u201d split.\n\nIt would be good to check that the x_{v->b} actually change significantly from their initialization values. The loss surface still looks pretty bad even for tied transformations, so they may actually not move much. Does the proposed model work better according to some measure, compared to a model where x_{v->b} are fixed and chosen from some reasonable range of parameter values (either randomly or spaced evenly)?\n\nOne of the conceptually interesting aspects of the paper is the idea of a tree of transformations, but the advantage of deeper trees is never demonstrated convincingly. It looks like the authors have only just gotten this approach to work on toy data with vertical and horizontal bars.\n\nFinally, it is not clear how the method could be extended to have multiple layers. The transformation operators T can be defined in the first layer because they act on the input space, but the same cannot be done in the learned feature space. It is also not clear how the pose information should be further processed in a hierarchical manner, or how learning in a deep version should work.\n\nIn summary, I do not recommend this paper for publication, because it is not clear what problem is being solved, the method is only moderately novel and the novel aspects are not convincingly shown to be beneficial. \n","label":0,"model":"human","source":"peerread","id":4754}
{"text":"Revision #1\n\nThank you for your comments and suggestions. We uploaded a new version of the paper to address them:\n\n1) We added all relevant references, corrected our claims and updated our \"Relevant Work\" section.\n\n2) We added a figure that shows the effects of each transformation.\n\n3) We added a formal definition of deeper trees and added a small example that shows learned structure.\n\n4) We added more figures of learned features.\n\n5) We changed our regularization. Instead of regularizing derivative features, we constrain root features to be of unit\nnorm and penalize transformations that change the magnitude. Inter- and intra- tree regularization is no longer required\nand we can use the feature-sign algorithm to infer the weights.\n\n6) We removed our parameter distance and magnitude figures, since these metrics depend heavily on the initialization approach of choice\nand hence are not very informative.\n\nAll results and figures are current.","label":0,"model":"human","source":"peerread","id":4755}
{"text":"This paper explores the use of Open Bigrams as a target representation of words, for application to handwriting image recognition. \n\nPros:\n- The use of OBs is novel and interesting.\n- Clearly written and explained.\n\nCons:\n- No comparison to previous state of the art, only with author-generated results. \n- More ablation studies needed -- i.e. fill in Table3 with rnn0,1 rnn0,1,2 rnn0,1' etc etc. It is not clear where the performance is coming from, as it seems that it is single character modelling (0) and word endings (') that are actually beneficial.\n- While the use of Open bigrams is novel, there are works which use bag of bigrams and ngrams as models which are not really compared to or explored. E.g.","label":0,"model":"human","source":"peerread","id":4756}
{"text":"There is consistent agreement towards the originality of this work and that the topic here is \"interesting\". Additionally there is consensus that the work is \"clearly written\", and (excepting questions of the word \"cortical\") all would be primed to accept this style of work. \n \n However there is a shared concern about the quality and potential impact of the work, in particularly in terms of the validity of empirical evaluations. Reviewers are generally not inclined to believe that the current empirical evidence validates the conclusions of the word. Suggestions are to: make greater use of a language model, compare to external baselines, or remove the handwriting aspects.","label":0,"model":"human","source":"peerread","id":4757}
{"text":"This paper explores the use of Open Bigrams as a target representation of words, for application to handwriting image recognition. \n\nPros:\n- The use of OBs is novel and interesting.\n- Clearly written and explained.\n\nCons:\n- No comparison to previous state of the art, only with author-generated results. \n- More ablation studies needed -- i.e. fill in Table3 with rnn0,1 rnn0,1,2 rnn0,1' etc etc. It is not clear where the performance is coming from, as it seems that it is single character modelling (0) and word endings (') that are actually beneficial.\n- While the use of Open bigrams is novel, there are works which use bag of bigrams and ngrams as models which are not really compared to or explored. E.g. ","label":0,"model":"human","source":"peerread","id":4758}
{"text":"This paper uses an LSTM model to predict what it calls \"open bigrams\" (bigrams of characters that may or may not have letters inbetween) from handwriting data. These open bigrams are subsequently used to predict the written word in a decoding step. The experiments indicate that the system does slightly better than a baseline model that uses Viterbi decoding. I have some major concerns about this paper:\n\n- I find the \"cortical inspired\" claim troublesome. If anything, it is psychology\/cognitive science inspired, in the sense that open bigrams appear to help for word recognition (Touzet et al. 2014). But the implied cortical characteristics, implicitly referred to e.g. by pointing to analogies between deep neural nets for object recognition and in that case the visual cortex, is unfounded. Is there any direct evidence from neuroscience that open-bigrams constitute a wholly separate layer in the cortex for a handwriting recognition task? Dehaene's work is a proposal, so you'll need to describe more \"findings in cognitive neurosciences [sic] research on reading\" (p. 8) to substantiate those claims. I am further worried by the fact that the authors seem to think that \"deep neural networks are based on a series of about five pairs of neurons [sic] layers\". Unless I misunderstand something, you are specifically referring to Krizhevsky's AlexNet here (which you should probably have cited there)? I hope you don't mean to imply that all deep neural nets need five layers. It is also not true that ten is \"quite close to the number of layers of an efficient deep NN\" -- what network? what task? etc.\n\n- The model is not clearly explained. There is a short paragraph in Appendix A.3. that roughly describes the setup, but this does not include e.g. the objective function, or answer why the network output is only considered each two consecutive time steps, rather than at each time step (or so it seems?). This is probably because the paper argues that it \"is focused on the decoder\" (p. 6), rather than on the whole problem. I find this problematic, because in that case we're effectively measuring how easy it is to reconstruct a word from its open bigrams, which has very little to do with handwriting recognition (it could have been evaluated on any text corpus). In fact, as the example on page 4 shows, handwriting is not necessary to illustrate the open bigram hypothesis. Which leads me to wonder why these particular tasks were chosen, if we are only interested in the decoding mechanism?\n\n- The comparison is not really fair. The Viterbi decoder only has access to unigrams, as far as I can tell. The only model that does better than that baseline has access to a lot more information, and does not do that much better. Did the Viterbi model have access to the word boundary information (at one point rather confusingly called \"extremities\") that pushed the open bigram model over the edge in terms of performance? Why is there no comparison to e.g. rnn_0,1' (unigram+bigram+boundary markers)? The dataset also appears to be biased in favor of the proposed approach (longer words, only ). I am not convinced that this paper really shows that open bigrams help.\n\nI very much like the idea of the paper, but I am simply not convinced by its claims.\n\nMinor points:\n- There are quite a few typos. Just a sample: \"independant\" (Fig.1), \"we evaluate an handwritten\", \", hand written words [..], an the results\", \"their approach include\", \"the letter bigrams of a word w is\", \"for the two considered database\"\n- Wouldn't it be easy to add how many times a bigram occurs, which would improve the decoding process? You can just normalize over the full counts instead of the binary occurrence counts.\n- The results in Table 5 are the same (but different precision) as the results in Table 2, except that edit distance and SER are added, this is confusing.","label":0,"model":"human","source":"peerread","id":4759}
{"text":"This submission investigates the usability of cortical-inspired distant bigram representations for handwritten word recognition. Instead of generating neural network based posterior features for character (optionally in local context), sets posterior for character bigrams of different length are used to represent words.  The aim here is to investigate the viability of this approach and to compare to the standard approach.\n\nOverall, the submission is well written, although information is missing w.r.t. to the comparison between the proposed approach and the standard approach, see below.\n\nIt would be desirable to see the model complexity of all the different models used here, i.e. the number of parameters used.\n\nLanguage models are not used here. Since the different models utilize different levels of context, language models can be expected to have a different effect on the different approaches. Therefore I suggest to include the use of language models into the evaluation.\n\nFor your comparative experiments you use only 70% of the data by choosing longer words only. On the other hand, it is well known that the shorter words are more prone to result in misrecognitions. The question remains, if this choice is advantageous for one of the tasks, or not - corresponding quantitative results should be provided to be able to better evaluate the effect of using this constrained corpus. Without clarification of this I would not readily agree that the error rates are competitive or better than the standard approach, as stated at the end of Sec. 5.\n\nI do see the motivation for introducing open-bigrams in an unordered way due to the corresponding evidence from cognitive research. However, decision theoretically I wonder, why the order should be given up, if the underlying sequential classification problem clearly is of a monotonous nature. It would be interesting to see an experiment, where only the use of the order is varied, to differentiate the effect of the order from the effect of other aspects of the approach.\n\nEnd of page 1: \"whole language method\" - please explain what is meant by this.\n\nPage 6: define your notation for rnn_d(x,t).\n\nThe number of target for the RNNs modeling order 0 (unigrams effectively) and the RNNs modeling order 1 and larger are very much different.  Therefore the precision and recall numbers in Table 2 do not seem to be readily comparable between order 0 and orders >=1. At least, the column for order 0 should be visually separated to highlight this.\n\n\nMinor comments: a spell check is recommended\np. 2: state-of-art -> state-of-the-art\np. 2: predict character sequence -> predict a character sequence\np. 3, top: Their approach include -> Their approach includes\np. 3, top: an handwritten -> a handwritten\np. 3, bottom: consituent -> constituent\np. 4, top: in classical approach -> in the classical approach\np. 4, top: transformed in a vector -> transformed into a vector\np. 5: were build -> were built\nReferences: first authors name written wrongly: Thodore Bluche -> Theodore Bluche\n","label":0,"model":"human","source":"peerread","id":4760}
{"text":"Strengths\n\n- interesting to explore the connection between ReLU DNN and simplified SFNN\n- small task (MNIST)  is used to demonstrate the usefulness of the proposed training methods experimentally\n- the proposed, multi-stage training methods are simple to implement (despite lacking theoretical rigor)\n\n\nWeaknesses\n\n-no results are reported on real tasks with large training set\n\n-not clear exploration on the scalability of the learning methods when training data becomes larger\n\n-when the hidden layers become stochastic, the model shares uncertainty representation with deep Bayes networks or deep generative models (Deep Discriminative and Generative Models for Pattern Recognition , book chapter in \u201cPattern Recognition and Computer Vision\u201d, November 2015, Download PDF). Such connections should be discussed, especially wrt the use of uncertainty representation to benefit pattern recognition (i.e. supervised learning via Bayes rule) and to benefit the use of domain knowledge such as \u201cexplaining away\u201d.\n\n-would like to see connections with variational autoencoder models and training, which is also stochastic with hidden layers","label":0,"model":"human","source":"peerread","id":4761}
{"text":"No reviewer was willing to champion the paper and the authors did not adequately address reviewer comments in a revision. Recommend rejection.","label":0,"model":"human","source":"peerread","id":4762}
{"text":"Strengths\n\n- interesting to explore the connection between ReLU DNN and simplified SFNN\n- small task (MNIST)  is used to demonstrate the usefulness of the proposed training methods experimentally\n- the proposed, multi-stage training methods are simple to implement (despite lacking theoretical rigor)\n\n\nWeaknesses\n\n-no results are reported on real tasks with large training set\n\n-not clear exploration on the scalability of the learning methods when training data becomes larger\n\n-when the hidden layers become stochastic, the model shares uncertainty representation with deep Bayes networks or deep generative models (Deep Discriminative and Generative Models for Pattern Recognition , book chapter in \u201cPattern Recognition and Computer Vision\u201d, November 2015, Download PDF). Such connections should be discussed, especially wrt the use of uncertainty representation to benefit pattern recognition (i.e. supervised learning via Bayes rule) and to benefit the use of domain knowledge such as \u201cexplaining away\u201d.\n\n-would like to see connections with variational autoencoder models and training, which is also stochastic with hidden layers\n","label":0,"model":"human","source":"peerread","id":4763}
{"text":"This paper builds connections between DNN, simplified stochastic neural network (SFNN) and SFNN and proposes to use DNN as the initialization model for simplified SFNN. The authors evaluated their model on several small tasks with positive results.\n\nThe connection between different models is interesting. I think the connection between sigmoid DNN and Simplified SFNN is the same as mean-field approximation that has been known for decades. However, the connection between ReLU DNN and simplified SFNN is novel.\n\nMy main concern is whether the proposed approach is useful when attacking real tasks with large training set. For tasks with small training set I can see that stochastic units would help generalize well.","label":0,"model":"human","source":"peerread","id":4764}
{"text":"Update: Because no revision of the paper has been provided by the authors, I am reducing my rating to \"marginally below acceptance\".\n\n----------\n\nThis paper addresses the problem of training stochastic feedforward neural networks.  It proposes to transfer weights from a deterministic deep neural network trained using standard procedures (including techniques such as dropout and batch normalization) to a stochastic network having the same topology.  The initial mechanism described for performing the transfer involves a rescaling of unit inputs and layer weights, and appropriate specification of the stochastic latent units if the DNN used for pretraining employs ReLU nonlinearities.  Initial experiments on MNIST classification and a toy generative task with a multimodal target distribution show that the simple transfer process works well if the DNN used for pretraining uses sigmoid nonlinearities, but not if the pretraining DNN uses ReLUs.  To tackle this problem, the paper introduces the \"simplified stochastic feedforward neural network,\" in which every stochastic layer is followed by a layer that takes an expectation over samples from its input, thus limiting the propagation of stochasticity in the network.  A modified process for transferring weights from a pretraining DNN to the simplified SFNN is described and justified.  The training process then occurs in three steps:  (1) pretrain a DNN, (2) transfer weights from the DNN to a simplified SFNN and continue training, and (3) optionally transfer the weights to a full SFNN and continue training or transfer them to a deterministic model (called DNN*) and continue training.  The third step can be skipped and the simplified SFNN may also be used directly as an inference model.  Experimental results on MNIST classification show that the use of simplified SFNN training can improve a deterministic DNN* model over a DNN baseline trained with batch normalization and dropout.  Experiments on two generative tasks (MNIST-half and the Toronto Faces Database) show that the proposed pretraining process improves test set negative log-likelihoods.  Finally, experiments on CIFAR-10, CIFAR-100, and SVHN with the LeNet-5, network-in-network, and wide residual network architectures show that use of a stochastic training step can improve performance of a deterministic (DNN*) model.\n\nIt is a bit confusing to refer to \"multi-modal\" tasks, when what is meant is \"generative tasks with a multimodal target distribution\" because \"multi-modal\" task can also refer to a learning task that crosses sensory modalities such as audio-visual speech recognition, text-based image retrieval, or image captioning.  I recommend that you use the more precise term (\"generative tasks with a multimodal target distribution\") early in the introduction and then say that you will refer to such tasks as \"multi-modal tasks\" in the rest of the paper for the sake of brevity.\n\nThe paper would be easier to read if \"SFNN\" were not used to refer to both the singular (\"stochastic feedforward neural network\") and plural (\"stochastic feedforward neural networks\") cases.  When the plural is meant, write \"SFNNs\".\n\nIn Table 1, why does the 3 hidden layer SFNN initialized from a ReLU DNN have so much worse of a test NLL than the 2 hidden layer SFNN initialized from a ReLU DNN?\n\nThe notation that uses superscripts to indicate layer indexes is confusing.  The reader naturally parses N\u00b2 as \"N squared\" and not as \"the number of units in the second layer.\"\n\nWhen you transfer weights back from the simplified SFNN to the DNN* model, do you need to perform some sort of rescaling that undoes the operations in Equation (8) in the paper?\n\nWhat does NCSFNN stand for in the supplementary material?\n\nPros\n+ The proposed model is easy to implement and apply to other tasks.\n+ The MNIST results showing that the stochastic model training can produce a deterministic model (called DNN* in the paper) that generalizes better than a DNN trained with batch normalization and dropout is quite exciting.\n\nCons\n- For the reasons outlined above, the paper is at times a bit hard to follow.\n- The results CIFAR-10, CIFAR-100, and SVHN would be more convincing if the baselines used dropout and batch normalization.  While this is shown on MINST, demonstration of a similar result on a more challenging task would strengthen the paper.\n\nMinor issues\n\nIt has been believed that stochastic \u2192 It is believed that stochastic\n\nunderlying these successes is on the efficient training methods \u2192 underlying these successes is efficient training methods\n\nnecessary in order to model complex stochastic natures in many real-world tasks \u2192 necessary in to model the complex stochastic nature of many real-world tasks\n\nstructured prediction, image generation and memory networks : memory networks are models, not tasks.\n\nFurthermore, it has been believed that SFNN \u2192 Furthermore, it is believed that SFNN\n\nusing backpropagation under the variational techniques and the reparameterization tricks  \u2192 using backpropagation with variational techniques and reparameterization tricks\n\nThere have been several efforts developing efficient training methods \u2192 There have been several efforts toward developing efficient training methods\n\nHowever, training SFNN is still significantly slower than doing DNN \u2192 However, training a SFNN is still significantly slower than training a DNN\n\ne.g., most prior works on this line have considered a \u2192 consequently most prior works in this area have considered a\n\nInstead of training SFNN directly \u2192 Instead of training a SFNN directly\n\nwhether pre-trained parameters of DNN \u2192 whether pre-trained parameters from a DNN\n\nwith further fine-tuning of light cost \u2192 with further low-cost fine-tuning\n\nrecent advances in DNN on its design and training \u2192 recent advances in DNN design and training\n\nit is rather believed that transferring parameters \u2192  it is believed that transferring parameters\n\nbut the opposite direction is unlikely possible \u2192 but the opposite is unlikely\n\nTo address the issues, we propose \u2192 To address these issues, we propose\n\nwhich intermediates between SFNN and DNN, \u2192 which is intermediate between SFNN and DNN,\n\nin forward pass and computing gradients in backward pass \u2192 in the forward pass and computing gradients in the backward pass\n\nin order to handle the issue in forward pass \u2192  in order to handle the issue in the forward pass\n\nNeal (1990) proposed a Gibbs sampling \u2192 Neal (1990) proposed Gibbs sampling\n\nfor making DNN and SFNN are equivalent \u2192 for making the DNN and SFNN equivalent\n\nin the case when DNN uses the unbounded ReLU \u2192 in the case when the DNN uses the unbounded ReLU\n\nare of ReLU-DNN type due to the gradient vanishing problem \u2192 are of the ReLU-DNN type because they mitigate the gradient vanishing problem\n\nmultiple modes in outupt space y \u2192 multiple modes in output space y\n\nThe only first hidden layer of DNN \u2192 Only the first hidden layer of the DNN\n\nis replaced by stochastic one, \u2192 is replaced by a stochastic layer,\n\nthe former significantly outperforms for the latter for the \u2192 the former significantly outperforms the latter for the\n\nsimple parameter transformations from DNN to SFNN are not clear to work in general, \u2192 simple parameter transformations from DNN to SFNN do not clearly work in general,\n\nis a special form of stochastic neural networks \u2192 is a special form of stochastic neural network\n\nAs like (3), the first layer is \u2192 As in (3), the first layer is\n\nThis connection naturally leads an efficient training procedure \u2192 This connection naturally leads to an efficient training procedure\n","label":0,"model":"human","source":"peerread","id":4765}
{"text":"When you transfer weights back from the simplified SFNN to the DNN* model, do you need to perform some sort of rescaling that undoes the operations in Equation (8) in the paper?\n","label":0,"model":"human","source":"peerread","id":4766}
{"text":"In Table 1, do the 4-layer SFNNs have one or two layers of stochastic units?  What about the 3-layer networks?  I suppose you could take the expectation in the output layer.","label":0,"model":"human","source":"peerread","id":4767}
{"text":"In this paper, citations are appearing with the authors' first initials and last names, e.g. (Hinton, G. et al., 2012a) instead of the authors last names and no initials, e.g. (Hinton et al., 2012a).  I find the first initials to be very distracting.  Please reformat the paper to match the citation style of the ICLR 2017 template.\n","label":0,"model":"human","source":"peerread","id":4768}
{"text":"Dear Authors,\n\nPlease resubmit your paper in the ICLR 2017 format with the margins to the correct spacing for your submission to be considered. Thank you!","label":0,"model":"human","source":"peerread","id":4769}
{"text":"The authors propose \"information dropout\", a variation of dropout with an information theoretic interpretation. A dropout layer limits the amount of information that can be passed through it, and the authors quantify this using a variational bound. \n\nIt remains unclear why such an information bottleneck is a good idea from a theoretical standpoint. Bayesian interpretations lend a theoretical basis to parameter noise, but activation noise has no such motivation. The information bottleneck indeed limits the information that can be passed through, but there is no rigorous argument for why this should improve generalization.\n\nThe experiments are not convincing. The CIFAR-10 results are worse than those in the paper that originally proposed the network architecture they use (Springenberg et al). The VAE results on MNIST are also horrible.","label":0,"model":"human","source":"peerread","id":4770}
{"text":"The authors all agree that the theory presented in the paper is of high quality and is promising but the experiments are not compelling. The reviewers are concerned that the presented idea and connections to existing methods, while neat, may not be impactful as the promise of the theory does not bear out in practice. One reviewer is concerned that the presented theory is still not useful, stating that the \"information bottleneck thus only becomes meaningful when the capacity of the encoding network is controlled in some measurable way, which is not discussed in the paper\". In general, they seem to agree that the experimental evaluation is still preliminary and unfinished. As such, it would seem that the authors could make the paper far more compelling by demonstrating more compelling improvements on benchmark experiments and submitting to a future conference.","label":0,"model":"human","source":"peerread","id":4771}
{"text":"Following the suggestions of the reviewers, we updated the paper with new experiments and plots.\n\nFirst, to empirically validate that, by increasing the value of the parameter \\beta, we can obtain representations that are increasingly minimal and invariant while remaining discriminative (sufficient), we created a new dataset, called \u2018Occluded CIFAR\u2019. The experiment in Sec. 6 shows precisely this effect, thus validating the theoretical intuition. Indeed, by increasing \\beta, we also prevent overfitting and the overall quality of the representation actually improves.\n\nIn Figure 5 in Appendix D, we added a comparison between Information Dropout and binary dropout using the same settings as [Springenberg et al., 2014]. For both methods we obtain a slightly better testing error than the original paper and, as also observed in the previous experiments, Information Dropout performs comparably or better than dropout.\n\nIn Figure 6, we plot the amount of information flowing through the dropout layers of a CNN as the number of filters varies. This plots supports some of the theoretical intuitions, and we show empirically that information dropout automatically selects a lower noise level for smaller networks, and that the units in the higher layers contain on average more information relative to the task than units in the bottom layers.","label":0,"model":"human","source":"peerread","id":4772}
{"text":"We thank all the reviewers for their comments. We would like to provide some clarification regarding the experiments in the paper, and address some of the concerns which were raised.\n\n>> The CIFAR-10 results are worse than those in the paper that originally proposed the network architecture they use (Springenberg et al). The VAE results on MNIST are also horrible.\n\nIf we use exactly the same architecture of Springenberg et al., then our results on CIFAR are, as predicted by the theory, comparable asymptotically, and better for smaller nets. We have added experiments that show this in the revised version to be uploaded soon. Also, our results on VAE are comparable to [KW13] for a similar architecture.\n\nNote, however, that the goal of our experiments is not to improve state-of-the-art on CIFAR-10 or MNIST, but to illustrate the effect of Information Dropout when compared to other forms of dropout, and to validate the intuition derived from the theory. For this reason, for the experiments in the paper we chose the simplest empirical settings, and modified the All Convolutional Net to isolate potentially confounding factors: we removed weight decay, increased the batch size to reduce gradient noise, simplified the architecture by removing the initial dropout layer, and used less aggressive learning rates and no fine tuning.  We also replaced ReLU with Softplus to make the results comparable with those of [KSW15]. This also served to validate the theory which applies to both ReLU and Softplus. \n\nMany factors affect empirical performance, only few of which are relevant to validating our theory. To the latter hand, we went to great length to ensure that the experiments are *controlled*. Only under careful control can the experiments be convincing in validating the theory.\n\nNevertheless, as suggested by the reviewers, we are currently exploring other experiments that would further illustrate the tradeoff between invariance to nuisances and sufficiency as mediated by the coefficient \\beta. We will add these along with the further tests using the same architecture of Springerberg, as described above.\n\n>> The results on CIFAR-10 in Figure 3(b) seem to be on a validation set\n\nWe are using the same nomenclature of [KSW15], since we want to make a direct comparison with their experiment. As customary for CIFAR, the data is divided into a disjoint training set (50,000 samples) and validation\/test set (10,000 samples). We feel that \"validation\" here is more appropriate.\n\n[KSW15] Diederik Kingma, Tim Salimans, and Max Welling, \"Variational Dropout and the Local Reparameterization Trick\", 2015\n\n[KW13] Diederik P Kingma, Max Welling, \"Auto-Encoding Variational Bayes\", 2013\n","label":0,"model":"human","source":"peerread","id":4773}
{"text":"Paper summary\nThis paper develops a generalization of dropout using information theoretic\nprinciples. The basic idea is that when learning a representation z of input x\nwith the aim of predicting y, we must choose a z such that it carries the least\namount of information about x, as long as it can predict y. This idea can be\nformalized using the Information Bottleneck Lagrangian. This leads to an\noptimization problem which is similar to the one derived for variational\ndropout, the difference being that Information dropout allows for a scaling\nfactor associated with the KL divergence term that encourages noise. The amount\nof noise being added is made a parameterized function of the data and this\nfunction is optimized along with the rest of the model. Experimental results on\nCIFAR-10 and MNIST show (small) improvements over binary dropout.\n\nStrengths\n- The paper highlights an important conceptual link between probabilistic\n  variational methods and information theoretic methods, showing that dropout\ncan be generalized using both formalisms to arrive at very similar models.\n- The presentation of the model is excellent.\n- The experimental results on cluttered MNIST are impressive.\n\nWeaknesses\n- The results on CIFAR-10 in Figure 3(b) seem to be on a validation set (unless\n  the axis label is a typo). It is not clear why the test set was not used. This\nmakes it hard to compare to results reported in Springenberg et al, as well as\nother results in literature.\n\nQuality\nThe theoretical exposition is high quality. Figure 2 gives a nice qualitative\nassessment of what the model is doing. However, the experimental results\nsection can be made better, for example, by matching the results on CIFAR-10 as\nreported in Springenberg et al. and trying to improve on those using information\ndropout.\n\nClarity\nThe paper is well written and easy to follow.\n\nOriginality\nThe derivation of the information dropout optimization problem using IB\nLagrangian is novel. However, the final model is quite close to variational\ndropout.\n\nSignificance\nThis paper will be of general interest to researchers in representation learning\nbecause it highlights an alternative way to think about latent variables (as\ninformation bottlenecks). However, unless the model can be shown to achieve\nsignificant improvements over simple dropout, its wider impact is likely to be\nlimited.\n\nOverall\nThe paper presents an insightful theoretical derivation and good preliminary\nresults. The experimental section can be improved.\n\nMinor comments and suggestions -\n- expecially -> especially\n- trough -> through\n- There is probably a minus sign missing in the expression for H(y|z) above Eq (2).\n- Figure 3(a) has error bars, but 3(b) doesn't. It might be a good idea to have those\nfor Figure 3(b) as well.\n- Please consider comparing Figure 2 with the activity map of a standard CNN\n  trained with binary dropout, so we can see if similar filtering out is\nhappening there already.","label":0,"model":"human","source":"peerread","id":4774}
{"text":"An interesting connection is made between dropout, Tishby et al's \"information bottleneck\" and VAEs. Specifically, classification of 'y' from 'x' is split in two faces: an inference model z ~ q(z|x), a prior p(z), and a classifier y ~ p(y|z). By optimizing the objective E_{(x,y)~data} [ E_{z~q(z|x)}[log p(x|y)] + lambda * KL(q(z|x)||p(z))], with lambda <= 1, an information bottleneck 'z' is formed, where lambda controls an upper bound on the number of bits traveling through 'z'.\n\nThe objective is equivalent to a VAE objective with downweighted KL(posterior|prior), an encoder that takes as input 'x', and a decoder that only predicts 'x'.\n\n- Related work (section 2) is discussed sufficiently. \n- In section 3, would be better to remind us the definition of mutual information.\n- Connection to VAEs in section 5 is interesting.\n- Unfortunately, the MNIST\/CIFAR-10 results are not great. Since the method is potentially more flexible than other forms of dropout, this is slightly disappointing.\n- It's unclear why the CIFAR-10 results seem to be substantially worse than the results originally reported for that architecture.\n- It's unclear which version of 'beta' was used in figure 3a.\n\nOverall I think the theory presented in the paper is promising. However, the paper lacks sufficiently convincing experimental results, and I encourage the authors to do further experiments that prove significant improvements, at least on CIFAR-10, perhaps on larger problems.","label":0,"model":"human","source":"peerread","id":4775}
{"text":"A personal communication asked whether there are cases in which a stochastic representation of the data can obtain a better value of the IB Lagrangian than any deterministic representation; in response to this, we added a remark in Section 3 saying that this indeed can happen. \n\nIn response to a question by the reviewer, we added to Section 2 a few examples of nuisances that act as a group on the data.\n\nWe updated the MNIST and CIFAR experiments: all the qualitative results are the same as before, but we slightly changed the hyperparameters and the optimization method to provide a more accurate and fairer comparison between the algorithms.\n\nFinally, we added an appendix to fill a gap in the narrative between Equation (2),  where the two distributions in the KL term were the actual prior and posterior of z, and Section 4, where we assume an approximated prior whose parameters are learned independently. Specifically, we show that if the approximated prior of the activations is chosen to be factorized, as we do, then our loss function differs from the actual IB Lagrangian by the total correlation of z. As a consequence, our approximation is correct when the components of z are mutually independent, and the loss function we use actually encourages this independence.\n\nWe would like to thank all the people that gave us early feedback on the paper.","label":0,"model":"human","source":"peerread","id":4776}
{"text":"I agree with the other reviewer that the application areas are limited in the paper. I agree with the overall sentiment of the paper to evaluate effectiveness of some of the more recent techniques in this area, in conjunction with the recurrent networks. \n\nThe paper advertises itself as a method (or a list of methods) of improving the recurrent baselines when performing experiments, however fails (or not shown) to generalize to other tasks. Effectiveness of these methods need to be shown across a wide variety of tasks if we intend to replace traditional baselines in general, rather than a specific subset of applications.\n\nI like the desire to evaluate many of the recent techniques and having many replications of experiments towards this end (which is a strong point of the paper). However, whether there are synergies of some of the enhancements with sentiment analysis or not, we cannot see from these results. It would be interesting to see whether some of these results generalize across a wide variety of tasks.","label":0,"model":"human","source":"peerread","id":4777}
{"text":"The paper attempts to perform an interesting exploration (how to combine different tricks for LSTM training) but does not take it far enough. \n \n Pros:\n - interesting attempt at studying different techniques to improve LSTM training results\n Cons:\n - not very strong baselines\n - limited set of domains were explored\n - low in novelty (which wouldn't be a problem if the comparison was more thorough -- see above 2 points).","label":0,"model":"human","source":"peerread","id":4778}
{"text":"The paper proposes and analyses three methods applied to traditional LSTMs: Monte Carlo test-time model averaging, average pooling, and residual connections. It shows that those methods help to enhance traditional LSTMs on sentiment analysis. \n\nAlthough the paper is well written, the experiment section is definitely its dead point. Firstly, although it shows some improvements over traditional LSTMs, those results are not on par with the state of the art. Secondly, if the purpose is to take those extensions as strong baselines for further research, the experiments are not adequate: the both two datasets which were used are quite similar (though they have different statistics). I thus suggest to carry out more experiments on more diverse tasks, like those in \"LSTM: A Search Space Odyssey\"). \n\nBesides, those extensions are not really novel.","label":0,"model":"human","source":"peerread","id":4779}
{"text":"This paper presents three improvements to the standard LSTM architecture used in many neural NLP models: Monte Carlo averaging, embed average pooling, and residual connections. Each of the modifications is trivial to implement, so the paper is definitely of interest to any NLP researchers experimenting with deep learning. \n\nWith that said, I am concerned about the experiments and their results. The residual connections do not seem to consistently help performance; on SST the vertical residuals help but the lateral residuals hurt, and on IMDB it is the opposite. More fundamentally, there need to be more tasks than just sentiment analysis here. I'm not quite sure why the paper's focus is on text classification, as any NLP task using an LSTM encoder could conceivably benefit from these modifications. It would be great to see a huge variety of tasks like QA, MT, etc., which would really make the paper much stronger. \n\nAt this point, while the experiments that are included in the paper are very thorough and the analysis is interesting, there need to be more tasks to convince me that the modifications generalize, so I don't think the paper is ready for publication.","label":0,"model":"human","source":"peerread","id":4780}
{"text":"I agree with the other reviewer that the application areas are limited in the paper. I agree with the overall sentiment of the paper to evaluate effectiveness of some of the more recent techniques in this area, in conjunction with the recurrent networks. \n\nThe paper advertises itself as a method (or a list of methods) of improving the recurrent baselines when performing experiments, however fails (or not shown) to generalize to other tasks. Effectiveness of these methods need to be shown across a wide variety of tasks if we intend to replace traditional baselines in general, rather than a specific subset of applications.\n\nI like the desire to evaluate many of the recent techniques and having many replications of experiments towards this end (which is a strong point of the paper). However, whether there are synergies of some of the enhancements with sentiment analysis or not, we cannot see from these results. It would be interesting to see whether some of these results generalize across a wide variety of tasks.  ","label":0,"model":"human","source":"peerread","id":4781}
{"text":"I find the experiments not convincing because the two datasets are quite similar (sentiment analysis). I was wondering if you have tried your proposed models\/methods on much more different tasks (e.g. machine translation, question answering, etc.)","label":0,"model":"human","source":"peerread","id":4782}
{"text":"This work proposes a model that can learn short binary codes via paragraph vectors to allow fast retrieval of documents. The experiments show that this is superior to semantic hashing. The approach is simple and not very technically interesting. For a code size of 128, the loss compared to a continuous paragraph vector seems moderate.\n\nThe paper asks the reader to refer to the Salakhutdinov and Hinton paper for the baseline numbers but I think they should be placed in the paper for easy reference. For simplicity, the paper could show the precision at 12.5%, 25% and 50% recall for the proposed model and semantic hashing. It also seems that the semantic hashing paper shows results on RCV2 and not RCV1. RCV1 is twice the size of RCV2 and is English only so it seems that these results are not comparable. It would be interesting to see how many binary bits are required to match the performance of the continuous representation. A comparison to the continuous PV-DBOW trained with bigrams would also make it a more fair comparison.\n\nFigure 7 in the paper shows a loss from using the real-binary PV-DBOW. It seems that if a user needed high quality ranking after the retrieval stage and they could afford the extra space and computation, then it would be better for them to use a standard PV-DBOW to obtain the continuous representation at that stage.\n\nMinor comments:\nFirst line after the introduction: is sheer -> is the sheer\n4th line from the bottom of P1: words embeddings -> word embeddings\nIn table 1: What does code size refer to for PV-DBOW? Is this the number of elements in the continuous vector?\n5th line from the bottom of P5: W -> We\n5th line after section 3.1: covers wide -> covers a wide","label":0,"model":"human","source":"peerread","id":4783}
{"text":"This paper proposes to binarize Paragraph Vector distributed representations in an end-to-end framework. Experiments demonstrate that this beats autoencoder-based binary codes. However, the performance is similar to using paragraph vectors followed by existing binarization techniques, failing to show an advantage of training end-to-end. Therefore, the novel contribution seems too limited for publication.","label":0,"model":"human","source":"peerread","id":4784}
{"text":"We have just uploaded an updated version of our paper. Below we list the most important changes. We will then follow with responses to the reviews.\n\n1) AnonReviewer3 pointed out that, apart from the Krizhevsky's binarization, one can also employ stochastic neurons in the coding layer. When comparing these two approaches we carried out a more thorough investigation of the dropout in the coding layers. In the initial experiments we employed small, 10% dropout. However, larger dropout rates (up to 50%) turned out to improve performance on the validation sets (in both binary and real-valued models). We therefore updated final results to reflect the dropout rates selected in validation experiments.\n\n2) We extended the comparison with baseline methods by adding results for two hashing techniques, namely random hyperplane projection and iterative quantization.\n\n3) Visualization of Binary PV codes (Figure 5 in the previous version of the paper) along with the corresponding text was moved to an appendix.","label":0,"model":"human","source":"peerread","id":4785}
{"text":"The method in this paper introduces a binary encoding level in the PV-DBOW and PV-DM document embedding methods (from Le & Mikolov'14). The binary encoding consists in a sigmoid with trained parameters that is inserted after the standard training stage of the embedding.\n \nFor a document to encode, the binary vector is obtained by forcing the sigmoid to output a binary output for each of the embedding vector components. The binary vector can then be used for compact storage and fast comparison of documents.\n \nPros:\n \n- the binary representation outperforms the Semantic hashing method from Salakhutdinov & Hinton '09\n \n- the experimental approach sound: they compare on the same experimental setup as Salakhutdinov & Hinton '09, but since in the meantime document representations improved (Le & Mikolov'14), they also combine this new representation with an RBM to show the benefit of their binary PV-DBOW\/PV-DM\n \nCons:\n \n- the insertion of the sigmoid to produce binary codes (from Lin & al. '15) in the training process is incremental\n \n- the explanation is too abstract and difficult to follow for a non-expert (see details below)\n \n- a comparison with efficient indexing methods used in image retrieval is missing. For large-scale indexing of embedding vectors, derivations of the Inverted multi-index are probably more interesting than binary codes. See eg. Babenko & Lempitsky, Efficient Indexing of Billion-Scale Datasets of Deep Descriptors, CVPR'16\n \nDetailed comments:\n \nSection 1: the motivation for producing binary codes is not given. Also, the experimental section could give some timings and mem usage numbers to show the benefit of binary embeddings\n \nfigure 1, 2, 3: there is enough space to include more information on the representation of the model: model parameters + training objective + characteristic sizes + dropout. In particular, in fig 2, it is not clear why \"embedding lookup\" and \"linear projection\" cannot be merged in a single smaller lookup table (presumably because there is an intermediate training objective that prevents this).\n \np2: \"This way, the length of binary codes is not tied to the dimensionality of word embeddings.\" -> why not?\n \nsection 3: This is the experimental setup of  Salakhutdinov & Hinton 2009. Specify this and whether there is any difference between the setups.\n \n\"similarity of the inferred codes\": say here that codes are compared using Hamming distances.\n \n\"binary codes perform very well, despite their far lower capacity\" -> do you mean smaller size than real vectors?\n \nfig 5: these plots could be dropped if space is needed.\n \nsection 3.1: one could argue that \"transferring\" from Wikipedia to anything else cannot be called transferring, since Wikipedia's purpose is to include all topics and lexical domains\n \nsection 3.2: specify how the 300D real vectors are compared. L2 distance? inner product?\n \nfig4: specify what the raw performance of the large embedding vectors is (without pre-filtering with binary codes), or equivalently, the perf of (code-size, Hamming dis) = (28, 28), (24, 24), etc.\n","label":0,"model":"human","source":"peerread","id":4786}
{"text":"This paper presents a method to represent text documents and paragraphs as short binary codes to allow fast similarity search and retrieval by using hashing techniques. The real-valued paragraph vectors by Le & Mikolov is extended by adding a stochastic binary layer on top of the neural network architecture. Two methods for binarizing the final activations are compared: (1) simply adding noise to sigmoid activations to encourage discritization. (2) binarizing the activations in the forward pass and keeping them real-valued in the backward pass (straight-through estimation). The paper presents encouraging results by using straight-through estimation on 20 newsgroup and RCV1 text datasets by using 128 and 32 bit binary codes.\n\nOn the plus side, the application presented in the paper is interesting and important. The exposition of the paper is clean and clear. However, the novelty of the approach is limited from a machine learning standpoint. The literature on binary hashing beyond semantic hashing and Krizhevsky's binary autoencoders in 2011 is not explained. An important baseline is missing where real-valued paragraph vectors are learned first, and then converted to binary codes using off-the-shelf hashing methods (e.g. random projection LSH by Charikar, BRE by Kulis & Darrell, ITQ by Gong & Lazebnik, MLH by Norouzi & Fleet, etc.)\n\nGiven the lack of novelty and the missing baseline, I do not recommend this paper in its current for publication in the ICLR conference's proceeding. Moving forward, this paper may be more suitable for NLP conferences as it is more on the applied side.\n\nMore comments:\n- I believe from an practical perspective it may be easier to first learn real-valued paragraph vectors and then quantize them for indexing. That said, an end-to-end approach as proposed in this paper may perform better. I would like to see an empirical comparison between the proposed end-to-end approach and a simpler two stage quantization method suggested here.\n- See \"Estimating or Propagating Gradients Through Stochastic Neurons\" By Bengio et al - discussing straight through estimation and some other alternatives.\n- The paper argues that the length of binary codes cannot be longer than 32 bits because longer codes are not suitable for document hashing. This is not quite right given multi-probe hashing mechanisms, for example see \"Mult-index Hashing\" by Norouzi et al.\n- See \"Hashing for Similarity Search: A Survey\" by Wang et al. for a survey of related work on binary hashing and quantization. You seem to ignore the extensive work done on binary hashing.\n","label":0,"model":"human","source":"peerread","id":4787}
{"text":"This work proposes a model that can learn short binary codes via paragraph vectors to allow fast retrieval of documents. The experiments show that this is superior to semantic hashing. The approach is simple and not very technically interesting. For a code size of 128, the loss compared to a continuous paragraph vector seems moderate.\n\nThe paper asks the reader to refer to the Salakhutdinov and Hinton paper for the baseline numbers but I think they should be placed in the paper for easy reference. For simplicity, the paper could show the precision at 12.5%, 25% and 50% recall for the proposed model and semantic hashing. It also seems that the semantic hashing paper shows results on RCV2 and not RCV1. RCV1 is twice the size of RCV2 and is English only so it seems that these results are not comparable. It would be interesting to see how many binary bits are required to match the performance of the continuous representation. A comparison to the continuous PV-DBOW trained with bigrams would also make it a more fair comparison.\n\nFigure 7 in the paper shows a loss from using the real-binary PV-DBOW. It seems that if a user needed high quality ranking after the retrieval stage and they could afford the extra space and computation, then it would be better for them to use a standard PV-DBOW to obtain the continuous representation at that stage.\n\nMinor comments:\nFirst line after the introduction: is sheer -> is the sheer\n4th line from the bottom of P1: words embeddings -> word embeddings\nIn table 1: What does code size refer to for PV-DBOW? Is this the number of elements in the continuous vector?\n5th line from the bottom of P5: W -> We\n5th line after section 3.1: covers wide -> covers a wide\n","label":0,"model":"human","source":"peerread","id":4788}
{"text":"This paper presents an approach to non-linear kernel dimensionality reduction with a trace norm regularizer in the feature space. The authors proposed an iterative minimization approach in order to obtain a local optimum of a relaxed problem. \nThe paper contains errors and the experimental evaluation is not convincing. Only old techniques are compared against in very toy datasets. \n\nThe authors claim state-of-the-art, however, the oil dataset is not a real benchmark, and the comparisons are to very old approaches. \nThe experimental evaluation should demonstrate robustness to more complex noise and outliers, as this was one of the motivations in the introduction.\n\nThe authors do not address the out-of-sample problem. This is a problem of kernel-based methods vs LVMs, and thus should be address here.\n\n\nThe paper contains errors:\n\n- The last paragraph of section 1 says that this paper proposes a closed form solution to robust KPCA. This is simply wrong, as the proposed approach consists of iteratively solving iterativey a set of closed form updates  and Levenberg-Marquard optimizationd. This is not any more closed form!\n\n- In the same paragraph (and later in the text) the authors claim that the proposed approach can be trivially generalized to incorporate other cost functions. This is not true, as in general there will be no more inner loop closed form updates and the authors will need to solve a much more complex optimization problem. \n\n- The third paragraph of section 2 claims that this paper presents a novel energy minimization framework to solve problems of the general form of eq. (2). However, this is not what the authors solve at the end. They solve a different problem that has been subject to at least two relaxations. It is not clear how solving for a local optima of this double relaxed problem is related to the original problem they want to solve. \n\n- The paper says that Geiger et al defined non linearities on a latent space of pre-defined dimensionality. This is wrong. This paper discovers the dimensionality of the latent space by means of a regularizer that encourages the singular values to be sparse. Thus, it does not have a fixed dimensionality, the latent space is just bounded to be smaller or equal than the dimensionality of the original space. \n\n\nIt is not clear to me why the author say for LVMs such as GPLVM that \"the latent space is learned a priority with clean training data\". One can use different noise models within the GP framework. Furthermore, the proposed approach assumes Gaussian noise (see eq. 6), which is also the trivial case for GP-based LVMs.  \n\n\nIt is not clear what the authors mean in the paper by \"pre-training\" or saying that techniques do not have a training phase. KPCA is trained via a closed-form update, but there is still training.","label":0,"model":"human","source":"peerread","id":4789}
{"text":"There is complete consensus among the reviewers that the KPCA formulation in this paper needs better motivation; that the paper has technical errors, and the experimental evaluation is not convincing. As such the paper is not up to ICLR standards. The authors are encouraged to revise the paper based on feedback from the reviews.","label":0,"model":"human","source":"peerread","id":4790}
{"text":"The paper proposes a nonlinear regularizer for solving ill-posed inverse problems. The latent variables (or causal factors) corresponding to the observed data are assumed to lie near a low dimensional subspace in an RKHS induced by a predetermined kernel. The proposed regularizer can be seen as an extension of the linear low-rank assumption on the latent factors. A nuclear norm penalty on the Cholesky factor of the kernel matrix is used as a relaxation for the dimensionality of the subspace. Empirical results are reported on two tasks involving linear inverse problems -- missing feature imputation, and estimating non-rigid 3D structures from a sequence of 2D orthographic projections -- and the proposed method is shown to outperform linear low-rank regularizer. \n\nThe clarity of the paper has scope for improvement (particularly, Introduction) - the back and forth b\/w dimensionality reduction techniques and inverse problems is confusing at times. Clearly defining the ill-posed inverse problem first and then motivating the need for a regularizer (which brings dimensionality reduction techniques into the picture) may be a more clear flow in my opinion. \n\nThe motivation behind relaxation of rank() in Eq 1 to nuclear-norm in Eq 2 is not clear to me in this setting. The relaxation does not yield a convex problem over S,C (Eq 5) and also increases the computations (Algo 2 needs to do full SVD of K(S) every time). The authors should discuss pros\/cons over the alternate approach that fixes the rank of C (which can be selected using cross-validation, in the same way as $\\tau$ is selected), leaving just the first two terms in Eq 5. For this simpler objective, an interesting question to ask would be -- are there kernel functions for which it can solved in a scalable manner? \n\nThe proposed alternating optimization approach in the current form is computationally intensive and seems hard to scale to even moderate sized data -- in every iteration one needs to compute the kernel matrix over S and perform full SVD over the kernel matrix (Algo 2). Empirical evaluations are also not extensive -- (i) the dataset used for feature imputation is old and non-standard, (ii) for structure estimation from motion on CMU dataset, the paper only compares with linear low-rank regularization, (iii) there is no comment\/study on the convergence of the alternating procedure (Algo 1). \n\n\n\n\n\n","label":0,"model":"human","source":"peerread","id":4791}
{"text":"This paper considers an alternate formulation of Kernel PCA with rank constraints incorporated as a regularization term in the objective. The writing is not clear. The focus keeps shifting from estimating \u201ccausal factors\u201d, to nonlinear dimensionality reduction to Kernel PCA to ill-posed inverse problems. The problem reformulation of Kernel PCA uses somewhat standard tricks and it is not clear what are the advantages of the proposed approach over the existing methods as there is no theoretical analysis of the overall approach or empirical comparison with existing state-of-the-art.  \n\n- Not sure what the authors mean by \u201ccausal factors\u201d. There is a reference to it in Abstract and in Problem formulation on page 3 without any definition\/discussion.\n\n- In KPCA, I am not sure why one is interested in step (iii) outlined on page 2 of finding a pre-image for each\n\n- Authors outline two key disadvantages of the existing KPCA approach. The first one, that of low-dimensional manifold assumption not holding exactly, has received lots of attention in the machine learning literature. It is common to assume that the data lies near a low-dimensional manifold rather than on a low-dimensional manifold. Second disadvantage is somewhat unclear as finding \u201ca data point (pre-image) corresponding to each projection in the input space\u201d is not a standard step in KPCA. \n\n- On page 3, you never define $\\mathcal{X} \\times N$, $\\mathcal{Y} \\times N$, $\\mathcal{H} \\times N$. Clearly, they cannot be cartesian products. I have to assume that notation somehow implies N-tuples. \n\n- On page 3, Section 2, $\\mathcal{X}$ and $\\mathcal{Y}$ are sets. What do you mean by $\\mathcal{Y} \\ll \\mathcal{X}$\n\n- On page 5, $\\mathcal{S}^n$ is never defined. \n\n- Experiments: None of the standard algorithms for matrix completion such as OptSpace or SVT were considered \n\n- Experiments: There is no comparison with alternate existing approaches for Non-rigid structure from motion.  \n\n- Proof of the main result Theorem 3.1: To get from (16) to (17) using the Holder inequality (as stated) one would end up with a term that involves sum of fourth powers of weights w_{ij}. Why would they equal to one using the orthonormal constraints? It would be useful to give more details here, as I don\u2019t see how the argument goes through at this point. ","label":0,"model":"human","source":"peerread","id":4792}
{"text":"This paper presents an approach to non-linear kernel dimensionality reduction with a trace norm regularizer in the feature space. The authors proposed an iterative minimization approach in order to obtain a local optimum of a relaxed problem. \nThe paper contains errors and the experimental evaluation is not convincing. Only old techniques are compared against in very toy datasets. \n\nThe authors claim state-of-the-art, however, the oil dataset is not a real benchmark, and the comparisons are to very old approaches. \nThe experimental evaluation should demonstrate robustness to more complex noise and outliers, as this was one of the motivations in the introduction.\n\nThe authors do not address the out-of-sample problem. This is a problem of kernel-based methods vs LVMs, and thus should be address here.\n\n\nThe paper contains errors:\n\n- The last paragraph of section 1 says that this paper proposes a closed form solution to robust KPCA. This is simply wrong, as the proposed approach consists of iteratively solving iterativey a set of closed form updates  and Levenberg-Marquard optimizationd. This is not any more closed form!\n\n- In the same paragraph (and later in the text) the authors claim that the proposed approach can be trivially generalized to incorporate other cost functions. This is not true, as in general there will be no more inner loop closed form updates and the authors will need to solve a much more complex optimization problem. \n\n- The third paragraph of section 2 claims that this paper presents a novel energy minimization framework to solve problems of the general form of eq. (2). However, this is not what the authors solve at the end. They solve a different problem that has been subject to at least two relaxations. It is not clear how solving for a local optima of this double relaxed problem is related to the original problem they want to solve. \n\n- The paper says that Geiger et al defined non linearities on a latent space of pre-defined dimensionality. This is wrong. This paper discovers the dimensionality of the latent space by means of a regularizer that encourages the singular values to be sparse. Thus, it does not have a fixed dimensionality, the latent space is just bounded to be smaller or equal than the dimensionality of the original space. \n\n\nIt is not clear to me why the author say for LVMs such as GPLVM that \"the latent space is learned a priority with clean training data\". One can use different noise models within the GP framework. Furthermore, the proposed approach assumes Gaussian noise (see eq. 6), which is also the trivial case for GP-based LVMs.  \n\n\nIt is not clear what the authors mean in the paper by \"pre-training\" or saying that techniques do not have a training phase. KPCA is trained via a closed-form update, but there is still training.  \n","label":0,"model":"human","source":"peerread","id":4793}
{"text":"Overall I think this is an interesting paper which shows empirical performance improvement over baselines. However, my main concern with the paper is regarding its technical depth, as the gist of the paper can be summarized as the following: instead of keeping the batch norm mean and bias estimation over the whole model, estimate them on a per-domain basis. I am not sure if this is novel, as this is a natural extension of the original batch normalization paper. Overall I think this paper is more fit as a short workshop presentation rather than a full conference paper.\n\nDetailed comments:\n\nSection 3.1: I respectfully disagree that the core idea of BN is to align the distribution of training data. It does this as a side effect, but the major purpose of BN is to properly control the scale of the gradient so we can train very deep models without the problem of vanishing gradients. It is plausible that intermediate features from different datasets naturally show as different groups in a t-SNE embedding. This is not the particular feature of batch normalization: visualizing a set of intermediate features with AlexNet and one gets the same results. So the premise in section 3.1 is not accurate.\n\nSection 3.3: I have the same concern as the other reviewer. It seems to be quite detatched from the general idea of AdaBN. Equation 2 presents an obvious argument that the combined BN-fully_connected layer forms a linear transform, which is true in the original BN case and in this case as well. I do not think it adds much theoretical depth to the paper. (In general the novelty of this paper seems low)\n\nExperiments:\n\n- section 4.3.1 is not an accurate measure of the \"effectiveness\" of the proposed method, but a verification of a simple fact: previously, we normalize the source domain features into a Gaussian distribution. the proposed method is explicitly normalizing the target domain features into the same Gaussian distribution as well. So, it is obvious that the KL divergence between these two distributions are closer - in fact, one is *explicitly* making them close. However, this does not directly correlate to the effectiveness of the final classification performance.\n\n- section 4.3.2: the sensitivity analysis is a very interesting read, as it suggests that only a very few number of images are needed to account for the domain shift in the AdaBN parameter estimation. This seems to suggest that a single \"whitening\" operation is already good enough to offset the domain bias (in both cases shown, a single batch is sufficient to recover about 80% of the performance gain, although I cannot get data for even smaller number of examples from the figure). It would thus be useful to have a comparison between these approaches, and also a detailed analysis of the effect from each layer of the model - the current analysis seems a bit thin.","label":0,"model":"human","source":"peerread","id":4794}
{"text":"The paper performs domain adaptation using a very simple trick inspired by BatchNorm. The paper received below margin scores. The reviewers both, liked the simplicity of the approach, and at the same time felt that the contribution was too thin. Given the high bar of ICLR, this paper falls short.","label":0,"model":"human","source":"peerread","id":4795}
{"text":"This work also inspired our another recent work \"Demystifying Neural Style Transfer\"(","label":0,"model":"human","source":"peerread","id":4796}
{"text":"According to the reviewers\u2019 helpful suggestions, we have revised and updated our paper. The main modifications are as follows:\n(1) We have updated the writing of our abstract and revised section 3.3 to make it clearer and merge it to the section 3.2.\n(2) We have removed the original section 4.3.1, and revised section \u201csensitivity to target domain size\u201d by adding the experimental results of using smaller number of images.\n(3) We have added a new analysis section \u201cAdaptation Effect for Different BN Layers\u201d in section 4.3.2.\n","label":0,"model":"human","source":"peerread","id":4797}
{"text":"We do not think simplicity is our drawback of our method. On the contrary, we believe this is a great advantage of our method. Being technically simple does not mean no novelty, and it should never be a reason to reject a paper. In fact, Batch Normalization is a very simple method, while dropout is even simpler. These techniques have had huge impacts to the field despite the simplicity. As the Reviewer2 indicates, there are also prior simple but important methods in domain adaptation. (e.g. \u201cFrustratingly Easy Domain Adaptation\u201d and \u201cReturn of Frustratingly Easy Domain Adaptation\u201d) Further, to our best knowledge, there are no prior works to exploit Batch Normalization for domain adaptation.\n\nThe main contributions in our paper is that we propose a simple yet effective AdaBN method for domain adaptation by modulating the statistics in all BN layers, which outperforms the states-of-the-art methods. Furthermore, we demonstrate that our method is complementary with other existing methods. Thus, we think it is valuable for the researchers in the field.\n","label":0,"model":"human","source":"peerread","id":4798}
{"text":"This paper proposes a simple domain adaptation technique in which batch normalization is performed separately in each domain.\n\n\nPros:\n\nThe method is very simple and easy to understand and apply.\n\nThe experiments demonstrate that the method compares favorably with existing methods on standard domain adaptation tasks.\n\nThe analysis in section 4.3.2 shows that a very small number of target domain samples are needed for adaptation of the network.\n\n\nCons:\n\nThere is little novelty -- the method is arguably too simple to be called a \u201cmethod.\u201d Rather, it\u2019s the most straightforward\/intuitive approach when using a network with batch normalization for domain adaptation.  The alternative -- using the BN statistics from the source domain for target domain examples -- is less natural, to me. (I guess this alternative is what\u2019s done in the Inception BN results in Table 1-2?)\n\nThe analysis in section 4.3.1 is superfluous except as a sanity check -- KL divergence between the distributions should be 0 when each distribution is shifted\/scaled to N(0,1) by BN.\n\nSection 3.3: it\u2019s not clear to me what point is being made here.\n\n\nOverall, there\u2019s not much novelty here, but it\u2019s hard to argue that simplicity is a bad thing when the method is clearly competitive with or outperforming prior work on the standard benchmarks (in a domain adaptation tradition that started with \u201cFrustratingly Easy Domain Adaptation\u201d).  If accepted, Sections 4.3.1 and 3.3 should be removed or rewritten for clarity for a final version.","label":0,"model":"human","source":"peerread","id":4799}
{"text":"Update: I thank the authors for their comments. I still think that the method needs more experimental evaluation: for now, it's restricted to the settings in which one can use pre-trained ImageNet model, but it's also important to show the effectiveness in scenarios where one needs to train everything from scratch. I would love to see a fair comparison of the state-of-the-art methods on toy datasets (e.g. see (Bousmalis et al., 2016), (Ganin & Lempitsky, 2015)). In my opinion, it's a crucial point that doesn't allow me to increase the rating.\n\nThis paper proposes a simple trick turning batch normalization into a domain adaptation technique. The authors show that per-batch means and variances normally computed as a part of the BN procedure are sufficient to discriminate the domain. This observation leads to an idea that adaptation for the target domain can be performed by replacing population statistics computed on the source dataset by the corresponding statistics from the target dataset.\n\nOverall, I think the paper is more suitable for a workshop track rather than for the main conference track. My main concerns are the following:\n\n1. Although the main idea is very simple, it feels like the paper is composed in such a way to make the main contribution less obvious (e.g. the idea could have been described in the abstract but the authors avoided doing so). \n\n2. (This one is from the pre-review questions) The authors are using much stronger base CNN which may account for the bulk of the reported improvement. In order to prove the effectiveness of the trick, the authors would need to conduct a fair comparison against competing methods. It would be highly desirable to conduct this comparison also in the case of a model trained from scratch (as opposed to reusing ImageNet-trained networks).\n","label":0,"model":"human","source":"peerread","id":4800}
{"text":"Strengths\n\uf06e-- An interesting proposal for a smaller CNN architecture designed for embedded CNN applications. \n\uf06e-- Balanced exploration of CNN macroarchitecture and microarchitecture with fire modules.\n\uf06e-- x50 less memory usage than AlexNet, keeping similar accuracy \n\uf06e-- strong experimental results\n\nWeaknesses\n\uf06e--Would be nice to test Sqeezenet on multiple tasks\n\n\uf06e--lack of insights and rigorous analysis into what factors are responsible for the success of SqueezeNet. For example, how are ResNet and GoogleNet connected to the current architecture? Another old paper (Analysis of correlation structure for a neural predictive model with application to speech recognition, Neural Networks, 1994) also showed that the \u201cby-pass\u201d architecture by mixing linear and nonlinear prediction terms improves long term dependency in NN based on rigorous perturbation analysis. Can the current work be placed more rigorously on theoretical analysis?","label":0,"model":"human","source":"peerread","id":4801}
{"text":"The paper proposes a ConvNet architecture (\"SqueezeNet\") and a building block (\"Fire module\") aimed at reducing the model size while maintaining the AlexNet level of accuracy. The novelty of the submission is very limited as very similar design choices have already been used for model complexity reduction in Inception and ResNet. Because of this, we recommend rejection and invite the authors to further develop their method.","label":0,"model":"human","source":"peerread","id":4802}
{"text":"Strengths\n\uf06e-- An interesting proposal for a smaller CNN architecture designed for embedded CNN applications. \n\uf06e-- Balanced exploration of CNN macroarchitecture and microarchitecture with fire modules.\n\uf06e-- x50 less memory usage than AlexNet, keeping similar accuracy \n\uf06e-- strong experimental results\n\nWeaknesses\n\uf06e--Would be nice to test Sqeezenet on multiple tasks\n\n\uf06e--lack of insights and rigorous analysis into what factors are responsible for the success of SqueezeNet. For example, how are ResNet and GoogleNet connected to the current architecture? Another old paper (Analysis of correlation structure for a neural predictive model with application to speech recognition, Neural Networks, 1994) also showed that the \u201cby-pass\u201d architecture by mixing linear and nonlinear prediction terms improves long term dependency in NN based on rigorous perturbation analysis. Can the current work be placed more rigorously on theoretical analysis?\n","label":0,"model":"human","source":"peerread","id":4803}
{"text":"Summary: The paper presents a smaller CNN architecture called SqueezeNet for embedded deployment. The paper explores CNN macroarchitecture and microarchitecture to develop SqueezeNet, which is composed of fire modules.\n\nPros: \nAchieves x50 less memory usage than AlexNet while keeping similar accuracy.\n\nCons & Questions:\nComplex by-pass has less accuracy than simple by-pass. And simple by-pass is like ResNet bottlenecks and complex by-pass is like inception modules in GoogLeNet. Can we say that these two valiants of SqueezeNet are adaptation of concepts seen in GoogLeNet and ResNet? If so, then shouldn\u2019t be there a SqueezeNet like model that achieves similar accuracy compared with GoogLeNet and ResNet?\n","label":0,"model":"human","source":"peerread","id":4804}
{"text":"The Squeezenet paper came out in Feb 2016, and I read it with interest. It has a series of completely reasonable engineering suggestions for how to save parameter memory for CNNs for object recognition (imagenet). The suggestions make a lot of sense, and provide an excellent compression of about 50x versus AlexNet. (Looks like ~500x if combined with Han, 2015). So, very nice results, definitely worth publishing.\n\nSince the arxiv paper came out, people have noticed and worked to extend the paper. This is already evidence that this paper will have impact --- and deserves to have a permanent published home.\n\nOn the negative side, the architecture was only tested on ImageNet -- unclear whether the ideas transfer to other tasks (e.g., audio or text recognition). And, as with many other architecture-tweaking papers, there is no real mathematical or theoretical support for the ideas: they are just sensible and empirically work.\n\nOh the whole, I think the paper deserves to appear at ICLR, being in the mainline of work on deep learning architectures.","label":0,"model":"human","source":"peerread","id":4805}
{"text":"This paper proposes a novel approach ParMAC, a parallel and distributed framework of MAC (the Method of Auxiliary Coordinates) to learn nested and non-convex models which is based on the composition of multiple processing layers (i.e., deep nets). The basic idea of MAC to optimise the nested objective function, which is traditionally learned using methods based on the chain-rule gradients but inconvenient and is hard to parallelise, is to break nested functional relationships judiciously by introducing new variables ( the auxiliary coordinates) as equality constraints, and then to optimise a penalised function using alternating optimisation over the original parameters (W step) and over the coordinates (Z step).  The minimisation (W step) updates the parameters by splitting the nested model into independent submodels and training them using existing algorithms, and the coordination (Z step) ensures that corresponding inputs and outputs of submodels eventually match.  In this paper, the basic assumptions of ParMAC are that with large datasets in distributed systems, it is imperative to minimise data movement over the network because of the communication time generally far exceeds the computation time in modern architectures. Thus, the authors propose the ParMAC to translate the parallelism inherent in MAC into a distributed system by data parallelism and model parallelism. They also analyse its parallel speedup and convergence, and demonstrated it with MPI-based implementation to optimise binary autoencoders. The proposed ParMAC is tested on 3 colour image retrieval datasets. \n\nThe organization of the paper is well written, and the presentation is clear. My questions are included in the following:\n- The MAC framework solves the original problem approximately. If people use the sigmoid function to smooth the stepwise function, the naive optimization methods can be easier applied. What is the difference between these two? Or why do we want to use a new approach to solve it?\n- The authors do not compare their ParMAC model with other distributed approaches for the same nested function optimization problem.","label":0,"model":"human","source":"peerread","id":4806}
{"text":"The work proposes a parallel\/distributed variant of the MAC decomposition method. In presents some theoretical and experimental results supporting the parallelization strategy. The reviews are mixed and indeed a common concern among the reviewers was the choice of test problem. To me it is ok to only concentrate on a single class of problems, but in this case it needs to be a problem that the ICLR community identifies as being of central importance. Otherwise, if a more esoteric problem is chosen then I (and the reviewers) would rather see that the method is useful on multiple problems. Otherwise, it's basically impossible to extrapolate the experiments to new settings and we are forced to re-implement the algorithm. I'm not saying that the authors necessarily need to consider deep networks and there are many alternative possible models (sparse coding, collaborative filtering, etc.). But it should be noted that, without further experimental comparisons, it is impossible to verify the author's claims that the method is effective for deeply-nested models.\n \n Other concerns brought up by the reviewers (beyond the clarity\/presentation issues, which should also be addressed): the experimental comparison would be more convincing with a comparison to an existing approach like a parallel SGD method. I appreciate that the authors have done a lot of work already on this problem, but doing such obvious comparisons should be the job of the author instead of the reader (focusing purely on parallelization would be ok if the MAC model was extremely-widely-used already and parallelizing was an open problem, but my impression is that this is not the case). As a minor aside, the memory issue will be more serious for deeply-nested models, due to the use of the decomposition approach (we don't want to store the activations for all layers for all examples), and this doesn't arise in SGD.","label":0,"model":"human","source":"peerread","id":4807}
{"text":"We thank the reviewers for their reviews. Below we reply individually to each one. Here, we address a comment that several reviewers made, namely that the binary autoencoder model we explore experimentally is not well known by other researchers. It is true that this model is less well known than deep nets, but it was a good choice for this paper for several reasons:\n- This type of binary autoencoders is actually well known in the area of binary hashing, where one wants to learn a fast hash function (e.g. linear) with binary outputs because the goal is to do fast image searches in large image databases or similar retrieval problems. We have worked in this area using the MAC algorithm and it was convenient for us to develop ParMAC for it.\n- The binary autoencoder allows us to highlight the ability of ParMAC to train non-differentiable models, for which the chain rule does not apply.\n- The binary hashing application also provides with large, public training sets (100 million images). This allowed us to test ParMAC in a realistic distributed setting (up to 128 processors over a network). For us it was important to get actual experimental numbers in a distributed cluster (rather than on cores in a machine or simulating network delays).\n\nFinally, perhaps it is not obvious, but implementing and debugging the algorithm in C and MPI costs significant effort, and running the experiments in the UCSD cluster costs real money (around $0.03 per processing core per hour, which quickly becomes hundreds of dollars). This isn't your usual Matlab or GPU experiment... For a team of one student and one faculty member this puts limitations on the size and number of the experiments.\n\nWe provide the full C\/MPI code in our website to recreate the experiments in either a shared- or a distributed-memory system.\n","label":0,"model":"human","source":"peerread","id":4808}
{"text":"The paper presents an architecture to parallelize the optimization of nested functions based on the method of auxiliary coordinates (MAC) (Carreira-Perpinan and Wang, 2012). This method decomposes the optimization into training individual layers and updating the auxiliary coordinates. The paper focuses on binary autoencoders and proposes to partition the data onto several machines allowing the parameters to move between machines. Relatively good speedup factors are reported especially on larger datasets and a theoretical model of performance is presented that matches with the experiments.\n\nMy main concern is that even though the method is presented as a general framework for nested functions, experiments focus on a restricted family of models (i.e. binary autoencoders with linear or kernel encoders and linear decoders) with only two components. While the speedup factors are encouraging, it is hard to get a sense of their importance as the binary autoencoder model considered is not well studied by other researchers and is not widely used. I encourage the authors to apply this framework to more generic architectures and problems.\n\nQuestions:\n1- Does this framework apply to some form of generic multi-layer neural network? If so, some experimental results are useful.\n2- What is the implication of applying this framework to more than two components (an encoder and a decoder) and non-linear components?\n3- It is desired to see a plot of performance as a function of time for different setups to demonstrate the speedup after convergence. It seems the paper only focuses on the speedup factors per iteration. For example, increasing the mini-batch size may improve the speed per iteration but may hurt the convergence speed.\n4- Did you consider a scenario where the dataset is too big that storing the data and auxiliary variables on multiple machines simultaneously is not possible?\n\nThe paper cites an ArXiv manuscript with the same title by the authors multiple times. Please make the paper self-contained and include any supplementary material in the appendix.\n\nI believe without applying this framework to a more generic architecture beyond binary autoencoders, this paper does not appeal to a wide audience at ICLR, hence weak reject.\n","label":0,"model":"human","source":"peerread","id":4809}
{"text":"UPDATE:\nI looked at the arxiv version of the paper. It is much longer and appears more rigorous. Fig 3 there is indeed more insightful.\nHowever, I am reviewing the submission and my overall assessment does not change. This is not a minor incremental contribution, and if you want to compress it into a conference submission of this type, I would recommend choosing message you want to convey, and focus on that. As you say, \"...ICLR submission focus on the ParMAC algorithm...\", I would focus on this properly - and remove or move to appendix all extensions and theoretical remarks, and have an extra page on explaining the algorithm. Additionally, make sure to clearly explain the relation of the arxiv paper, in particular that the submission was a compressed version.\n\nORIGINAL REVIEW:\nThe submission proposes ParMAC, based on MAC (Method of Auxiliary Coordinates), formulating a distributed variant of the idea.\n\nRelated Work: In the part on convex ERM and methods, I would recommend citing general communication efficient frameworks, COCOA (Ma et al.) and AIDE (Reddi et al.). I believe these works are most related to the practical objectives authors of this paper set, while number of the papers cited are less relevant.\n\nSection 2, explaining MAC, is quite clearly written, but I do not find part on MAC and EM particularly useful.\n\nSection 3 is much less clearly written. I have trouble following notation, particularly in the speedups part, as different symbols were introduced at different places. Perhaps a quick summary or paragraph on notation in the introduction would be helpful. In paragraph 2, you write as if reader knew how data\/anything is distributed, but this was not mentioned yet; it is specified later. It is not clear what is meant by \"submodel\". Perhaps a more precise example pointing back to eqs (1) & (2) would be useful. As far as I understand from what is written, there are P independent sets of submodels, that traverse the machines in circular fashion. I don't understand how are they initialized (identically?), and more importantly I don't understand what would be a single output of the algorithm (averaging? does not seem to make sense). Since this is not addressed, I suppose I get it wrong, leaving me to guess what was actually meant. \nThe fact that I am not able to understand what is actually happening, I see as major issue.\n\nI don't like the later paragraphs on extensions, model for speedup, convergence and topologies. I don't understand whether these are novel contributions or not, as the authors refer to other work for details. If these are novel, the explanation is not sufficient, particularly speedup part, which contains undefined quantities, e.g. T(P) (or I can't find it). If this is not novel, It does not provide enough explanation to understand anything more, compared with a its version compressed to 1\/4 of its size and referring to the other work. The statement that we can recover the original convergence guarantees seems strong and I don't see why it should be trivial to show (but author point to other work which I did not look at). In topologies part, claiming that something does \"true SGD\", without explaining what is \"true SGD\" seems very strange. Other statements in this section seem also very vague and unjustified\/unexplained.\n\nExperimental section seems to suggest that the method is interesting for binary autoencoders, but I don't see how would I conclude anything about any other models. ParMAC is also not compared to alternative methods, only with itself, focusing on scaling properties.\n\nConclusion contains statements that are too strong or misleading based on what I saw. In particular, \"we analysed its parallel speedup and convergence\" seems ungrounded. Further, the claim \"The convergence properties of MAC remain essentially unaltered in ParMAC\" is unsupported, regardless of the meaning of \"essentially unchanged\".\n\nIn summary, the method seems relevant for particular model class, binary autoencoders, but clarity of presentation is insufficient - I wouldn't be able to recreate the algorithm used in experiments - and the paper contains a number of questionable claims.","label":0,"model":"human","source":"peerread","id":4810}
{"text":"This paper proposes an extension of the MAC method in which subproblems are trained on a distributed cluster arranged in a circular configuration. The basic idea of MAC is to decouple the optimization between parameters and the outputs of sub-pieces of the model (auxiliary coordinates); optimization alternates between updating the coordinates given the parameters and optimizing the parameters given the outputs. In the circular configuration. Because each update is independent, they can be massively parallelized.\n\nThis paper would greatly benefit from more concrete examples of the sub-problems and how they decompose. For instance, can this be applied effectively for deep convolutional networks, recurrent models, etc? From a practical perspective, there's not much impact for this paper beyond showing that this particular decoupling scheme works better than others. \n\nThere also seem to be a few ideas worth comparing, at least:\n- Circular vs. parameter server configurations\n- Decoupled sub-problems vs. parallel SGD\n\nParallel SGD also has the benefit that it's extremely easy to implement on top of NN toolboxes, so this has to work a lot better to be practically useful. \n\nAlso, it's a bit hard to understand what exactly is being passed around from round to round, and what the trade-offs would be in a deep feed-forward network. Assuming you have one sub-problem for every hidden unit, then it seems like:\n\n1. In the W step, different bits of the NN walk their way around the cluster, taking SGD steps w.r.t. the coordinates stored on each machine. This means passing around the parameter vector for each hidden unit.\n2. Then there's a synchronization step to gather the parameters from each submodel, requiring a traversal of the circular structure.\n3. Then each machine updates it's coordinates based on the complete model for a slice of the data. This would mean, for a feed-forward network, producing the intermediate activations of each layer for each data point.\n\nSo for something comparable to parallel SGD, you could do the following: put a mini-batch of size B on each machine with ParMAC, compared to running such mini-batches in parallel. Completing steps 1-2-3 above would then be roughly equivalent to one synchronized PS type implementation step (distribute model to workers, get P gradients back, update model.)\n\n It would be really helpful to see how this compares in practice. It's hard for me to understand intuitively why the proposed method is theoretically any better than parallel SGD (except for the issue of non-smooth function optimization); the decoupling also can fundamentally change the problem since you're not doing back-propagation directly anymore, so that seems like it would conflate things as well and it's not necessarily going to just work for other types of architectures.","label":0,"model":"human","source":"peerread","id":4811}
{"text":"Summary\n===\nThis paper presents tic-tac-toe as toy problem for investigating CNNs.\nA dataset is created containing tic-tac-toe boards where one player is one\nmove away from winning and a CNN is trained to label boards according\nto (1) the player who can win (2 choices) and (2) the position they may move\nto win (9 choices), resulting in 18 labels. The CNN evaluated in this paper\nperforms perfectly at the task and the paper's goal is to inspect how the\nCNN works.\n\nThe fundamental mechanism for this inspection is Class Activation\nMapping (CAM) (Zhou et. al. 2016), which identifies regions of implicit attention\nin the CNN. These implicit attention maps (localization heat maps) are used to\nderive actions (which square each player should move). The attention maps  \n\n(1) attend to squares in the tic-tac-toe board rather than arbitrary\nblobs, despite the fact that one square in a board has uniform color, and\n\n(2) they can be used to pick correct (winning) actions.\n\nThis experiment are used to support assertions that the network understands\n(1) chess (tic-tac-toe) boards\n(2) a rule for winning tic-tac-toe\n(3) that there are two players.\n\nSome follow up experiments indicate similar results under various renderings\nof the tic-tac-toe boards and an incomplete training regime.\n\n\nMore Clarifying Questions\n===\n\n* I am not quite sure precisely how CAM is implemented here. In the original CAM\none must identify a class of interest to visualize (e.g., cat or dog). I don't\nthink this paper identifies such a choice. How is one of the 18 possible classes\nchosen for creating the CAM visualization and through that visualization\nchoosing an action?\n\n* How was the test set for this dataset for the table 1 results created?\nHow many of the final 1029 states were used for test and was the\ndistribution of labels the same in train and test?\n\n* How is RCO computed? Is rank correlation or Pearson correlation used?\nIf Pearson correlation is used then it may be good to consider rank correlation,\nas argued in \"Human Attention in Visual Question Answering: Do Humans and\nDeep Networks Look at the Same Regions?\" by Das et. al. in EMNLP 2016.\nIn table 1, what does the 10^3 next to RCO mean?\n\n\nPros\n===\n\n* The proposed method, deriving an action to take from the result of a\nvisualization technique, is very novel.\n\n* This paper provides an experiment that clearly shows a CNN relying on context\nto make accurate predictions.\n\n* The use of a toy tic-tac-toe domain to study attention in CNNs\n(implicit or otherwise) is a potentially fruitful setting that may\nlead to better understanding of implicit and maybe explicit attention mechanisms.\n\n\nCons\n===\n\n* This work distinguishes between predictions about \"what will happen\"\n(will the white player win?) and \"what to do\" (where should the white\nplayer move to win?). The central idea is generalization from \"what will happen\"\nto \"what to do\" indicates concept learning (sec. 2.1). Why should an ability to\nact be any more indicative of a learned concept than an ability to predict\nfuture states. I see a further issue with the presentation of this approach and\na potential correctness problem:\n\n1. (correctness)\nIn the specific setting proposed I see no difference between \"what to do\"\nand \"what will happen.\"\n\nSuppose one created labels dictating \"what to do\" for each example in the\nproposed dataset. How would these differ from the labels of \"what will happen\"\nin the proposed dataset? In this case \"what will happen\" labels include\nboth player identity (who wins) and board position (which position they move\nto win). Wouldn't the \"what to do\" labels need to indicate board position?\nThey could also chosen to indicate player identity, which would make them\nidentical to the \"what will happen\" labels (both 18-way softmaxes).\n\n2. (presentation)\nI think this distinction would usually be handled by the Reinforcement Learning\nframework, but the proposed method is not presented in that framework or\nrelated to an RL based approach. In RL \"what will happen\" is the reward an\nagent will receive for making a particular action and \"what to do\" is the\naction an agent should take. From this point of view, generalization from\n\"what will happen\" to \"what to do\" is not a novel thing to study.\n\nAlternate models include:\n    * A deep Q network (Mnih. et. al. 2015) could predict the value of\n      every possible action where an action is a (player, board position) tuple.\n    * The argmax of the current model's softmax could be used as an action\n      prediction.\nThe deep Q network approach need not be implemented, but differences between\nmethods should be explained because of the uniqueness of the proposed approach.\n\n\n* Comparison to work that uses visualization to investigate deep RL networks\nis missing. In particular, other work in RL has used Simonyan et. al.\n(arXiv 2013) style saliency maps to investigate network behavior. For example, \n\"Dueling Network Architectures for Deep Reinforcement Learning\" by Wang et. al.\nin (ICML 2016) uses saliency maps to identify differences between their\nstate-value and advantage networks. In \"Graying the black box:\nUnderstanding DQNs\" by Zahavy et. al. (ICML 2016) these saliency maps are\nalso used to analyze network behavior.\n\n\n* In section 2.3, saliency maps of Simonyan et. al. are said to not be able to\nactivate on grid squares because they have constant intensity, yet no empirical\nor theoretical evidence is provided for this claim.\n\nOn a related note, what precisely is the notion of information referenced in\nsection 2.3 and why is it relevant? Is it entropy of the distribution of pixel\nintensities in a patch? To me it seems that any measure which depends only\non one patch is irrelevant because the methods discussed (e.g., saliency maps)\ndepend on context as well as the intensities within a patch.\n\n\n* The presentation in the paper would be improved if the results in section 7\nwere presented along with relevant discussion in preceding sections.\n\n\nOverall Evaluation\n===\nThe experiments presented here are novel, but I am not sure they are very\nsignificant or offer clear conclusions. The methods and goals are not presented\nclearly and lack the broader relevant context mentioned above. Furthermore, I\nfind the lines of thought mentioned in the Cons section possibly incorrect\nor incomplete. As detailed with further clarifying questions, upon closer\ninspection I do not see how some aspects of the proposed approach were\nimplemented, so my opinion may change with further details.","label":0,"model":"human","source":"peerread","id":4812}
{"text":"Game of tic-tac-toe is considered. 1029 tic-tac-toe board combinations are chosen so that a single move will result into victory of either the black or the white player. There are 18 possible moves - 2 players x 9 locations. A CNN is trained from a visual rendering of the game board to these 18 possible outputs. CAM technique is used to visualize the salient regions in the inputs responsible for the prediction that CNN makes. Authors find that predictions correspond to the winning board locations. \n\nAuthors claim that this:\n1. is a very interesting finding. \n2. CNN has figured out game rules. \n3. Cross modal supervision is applicable to higher-level semantics. \n\nI don't think (2) be can be claimed because the knowledge of game rules is not tested by any experiment. There is only \"one\" stage of a game - i.e. last move that is considered. Further, the results are on the training set itself - the bare minimum requirement of any implicit or explicit representation of game rules is the ability to act in previously unseen states (i.e. generalization). Even if the CNN did generalize, I would avoid making any claims about knowledge of game rules. \n\nFor (3), author's definition of cross-modal seems to be training from images to games moves. In image-classification we go from images --> labels (i.e. between two different domains). We already know CNNs can perform such mappings. CNNs have been used to map images to actions such as in DQN my Mnih et al., or DDPG by Lillicrap et al. and a lot of other classical work such as ALVIN. It's unclear what points authors are trying to make. \n\nFor (1): how interesting is an implicit attention mechanism is a subjective matter. The authors claim a difference between the concepts of \"what do do\" and \"what will happen\". They claim by supervising for \"what will happen\", the CNN can automatically learn about \"what to do\". This is extensively studied in the model predictive control literature. Where model is \"what will happen next\", and the model is used to infer a control law - \"what to do\". However, in the experimental setup presented in the paper what will happen and what to do seem to be the exact same things. \n\nFor further analysis of what the CNN has learnt I would recommend:\n(a) Visualizing CAM with respect to incorrect classes. For eg, visualize the CAM with respect to player would lose (instead of winning).\n\n(b) Split the data into train\/val and use the predictions on the val-set for visualization. These would be much more informative about what kind of \"generalizable\" features the CNN pays attention to. \n\nIn summary, understanding why CNN's make what decisions they make is a very interesting area of research. While the emergence of an implicit attention mechanism may be considered to be an interesting finding by some, many claims made by the authors are not supported by experiments (see comments above). \n\n\n \n\n\n\n","label":0,"model":"human","source":"peerread","id":4813}
{"text":"1029 tic-tac-toe boards are rendered (in various ways). These 1029 boards are legal boards where the next legal play can end the game. There are 18 categories of such boards -- 9 for the different locations of the next play, and 2 for the color of the next play. The supervision is basically saying \"If you place a black square in the middle right, black will win\" or \"if you place a white square in the upper left, white will win\". A CNN is trained to predict these 18 categories and can do so with 100% accuracy.\n\nThe focus of the paper is using Zhou et al's Class Activation Mapping to show where the CNN focuses when making it's decision. As I understand it, an input to CAM is the class of interest. So let's say it is class 1 (black wins with a play to the bottom right square, if I've deciphered figure 2 correctly. Figure 2 should really be more clear about what each class is). So we ask CAM to determine the area of focus of the CNN for deciding whether class 1 is exhibited. The focus ends up being on the empty bottom right square (because certainly you can't exhibit class 1 if the bottom right square is occupied). The CNN also needs to condition its decision on other parts of the board -- it needs to know whether there will be 3 in a row from some direction. But maybe that conditioning is weaker?\n\nThat's kind of interesting but I'm not sure about the deeper statements about discovering game rules that the paper hints at. I'm also not sure about the connection of this work to weakly supervised learning or multi-modal learning.\n\nThe paper is pretty well written, overall, with some grammatical mistakes, but I simply don't see the surprising discovery of this work. \n\nI also have some concerns about how contrived this scenario is -- using a big, expressive CNN for such a simple game domain and using a particular CNN visualization method.\n\nI am not an expert in reinforcement learning (which isn't happening in this paper, but is in related works on CNN game playing), so maybe I'm not appreciating the paper appropriately.","label":0,"model":"human","source":"peerread","id":4814}
{"text":"SUMMARY \nThis paper studies the expressive power of deep neural networks under various related measures of expressivity. \nIt discusses how these measures relate to the `trajectory length', which is shown to depend exponentially on the depth of the network, in expectation (at least experimentally, at an intuitive level, or theoretically under certain assumptions). \nThe paper also emphasises the importance of the weights in the earlier layers of the network, as these have a larger influence on the represented classes of functions, and demonstrates this in an experimental setting. \n\nPROS \nThe paper further advances on topics related to the expressive power of feedforward neural networks with piecewise linear activation functions, in particular elaborating on the relations between various points of view. \n\nCONS \nThe paper further advances and elaborates on interesting topics, but to my appraisal it does not contribute significantly new aspects to the discussion. \n\nCOMMENTS\n- The paper is a bit long (especially the appendix) and seems to have been written a bit in a rush. \nOverall the main points are presented clearly, but the results and conclusions could be clearer about the assumptions \/ experimental vs theoretical nature. \nThe connection to previous works could also be clearer. \n\n- On page 2 one finds the statement ``Furthermore, architectures are often compared via \u2018hardcoded\u2019 weight values -- a specific function that can be represented efficiently by one architecture is shown to only be inefficiently approximated by another.'' \n\nThis is partially true, but it neglects important parts of the discussion conducted in the cited papers. \nIn particular, the paper [Montufar, Pascanu, Cho, Bengio 2014] discusses not one hard coded function, but classes of functions with a given number of linear regions. \nThat paper shows that deep networks generically* produce functions with at least a given number of linear regions, while shallow networks never do. \n* Generically meaning that, after fixing the number of parameters, any function represented by the network, for parameter values form an open, positive -measure, neighbourhood, belongs to the class of functions which have at least a certain number of linear regions. \nIn particular, such statements can be directly interpreted in terms of networks with random weights. \n\n- One of the measures for expressivity discussed in the present paper is the number of Dichotomies. In statistical learning theory, this notion is used to define the VC-dimension. In that context, a high value is associated with a high statistical complexity, meaning that picking a good hypothesis requires more data. \n\n- On page 2 one finds the statement ``We discover and prove the underlying reason for this \u2013 all three measures are directly proportional to a fourth quantity, trajectory length.'' \nThe expected trajectory length increasing exponentially with depth can be interpreted as the increase (or decrease) in the scale by a composition of the form a*...*a x, which scales the inputs by a^d. Such a scaling by itself certainly is not an underlying cause for an increase in the number of dichotomies or activation patterns or transitions. Here it seems that at least the assumptions on the considered types of trajectories also play an important role. \nThis is probably related to another observation from page 4: ``if the variance of the bias is comparatively too large... then we no longer see exponential growth.''\n\nOTHER SPECIFIC COMMENTS \nIn Theorem 1 \n- Here it would be good to be more specific about ``random neural network'', i.e., fixed connectivity structure with random weights, and also about the kind of one-dimensional trajectory, i.e., finite in length, closed, differentiable almost everywhere, etc. \n\n- The notation ``g \\geq O(f)'' used in the theorem reads literally as |g| \\geq \\leq k |f| for some k>0, for large enough arguments. It could also be read as g being not smaller than some function that is bounded above by f, which holds for instance whenever g\\geq 0. \nFor expressing asymptotic lower bounds one can use the notation \\Omega (see","label":0,"model":"human","source":"peerread","id":4815}
{"text":"While the reviewers saw some value in your contribution, there were also serious issues, so the paper does not reach the acceptance threshold.","label":0,"model":"human","source":"peerread","id":4816}
{"text":"This paper presents a theoretical and empirical approach to the problem of understanding the expressivity of deep networks.\n\nRandom networks (deep networks with random Gaussian weights, hard tanh or ReLU activation) are studied according to several criterions: number of neutron transitions, activation patterns, dichotomies and trajectory length.\n\nThere doesn't seem to be a solid justification for why the newly introduced measures of expressivity really measure expressivity.\nFor instance the trajectory length seems a very discutable measure of expressivity. The only justification given for why it should be a good measure of expressivity is proportionality with other measures of expressivity in the specific case of random networks.\n\nThe paper is too obscure and too long. The work may have some interesting ideas but it does not seem to be properly replaced in context.\n\nSome findings seem trivial.\n\ndetailed comments\n\np2 \n\n\"Much of the work examining achievable functions relies on unrealistic architectural assumptions such as layers being exponentially wide\"\n\nI don\u2019t think so. In \"Deep Belief Networks are Compact Universal Approximators\" by Leroux et al., proof is given that deep but narrow feed-forward neural networks with sigmoidal units can represent any Boolean expression i.e. A neural network with 2n\u22121 + 1 layers of n units (with n the number of input neutron).\n\n\u201cComparing architectures in such a fashion limits the generality of the conclusions\u201d\n\nTo my knowledge much of the previous work has focused on mathematical proof, and has led to very general conclusions on the representative power of deep networks (one example being Leroux et al again).\n\nIt is much harder to generalise the approach you propose, based on random networks which are not used in practice.\n\n\u201c[we study] a family of networks arising in practice: the behaviour of networks after random initialisation\u201d\n\nThese networks arise in practice as an intermediate step that is not used to perform computations; this means that the representative power of such intermediate networks is a priori irrelevant. You would need to justify why it is not.\n\n\u201cresults on random networks provide natural baselines to compare trained networks with\u201d\n\nrandom networks are not \u201cnatural\u201d for the study of expressivity of deep networks. It is not clear how the representative power of random networks (what kind of random networks seems an important question here) is linked to the representative power of (i) of the whole class of networks or (ii) the class of networks after training. Those two classes of networks are the ones we would a priori care about and you would need to justify why the study of random networks helps in understanding either (i) or (ii).\n\np5\n\n\u201cAs FW is a random neural network [\u2026] it would suggest that points far enough away from each other would have independent signs, i.e. a direct proportionality between the length of z(n)(t) and the number of times it crosses the decision boundary.\u201d\n\nAs you say, it seems that proportionality of the two measures depends on the network being random. This seems to invalidate generalisation to other networks, i.e. if the networks are not random, one would assume that path lengths are not proportional.\n\np6\n\nthe expressivity w.r.t. remaining depth seems a trivial concerns, completely equivalent to the expressivity w.r.t. depth. This makes the remark in figure 5 that the number of achievable dichotomies only depends *only* on the number of layers above the layer swept seem trivial\n\np7\n\nin figure 6 a network width of 100 for MNIST seems much too small. Accordingly performance is very poor and it is difficult to generalise the results to relevant situations.\n","label":0,"model":"human","source":"peerread","id":4817}
{"text":"SUMMARY \nThis paper studies the expressive power of deep neural networks under various related measures of expressivity. \nIt discusses how these measures relate to the `trajectory length', which is shown to depend exponentially on the depth of the network, in expectation (at least experimentally, at an intuitive level, or theoretically under certain assumptions). \nThe paper also emphasises the importance of the weights in the earlier layers of the network, as these have a larger influence on the represented classes of functions, and demonstrates this in an experimental setting. \n\nPROS \nThe paper further advances on topics related to the expressive power of feedforward neural networks with piecewise linear activation functions, in particular elaborating on the relations between various points of view. \n\nCONS \nThe paper further advances and elaborates on interesting topics, but to my appraisal it does not contribute significantly new aspects to the discussion. \n\nCOMMENTS\n- The paper is a bit long (especially the appendix) and seems to have been written a bit in a rush. \nOverall the main points are presented clearly, but the results and conclusions could be clearer about the assumptions \/ experimental vs theoretical nature. \nThe connection to previous works could also be clearer. \n\n- On page 2 one finds the statement ``Furthermore, architectures are often compared via \u2018hardcoded\u2019 weight values -- a specific function that can be represented efficiently by one architecture is shown to only be inefficiently approximated by another.'' \n\nThis is partially true, but it neglects important parts of the discussion conducted in the cited papers. \nIn particular, the paper [Montufar, Pascanu, Cho, Bengio 2014] discusses not one hard coded function, but classes of functions with a given number of linear regions. \nThat paper shows that deep networks generically* produce functions with at least a given number of linear regions, while shallow networks never do. \n* Generically meaning that, after fixing the number of parameters, any function represented by the network, for parameter values form an open, positive -measure, neighbourhood, belongs to the class of functions which have at least a certain number of linear regions. \nIn particular, such statements can be directly interpreted in terms of networks with random weights. \n\n- One of the measures for expressivity discussed in the present paper is the number of Dichotomies. In statistical learning theory, this notion is used to define the VC-dimension. In that context, a high value is associated with a high statistical complexity, meaning that picking a good hypothesis requires more data. \n\n- On page 2 one finds the statement ``We discover and prove the underlying reason for this \u2013 all three measures are directly proportional to a fourth quantity, trajectory length.'' \nThe expected trajectory length increasing exponentially with depth can be interpreted as the increase (or decrease) in the scale by a composition of the form a*...*a x, which scales the inputs by a^d. Such a scaling by itself certainly is not an underlying cause for an increase in the number of dichotomies or activation patterns or transitions. Here it seems that at least the assumptions on the considered types of trajectories also play an important role. \nThis is probably related to another observation from page 4: ``if the variance of the bias is comparatively too large... then we no longer see exponential growth.''\n\nOTHER SPECIFIC COMMENTS \nIn Theorem 1 \n- Here it would be good to be more specific about ``random neural network'', i.e., fixed connectivity structure with random weights, and also about the kind of one-dimensional trajectory, i.e., finite in length, closed, differentiable almost everywhere, etc. \n\n- The notation ``g \\geq O(f)'' used in the theorem reads literally as |g| \\geq \\leq k |f| for some k>0, for large enough arguments. It could also be read as g being not smaller than some function that is bounded above by f, which holds for instance whenever g\\geq 0. \nFor expressing asymptotic lower bounds one can use the notation \\Omega (see ","label":0,"model":"human","source":"peerread","id":4818}
{"text":"Thank you for the review! We will take the comments into account and endeavour to make the text even clearer.\n\nA quick comment about motivation: our goal in this work improve interpretability in deep neural networks through a better understanding of neural network expressivity. In particular, we look at different \"diagnostics\" (transitions\/activation patterns\/dichotomies) for measuring the expressiveness of different neural network architectures, and their practical consequences. The surprising fact that three natural measures of expressiveness are related by *direct* proportion (see below) to trajectory length suggests consequences on remaining depth (earlier parameters are more important to fit the final function) and a trade off between expressivity and stability during training due to initialization choices.  \n\nResponses inline to other specific comments below:\n\nTrajectory Length: We will add this as a definition before Theorem 1. We take a 1-d trajectory to be a 1-d curve in the high dimensional space, and we measure the length  -- ","label":0,"model":"human","source":"peerread","id":4819}
{"text":"Summary of the paper:\n\nAuthors study in this paper quantities related to the expressivity of neural networks.The analysis is done for a random network. authors define the \u2018trajectory length\u2019 of a one dimensional trajectory as the length of the trajectory as the points (in a m- dimensional space) are embedded by layers of the network. They provide growth factors as function of hidden units k, and number of layers d.  the growth factor is exponential in the number of layers. Authors relates this trajectory length to authors quantities : \u2018transitions\u2019,\u2019activation patterns \u2019 and \u2018Dichotomies\u2019. \nAs a consequence of this study authors suggest that training only  earlier layers in the network  leads higher accuracy then just training later layers. Experiments are presented on MNIST and CIFAR10.\n\nClarity:\n\nThe  paper is a little hard to follow, since  the motivations are not clear in the introduction and the definitions across the paper are not clear. \n\nNovelty:\n\nStudying the trajectory length as function of transforming the data by a multilayer network is   new and interesting idea. The relation to transition numbers is in term of the growth factor, and not as a quantity to quantity relationship. Hence it is hard to understand what are the implications.\n\nSignificance:\n\nThe geometry of the input set (of dimension m)  shows up only weakly in the activation patterns analysis.  The trajectory study should tell us how the network organizes the input set. As observed in the experiments the network becomes contractive\/selective as we train the network. It would be interesting to study those phenomenas using this trajectory length , as a measure for disentangling nuisance factors ( such as invariances etc.). In the supervised setting the network need not to be contractive every where , so it needs to be selective to the class label, a  theoretical study of the selectivity and contraction using the trajectory length would be more appealing.\n\nDetailed comments:\n\nTheorem 1:\n\n- As raised by reviewer one the definition of a one dimensional input trajectory is missing. \n- What does theorem 1 tells us about the design and the architecture to use in neural networks as promised in the introduction is not clear. The connection to transitions in Theorem 2 is rather weak. \n\nTheorem 2:\n\n- in the proof of theorem 2 it not clear what is meant by T and t. Notations are confusing, the expectation is taken with respect to which weight: is it W_{d+1} or (W_{d+1} and W_{d})? I understand you don't want to overload notation but maybe E_{d+1} can help keeping track. I don't see how the recursion is applied if T and t in it, have different definitions. seems T_{d+1} for you is a random variable and t_{d} is fixed. Are you fixing W_d and then looking at W_{d+1} as  random?\n\n- In the same proof:  the recursion  is for d>1  ? your analysis is for W \\in R^{k\\times k}, you don't not study the W \\in \\mathbb{R}^{k\\times m}. In this case you can not assume assume that |z^(0)|=1.\n\n- should d=1, be analyzed alone to know how it scales with m?\n\nTheorem 4 in main text:\n\n- Is the proof missing? or Theorem 4 in the main text is Theorem 6 in the appendix?\n\nFigures 8 and 9:\n\n- the trajectory length reduction in the training isn't that just the network becoming contractive to enable mapping the training points to the labels? See for instance  on contraction in deep networks ","label":0,"model":"human","source":"peerread","id":4820}
{"text":"Dear Authors,\n\nPlease resubmit your paper in the ICLR 2017 format with the correct font type for your submission to be considered. Thank you!","label":0,"model":"human","source":"peerread","id":4821}
{"text":"The idea of universality that is independent of input distribution and dimension, depending only on the algorithm is an appealing one. However, as an empirical study, this paper comes up somewhat short:\n\n1. Exactly one algorithm is shown for the deep learning example. It would have been more convincing to compare distributions with one or more algorithms. \n\n2. The definition (1),  and much of the work of Section 2.1 seems to have already been covered in Deift (2014), Section 1.3. In that paper, a number of different algorithms for the solution of linear systems are considered, and then the concept of universality becomes more plausible. I do not see enough of such algorithmic comparisons in this paper (same problem setup, different algorithms).\n\n3.  It seems to me that what practitioners might care about in practice are both the mean and variance in running times; these quantities are buried in (1). So I question how useful the distribution itself might be for algorithm tuning. \n\nAt the least, many more empirical comparisons should be provided to convince me that the universality holds across a broad range of algorithms.","label":0,"model":"human","source":"peerread","id":4822}
{"text":"My overall conclusion is that the paper needs more work to be sufficiently convincing. I think the reviews are sufficiently careful. My recommendation is based on the fact that none of the reviewers supports acceptance.","label":0,"model":"human","source":"peerread","id":4823}
{"text":"The authors explore whether the halting time distributions for various algorithms in various settings exhibit \"universality\", i.e. after rescaling to zero mean and unit variance, the distribution does not depend on stopping parameter, dimensionality and ensemble.\n\nThe idea of the described universality is very interesting. However I see several shortcomings in the paper:\n\nIn order to be of practical relevance, the actual stopping time might be more relevant than the scaled one. The discussion of exponential tailed halting time distributions is a good start, but I am not sure how often this might be actually helpful. Still, the findings in the paper might be interesting from a theoretical point of view.\n\nEspecially for ICLR, I think it would have been more interesting to look into comparisons between stochastic gradient descent, momentum, ADAM etc on different deep learning architectures. Over which of those parameters does universality hold?. How can different initializations influence the halting time distribution? I would expect a sensible initialization to cut of part of the right tail of the distribution.\n\nAdditionally, I found the paper quite hard to read. Here are some clarity issues:\n\n- abstract: \"even when the input is changed drastically\": From the abstract I'm not sure what \"input\" refers to, here\n- I. Introduction: \"where the stopping condition is, essentially, the time to find the minimum\": this doesn't seem to make sense, a condition is not a time. I guess the authors wanted to say that the stopping condition is that the minimum has been reached?\n- I.1 the notions of dimension N, epsilon and ensemble E are introduced without any clarification what they are. From the later parts of the paper I got some ideas and examples, but here it is very hard to understand what these parameters should be (just some examples would be already helpful)\n- I.3 \"We use x^\\ell for \\ell \\in Z=\\{1, \\dots, S\\} where Z is a random sample from of training samples\" This formulation doesn't make sense. Either Z is a random sample, or Z={1, ..., S}.\n- II.1 it took me a long time to find the meaning of M. As this parameter seems to be crucial for universality in this case, it would be very helpful to point out more explicitly what it refers to.\n","label":0,"model":"human","source":"peerread","id":4824}
{"text":"Summary\n\n\nFor several algorithms, previous research has shown that the halting time follows a two-parameter distribution (the so-called universal property investigated by the authors). In this work, the authors extend the investigation to new algorithms (spin-glass, gradient descent in deep learning).\n\nAn algorithm is considered to satisfy the universality property when the centered\/scaled halting time fluctuations (empirical distribution of halting times) depend on the algorithm but do not depend on the target accuracy epsilon, an intrinsic measure of dimension N, the probability distribution\/random ensemble. (This is clear from Eq 1 where on the left the empirical halting time distribution depends on epsilon, N, A, E and on the right, the approximation only depends on the algorithm)\n\nThe authors argue that empirically, the universal property is observed when both algorithms (spin glass and deep learning) perform well and that it is not observed when they do not perform well.\n\nA moment-based indicator is introduced to assess whether universality is observed.\n\n\nReview\n\n\nThis paper presents several problems.\n\n\n\npage 2: \u201c[\u2026] for sufficiently large N and eps = eps(N)\u201d\n\nThe dependence of epsilon on N is troubling.\n\n\n\npage 3: \u201cUniversality is a measure of stability in an algorithm [\u2026] For example [\u2026] halting time for the power method [\u2026] has infinite expectation and hence this type of universality is *not* present. One could use this to conclude that the power method is naive. Therefore the presence of universality is a desirable feature of a numerical method\u201d\n\nNo. An algorithm is naive if there are better ways to answer the problem. One could not conclude from a halting time with infinite expectation (e.g. solving a problem extremely quickly 99% of the time, and looping forever in 1% of cases) or infinite variance, that the algorithm is naive.\u2028\nMoreover the universal property is more restrictive than having a finite halting time expectation. Even if in many specific cases, having a finite halting time expectation is a desirable property, showing that the presence of universality is desirable would require a demonstration that the other more restrictive aspects are also desirable.\u2028\nAlso, the paragraph only concerns one algorithm. why would the conclusions generalise to all numerical methods ?\u2028\nEven if the universality property is arguably desirable (i.e. event if the conclusion of this paragraph is assumed correct), the paragraph does not support the given conclusion.\n\n\n\nComparing Eq 1 and figures 2,3,4,5\u2028\nFrom Eq 1, universality means that the centered\/scaled halting time fluctuations (which depend on A, epsilon, N, E) can be approximated by a distribution that only depends on A (not on epsilon, N, E) but in the experiments only E varies (figures 2,3,4,5). The validity of the approximation with varying epsilon or N is never tested\n\n\n\nThe ensembles\/distributions parameter E (on which halting fluctuations should not depend) and the algorithm A (on which halting fluctuations are allowed to depend) are not well defined, especially w.r.t. the common use of the words. In the optimisation setting we are told that the functional form of the landscape function is part of A (in answer to the question of a reviewer) but what is part of the functional form ? what about computations where the landscape has no known functional form (black box) ?\n\n\n\nThe conclusion claims that the paper \u201cattempts to exhibit cases\u201d where one can answer 5 questions in a robust and quantitative way.\n\nQuestion 1: \u201cWhat are the conditions on the ensembles and the model that lead to such universality ?\u201d\u2028The only quantitative way would be to use the moments based indicator however there is only one example of universality not being observed which concerns only one algorithm (conjugate gradient) and one type of failure (when M = N). This does not demonstrate robustness of the method.\n\nQuestion 2: \u201cWhat constitutes a good set of hyper parameters for a given algorithm ?\u201d\nThe proposed way to choose would be to test whether universality is observed. If it is then the hyper parameters are good, if not the hyper parameters are bad. The correspondance between bad hyper-parameters and observing no universality concerns only one algorithm and one type of failure. Other algorithms may fail in the universal regime or perform well in the non universal regime. The paper does not show how to answer this question in a robust way.\n\nQuestion 3: \"How can we go beyond inspection when tuning a system ?\u2028\"\nThe question is too vague and general and there is probably no robust and quantitative way to answer it at all.\n\nQuestion 4: \"How can we infer if an algorithm is a good match to the system at hand ?\u2028\"\nThe paper fails to demonstrate convincingly that universality is either a good or robust way to approach the very few studied algorithms. The suggested generalisation to all systems and algorithms is extremely far fetched.\n\nQuestion 5: \"What is the connection between the universal regime and the structure of the landscape ?\"\n\u2028Same as before, the question is extremely vague and cannot be answered in a robust or quantitative way at all. The fact that what corresponds to A and what corresponds to E is not clear does not help.\n\n\nIn the conclusion it is written that the paper validates the claim that universality is present in all or nearly all sensible computation. It does not. The paper does not properly test whether universality is present (only 1 parameter in 3 that should not vary is tested). The paper does not properly test whether universality is lost when the computation is no longer sensible (only one failure case tested). Finally the experiments do not apply to all or nearly all computations but only to very few  specific algorithms.\n","label":0,"model":"human","source":"peerread","id":4825}
{"text":"It is an interesting paper. However, I would like to describe a hypothesis\/counter-example and I hope you will\neasily reject it. Otherwise, I will fail to see any universality in the discussed \"universality in halting time\".\n\nI would like to present a toy minimization problem where f(x) is defined in [0,1] and has two global\noptima in 0.0 and 1.0 respectively. f(x) has its maximum in 0.5 but f(x) decays somewhat differently to the \"left\"\nand to the \"right\" optima. These somewhat different decays (shapes of f(x)) impact the way some optimizer A minimizes\nf(x). My A can be e.g. a deterministic local search algorithm such that if it is initialized in the basin of attraction [0,0.5], it will stay\/search there. Thus, the starting position will determine in which global optima it will end up. To observe the halting distribution, I run A starting from different starting positions generated uniformly at random in [0,1]. I expect the halting distribution to have two peaks: the first peak corresponds to a typical number of iterations required to reach some epsilon around the \"left\" optima, the second peak would correspond to the \"right\" optima. Question: Why there are two peaks and not one? Answer: Because, as mentioned before, the shapes\/decays of f(x) on the left and right side are different and this affects our algorithm A, i.e., the number of iterations to reach some epsilon precision. Question: Is this a problem to have two peaks in the halting distribution not like the ones shown in figures? Answer: No, it is perfectly fine. What is not fine is that when I run the same algorithm A on another problem which is unimodal, then I will get a completely different halting distribution, e.g., like the \"one peak\" ones shown in the paper. Question: Where is the problem? Answer: The problem is that these observations contradict the discussed universality in halting time distribution which is an intrinsic thing to A and does not depend on the problem. Question: This was stated for large dimension N and not one-dimensional problem. Answer: Increase the dimensionality of the toy problem preserving the idea of the two (multiple) optima with, e.g., basins of attraction of equal size (keep in mind the  course of dimensionality). Question: There is a restriction on the class of ensembles E. Answer: If we restrict our-self to unimodal problems then I barely see \"universality\". Moreover, unimodal non-convex problems may exhibit the same properties as multi-modal ones, e.g., make a horizontal one-dimensional slice of 2-dimensional Rosenbrock function. Question: Why we see these nice-looking distribution for very different problems? Answer: because for some problems the multi-modality is negligeable and the dynamical system behind the optimizer may behave similarly for different values of N, especially for the large ones. Nevertheless, to claim it to be the \"universal law\" is misleading since it depends on the properties of the problem at hand and is not strictly intrinsic to the optimizer. Moreover, for the same (class of) problem the landscape may qualitatively change with increasing N. Rosenbrock function is a good example, it is uni-modal for 2 variables and multi-modal for multiple variables which would affect the halting time distribution. The \"threshold\" dimension value 2 can be turned to 2 billion if needed.","label":0,"model":"human","source":"peerread","id":4826}
{"text":"The paper presents a layer architecture where a single parameter is used to  gate the output response of layer to amplify or suppress it. It is shown that such an architecture can ease optimization of a deep network as it is easy to learn identity mappings in layers helping in better gradient propagation to lower layers (better supervision). \n\nUsing an introduced SDI metric it shown that gated residual networks can most easily learn identity mappings compared to other architectures. \n\nAlthough good theoretical reasoning is presented the observed experimental evidence of learned k values does not seem to strongly support the theory given that learned  k values are mostly very small and not varying much across layers. Also, experimental validation of the approach is not quite strong in terms of reported performances and number of large scale experiments.","label":0,"model":"human","source":"peerread","id":4827}
{"text":"Although this was a borderline paper, the reviewers ultimately concluded that, given how easy it would be for a practitioner to independently devise the methodological trick of the paper, the paper did not demonstrate that the idea was sufficiently useful to merit acceptance.","label":0,"model":"human","source":"peerread","id":4828}
{"text":"We have also added a section (3.3) that connects our technique to the Unrolled Iterative Estimation interpretation of Highway and ResNets. We further elaborate on how the learned k values are indicators of the abstraction jumps in representations defined in Greff et al, and analyze the k values for both experiments (MNIST and CIFAR) under this perspective.","label":0,"model":"human","source":"peerread","id":4829}
{"text":"In the last revision of our work, we have performed the following main changes:\n\n- Added 4 extra experiments comparing Wide ResNet and their augmented counterparts (Table 5) on CIFAR-100. With this we hope to provide stronger indications of the generality and advantages of our technique.\n\nWe are currently running Gated PlainNets on CIFAR-100.\n\nWe thank all the received feedback.","label":0,"model":"human","source":"peerread","id":4830}
{"text":"In the last revision of our work, we have performed the following main changes:\n\n- Added 6 extra experiments comparing Wide ResNet and their augmented counterparts (Table 4). With this we hope to provide stronger indications of the generality and advantages of our technique.\n\n\nWe are currently running the same 6 models on CIFAR-100 to add them to the table. Are have also run Gated PlainNets (u = g(k)f(x) + (1 - g(k))x) on CIFAR-10, as suggested by reviewer 3, and will add results after running on CIFAR-100 as well.\n\n\nWe thank all the received feedback.","label":0,"model":"human","source":"peerread","id":4831}
{"text":"In the last revision of our work, we have performed the following main changes:\n\n- Added results with u = g(k)f(x) + (1 - g(k))x (Gated Plain Networks), along with some intuitions of why it outperformed non-augmented Residual Networks. This result suggests that an extensive study on different gating mechanisms for Highway Neural Networks can be extremely fruitful, once the original design is equivalent to a Highway Net with scalar gates. This also goes against the suggestions in the literature not to add gates to shortcut connections in order to keep an uncorrupted gradient flow through the network.\n\n- Added Gated Plain Networks to the table with mean k values, along with an explanation for the significant difference when compared to mean k's of Gated ResNets.\n\n- Added 'Understanding deep learning requires rethinking generalization' to bibliography.\n\n- Rephrased a few parts when augmented models are compared to Highway Nets, showing more clearly the differences between the two designs. Also added a brief discussion regarding the impact for Highway Nets of the new results (Gated Plain Nets).\n\n\nWe are currently gathering results to introduce the following changes to the next revision:\n\n- Add results for more aggressive depths (200+ layers), in order to better compare different models.\n\n- Add results for Gated Plain Net on CIFAR.\n\nWe thank all the received feedback.","label":0,"model":"human","source":"peerread","id":4832}
{"text":"\nThis paper proposes a network called Gated Residual Networks layer design that adds gating to shortcut connections with a scalar to regulate the gate. The authors claim that this approach will improve the training Residual Networks.\n\nIt seems the authors could get competitive performance on CIFAR-10 to state of art models with only Wide Res Nets. Wide Gated ResNet requires much more parameters than DenseNet (and other Res Net variants) for obtaining a little improvement over Dense Net.  More importantly, the authors state that they obtained the best results on CIFAR-10 and CIFAR-100 but the updated version of DenseNet (Huang et al. (2016b)) has new results for a version called DenseNet-BC which outperforms all of the results that authors reported (3.46 for CIFAR-10 and 17.18 for CIFAR-100 with 25.6M parameters, DenseNet-BC still outperforms with 15.3M parameters which is much less that 36.5M). The Res Net variants papers with state of art results report result for Image Net. Therefore the empirical results need also the Image Net to demonstrate that improvement claimed is achieved.\n\nThe proposed trick adopts Highway Neural Networks and Residual Networks with an intuitive motivation. It is not sufficiently novel and the empirical results do not prove sufficient effectiveness of this incremental approach.\n\n","label":0,"model":"human","source":"peerread","id":4833}
{"text":"This paper proposes to learn a single scalar gating parameter instead of a full gating tensor in highway networks. The claim is that such gating is easier to learn and allows a network to flexibly utilize computation.\n\nThe basic idea of the paper is simple and is clearly presented. It is a natural simplification of highway networks to allow easily \"shutting off\" layers while keeping number of additional parameters low. However, in this regard the paper leaves out a few key points. Firstly, it does not mention that the gates in highway networks are data-dependent which is potentially more powerful than learning a fixed gate for all units and independent of data. Secondly, it does not do a fair comparison with highway networks to show that this simpler formulation is indeed easier to learn.\n\nDid the authors try their original design of u = g(k)f(x) + (1 - g(k))x where f(x) is a plain layer instead of a residual layer? Based on the arguments made in the paper, this should work fine. Why wasn't it tested? If it doesn't work, are the arguments incorrect or incomplete?\n\nFor the MNIST experiments, since the hyperparameters are fixed, the plots are misleading if any dependence on hyperparameters exists for the different models. This experiment appears to be based on Srivastava et al (2015). If it is indeed designed to test optimization at aggressive depths, then apart from doing a hyperparameter search, the authors should not use regularization such as dropout or batch norm, which do not appear in the theoretical arguments for the architecture.\n\nFor CIFAR experiments, the obtained improvements compared to the baseline (wide resnets) are very small and therefore it is important to report the standard deviations (or all results) in both cases. It's not clear that the differences are significant.\n\nSome questions regarding g(): Was g() always ReLU? Doesn't this have potential problems with g(k) becoming 0 and never recovering? Does this also mean that for the wide resnet in Fig 7, most residual blocks are zeroed out since k < 0?","label":0,"model":"human","source":"peerread","id":4834}
{"text":"In the recent revisions of our work, we have performed the following main changes:\n\n- Run complex models on CIFAR-10 and on CIFAR-100, achieving 3.65% and 18.27% test error, respectively. We have then removed our belief that the technique could achieve SOTA results, since now we have empirical indications of it.\n- Added evaluation of the final k parameters for fully-connected and convolutional networks, showing interesting patterns for both architectures.\n- Added layer pruning experiment to Wide GResNet.\n- Made the intuition behind the distance to identity clearer.\n\nWe thank all the received feedback.","label":0,"model":"human","source":"peerread","id":4835}
{"text":"\n- \"assume that the squared parameter-wise distance between .. surrogate to the paths length\". Can you elaborate more on this? Why should this be the case? \n\n- Why do you substitute mean and variance with initialization mean variance in derivation in Eq 15 and 16?\n\n-  Regarding \"A comparison of the Total Squared Distance to Identity\u2026\": where is the comparison?\n\n- Why there is no Highway Neural Networks result in Table 5? \n\n- In Table 5, why does  He et al. (2015b)  give 6.61 which does not match with 6.43 in that paper? \n\n- Regarding \"Our results don\u2019t surpass the state-of-the-art, which was expected considering the hardware limitations. However, taking into account the improvement observed when augmenting a smaller Wide ResNet, we believe that the technique proposed can be used to surpass the state-of-the-art\", you may use that result for sanity check during the empirical work but how can you claim that you will get any improvement over baselines in final comparison? \n\n- Baselines have Imagenet results. In order to obtain satisfactory comparison, you would need that too. Are there any results on Imagenet comparison?\n","label":0,"model":"human","source":"peerread","id":4836}
{"text":"I was holding off on this review hoping to get the missing details from the code at","label":0,"model":"human","source":"peerread","id":4837}
{"text":"The area chair agrees with the reviewers that this paper is not ready for ICLR yet. There are significant issues with the writing, making it difficult to follow the technical details. Writing aside, the technique seems somewhat limited in its applicability. The authors also promised an updated version, but this version was never delivered (latest version is from Nov 13).","label":0,"model":"human","source":"peerread","id":4838}
{"text":"Dear authors,\n\ndo you plan to address the third reviewer's comments? Your responses could help bring some more clarity and improve the confidence for the final decision...\n\nThanks!","label":0,"model":"human","source":"peerread","id":4839}
{"text":"I was holding off on this review hoping to get the missing details from the code at ","label":0,"model":"human","source":"peerread","id":4840}
{"text":"The basic idea of this contribution is very nice and worth pursuing: how to use the powerful \u201cdivide and conquer\u201d algorithm design strategy to learn better programs for tasks such as sorting or planar convex hull. However, the execution of this idea is not convincing and needs polishing before acceptance. As it is right now, the paper has a proof-of-concept feel that makes it great for a workshop contribution.\n\nMy main concern is that the method presented is currently not easily applicable to other tasks. Typically, demonstrations of program induction from input-output examples on well known tasks serves the purpose of proving, that a generic learning machine is able to solve some well known tasks, and will be useful on other tasks due to its generality. This contribution, however, presents a learning machine that is very hand-tailored to the two chosen tasks. The paper essentially demonstrates that with enough engineering (hardcoding the recurrency structure, designing problem-specific rules of supervision at lower recurrency levels) one can get a partially trainable sorter or convex hull solver.\n\nI found the contribution relatively hard to understand. High level ideas are mixed with low-level tricks required to get the model to work and it is not clear either how the models operate, nor how much of them was actually learned, and how much was designed. The answer to the questions did hep, nut didn't make it into the paper. Mixing the descriptions of the tricks required to solve the two tasks makes things even more confusing. I believe that the paper would be much more accessible if instead of promising a general solution it clearly stated the challenges faced by the authors and the possible solutions.\n\nHighlights:\n+ Proof-of-concept of a partially-trainable implementation of the important \u201cdivide and conquer\u201d paradigm\n++ Explicit reasoning about complexity of induced programs\n- The solution isn\u2019t generic enough to be applicable to unknown problems - the networks require tricks specific to each problem\n- The writing style pictures the method as very general, but falls back on very low level details specific to each task\n","label":0,"model":"human","source":"peerread","id":4841}
{"text":"I find this paper extremely hard to read. The main promise of the paper is to train models for combinatorial search procedures, especially for dynamic programming to learn where to split and merge. The present methodology is supposed to make use of some form of scale invariance property which is scarcely motivated for most problems this approach should be relevant for. However, the general research direction is fruitful and important.\n\nThe paper would be much more readable if it would start with a clear, formal problem formulation, followed by some schematic view on the overall flow and description on which parts are supervised, which parts are not. Also a tabular form and sample of the various kinds problems solved by this method could be listed in the beginning as a motivation with some clear description on how they fit the central paradigm and motivate the rest of the paper in a more concrete manner.\n\nInstead, the paper is quite chaotic, switching between low-level and high level details, problem formulations and their solutions in a somewhat random, hard to parse order.\n\nBoth split and merge phases seem to make a lot of discrete choices in a hierarchical manner during training. The paper does not explain how those discrete choices are backpropagated through the network in an unbiased manner, if that is the case at all.\n\nIn general, the direction this paper is exciting, but the paper itself is a frustrating read in its present form. I have spent several hours on it without having to manage to achieve a clear mental image on how all the presented pieces fit together. I would revise my score if the paper would be improved greatly from a readability perspective, but I think it would require a major rewrite.","label":0,"model":"human","source":"peerread","id":4842}
{"text":"This work proposes to use visualization of gradients to further understand the importance of features (i.e. pixels) for visual classification. Overall, this presented visualizations are interesting, however, the approach is very ad hoc. The authors do not explain why visualizing regular gradients isn't correlated with the importance of features relevant to the given visual category and proceed to the interior gradient approach. \n\nOne particular question with regular gradients at features that form the spatial support of the visual class. Is it the case that the gradients of the features that are confident of the prediction remain low, while those with high uncertainty will have strong gradients?\n\nWith regards to the interior gradients, it is unclear how the scaling parameter \\alpha affects the feature importance and how it is related to attention.\n\nFinally, does this model use batch normalization?","label":0,"model":"human","source":"peerread","id":4843}
{"text":"This paper was reviewed by 3 experts. All 3 seem unconvinced of the contributions, point to several shortcomings, and recommend rejection. I see no basis for overturning their recommendation. To be clear, the problem of achieving insight into the inner workings of deep networks is of significant importance and I encourage the authors to use the feedback to improve the manuscript.","label":0,"model":"human","source":"peerread","id":4844}
{"text":"We added two new sections (2.5 and 2.6) to the paper. Section 2.5\nproposes two very desirable axioms for attribution methods,\nand uses them to rule out other attribution methods from the literature.\nSection 2.6 proposes a full axiomatization under which our method is\nunique.\n\nIt is possible that sections 2.3-2.7 may constitute a shorter 6-page\nself-contained paper with the title --- \"attributions using interior\ngradients\".\n","label":0,"model":"human","source":"peerread","id":4845}
{"text":"We thank the reviewers for a detailed review.  The rebuttal below addresses some of the mentioned concerns.\n\nRegarding \u201cfar too long\u201d and \u201cunnecessarily grandiose name for literally, a scaled image\u201d: \n\nWe\u2019d agree that the paper is long for the ideas in it. The length stems from the difficulty of not having a crisp evaluation technique for feature importance. So we try to resort to qualitative discussions together with images. But we  can definitely try to tighten the writing. We are open to changing the title of the paper to \u201cInterior Gradients\u201d or something like it, though it is worth noting that while scaling intensities seems natural for images, analogous scaling for Text or Drug Discovery models results in inputs that are more obviously fake, i.e., counterfactual.\n\nRegarding \u201chow the proposed scheme for feature importance ranking is useful\u201d: \n\nWhile debugging deep networks is hard in general, examining feature importance scores offers a limited but useful insight into the operation of the network on a particular input. For us, the experience with the Drug Discovery network where we found, via our attributions, that the bond features were severely underused (see Section 3.1) was a concrete instance of how feature importance analysis could help debug and improve networks. As we discussed in section 2.7, we do mention the limitations of our technique in understanding what the network does. The same pros and cons would seem to apply to other feature importance techniques (see Section 2.8). The key difference is that ours is much easier to implement--- as simple as computing a gradient.\n\nRegarding \u201cThe quantitative evidence is quite limited and most of the paper is spent on qualitative results\u201d: \n\nWe address with the following multipart response; apologies for the lengthy response.\n\nFirst, we do plan to produce a comparison with side by sides for LRP and our method for the MNIST data set over the next few weeks as a sanity check.\n\nHowever, we don\u2019t think that that there is a strong metric to compare different feature importance techniques. This is acknowledged by Samek et al in their 2015 ICML Visualization Workshop work. We elaborate on this further at the end of this rebuttal.  \n\nMethods like DeepLift and Layer-wise Relevance Propagation (LRP) break a fundamental axiom in our mind: the attributions depend on the implementation, i.e. two networks that implement identical input-output  relationships can have different attributions. This seems odd---see Section 2.4 and Figure 14.  Perhaps, we did not emphasize this enough in the paper. \n\nThe main focus for us in the evaluation conducted so far has been to ensure that our output was sensible. In Section 2.5, we discuss a combination of approaches that we used to assess the attributions, including eyeballing, localization, and ablations. We welcome you to visualize more attributions at: ","label":0,"model":"human","source":"peerread","id":4846}
{"text":"This paper proposes a new method, interior gradients, for analysing feature importance in deep neural networks.  The interior gradient is the gradient measured on a scaled version of the input.  The integrated gradient is the integral of interior gradients over all scaling factors.  Visualizations comparing integrated gradients with standard gradients on real images input to the Inception CNN show that integrated gradients correspond to an intuitive notion of feature importance.\n\nWhile motivation and qualitative examples are appealing, the paper lacks both qualitative and quantitative comparison to prior work.  Only the baseline (simply the standard gradient) is presented as reference for qualitative comparison.  Yet, the paper cites numerous other works (DeepLift, layer-wise relevance propagation, guided backpropagation) that all attack the same problem of feature importance.  Lack of comparison to any of these methods is a major weakness of the paper.  I do not believe it is fit for publication without such comparisons.  My pre-review question articulated this same concern and has not been answered.\n","label":0,"model":"human","source":"peerread","id":4847}
{"text":"The authors propose to measure \u201cfeature importance\u201d, or specifically, which pixels contribute most to a network\u2019s classification of an image. A simple (albeit not particularly effective) heuristic for measuring feature importance is to measure the gradients of the predicted class wrt each pixel in an input image I. This assigns a score to each pixel in I (that ranks how much the output prediction would change if a given pixel were to change). In this paper, the authors build on this and propose to measure feature importance by computing gradients of the output wrt scaled version of the input image, alpha*I, where alpha is a scalar between 0 and 1, then summing across all values of alpha to obtain their feature importance score. Here the scaling is simply linear scaling of the pixel values (alpha=0 is all black image, alpha=1 is original image). The authors call these scaled images \u201ccounterfactuals\u201d which seems like quite an unnecessarily grandiose name for literally, a scaled image. \n\nThe authors show a number of visualizations that indicate that the proposed feature importance score is more reasonable than just looking at gradients only with respect to the original image. They also show some quantitative evidence that the pixels highlighted by the proposed measure are more likely to fall on the objects rather than spurious parts of the image (in particular, see figure 5). The method is also applied to other types of networks. The quantitative evidence is quite limited and most of the paper is spent on qualitative results.\n\nWhile the goal of understanding deep networks is of key importance, it is not clear whether this paper really help elucidate much. The main interesting observation in this paper is that scaling an image by a small alpha (i.e. creating a faint image) places more \u201cimportance\u201d on pixels on the object related to the correct class prediction. Beyond that, the paper builds a bit on this, but no deeper insight is gained. The authors propose some hand-wavy explanation of why using small alpha (faint image) may force the network to focus on the object, but the argument is not convincing. It would have been interesting to try to probe a bit deeper here, but that may not be easy.\n\nUltimately, it is not clear how the proposed scheme for feature importance ranking is useful. First, it is still quite noisy and does not truly help understand what a deep net is doing on a particular image. Performing a single gradient descent step on an image (or on the collection of scaled versions of the image) hardly begins to probe the internal workings of a network. Moreover, as the authors admit, the scheme makes the assumption that each pixel is independent, which is clearly false.\n\nConsidering the paper presents a very simple idea, it is far too long. The main paper is 14 pages, up to 19 with references and appendix. In general the writing is long-winded and overly verbose. It detracted substantially from the paper. The authors also define unnecessary terminology. \u201cGradients of Coutnerfactuals\u201d sounds quite fancy, but is not very related to the ideas explored in the writing. I would encourage the authors to tighten up the writing and figures down to a more readable page length, and to more clearly spell out the ideas explored early on.","label":0,"model":"human","source":"peerread","id":4848}
{"text":"This paper is the first (I believe) to establish a simple yet important result that Convnets for NMT encoders can be competitive to RNNs. The authors present a convincing set of results over many translation tasks and compare with very competitive baselines. I also appreciate the detailed report on training and generation speed. I find it's very interesting when position embeddings turn out to be hugely important (beside residual connections); unfortunately, there is little analysis to shed more lights on this aspect and perhaps compare other ways of capturing positions (a wild guess might be to use embeddings that represent some form of relative positions). The only concern I have (similar to the other reviewer) is that this paper perhaps fits better in an NLP conference.\n\nOne minor comment: it's slight strange that this well-executed paper doesn't have a single figure on the proposed architecture :) It will also be even better to draw a figure for the biLSTM architecture as well (it does take some effort to understand the last paragraph in Section 2, especially the part on having a linear layer to compute z).","label":0,"model":"human","source":"peerread","id":4849}
{"text":"This work demonstrates architectural choices to make conv nets work for NMT. In general the reviewers liked the work and were convinced by the results but found the main contributions to be \"incremental\". \n \n Pros:\n - Clarity: The work was clearly presented, and besides for minor comments (diagrams) the reviewers understood the work\n - Quality: The experimental results were thorough, \"very extensive and leaves no doubt that the proposed approach works well\".\n \n Mixed:\n - Novelty: There is appreciation that the work is novel. However as the work is somewhat \"application-specific\" the reviewers felt the technical contribution was not an overwhelming contribution.\n - Impact: While some speed ups were shown, not all reviewers were convinced that the benefit was sufficient, or \"main speed-up factor(s)\" were. \n \n This work is clearly worthwhile, but the reviews place it slightly below the top papers in this area.","label":0,"model":"human","source":"peerread","id":4850}
{"text":"Authors, It would be great to have a rebuttal for this paper as reviewers will be discussing over the next week. Thanks.","label":0,"model":"human","source":"peerread","id":4851}
{"text":"The system described works comparably to bi-directional LSTM baseline for NMT, and CNN's are naturally parallelizable.\n\nKey ideas include the use of two stacked CNN's (one for each of encoding and decoding) for translation, with res connections and position embeddings. The use of CNN's for translation has been attempted previously (as described by the authors), but presumably it is the authors' combination of various architectural choices (attention, position embeddings, etc) that make the present system competitive with RNN's, whereas earlier attempts were not. They describe system's sensitivity to some of these choices (e.g. experiments to choose appropriate number of layers in each of the CNN's).\n\nThe experimental results are well reported in detail.\n\nOne or two figures would definitely be required to help clarify the architecture.\n\nThis paper is less about new ways of learning representations than about the combination of choices made (over the set of existing techniques) in order to get the good results that they do on the reported NMT tasks. In this respect, while I am fairly confident that the paper represents good work in machine learning, I am not quite as confident about its fit for this particular conference.","label":0,"model":"human","source":"peerread","id":4852}
{"text":"The paper reports a very clear and easy to understand result that a convolutional network can be used instead of the recurrent encoder for neural machine translation. \n\nApart from the known architectural elements, such as convolution, pooling, residual connections, position embeddings, the paper features one unexpected architectural twist: two stacks of convolutions, one for computing alignment and another for computing the representations.\nThe empirical evidence that this was necessary is provided, however the question of *why* it is necessary remains open. \n\nThe experimental evaluation is very extensive and leaves no doubt that the proposed approach works well. The convnet-based model was faster at evaluation, but it is not very clear what is the main speed-up factor. It\u2019s however hard to argue against the fact that the speed advantage of convnets is likely to increase if a more parallel implementation is considered. \n\nMy main concern is whether or not the paper is appropriate for ICLR, because the contribution is quite incremental and rather application-specific. ACL, EMNLP and other NLP conferences would be a better venue, I think. ","label":0,"model":"human","source":"peerread","id":4853}
{"text":"Hi,\n\nThe conditional input is denoted by both c_i and c_t in sections 2 and 3.1 respectively. c_i is reused in section 3.2\n","label":0,"model":"human","source":"peerread","id":4854}
{"text":"I want to draw attention to the fact that you compare tokenized BLEU results (with multi-bleu.perl) and detokenized BLEU results (with mteval-v13a.pl). The two should *never* be mixed in the same table, as the tokenization will have a big effect on results. Even when comparing systems that all have tokenized BLEU, and all use the Moses tokenizer, using different parameters (such as the \"-a\" option for aggressive hyphen splitting) will skew the results.\n\nDetokenized BLEU is standard for WMT, and reported by Sennrich et al. (2016a,b).\n\nI re-ran BLEU on our EN-RO system for comparison:\n\ndetokenized BLEU, mteval-v13a.pl: 28.1 BLEU\ntokenized BLEU, multi-bleu.perl: 29.4 BLEU\n\nyour reported result (multi-bleu.perl): 28.5","label":0,"model":"human","source":"peerread","id":4855}
{"text":"The paper proposes to use a last-layer feature penalty as regularization on the last layer of a neural net.\nAlthough the equations suggest a weighting per example, dropping this weight (alpha_i) works equally well.\nThe proposed approach relates to Batch Norm and weight decay.\nExperiments are given on \"low-shot\" settting.\nThere seem to be two stories in the paper: feature penalty as a soft batch norm version, and low-shot learning; why is feature penalty specifically adapted to low-shot learning and not a more classical supervised task?\nRegarding your result on Omniglot, 91.5, I believe it is still about 2% worse than the Matching Networks, which you refer to but don't put in Table 1. Why?\nOverall, the idea is simple but feels like preliminary: while it is supposed to be a \"soft BN\", BN itself gets better performance than feature penalty, and both together give even better results. Is something still missing in the explanation?\n\n-- edits after revised version:\n\nThank you for adding more information to the paper. I feel it is still too long but hopefully you can reduce it to 9 pages as promised. However, I'm still not convinced the paper is ready to be accepted, mainly for the following reasons:\n- on Omniglot, the paper is still significantly far from the current state of the art.\n- the new experiments do not really confirm\/infirm the relationship with BN.\n- you added an explanation of why FP works for low-shot setting, by showing it controls the VC dimension and hence is good to control overfitting with a small number of training examples, but this discussion is basic and does not really shed more light than the obvious.\nI'm pushing up your score from 4 to 5 for the improved version, but I still think it is below acceptance level.","label":0,"model":"human","source":"peerread","id":4856}
{"text":"The paper extends a regularizer on the gradients recently proposed by Hariharan and Girshick. I agree with the reviewers that while the analysis is interesting, it is unclear why this particular regularizer is especially relevant for low-shot learning. And the experimental validation is not strong enough to warrant acceptance.","label":0,"model":"human","source":"peerread","id":4857}
{"text":"We would like to thank all reviewers for their valuable comments and suggestions. We modify our paper accordingly. \nEspecially, we add classification performance comparison between our methods and batch normalization.\nMoreover, we also add our preliminary analysis why the proposed regularizer could improve generalization performance, which is closely related to the improved \"low-shot\" improvement.\n\nAdmittedly, current version is a little bit beyond page-limit (11 pages now). We managed to include some new experimental results and analysis in our revised paper, and will trim it within 9 pages with some detailed deductions left in supplemental materials.","label":0,"model":"human","source":"peerread","id":4858}
{"text":"Summary\n===\nThis paper extends and analyzes the gradient regularizer of Hariharan and\nGirshick 2016. In that paper a regularizer was proposed which penalizes\ngradient magnitudes and it was shown to aid low-shot learning performance.\nThis work shows that the previous regularizer is equivalent to a direct penalty\non the magnitude of feature values weighted differently per example.\n\nThe analysis goes to to provide two examples where a feature penalty\nfavors a better representation. The first example addresses the XOR\nproblem, constructing a network where a feature penalty encourages\na representation where XOR is linearly separable.\nThe second example analyzes a 2 layer linear network, showing improved stability\nof a 2nd order optimizer when the feature penalty is added.\nOne last bit of analysis shows how this regularizer can be interpreted as\na Gaussian prior on both features and weights. Since the prior can be\ninterpreted as having a soft whitening effect, the feature regularizer\nis like a soft version of Batch Normalization.\n\nExperiments show small improvements on a synthetic XOR test set.\nOn the Omniglot dataset feature regularization is better than most baselines,\nbut is worse than Moment Matching Networks. An experiment on ImageNet similar\nto Hariharan and Girshick 2016 also shows effective low-shot learning.\n\n\nStrengths\n===\n\n* The core proposal is a simple modification of Hariharan and Girshick 2016.\n\n* The idea of feature regularization is analyzed from multiple angles\nboth theoretically and empirically.\n\n* The connection with Batch Normalization could have broader impact.\n\n\nWeaknesses\n===\n\n* In section 2 the gradient regularizer of Hariharan and Girshick is introduced.\nWhile introducing the concept, some concern is expressed about the motivation:\n\"And it is not very clear why small gradients on every sample produces\ngood generalization experimentally.\" This seems to be the central issue to me.\nThe paper details some related analysis, it does not offer a clear answer to\nthis problem.\n\n\n* The purpose and generality of section 2.1 is not clear.\n\nThe analysis provides a specific case (XOR with a non-standard architecture)\nwhere feature regularization intuitively helps learn a better representation.\nHowever, the intended take-away is not clear.\n\nThe take-away may be that since a feature penalty helps in this case it\nshould help in other cases. I am hesitant to buy that argument because of the\nspecific architecture used in this section. The result seems to rely on the\nchoice of an x^2 non-linearity, which is not often encountered in recent neural\nnet literature.\n\nThe point might also be to highlight the difference between a weight\npenalty and a feature penalty because the two seem to encourage\ndifferent values of b in this case. However, there is no comparison to\na weight penalty on b in section 2.1.\n\n\n* As far as I can tell, eq. 3 depends on either assuming an L2 or cross-entropy\nloss. A more general class of losses for which eq. 3 holds is not provided. This\nshould be made clear before eq. 3 is presented.\n\n\n* The Omniglot and ImageNet experiments are performed with Batch Normalization,\nyet the paper points out that feature regularization may be similar in effect\nto Batch Norm. Since the ResNet CNN baseline includes Batch Norm and there are\nclear improvements over that baseline, the proposed regularizer has a clear\nadditional positive effect. However, results should be provided without\nBatch Norm so a 1-1 comparison between the two methods can be performed.\n\n\n* The ImageNet experiment should be more like Hariharan and Girshick.\nIn particular, the same split of classes should be used (provided in\nthe appendix) and performance should be measured using n > 1 novel examples\nper class (using k nearest neighbors).\n\n\nMinor:\n\n* A brief comparison to Matching Networks is provided in section 3.2, but the\nperformance of Matching Networks should also be reported in Table 1.\n\n* From the approach section: \"Intuitively when close to convergence, about half\nof the data-cases recommend to update a parameter to go left, while\nthe other half recommend to go right.\"\n\nCould the intuition be clarified? There are many directions in high\ndimensional space and many ways to divide them into two groups.\n\n* Is the SGM penalty of Hariharan and Girshick implemented for this paper\nor using their code? Either is acceptable, but clarification would be appreciated.\n\n* Should the first equal sign in eq. 13 be proportional to, not equal to?\n\n* The work is dense in nature, but I think the presentation could be improved.\nIn particular, more detailed derivations could be provided in an appendix\nand some details could be removed from the main version in order to increase\nfocus on the results (e.g., the derviation in section 2.2.1).\n\n\nOverall Evaluation\n===\n\nThis paper provides an interesting set of analyses, but their value is not clear.\nThere is no clear reason why a gradient or feature regularizer should improve\nlow-shot learning performance. Despite that, experiments support that conclusion,\nthe analysis is interesting by itself, and the analysis may help lead to a\nclearer explanation.\n\nThe work is a somewhat novel extension and analysis of Hariharan and Girshick 2016.\nSome points are not completely clear, as mentioned above.","label":0,"model":"human","source":"peerread","id":4859}
{"text":"This paper proposes analysis of regularization, weight Froebius-norm and feature L2 norm, showing that it is equivalent to another proposed regularization, gradient magnitude loss. They then argue that: 1) it is helpful to low-shot learning, 2) it is numerically stable, 3) it is a soft version of Batch Normalization. Finally, they demonstrate experimentally that such a regularization improves performance on low-shot tasks.\n\nFirst, this is a nice analysis of some simple models, and proposes interesting insights in some optimization issues. Unfortunately, the authors do not demonstrate, nor argue in a convincing manner, that such an analysis extends to deep non-linear computation structures. I feel like the authors could write a full paper about \"results can be derived for \u03c6(x) with convex differentiable non-linear activation functions such as ReLU\", both via analysis and experimentation to measure numerical stability.\n\nSecond, the authors again show an interesting correspondance to batch normalization, but IMO fail to experimentally show its relevance.\n\nFinally, I understand the appeal of the proposed method from a numerical stability point of view, but am not convinced that it has any effect on low-shot learning in the high dimensional spaces that deep networks are used for.\n\nI commend the authors for contributing to the mathematical understanding of our field, but I think they have yet to demonstrate the large scale effectiveness of what they propose. At the same time, I feel like this paper does not have a clear and strong message. It makes various (interesting) claims about a number of things, but they seem more or less disparate, and only loosely related to low-shot learning.\n\nnotes:\n- \"an expectation taken with respect to the empirical distribution generated by the training set\", generally the training set is viewed as a \"montecarlo\" sample of the underlying, unknown data distribution \\mathcal{D}.\n- \"we can see that our model learns meaningful representations\", it gets a 6.5% improvement on the baseline, but there is no analysis of the meaningfulness of the representations.\n- \"Table 13.2\" should be \"Table 2\".\n- please be mindful of formatting, some citations should be parenthesized and there are numerous extraneous and missing spacings between words and sentences.\n","label":0,"model":"human","source":"peerread","id":4860}
{"text":"The paper proposes to use a last-layer feature penalty as regularization on the last layer of a neural net.\nAlthough the equations suggest a weighting per example, dropping this weight (alpha_i) works equally well.\nThe proposed approach relates to Batch Norm and weight decay.\nExperiments are given on \"low-shot\" settting.\nThere seem to be two stories in the paper: feature penalty as a soft batch norm version, and low-shot learning; why is feature penalty specifically adapted to low-shot learning and not a more classical supervised task?\nRegarding your result on Omniglot, 91.5, I believe it is still about 2% worse than the Matching Networks, which you refer to but don't put in Table 1. Why?\nOverall, the idea is simple but feels like preliminary: while it is supposed to be a \"soft BN\", BN itself gets better performance than feature penalty, and both together give even better results. Is something still missing in the explanation?\n\n-- edits after revised version:\n\nThank you for adding more information to the paper. I feel it is still too long but hopefully you can reduce it to 9 pages as promised. However, I'm still not convinced the paper is ready to be accepted, mainly for the following reasons:\n- on Omniglot, the paper is still significantly far from the current state of the art.\n- the new experiments do not really confirm\/infirm the relationship with BN.\n- you added an explanation of why FP works for low-shot setting, by showing it controls the VC dimension and hence is good to control overfitting with a small number of training examples, but this discussion is basic and does not really shed more light than the obvious.\nI'm pushing up your score from 4 to 5 for the improved version, but I still think it is below acceptance level.\n\n","label":0,"model":"human","source":"peerread","id":4861}
{"text":"This paper addresses an important and timely topic in a creative way. I consider it to have three flaws (and one good idea).\n\n1) insufficient context of what is known and had been studied before (in shallow RL), for example within the field of \u201crobust RL\u201d. A good place to start might be with the work of Shie Mannor.\n\n2) an ill-defined general problem setup. Does it make sense to do post-hoc labeling of certain actions as \u201ccatastrophic\u201d if the agent is not informed about that metric during learning? Training a system to do one thing (maximize reward), but then evaluating it with a different metric is misleading. On the training metric, it could even be that the baseline outperforms the new algorithm? So I\u2019d want to see plots for \u201caverage reward\u201d in fig 3 as well. Also, what would the baseline learn if it was given large negative rewards for entering these otherwise invisible \u201cdanger states\u201d?\n\n3) a somewhat ad-hoc solution, that introduces new domain-specific hyperparameters (k_r, k_lambda and lambda) a second deep network and and two additional replay memories. In terms of results, I\u2019m also unsure whether I can trust the results, given the long-standing track-record of cart-pole being fully solved by many methods: is DQN an outlier here? Or is the convnet not an appropriate function-approximator? Actually: which exact variant \u201cstate-of-the-art\u201d variant of DQN are you using?\n\nThe good idea that I encourage the authors to pursue further is D_d, this set of rare but dangerous states, that should be kept around in some form. I see it as an ingredient for continual learning that most typical methods lack -- it is also one of the big differences between RL and supervised learning, where such states would generally be discarded as outliers.\n\nGiven my comment a couple of weeks ago, and the prompt response (\u201cwe implemented expected SARSA\u201d), I would have expected that the paper had been revised with the new results by now? In any case, I\u2019m open to discussing all these points and revising my opinion based on an updated version of the paper.\n\nMinor comment: the bibliography is done sloppily, with missing years, conference venues and missing\/misspelled author lists, e.g. \u201cSergey et al. Levine\u201d. I also think it is good form to cite the actual conference publications instead of arXiv where applicable.","label":0,"model":"human","source":"peerread","id":4862}
{"text":"This paper presents a few interesting ideas, namely the idea of keeping around a set of \"danger states\" and treating these states with some special consideration in reply to make sure that their impact is not neglected after collecting a lot of additional data.\n \n However, there are two main problems: 1) the actual implementation here seems fairly ad-hoc, and it's not at all clear to me that this particular algorithm (building a classifier with equal numbers of good and danger states, and then injecting an additional reward into the Q-learning task based upon this classifier), is the right way to go about this. The presentation is also difficult to follow, and the final results imply aren't that compelling (though this is improving after the revisions, but still has a way to go. We therefore encourage the authors to resubmit their work at a future conference venue.\n \n Pros:\n + Interesting idea of keeping around danger states and injecting them into training\n \n Cons:\n - Algorithm doesn't seem that well motivated\n - Presentation is a bit unclear, takes until page 6 to actually present the basic approach.\n - Experiments aren't that convincing (better after revisions, but still need work)","label":0,"model":"human","source":"peerread","id":4863}
{"text":"Thanks for an interesting take on an important question :) \nSome high level comments\/questions about the issue of \"safety\" in RL:\n\n1. What is the justification that there is any hope of behaving safely when a model is unavailable? Some amount of exploration is needed anyway, and without exploration you are doomed to not find the optimal policy. Essentially you either get stuck with a sub-optimal policy or accept some amount of failures. Whether such failures are acceptable or not is highly application specific, and hence I would encourage the authors to consider a concrete application that has some use (as opposed to toy problems) and provide a reasonable solution tailored to the same. A general solution is unlikely to exist, or be useful in practice.\n\n2. If a model is available, then much of the motivations raised become irrelevant. A simulated car can hit simulated pedestrians many times, and learn from the mistake in simulation -- it does not pose any threat. Further, measures like risk sensitive RL or robust RL can be put in place to ensure sim2real transfer.\n\n3. In light of (2), I would actually encourage the authors to think of their danger model as a form of reward shaping. Can we think of fear as a way to guide the exploration space when a model is made available? That way, the question shifts away from \"safety\" to sample efficiency and transfer. I think the danger model could be a good approach for these considerations (e.g. similar to SARSA). Though these have been mentioned in the paper, I look forward to an expanded discussion along these lines.","label":0,"model":"human","source":"peerread","id":4864}
{"text":"This paper presents a heuristic for avoiding large negative rewards which have already been experienced by distilling such events into a \"danger model\". The paper is well written including some rather poetic language [*].\n\nThe heuristic is evaluated in two toy domains. I would think that in order to properly evaluate this one would use a well known benchmark e.g. Atari. Atari seems particularly apt since those games are full of catastrophes (i.e. sudden death).\n\n[*] this reviewer's favourite quotes:\n\"Imagine a self-driving car that had to periodically hit a few pedestrians in order to remember that it\u2019s undesirable.\"\n\"The child can learn to adjust its behaviour without actually having to stab someone.\"\n\"... the catastrophe lurking just past the optimal shave.\"","label":0,"model":"human","source":"peerread","id":4865}
{"text":"- The topic of keeping around highly rewarding or dangerous states is important and has been studied extensively in the RL literature. After the pre-review comments, authors do mention that they compared against expected SARSA but I would really like to see these and other extensive baselines before accepting this paper. \n\n- There is also an increasing amount of literature of using reward replay buffers in deep RL agents (c.f. Jaderberg, Max, et al. \"Reinforcement learning with unsupervised auxiliary tasks.\", Blundell, Charles, et al. \"Model-free episodic control.\" , Narasimhan et al. \"Language understanding for text-based games using deep reinforcement learning\"), which could perhaps reinforce the agent to avoid revisiting catastrophic states. \n\n- Overall, the approach presented is not very principled. For instance, why isn't catastrophe directly provided as a signal to the learner instead of a separate model? ","label":0,"model":"human","source":"peerread","id":4866}
{"text":"This paper takes a standard auto-regressive model of source code and augments it with a fixed attention policy that tracks the use of certain token types, like identifiers. Additionally they release a Python open source dataset. As expected this augmentation, the fixed attention policy, improves the perplexity of the model. It seems important to dig a bit deeper into these results and show the contribution of different token types to the achieve perplexity. This is alluded to in the text, but a more thorough comparison would be welcome. The idea of an attention policy that takes advantage of expert knowledge is a nice contribution, but perhaps if limited novelty --- for example the Maddison and Tarlow 2014 paper, which the authors cite, has scoping rules that track previously used identifiers in scope.","label":0,"model":"human","source":"peerread","id":4867}
{"text":"This paper augments language models with attention to to capture long range dependencies through a sparse pointer network that is restricted to previously introduced identifiers, and demonstrates the proposed architecture over a new, released large-scale code suggestion corpus of 41M lines of Python code. The addition of long range attention over 20 identifiers improves perplexity compared to an LSTM with an attentional context of 50 words, but degrades accuracy (hit @1), while improving hit@5.\n The experimental validation however requires a more thorough analysis and more detailed ablation experiments and discussions, and more thorough comparison to related work. As is, many choices seem quite arbitrary and make it hard to determine if the model is really performing well (minibatch sizes, size of the memory for the LSTM, choice and number of identifiers for the sparse pointers, etc).","label":0,"model":"human","source":"peerread","id":4868}
{"text":"This paper presents an improved neural language models designed for selected long-term dependency, i.e., to predict more accurately the next identifier for the dynamic programming language such as Python. The improvements are obtained by:\n\n1) replacing the fixed-widow attention with a pointer network, in which the memory only consists of context representation of the previous K identifies introduced for the entire history. \n2) a conventional neural LSTM-based language model is combined with such a sparse pointer network with a controller, which linearly combines the prediction of both components using a dynamic weights, decided by the input, hidden state, and the context representations at the time stamp.\n\nSuch a model avoids the the need of large window size of the attention to predict next identifier, which usually requires a long-term dependency in the programming language. This is partly validated by the python codebase (which is another contribution of this paper) experiments in the paper.\n\nWhile the paper still misses some critical information that I would like to see, including how the sparse pointer network performance chances with different size of K, and how computationally efficient it is for both training and inference time compared to LSTM w\/ attention of various window size, and ablation experiments about how much (1) and (2) contribute respectively, it might be of interest to the ICLR community to see it accepted.\n\n","label":0,"model":"human","source":"peerread","id":4869}
{"text":"This paper uses a pointer network over a sparse window of identifiers to improve code suggestion for dynamically-typed languages. Code suggestion seems an area where attention and\/or pointers truly show an advantage in capturing long term dependencies.\n\nThe sparse pointer method does seem to provide better results than attention for similar window sizes - specifically, comparing a window size of 20 for the attention and sparse pointer method shows the sparse pointer winning fairly definitively across the board. Given a major advantage of the pointer method is being able to use a large window size well thanks to the supervision the pointer provides, it was unfortunate (though understandable due to potential memory issues) not to see larger window sizes. Having a different batch size for the sparse pointer and attention models is unfortunate given it complicates an otherwise straight comparison between the two models.\n\nThe construction and filtering of the Python corpus sounds promising but as of now it is still inaccessible (listed in the paper as TODO). Given that code suggestion seems an interesting area for future long term dependency work, it may be promising as an avenue for future task exploration.\n\nOverall this paper and the dataset are likely an interesting contribution even though there are a few potential issues.","label":0,"model":"human","source":"peerread","id":4870}
{"text":"This paper takes a standard auto-regressive model of source code and augments it with a fixed attention policy that tracks the use of certain token types, like identifiers. Additionally they release a Python open source dataset. As expected this augmentation, the fixed attention policy, improves the perplexity of the model. It seems important to dig a bit deeper into these results and show the contribution of different token types to the achieve perplexity. This is alluded to in the text, but a more thorough comparison would be welcome. The idea of an attention policy that takes advantage of expert knowledge is a nice contribution, but perhaps if limited novelty --- for example the Maddison and Tarlow 2014 paper, which the authors cite, has scoping rules that track previously used identifiers in scope. ","label":0,"model":"human","source":"peerread","id":4871}
{"text":"Some of the claimed contributions the paper makes already exist in prior work. For instance:\n\n\u2192 There is already a Python data set available: ","label":0,"model":"human","source":"peerread","id":4872}
{"text":"ResNet and other architectures that use shortcuts have shown empirical success in several domains and therefore, studying the optimization for such architectures is very valuable. This paper is an attempt to address some of the properties of networks that use shortcuts. Some of the experiments in the paper are interesting. However, there are two main issues with the current paper:\n\n1- linear vs non-linear: I think studying linear networks is valuable but we should be careful not to extend the results to networks with non-linear activations without enough evidence. This is especially true for Hessian as the Hessian of non-linear networks have very large condition number (see the ICLR submission \"Singularity of Hessian in Deep Learning\") even in cases where the optimization is not challenging. Therefore, I don't agree with the claims in the paper on non-linear networks. Moreover, one plot on MNIST is not enough to claim that non-linear networks behave similar to linear networks.\n\n2- Hessian at zero initial point: The explanation of why we should be interested in Hessain at zero initial point is not acceptable. The zero initial point is not interesting because it is a very particular point that cannot tell us about the Hessian during optimization.","label":0,"model":"human","source":"peerread","id":4873}
{"text":"This paper endeavors to offer theoretical explanations of the performance of ResNet. Providing better theoretical understanding of existing empirically powerful architectures is very important work and I commend the authors for tackling this. Unfortunately, this paper falls short in its current form: the particular choices and restrictions made (0 weights, linear regime) limit applicability to ResNet, and do not seem to offer insights sufficient to capture the causes of ResNet's performance.","label":0,"model":"human","source":"peerread","id":4874}
{"text":"I think the write-up can be improved. The results of the paper also might be somewhat misleading. The behavior for when weights are 0 is not revealing of how the model works in general. I think the work also underestimates the effect of the nonlinearities on the learning dynamics of the model.","label":0,"model":"human","source":"peerread","id":4875}
{"text":"This paper studies the optimization issue of linear ResNet, and shows mathematically that for 2-shortcuts and zero initialization, the Hessian has condition number independent of depth. I skimmed through the proof but have not checked them carefully. \n\nThis result is a nice observation for training deep linear networks.  But I do not think the paper has fully resolved the linear vs nonlinear issue. Some question:\n\n1. Though the revision has added some results using ReLU units, it seems it is only added to the mid positions of the network (sec 5.3), is this how it is typically done in ResNet? Moreover, ReLU is not differentiable at zero point, which does not satisfy the condition you had in Theorem 1. Why not use differentiable activations like sigmoid or tanh?\n\n2. From equation (22) in the appendix, it seems for nonlinear activations, the condition number depends on the derivative \\sigma^\\prime at 0. Therefore, if we use tanh which has derivative 1 at zero, the condition number is the same for linear and tanh activations. But this probably is not enough to explain the bit difference in performance or optimization for linear and nonlinear networks, or how the situations evolve after learning the 0 point.\n\n3. As for the success of ResNet (or convnets in general) in computer vision, I believe there are more types of nonlinearity such as pooling? Can the result here generalizes to pooling as well?\n\nMinor: \n- sec 1 last paragraph, low approximation error typically means more powerful model class and better training error, but not necessarily better test error\n- sec 4.1 what do you mean by \"zero initialization with small random perturbations\"? why not exactly zero initialization, how large is the random perturbation?\n\n","label":0,"model":"human","source":"peerread","id":4876}
{"text":"ResNet and other architectures that use shortcuts have shown empirical success in several domains and therefore, studying the optimization for such architectures is very valuable. This paper is an attempt to address some of the properties of networks that use shortcuts. Some of the experiments in the paper are interesting. However, there are two main issues with the current paper:\n\n1- linear vs non-linear: I think studying linear networks is valuable but we should be careful not to extend the results to networks with non-linear activations without enough evidence. This is especially true for Hessian as the Hessian of non-linear networks have very large condition number (see the ICLR submission \"Singularity of Hessian in Deep Learning\") even in cases where the optimization is not challenging. Therefore, I don't agree with the claims in the paper on non-linear networks. Moreover, one plot on MNIST is not enough to claim that non-linear networks behave similar to linear networks.\n\n2- Hessian at zero initial point: The explanation of why we should be interested in Hessain at zero initial point is not acceptable. The zero initial point is not interesting because it is a very particular point that cannot tell us about the Hessian during optimization. ","label":0,"model":"human","source":"peerread","id":4877}
{"text":"Hi Authors,\n\nI went through your paper in detail and it was very good linking the Hessian at the zero initial point to the success of the ResNet. However, I am a little unsure of the definition of the condition number in the paper. Across several standard refernces, the condition number is defined as the ratio the maximum and minimum singular values where as you defined it as $ |\\lambda|_max \/ |\\lambda|_min  $.  Unless the matrix, which is the Hessian H in your case, is positive semi-definite, these two are not the same and the spectra of the singular values and the eigen values can be quite different in general. Especially, for n=2, the initial point zero is a saddle point as stated in your theorem and as such the Hessian is not positive-definite. Please let me know if I am missing out on something and whether or not this modification affects the analysis in the paper. Thanks!","label":0,"model":"human","source":"peerread","id":4878}
{"text":"This paper presents a generative model for binary images.  Images are composed by placing a set of binary features at locations in the image.  These features are OR'd together to produce an image.  In a hierarchical variant, features\/classes can have a set of possible templates, one of which can be active.  Variables are defined to control which template is present in each layer.  A joint probability distribution over both the feature appearance and instance\/location variables is defined.\n\nOverall, the goal of this work is interesting -- it would be satisfying if semantically meaningful features could be extracted, allowing compositionality in a generative model of images.  However, it isn't clear this would necessarily result from the proposed process.\nWhy would the learned features (building blocks) necessarily semantically meaningful?  In the motivating example of text, rather than discovering letters, features could correspond to many other sub-units (parts of letters), or other features lacking direct semantic meaning.\n\nThe current instantiation of the model is limited.  It models binary image patterns.  The experiments are done on synthetic data and MNIST digits.  The method recovers the structure and is effective at classification on synthetic data that are directly compositional.  On the MNIST data, the test errors are quite large, and worse than a CNN except when synthetic data corruption is added.  Further work to enhance the ability of the method to handle natural images or naturally occuring data variation would enhance the paper.","label":0,"model":"human","source":"peerread","id":4879}
{"text":" The reviewer's opinions were clear for this paper. Mainly it seems that the fact that this work focuses on binary image patterns limited the ability of reviewers to assess the significance of this work based on the instantiation of the model explored in this work. It was also noted that the writing could have been clearer when describing the intuitions for the approach and that the derivations could have been explained in more detail.","label":0,"model":"human","source":"peerread","id":4880}
{"text":"The paper discusses a method to learn interpretable hierarchical template representations from given data. The authors illustrate their approach on binary images.\n\nThe paper presents a novel technique for extracting interpretable hierarchical template representations based on a small set of standard operations. It is then shown how a combination of those standard operations translates into a task equivalent to a boolean matrix factorization. This insight is then used to formulate a message passing technique which was shown to produce accurate results for these types of problems.\n\nSummary:\n\u2014\u2014\u2014\nThe paper presents an novel formulation for extracting hierarchical template representations that has not been discussed in that form. Unfortunately the experimental results are on smaller scale data and extension of the proposed algorithm to more natural images seems non-trivial to me.\n\nQuality: I think some of the techniques could be described more carefully to better convey the intuition.\nClarity: Some of the derivations and intuitions could be explained in more detail.\nOriginality: The suggested idea is reasonable but limited to binary data at this point in time.\nSignificance: Since the experimental setup is somewhat limited according to my opinion, significance is hard to judge.\n\nDetails:\n\u2014\u2014\u2014\n1. My main concern is related to the experimental evaluation. While the discussed approach is valuable, its application seems limited to binary images at this point in time. Can the authors comment?\n\n2. There are existing techniques to extract representations of images which the authors may want to mention, e.g., work based on grammars.","label":0,"model":"human","source":"peerread","id":4881}
{"text":"This paper presents an approach to learn object representations by composing a set of templates which are leaned from binary images. \nIn particular, a hierarchical model is learned by combining AND, OR and POOL operations. Learning is performed by using approximated inference with MAX-product BP follow by a heuristic to threshold activations to be binary. \n\nLearning hierarchical representations that are interpretable is a very interesting topic, and this paper brings some good intuitions in light of modern convolutional neural nets. \n\nI have however, some concerns about the paper:\n\n1) the paper fails to cite and discuss relevant literature and claims to be the first one that is able to learn interpretable parts. \nI would like to see a discussion of the proposed approach compared to a variety of papers e.g.,:\n\n- Compositional hierarchies of Sanja Fidler\n- AND-OR graphs used by Leo Zhu and Alan Yuille to model objects\n- AND-OR templates of Song-Chun Zhu's group at UCLA \n\nThe claim that this paper is the first to discover such parts should be removed. \n\n2) The experimental evaluation is limited to very toy datasets. The papers I mentioned have been applied to real images (e.g., by using contours to binarize the images). \nI'll also like to see how good\/bad the proposed approach is for classification in more well known benchmarks. \nA comparison to other generative models such as VAE, GANS, etc will also be useful.\n\n3) I'll also like to see a discussion of the relation\/differences\/advantages of the proposed approach wrt to sum product networks and grammars.\n\nOther comments:\n\n- the paper claims that after learning inference is feed-forward, but since message passing is used, it should be a recurrent network. \n\n- the algorithm and tech discussion should be moved from the appendix to the main paper\n\n- the introduction claims that compression is a prove for understanding. I disagree with this statement, and should be removed. \n\n- I'll also like to see a discussion relating the proposed approach to the Deep Rendering model. \n\n- It is not obvious how some of the constraints are satisfied during message passing. Also constraints are well known to be difficult to optimize with max product. How do you handle this?\n\n- The learning and inference algorithms seems to be very heuristic (e.g., clipping to 1, heuristics on which messages are run). Could you analyze the choices you make?\n\n- doing multiple steps of 5) 2) is not a single backward pass \n\nI'll reconsider my score in light of the answers","label":0,"model":"human","source":"peerread","id":4882}
{"text":"This paper presents a generative model for binary images.  Images are composed by placing a set of binary features at locations in the image.  These features are OR'd together to produce an image.  In a hierarchical variant, features\/classes can have a set of possible templates, one of which can be active.  Variables are defined to control which template is present in each layer.  A joint probability distribution over both the feature appearance and instance\/location variables is defined.\n\nOverall, the goal of this work is interesting -- it would be satisfying if semantically meaningful features could be extracted, allowing compositionality in a generative model of images.  However, it isn't clear this would necessarily result from the proposed process.\nWhy would the learned features (building blocks) necessarily semantically meaningful?  In the motivating example of text, rather than discovering letters, features could correspond to many other sub-units (parts of letters), or other features lacking direct semantic meaning.\n\nThe current instantiation of the model is limited.  It models binary image patterns.  The experiments are done on synthetic data and MNIST digits.  The method recovers the structure and is effective at classification on synthetic data that are directly compositional.  On the MNIST data, the test errors are quite large, and worse than a CNN except when synthetic data corruption is added.  Further work to enhance the ability of the method to handle natural images or naturally occuring data variation would enhance the paper.\n","label":0,"model":"human","source":"peerread","id":4883}
{"text":"The authors explore the idea of deep-learning a static analyzer. They do it with a toy programming language and a very simplified analysis problem -- just checking if all variables are initalized.\n\nWhile the idea is interesting and might be developped into a tool in the future, the toy task presented in this paper is too simple to warrant an ICLR submission. Just detecting whether a variable is initialized in a string is a toy algorihtmic task, similar to the ones solved in a number of paper in recent years by models such as the Neural Turing Machine, Stack RNNs, Neural GPU, or Differentiable Neural Computer. All these architectures perform almost perfectly on a number of algorithmic tasks, so it is highly probable that they would also solve this one. Unluckily, the authors only compare to much more basic models, such as HMMs. Since the code for many of the above-mentioned models is available online, a paper without these baselines is not ready for ICLR. Moreover, there is a risk that existing models already solve this problem very well, making the contribution unclear.","label":0,"model":"human","source":"peerread","id":4884}
{"text":"There is a general consensus that, though the idea is interesting, the work is not mature enough for a conference publication (e.g., the problem is too toy, not clear that really solves any, even artificial problem, better than existing techniques).","label":0,"model":"human","source":"peerread","id":4885}
{"text":"This paper takes a first step towards learning to statically analyze source code. It develops a simple toy programming language that includes loops and branching. The aim is to determine whether all variables in the program are defined before they are used. The paper tries a variety of off-the-shelf sequence classification models and develops a new model that makes use of a ``differentiable set'' to keep track of which variables have been defined so far. Result show that an LSTM model can achieve 98% accuracy, and the differentiable set model can achieve 99.3% accuracy with sequence-level supervision and 99.7% accuracy with strong token-level supervision. An additional result is used whereby an LSTM language model is trained over correct code, and then low probability (where a threshold to determine low is tuned by hand) tokens are highlighted as sources of possible error.\n\nOne further question is if the authors could clarify what reasoning patterns are needed to solve these problems. Does the model need to, e.g., statically determine whether an `if` condition can ever evaluate to true in order to solve these tasks? Or is it just as simple as checking whether a variable appears on a LHS before it appears on a RHS later in the textual representation of the program?\n\nStrengths:\n- Learning a static analyzer is an interesting concept, and I think there is good potential for this line of work\n- The ability to determine whether variables are defined before they are used is certainly a prerequisite for more complicated static analysis.\n- The experimental setup seems reasonable\n- The differentiable set seems like a useful (albeit simple) modelling tool\n\nWeaknesses:\n- The setup is very toy, and it's not clear to me that this makes much progress towards the challenges that would arise if one were trying to learn a static analyzer \n- The models are mostly very simple. The one novelty on the modelling front (the differentiable set) provides a small win on this task, but it's not clear if it is a useful general construct or not.\n\nOverall:\nI think it's an interesting start, and I'm eager to see how this line of work progresses. In my opinion, it's a bit too early to accept this work to ICLR, but I'd be excited about seeing what happens as the authors try to push the system to learn to analyze more properties of code, and as they push towards scenarios where the learned static analyzer would be useful, perhaps leveraging strengths of machine learning that are not available to standard programming languages analyses.","label":0,"model":"human","source":"peerread","id":4886}
{"text":"The authors are trying to understand whether static analysis can be learned. As I hinted in my question, I think that all of the interesting complexity of static analysis has been removed in the toy language --- extraordinarily simple logic using a set can solve the problem posed, and an LSTM (unsurprisingly) can learn the extraordinarily simple logic (when given a differentiable set object). This extreme simplicity gives me no confidence that a more realistic static analysis problem can be solved.\n\nLSTMs (and deep learning) have had remarkable successes in solving messy real-world language problems. It's certainly possible that LSTMs could solve static analysis -- but being technically timid is not the right way to go about it.","label":0,"model":"human","source":"peerread","id":4887}
{"text":"-- The motivation behind the work is somewhat unclear: by now, it is very well understood how to design analyzers and what sound\/optimal means. Creating a ``black box'' analyzer that can make basic predictions (that are sometimes incorrect) without being able to modify it would be useful if the predictions were challenging and need not be sound all the time (like in some of the cited papers). Note that when analyzers are not sound there are typically clear  reasons for why this is so, e.g., dealing with native methods, frameworks, dynamic evaluation, etc. They are not unsound for 'random reasons'. \n\n-- There is also related work, already pointed out: the one of Hindle and others which already addresses what is in section 4.\n\n-- Here is also recent related work on learning (the transformers of the) static analyzers from data, one that is more elaborate as it learns the transformers of real analyzers and even finds real-world issues in Facebook's Flow: ","label":0,"model":"human","source":"peerread","id":4888}
{"text":"This paper is motivated by the ability that human's visual system can recognize contents of environment by from critical features, and tried to investigate whether neural networks can also have this kind of ability.  Specifically, the paper proposed to use Auto-Encoder (AE) as the network to reconstruct the low fidelity of visual input. Moreover, similar to Mnih et al. (2014),  the paper also proposed to use a recurrent fashion to mimic the sequential behavior the  human visual system. \n\nI think the paper is well motivated. However, there are several concerns:\n1. The baselines of the paper are too weak. Nearest neighbor, bilinear, bicubic and cubic interpolations without any learning procedure are of course performed worse than AE based models. The author should compare with the STOA methods such as","label":0,"model":"human","source":"peerread","id":4889}
{"text":"The program committee appreciates the authors' response to concerns raised in the reviews. Unfortunately, reviews are not leaning sufficiently towards acceptance. Reviewers find this direction of exploration to be interesting, but a bit preliminary at the moment. Authors are strongly encouraged to incorporate reviewer comments to make future iterations of the work stronger.","label":0,"model":"human","source":"peerread","id":4890}
{"text":"This paper aims to characterize the perceptual ability of a neural network under different input conditions.  This is done by manipulating the input image x in various ways (e.g. downsamplig, foveating), and training an auto-encoder to reconstruct the original full-resolution image.  MSE and qualitative results are shown and compared for the different input conditions.\n\nUnfortunately, this paper seems to lack focus, presenting a set of preliminary inspections with few concrete conclusions.  For example, at the end of sec 4.4, \"This result is not surprising, given that FOV-R contains additional information .... These results suggests that a small number of foveations containing rich details might be all these neural networks need....\".  But this hypothesis is left dangling:  What detailed regions are needed, and from where?  For what sort of tasks?\n\nSecondly, it isn't clear to me what reconstruction behaviors are caused by a fundamental perception of the input, and what are artifacts of the autoencoder and pixelwise l2 loss?  A prime example is texture, which the autoencoder fails to recover.  But with a pixelwise loss, the network must predict high-frequency textures nearly pixel-for-pixel at training time; if this is impossible, then it will generate a pixelwise average of the training samples --- a flat region.  So then the network's inability to reconstruct textures is due to a problem generating them, specifically averaging from the training loss, not necessarily an issue in perceiving textures.  A network trained a different way (perhaps an adversarial network) may infer a texture is there, even if it wouldn't be able to generate it in a pixelwise l2 sense.\n\nSimilarly, the ability to perform color reconstruction given a color glimpse I think has much to do with disambiguating the color of an object\/scene:  If there is an ambiguity, the network won't know which to \"choose\" (white flower or yellow flower?) and output an average, which is why there are so many sepia tones.  However, in its section on this, the paper only measures the reconstruction error for different amounts of color given, and does not drill very far into any hypotheses for why this behavior occurs.\n\nThere are some interesting measurements here, such as the amount of color needed in the foveation to reconstruct a color image, and the discussion on global features, which may start to get at a mechanism by which glimpses may propagate to an entire reconstruction.  But overall it's hard to know what to take away from this paper.  What are larger concrete conclusions that can be garnered from the details, and what mechanisms bring them about?  Can these be more thoroughly explored with more focus?\n","label":0,"model":"human","source":"peerread","id":4891}
{"text":"I like the idea the paper is exploring. Nevertheless I see some issues with the analysis:\n\n- To get a better understanding of the quality of the results, I think at least some state-of-the-art comparisons should be included (e.g. by setting d times d pixel patches too their average and applying a denoising autoencoder). If they perform significantly better, then this indicates that the presented model is not yet taking all the information from the input image that could be used.\n- SCT-R and FOV-R are supposed to test how much information can be restored from the Fovea alone as opposed to the Fovea together with low resolution periphery. However, there is an additional difference between the two conditions: According to the paper, in SCT-R, part of the image was set to zero, while in FOV-R it was removed alltogether. With only one or two hidden layers, I could easily imagine this making a difference.\n- On page 4, you compare the performance of FOV-R (1% error) with that of DS-D (1.5%) and attribute this to information about the periphery that the autoencoder extracts from the fovea. While this might be the case, at least part of the reduced error will be due to the fact that the fovea is (hopefully) perfectly reconstructed. To answer the actual question \"how much additional information about the periphery can be extracted from the fovea\", you should consider calculating the error only in the periphery, i.e. the part of the image where DS-D and FOV-R got exactly the same input for. Then any decreased error is only due to the additional fovea information.\n\nOther issues:\n- The images in Figure 2 (a) and (b) in the rows \"factor 2\", \"factor 4\", \"factor 8\" look very blurry. There seems some interpolation to be going on (although slighly different than the bilinear interpolation). This makes it hard to asses how much information is in these images. I think it would be much more insightfull to print them with \"nearest\" interpolation.\n- Figure 3 caption too vague. Maybe add something like footnote 2?\n- Often figures appear too early in paper which leads to lots of distance between text and figures.\n","label":0,"model":"human","source":"peerread","id":4892}
{"text":"This paper is motivated by the ability that human's visual system can recognize contents of environment by from critical features, and tried to investigate whether neural networks can also have this kind of ability.  Specifically, the paper proposed to use Auto-Encoder (AE) as the network to reconstruct the low fidelity of visual input. Moreover, similar to Mnih et al. (2014),  the paper also proposed to use a recurrent fashion to mimic the sequential behavior the  human visual system. \n\nI think the paper is well motivated. However, there are several concerns:\n1. The baselines of the paper are too weak. Nearest neighbor, bilinear, bicubic and cubic interpolations without any learning procedure are of course performed worse than AE based models. The author should compare with the STOA methods such as ","label":0,"model":"human","source":"peerread","id":4893}
{"text":"Because the authors did not respond to reviewer feedback, I am maintaining my original review score.\n\n-----\n\nThis paper proposes to model relational (i.e., correlated) time series using a deep learning-inspired latent variable approach: they design a flexible parametric (but not generative) model with Gaussian latent factors and fit it using a rich training objective including terms for reconstruction (of observed time series) error, smoothness in the latent state space (via a KL divergence term encouraging neighbor states to be similarly distributed), and a final regularizer that encourages related time series to have similar latent state trajectories. Relations between trajectories are hard coded based on pre-existing knowledge, i.e., latent state trajectories for neighboring (wind speed) base stations should be similar. The model appears to be fit using gradient simple descent. The authors propose several elaborations, including a nonlinear transition function (based on an MLP) and a reconstruction error term that takes variance into account. However, the model is restricted to using a linear decoder. Experimental results are positive but not convincing.\n\nStrengths:\n- The authors target a worthwhile and challenging problem: incorporating the modeling of uncertainty over hidden states with the power of flexible neural net-like models.\n- The idea of representing relationships between hidden states using KL divergence between their (distributions over) corresponding hidden states is clever. Combined with the Gaussian distribution over hidden states, the resulting regularization term is simple and differentiable.\n- This general approach -- focusing on writing down the problem as a neural network-like loss function -- seems robust and flexible and could be combined with other approaches, including variants of variational autoencoders.\n\nWeaknesses:\n- The presentation is a muddled, especially the model definition in Sec. 3.3. The authors introduce four variants of their model with different combinations of decoder (with and without variance term) and linear vs. MLP transition function. It appears that the 2,2 variant is generally better but not on all metrics and often by small margins. This makes drawing a solid conclusions difficult: what each component of the loss contributes, whether and how the nonlinear transition function helps and how much, how in practice the model should be applied, etc. I would suggest two improvements to the manuscript: (1) focus on the main 2,2 variant in Sec. 3.3 (with the hypothesis that it should perform best) and make the simpler variants additional \"baselines\" described in a paragraph in Sec. 4.1; (2) perform more thorough experiments with larger data sets to make a stronger case for the superiority of this approach.\n- The authors only allude to learning (with references to gradient descent and ADAM during model description) in this framework. Inference gets its one subsection but only one sentence that ends in an ellipsis (?).\n- It's unclear what is the purpose of introducing the inequality in Eq. 9.\n- Experimental results are not convincing: given the size of the data, the differences vs. the RNN and KF baselines is probably not significant, and these aren't particularly strong baselines (especially if it is in fact an RNN and not an LSTM or GRU).\n- The position of this paper is unclear with respect to variational autoencoders and related models. Recurrent variants of VAEs (e.g., Krishnan, et al., 2015) seem to achieve most of the same goals as far as uncertainty modeling is concerned. It seems like those could easily be extended to model relationships between time series using the simple regularization strategy used here. Same goes for Johnson, et al., 2016 (mentioned in separate question).\n\nThis is a valuable research direction with some intriguing ideas and interesting preliminary results. I would suggest that the authors restructure this manuscript a bit, striving for clarity of model description similar to the papers cited above and providing greater detail about learning and inference. They also need to perform more thorough experiments and present results that tell a clear story about the strengths and weaknesses of this approach.","label":0,"model":"human","source":"peerread","id":4894}
{"text":"The reviews of this paper seem to be very aligned: many of the ideas presented in the paper are interesting, the problem is important, and the results encouraging but preliminary. R2 thought the paper could be improved in terms of clarity and offered several specific suggestions to this end. R2 and R1 mentioned the limitations of the linear decoder; which is not a critical flaw, in my opinion, but as R1 points out, many recent works have explored nonlinear decoders and these could be at least discussed, if not compared. All of the reviewers have worked in this area and expressed high-confidence reviews.\n \n I was surprised that the authors did not provide feedback or revise the paper at least with reference to the clarity\/presentation suggestions. It seems this may have had an impact on the perception of the reviewers. I encourage the authors to revise the paper in light of the reviews and re-submit to another venue.","label":0,"model":"human","source":"peerread","id":4895}
{"text":"This manuscript proposes an approach for modeling correlated timeseries through a combination of loss functions which depend on neural networks. The loss functions correspond to: data fit term, autoregressive latent state term, and a term which captures relations between pairs of timeseries (relations have to be given as prior information).\n\nModeling relational timeseries is a well-researched problem, however little attention has been given to it in the neural network community. Perhaps the reason for this is the importance of having uncertainty in the representation. The authors correctly identify this need and consider an approach which considers distributions in the state space.\n\nThe formulation is quite straightforward by combining loss functions. The model adds to Ziat et al. 2016 in certain aspects which are well motivated, but unfortunately implemented in an unconvincing way. To start with, uncertainty is not treated in a very principled way, since the inference in the model is rather naive; I'd expect employing a VAE framework [1] for better uncertainty handling. Furthermore, the Gaussian co-variance collapses into a variance, which is the opposite of what one would want for modelling correlated time-series. There are approaches which take these correlations into account in the states, e.g. [2].\n\nMoreover, the treatment of uncertainty only allows for linear decoding function f. This significantly reduces the power of the model. State of the art methods in timeseries modeling have moved beyond this constraint, especially in the Gaussian process community e.g. [2,3,4,5]. Comparing to a few of these methods, or at least discussing them would be useful.\n\n\nReferences:\n[1] Kingma and Welling. Auto-encoding Variational Bayes. arXiv:1312.6114\n[2] Damianou et al. Variational Gaussian process dynamical systems. NIPS 2011.\n[3] Mattos et al. Recurrent Gaussian processes. ICLR 2016.\n[4] Frigola. Bayesian Time Series Learning with Gaussian Processes, University of Cambridge, PhD Thesis, 2015. \n[5] Frigola et al. Variational Gaussian Process State-Space Models. NIPS 2014\n\n\nOne innovation is that the prior structure of the correlation needs to be given. This is a potentially useful and also original structural component. However, it also constitutes a limitation in some sense, since it is unrealistic in many scenarios to have this prior information. Moreover, the particular regularizer that makes \"similar\" timeseries to have closeness in the state space seems problematic. Some timeseries groups might be more \"similar\" than others, and also the similarity might be of different nature across groups. These variations cannot be well captured\/distilled by a simple indicator variable e_ij. Furthermore, these variables are in practice taken to be binary (by looking at the experiments), which would make it even harder to model rich correlations. \n\nThe experiments show that the proposed method works, but they are not entirely convincing. Importantly, they do not shed enough light into the different properties of the model w.r.t its different parts. For example, the effect and sensitivity of the different regularizers. The authors state in a pre-review answer that they amended with some more results, but I can't see a revision in openreview (please let me know if I've missed it). From the performance point of view, the results are not particularly exciting, especially given the fact that it's not clear which loss is better (making it difficult to use the method in practice). \n\nIt would also be very interesting to report the optimized values of the parameters \\lambda, to get an idea of how the different losses behave.\n\nTimeseries analysis is a very well-researched area. Given the above, it's not clear to me why one would prefer to use this model over other approaches. Methodology wise, there are no novel components that offer a proven advantage with respect to past methods. The uncertainty in the states and the correlation of the time-series are the aspects which could add an advantage, but are not adequately researched in this paper.\n","label":0,"model":"human","source":"peerread","id":4896}
{"text":"In absence of authors' response, the rating is maintained.\n\n---\n\nThis paper introduces a nonlinear dynamical model for multiple related multivariate time series. It models a linear observation model conditioned on the latent variables, a linear or nonlinear dynamical model between consecutive latent variables and a similarity constraint between any two time series (provided as prior data and non-learnable). The predictions\/constraints given by the three components of the model are Gaussian, because the model predicts both the mean and the variance or covariance matrix. Inference is forward only.\n\nThe model is evaluated on four datasets, and compared to several baselines: plain auto-regressive models, feed-forward networks, RNN and dynamic factor graphs DFGs, which are RNNs with forward and backward inference of the latent variables.\n\nThe model, which introduces lateral constraints between different time series, and which predicts both the mean and covariance seems interesting, but presents two limitations.\n\nFirst of all, the paper should refer to variational auto-encoders \/ deep gaussian models, which also predict the mean and the variance during inference.\n\nSecondly, the datasets are extremely small. For example, the WHO contains only 91 times series of 52*10 = 520 time points. Although the experiments seem to suggest that the proposed model tends to outperform RNNs, the datasets are very small and the high variance in the results indicates that further experiments, with longer time series, are required. The paper could also easily be extended with more information about the model (what is the architecture of the MLP) as well as time complexity comparison between the models (especially between DFGs and this model).\n\nMinor remark:\nThe footnote 2 on page 5 seems to refer to the structural regularization term, not to the dynamical term.\n\n","label":0,"model":"human","source":"peerread","id":4897}
{"text":"The topic of the paper, model-based RL with a learned model, is important and timely. The paper is well written. I feel that the presented results are too incremental. Augmenting the frame prediction network with another head that predicts the reward is a very sensible thing to do. However neither the methodology not the results are novel \/ surprising, given that the original method of [Oh et al. 2015] already learns to successfully increment score counters in predicted frames in many games.\n\nI\u2019m very much looking forward to seeing the results of applying the learned joint model of frames and rewards to model-based RL as proposed by the authors.","label":0,"model":"human","source":"peerread","id":4898}
{"text":"The authors have combined two known areas of research - frame prediction and reward prediction - and combined them in a feedforward network trained on sequences from Atari games. The fact that this should train well is unsurprising for this domain, and the research yields no other interesting results. Pros - the paper is clearly written and the experiments are sound. Cons - there is very little novelty or contribution.","label":0,"model":"human","source":"peerread","id":4899}
{"text":"This paper introduces an additional reward-predicting head to an existing NN architecture for video frame prediction. In Atari game playing scenarios, the authors show that this model can successfully predict both reward and next frames.\n\nPros:\n- Paper is well written and easy to follow.\n- Model is clear to understand.\n\nCons:\n- The model is incrementally different than the baseline. The authors state that their purpose is to establish a pre-condition, which they achieve. But this makes the paper quite limited in scope.\n\nThis paper reads like the start of a really good long paper, or a good short paper. Following through on the future work proposed by the authors would make a great paper. As it stands, the paper is a bit thin on new contributions.","label":0,"model":"human","source":"peerread","id":4900}
{"text":"The paper extends a recently proposed video frame prediction method with reward prediction in order to learn the unknown system dynamics and reward structure of an environment. The method is tested on several Atari games and is able to predict the reward quite well within a range of about 50 steps. The paper is very well written, focussed and is quite clear about its contribution to the literature. The experiments and methods are sound. However, the results are not really surprising given that the system state and the reward are linked deterministically in Atari games. In other words, we can always decode the reward from a network that successfully encodes future system states in its latent representation. The contribution of the paper is therefore minor. The paper would be much stronger if the authors could include experiments on the two future work directions they suggest in the conclusions: augmenting training with artificial samples and adding Monte-Carlo tree search. The suggestions might decrease the number of real-world training samples and increase performance, both of which would be very interesting and impactful.","label":0,"model":"human","source":"peerread","id":4901}
{"text":"The topic of the paper, model-based RL with a learned model, is important and timely. The paper is well written. I feel that the presented results are too incremental. Augmenting the frame prediction network with another head that predicts the reward is a very sensible thing to do. However neither the methodology not the results are novel \/ surprising, given that the original method of [Oh et al. 2015] already learns to successfully increment score counters in predicted frames in many games.\n\nI\u2019m very much looking forward to seeing the results of applying the learned joint model of frames and rewards to model-based RL as proposed by the authors. ","label":0,"model":"human","source":"peerread","id":4902}
{"text":"This paper presents experimental results from an EdgeBoxes + Fast R-CNN detector on the task of localizing pedestrians. It uses an AlexNet (CaffeNet) backbone architecture modified to include batch normalization. Experimental results are presented on the INRIA and ETH datasets.\n\nPros\n- The paper is clearly written and easy to follow\n\nCons\n- The paper's two contributions are too minor to merit publication\n- Experimental results should include at least the Caltech pedestrian dataset but likely also the KITTI pedestrian dataset\n- Recent work from ECCV 2016 [a], with superior results and much more experimental evaluation, is not cited or discussed\n\nMy rating is due primarily to the lack luster contributions. The first claimed contribution is the use of EdgeBoxes as proposals for pedestrian detection. Unless the result of this choice produced a truly surprising experimental result, this is simply too minor to be considered a contribution. Moreover, if this choice is important, then the paper should justify it by showing that other proposal methods (of which there are a great many in addition to Selective Search and Edge Boxes) are worse performing in some regard (speed, accuracy, memory, etc.). The second claimed contribution is the use of batch normalization (BN) in their network architecture. There is a case to be made that BN hasn't been explored in Fast R-CNN. However, if the goal of the paper was to thoroughly explore BN + Fast R-CNN, then why focus narrowly on pedestrian detection? Instead, it should focus more broadly on generic object category detection for which there are well established Fast R-CNN baselines on PASCAL VOC and COCO. The use of BN + Fast R-CNN only for pedestrian detection does not provide much signal about this choice. There are also potential technical issues that are not discussed. BN is typically avoided in Fast R-CNN because the batch size seen by most of the network is usually only one or two images. This is likely too few images for the naive application of BN.\n\n\n[a] \"Is Faster R-CNN Doing Well for Pedestrian Detection?\" Zhang et al.","label":0,"model":"human","source":"peerread","id":4903}
{"text":"Four knowledgable reviewers recommend rejection due to too weak of a contribution. The authors did not post a rebuttal. The AC agrees with the reviewers' recommendation.","label":0,"model":"human","source":"peerread","id":4904}
{"text":"Paper summary: the authors proposed to use EdgeBoxes + Fast-RCNN with\nbatch normalization for pedestrian detection\n\nReview summary: results do not cover enough datasets, the reported\nresults do not improve over state of the art, writing is poor, and\noverall the work lacks novelty. This is a clear reject.\n\nPros:\n* Shows that using batch normalization does improve results\n\nCons:\n* Only results on ETH and INRIA. Should include Caltech or KITTI.\n* Reported results are fair, but not improving over state of the art\n* Overall idea of limited interest when considering works like S.\nZhang CVPR 2016 (Fast R-CNN for pedestrian detection) and L. Zhang\nECCV 2017 (Faster R-CNN for pedestrian detection)\n* Issues with the text quality\n* Limited takeaways\n\nQuality: low\nClarity: fair, but poor English\nOriginality: low\nSignificance: low\n\nFor acceptance at future conferences, this work would need more\npolish, improving over best known results on INRA, ETH, and Caltech or\nKITTI. And ideally, present additional new insights.\n\nMinor comments:\n* The text lacks polish. E.g. influent -> influence, has maken ->\nmade, is usually very important -> is important, achieve more\nexcellent results -> achieve better results; etc. Please consider\nasking help from a native speaker for future submissions. There are\nalso non-sense sentences such as \u201cit is computational\u201d.\n* Citations should be in parentheses\n* Some of the citations are incorrect because the family name is in\nthe wrong position, e.g. Joseph Lim, Lawrence Zitnick, and Rodrigo Benenson.","label":0,"model":"human","source":"peerread","id":4905}
{"text":"This paper proposes a pedestrian detection method using Fast RCNN framework with batch normalization, where EdgeBoxes is used to collect pedestrian proposals instead of selective search as used in the original Fast RCNN method. The proposed method is evaluated in INRIA and ETH dataset.\n\nPros:\n- The proposed method shows good performance(but not state-of-the-art).\n\nCons:\n- Lack of novelty. Fast RCNN and its variants (e.g. FasterRCNN, ","label":0,"model":"human","source":"peerread","id":4906}
{"text":"The authors apply the commonly used Fast RCNN detection system to pedestrian detection. They use \u201cEdgeBoxes\u201d object proposals and incorporate batch norm into their network. Results are shown on the INRIA and ETH pedestrian datasets. They are reasonable but not state-of-the-art. Results are not shown on Caltech Pedestrians, the standard modern dataset used to evaluate pedestrian detection. Perhaps more importantly, the paper has no novelty.\n\nThe detection system described in this paper is a standard application of Fast RCNN to pedestrian detection. The implementation is not state-of-the-art, and there is no novelty in this work. EdgeBoxes has been used with Fast RCNN before. The authors don\u2019t seem to be aware of more recent developments in object detection, including Faster RCNN (","label":0,"model":"human","source":"peerread","id":4907}
{"text":"The starting point of this work is the understanding that by having decorrelated neurons (e.g. neurons that only fire on background, or only on foreground regions) one provides independent pieces of information to the subsequent decisions. As such one gives \"complementary viewpoints\" of the input to the subsequent layers, which can be thought of as performing ensembling\/expert combination within the model, rather than using an ensemble of networks. \n\nFor this, the authors propose a sensible method to decorrelate the activations of intermediate neurons, with the aim of delivering complementary inputs to the final classification layers: they split intermediate neurons to a \"foreground\" and a \"background\" subset, and append side-losses that force them to be zero on background and foreground pixels respectively. \n\nThey demonstrate that this can improve classification on a mid-scale classification example (a fraction of imagenet, and a ResNet with 18, rather than 150 layers), when compared to a \"vanilla\" baseline that does not use these losses.\n\nI enjoyed reading the paper because the idea is simple, smart, and seems to be effective. \nBut there are a few concerns;\n-firstly, the way of doing this seems very particular to vision. In vision one knows that masking the features (during both training and testing) helps, e.g.","label":0,"model":"human","source":"peerread","id":4908}
{"text":"This paper was reviewed by three experts. While they find interesting ideas in the manuscript, all three point to deficiencies (lack of clean experiments, clarity in the manuscript, etc) and recommend rejection. I believe there are promising ideas here, and this manuscript will be stronger for a future deadline.","label":0,"model":"human","source":"peerread","id":4909}
{"text":"We've already updated the paper. \n- The abstract and introduction have been rewritten with more explanation (on the motivation) and comparison. \n- The difference from ensemble models was highlighted in the related works.\n- We found that the Fig 1. is a bit confusing and have already updated it in the revised revision.\n- Eqn 3. has been corrected.\n- New results on ImageNet dataset.","label":0,"model":"human","source":"peerread","id":4910}
{"text":"This paper proposes to learn groups of orthogonal features in a convnet by penalizing correlation among features in each group.  The technique is applied in the setting of image classification with \u201cprivileged information\u201d in the form of foreground segmentation masks, where the model is trained to learn orthogonal groups of foreground and background features using the correlation penalty and an additional \u201cbackground suppression\u201d term.\n\n\nPros:\n\nProposes a \u201cgroup-wise model diversity\u201d loss term which is novel, to my knowledge.\n\nThe use of foreground segmentation masks to improve image classification is also novel.\n\nThe method is evaluated on two standard and relatively large-scale vision datasets: ImageNet and PASCAL VOC 2012.\n\n\nCons:\n\nThe evaluation is lacking.  There should be a baseline that leaves out the background suppression term, so readers know how much that term is contributing to the performance vs. the group orthogonal term.  The use of the background suppression term is also confusing to me -- it seems redundant, as the group orthogonality term should already serve to suppress the use of background features by the foreground feature extractor.\n\nIt would be nice to see the results with \u201cIncomplete Privileged Information\u201d on the full ImageNet dataset (rather than just 10% of it) with the privileged information included for the 10% of images where it\u2019s available.  This would verify that the method and use of segmentation masks remains useful even in the regime of more labeled classification data.\n\nThe presentation overall is a bit confusing and difficult to follow, for me.  For example, Section 4.2 is titled \u201cA Unified Architecture: GoCNN\u201d, yet it is not an overview of the method as a whole, but a list of specific implementation details (even the very first sentence).\n\nMinor: calling eq 3 a \u201cregression loss\u201d and writing \u201c||0 - x||\u201d rather than just \u201c||x||\u201d is not necessary and makes understanding more difficult -- I\u2019ve never seen a norm regularization term written this way or described as a \u201cregression to 0\u201d.\n\nMinor: in fig. 1 I think the FG and BG suppression labels are swapped: e.g., the \u201csuppress foreground\u201d mask has 1s in the FG and 0s in the BG (which would suppress the BG, not the FG).\n\n\nAn additional question: why are the results in Table 4 with 100% privileged information different from those in Table 1-2?  Are these not the same setting?\n\nThe ideas presented in this paper are novel and show some promise, but are currently not sufficiently ablated for readers to understand what aspects of the method are important.  Besides additional experiments, the paper could also use some reorganization and revision for clarity.\n\n===============\n\nEdit (1\/29\/17): after considering the latest revisions -- particularly the full ImageNet evaluation results reported in Table 5 demonstrating that the background segmentation 'privileged information' is beneficial even with the full labeled ImageNet dataset -- I've upgraded my rating from 4 to 6.\n\n(I'll reiterate a very minor point about Figure 1 though: I still think the \"0\" and \"1\" labels in the top part of the figures should be swapped to match the other labels.  e.g., the topmost path in figure 1a, with the text \"suppress foreground\", currently has 0 in the background and 1 in the foreground, when one would want the reverse of this to suppress the foreground.)","label":0,"model":"human","source":"peerread","id":4911}
{"text":"This paper proposes a modification to ConvNet training so that the feature activations before the linear classifier are divided into groups such that all pairs of features across all pairs of groups are encouraged to have low statistical correlation. Instead of discovering the groups automatically, the work proposes to use supervision, which they call privileged information, to assign features to groups in a hand-coded fashion. The developed method is applied to image classification.\n\nPros:\n- The paper is clear and easy to follow\n- The experimental results seem to show some benefit from the proposed approach\n\nCons:\n(1) The paper proposes one core idea (group orthogonality w\/ privileged information), but then introduces background feature suppression without much motivation and without careful experimentation\n(2) No comparison with an ensemble\n(3) Full experiments on ImageNet under the \"partial privileged information\" setting would be more impactful\n\nThis paper is promising and I would be willing to accept an improved version. However, the current version lacks focus and clean experiments.\n\nFirst, the abstract and intro focus on the need to replace ensembles with a single model that has diverse (ensemble like) features. The hope is that such a model will have the same boost in accuracy, while requiring fewer FLOPs and less memory. Based on this introduction, I expect the rest of the paper to focus on this point. But it does not; there are no experimental results on ensembles and no experimental evidence that the proposed approach in able to avoid the speed and memory cost of ensembles while also retaining the accuracy benefit.\n\nSecond, the technical contribution of the paper is presented as group orthogonality (GO). However, in Sec 4.1 the idea of background feature suppression is introduced. While some motivation for it is given, the motivation does not tie into GO. GO does not require bg suppression and the introduction of it seems ad hoc. Moreover, the experiments never decouple GO and bg suppression, so we are unable to understand how GO works on its own. This is a critical experimental flaw in my reading.\n\nMinor suggestions \/ comments:\n- The equation in definition 2 has an incorrect normalizing factor (1\/c^(k)^2)\n- Figure 1 seems to have incorrect mask placements. The top mask is one that will mask out the background and only allow the fg to pass","label":0,"model":"human","source":"peerread","id":4912}
{"text":"The starting point of this work is the understanding that by having decorrelated neurons (e.g. neurons that only fire on background, or only on foreground regions) one provides independent pieces of information to the subsequent decisions. As such one gives \"complementary viewpoints\" of the input to the subsequent layers, which can be thought of as performing ensembling\/expert combination within the model, rather than using an ensemble of networks. \n\nFor this, the authors propose a sensible method to decorrelate the activations of intermediate neurons, with the aim of delivering complementary inputs to the final classification layers: they split intermediate neurons to a \"foreground\" and a \"background\" subset, and append side-losses that force them to be zero on background and foreground pixels respectively. \n\nThey demonstrate that this can improve classification on a mid-scale classification example (a fraction of imagenet, and a ResNet with 18, rather than 150 layers), when compared to a \"vanilla\" baseline that does not use these losses.\n\nI enjoyed reading the paper because the idea is simple, smart, and seems to be effective. \nBut there are a few concerns;\n-firstly, the way of doing this seems very particular to vision. In vision one knows that masking the features (during both training and testing) helps, e.g. ","label":0,"model":"human","source":"peerread","id":4913}
{"text":"This paper addresses the practical problem of generating rare or unseen words in the context of language modeling. Since language follows a Zipf\u2019s law, most approaches limit the vocabulary (because of computation reasons) and hence rare words are often mapped to a UNK token. Rare words are especially important in context of applications such as question answering. MT etc. This paper proposes a language modeling technique which incorporates facts from knowledge bases (KBs) and thus has the ability to generate (potentially unseen) words from KBs. This paper also releases a dataset by aligning words with Freebase facts and corresponding Wikipedia descriptions.\n\nThe model first selects a KB fact based on the previously generated words and facts. Based on the selected fact, it then predicts whether to generate a word based on the vocabulary or to output a symbolic word from the KB. For the latter, the model is trained to predict the position of the word from the fact description.\n\nOverall the paper could use some rewriting especially the notations in section 3. The experiments are well executed and they definitely get good results. The heat maps at the end are very insightful. \n\nComments\n\nThis contributions of this paper would be much stronger if it showed improvements in a practical applications such as Question Answering (although the paper clearly mentions that this technique could be applied to improve QA)\nIn section 3, it is unclear why the authors refer the entity as a \u2018topic'. This makes the text a little confusing since a topic can also be associated with something abstract, but in this case the topic is always a freebase entity. \nIs it really necessary to predict a fact at every step before generating a word. In other words, how many distinct facts on average does the model choose to generate a sentence. Intuitively a natural language sentence would be describe few facts about an entity. If the fact generation step could be avoided (by adding a latent variable which decides if the fact should be generated or not), the model will also be faster.\nIn equation 2, the model has to make a hard decision to choose the fact. For this to be end to end trained, every word needs to be annotated with a corresponding fact which might not be always a realistic scenario. For e.g., in domains such as social media text.\nLearning position embeddings for copying knowledge words seems a little counter-intuitive. Does the sequence of knowledge words follow any particular structure like word O_2 is always the last name (e.g. Obama).\nIt would also be nice to compare to char-level LM's which inherently solves the unknown token problem.","label":0,"model":"human","source":"peerread","id":4914}
{"text":"This work introduces a combination of a LM with knowledge based retrieval system. This builds upon the recent trend of incorporating pointers and external information into generation, but includes some novelty, making the paper \"different and more interesting\". Generally though the reviewers found the clarity of the work to be sufficiently an issue that no one strongly defended its inclusion.\n \n Pros:\n - The reviewers seemed to like the work and particularly the problem space. Issues were mainly on presentation and experiments. \n \n Mixed:\n - Reviewers were divided on experimental quality. The work does introduce a new dataset, but reviewers would also have liked use on some existing tasks. \n \n Cons:\n - Clarity and writing issues primarily. All reviewers found details missing and generally struggled with comprehension.\n - Novelty was a question. Impact of work could also be improved by more clearly defining new contributions","label":0,"model":"human","source":"peerread","id":4915}
{"text":"Dear Reviewers and AreaChair, \n\nApproaching to the deadline, I'd like to remind the reviewers of the fact that the revised version of the paper (that we uploaded in Dec. 27) has significant improvements with respect to most of the comments pointed by the reviewers, which are mainly in terms of the writing clarity in Section 3.","label":0,"model":"human","source":"peerread","id":4916}
{"text":"This paper proposes to incorporate knowledge base facts into language modeling, thus at each time step, a word is either generated from the full vocabulary or relevant KB entities.\n\nThe authors demonstrate the effectiveness on a new generated dataset WikiFacts which aligns Wikipedia articles with Freebase facts.  The authors also suggest a modified perplexity metric which penalizes the likelihood of unknown words.\n\nAt a high level, I do like the motivation of this paper -- named entity words are usually important for downstream tasks, but difficult to learn solely based on statistical co-occurrences. The facts encoded in KB could be a great supply for this.\n\nHowever, I find it difficult to follow the details of the paper (mainly Section 3) and think the paper writing needs to be much improved. \n- I cannot find where  f_{symbkey} \/ f_{voca} \/ f_{copy} are defined\n- w^v, w^s are confusing.\n- e_k seems to be the average of all previous fact embeddings? It is necessary to make it clear enough.\n- (h_t, c_t) = f_LSTM(x_{t\u22121}, h_{t\u22121})  c_t is not used?\n- The notion of \u201cfact embeddings\u201d is also not that clear (I understand that they are taken as the concatenation of relation and entity (object) entities in the end).  For the anchor \/ \u201ctopic-itself\u201d facts, do you learn the embedding for the special relations and use the entity embeddings from TransE?\n\nOn generating words from KB entities (fact description), it sounds a bit strange to me to generate a symbol position first.  Most entities are multiple words, and it is necessary to keep that order. Also it might be helpful to incorporate some prior information, for example, it is common to only mention \u201cObama\u201d for the entity \u201cBarack Obama\u201d?\n","label":0,"model":"human","source":"peerread","id":4917}
{"text":"The paper proposes an evolution upon traditional Recurrent Language Models to give the capability to deal with unknown words. It is done by pairing the traditional RNNLM with a module operating on a KB and able to copy from KB facts to generate unseen words. It is shown to be efficient and much better than plain RNNLM on a new dataset.\n\nThe writing could be improved. The beginning of Section 3 in particular is hard to parse.\n\nThere have been similar efforts recently (like \"Pointer Sentinel Mixture Models\" by Merity et al.) that attempt to overcome limitations of RNNLMs with unknown words; but they usually do it by adding a mechanism to copy from a longer past history. The proposal of the current paper is different and more interesting to me in that it try to bring knowledge from another source (KB) to the language model. This is harder because one needs to leverage the large scale of the KB to do so. Being able to train that conveniently is nice.\n\nThe architecture appears sound, but the writing makes it hard to fully understand completely so I can not give a higher rating. \n\n\nOther comments:\n* How to cope with the dependency on the KB? Freebase is not updated anymore so it is likely that a lot of the new unseen words in the making are not going to be in Freebase.\n* What is the performance on standard benchmarks like Penn Tree Bank?\n* How long is it to train compare to a standard RNNLM?\n* What is the importance of the knowledge context $e$?\n* How is initialized the fact embedding $a_{t-1}$ for the first word?\n* When a word from a fact description has been chosen as prediction (copied), how is it encoded in the generation history for following predictions if it has no embedding (unknown word)? In other words, what happens if \"Michelle\" in the example of Section 3.1 is not in the embedding dictionary, when one wants to predict the next word?\n\n\n\n","label":0,"model":"human","source":"peerread","id":4918}
{"text":"\nThis paper addresses the practical problem of generating rare or unseen words in the context of language modeling. Since language follows a Zipf\u2019s law, most approaches limit the vocabulary (because of computation reasons) and hence rare words are often mapped to a UNK token. Rare words are especially important in context of applications such as question answering. MT etc. This paper proposes a language modeling technique which incorporates facts from knowledge bases (KBs) and thus has the ability to generate (potentially unseen) words from KBs. This paper also releases a dataset by aligning words with Freebase facts and corresponding Wikipedia descriptions.\n\nThe model first selects a KB fact based on the previously generated words and facts. Based on the selected fact, it then predicts whether to generate a word based on the vocabulary or to output a symbolic word from the KB. For the latter, the model is trained to predict the position of the word from the fact description.\n\nOverall the paper could use some rewriting especially the notations in section 3. The experiments are well executed and they definitely get good results. The heat maps at the end are very insightful. \n\nComments\n\nThis contributions of this paper would be much stronger if it showed improvements in a practical applications such as Question Answering (although the paper clearly mentions that this technique could be applied to improve QA)\nIn section 3, it is unclear why the authors refer the entity as a \u2018topic'. This makes the text a little confusing since a topic can also be associated with something abstract, but in this case the topic is always a freebase entity. \nIs it really necessary to predict a fact at every step before generating a word. In other words, how many distinct facts on average does the model choose to generate a sentence. Intuitively a natural language sentence would be describe few facts about an entity. If the fact generation step could be avoided (by adding a latent variable which decides if the fact should be generated or not), the model will also be faster.\nIn equation 2, the model has to make a hard decision to choose the fact. For this to be end to end trained, every word needs to be annotated with a corresponding fact which might not be always a realistic scenario. For e.g., in domains such as social media text.\nLearning position embeddings for copying knowledge words seems a little counter-intuitive. Does the sequence of knowledge words follow any particular structure like word O_2 is always the last name (e.g. Obama).\nIt would also be nice to compare to char-level LM's which inherently solves the unknown token problem. ","label":0,"model":"human","source":"peerread","id":4919}
{"text":"This paper introduces the concept of fuzzy paraphrases to aid in the learning of distributed word representations from a corpus augmented by a lexicon or ontology. Sometimes polysemy is context-dependent, but prior approaches have neglected this fact when incorporating external paraphrase information during learning. The main idea is to introduce a function that essentially judges the context-sensitivity of paraphrase candidates, down-weighting those candidates that depend strongly on context. This function is inferred from bilingual translation agreement.\n\nThe main argumentation leading to the model selection is intuitive, and I believe that the inclusion of good paraphrases and the elimination of bad paraphrases during training should in principle improve word representation quality. However, the main questions are how well the proposed method achieves this goal, and, even if it achieves it well, whether it makes much difference in practical terms.\n\nRegarding the first question, I am not entirely convinced that the parameterization of the control function f(x_ij) is optimal. It would have been nice to see some experiments investigating different choices, in particular some baselines where the effect of f is diminished (so that it reduces to f=1 in the limit) would have been interesting. I also feel like there would be a lot to gain from having f be a function of the nearby word embeddings, though this would obvious incur a significant slowdown. (See for example 'Efficient Non-parametric Estimation of Multiple Embeddings per Word in Vector Space' by Neelakantan et al, which should probably be cited.) As it stands, the experimental results do not clearly distinguish the fuzzy paraphrase approach from prior work, i.e. tables 3 and 4 do not show major trends one way or the other.\n\nRegarding the second question, it is hard to draw many conclusions from analogy tasks alone, especially when effects unrelated to good\/bad paraphrasing such as corpus size\/content, window size, vocabulary size, etc., can have an outsize effect on performance. \n\nOverall, I think this is a good paper presenting a sensible idea, but I am not convinced by the experiments that the specific approach is achieving its goal. With some improved experiments and analysis, I would wholeheartedly recommend this paper for acceptance; as it stands, I am on the fence.","label":0,"model":"human","source":"peerread","id":4920}
{"text":"The reviewers agree that the paper's clarity and experimental evaluation can be improved.","label":0,"model":"human","source":"peerread","id":4921}
{"text":"This paper proposes a method for estimating the context sensitivity of paraphrases and uses that to inform a word embedding learning model. The main idea and model are presented convincingly and seem plausible. The main weaknesses of the paper are shortcomings in the experimental evaluation and in the model exploration. The evaluation does not convincingly determine whether the model is a significant improvement over simpler methods (particularly those that do not require the paraphrase database!). Likewise, the model section did not convince me that this was the most obvious model formulation to try. The paper would be stronger if model choices were explained more convincingly or - better yet - alternatives were explored.\n\nOn balance I lean towards rejecting the paper and encouraging the authors to submit a revised and improved version at a near point in the future.\n\nDetailed\/minor points below:\n\n1) While the paper is grammatically mostly correct, it would benefit from revision with the help of a native English speaker. In its current form long sections are very difficult to understand due to the unconventional sentence structure.\n2) The tables need better and more descriptive labels.\n3) The results are somewhat inconclusive. Particularly in the analogy task in Table 4 it is surprising that CBOW does better on the semantic aspect of the task than your embeddings which are specifically tailored to be good at this?\n4) Why was \"Enriched CBOW\" not included in the analogy task?\n5) In the related work section several papers are mentioned that learn embeddings from a combination of lexica and corpora, yet it is repeatedly said that this was the first work of such a kind \/ that there hasn't been enough work on this. That feels a little misleading.","label":0,"model":"human","source":"peerread","id":4922}
{"text":"This paper tries to leverage an external lexicon \/ knowledge base to improve corpus-based word representations by determining (in a fuzzy way) which potential paraphrase is the most appropriate in a particular context.\n\nI think this paper is a bit lost in translation. The grammatical and storytelling styles made it really difficult for me to concentrate, and even unintelligible at times. One of the most important criteria in a conference paper is to communicate one's ideas clearly; unfortunately, I do not feel that this paper meets that standard.\n\nIn addition, the evaluation is rather lacking. There are many ways to evaluate word representations, and Google's analogy dataset has many issues (see, for example, Linzen's paper from RepEval 2016, as well as Drozd et al., COLING 2016).\n\nFinally, this work does not provide any qualitative result or motivation. Why does this method work better? Where does it fail? What have we learned about word representations \/ lexicons \/ corpus-based methods in general?","label":0,"model":"human","source":"peerread","id":4923}
{"text":"This paper explores different strategies for instance-level image retrieval with deep CNNs. The approach consists of extracting features from a network pre-trained for image classification (e.g. VGG), and post-process them for image retrieval. In other words, the network is off-the-shelf and solely acts as a feature extractor. The post-processing strategies are borrowed from traditional retrieval pipelines relying on hand-crafted features (e.g. SIFT + Fisher Vectors), denoted by the authors as \"traditional wisdom\".\n\nSpecifically, the authors examine where to extract features in the network (i.e. features are neurons activations of a convolution layer), which type of feature aggregation and normalization performs best, whether resizing images helps, whether combining multiple scales helps, and so on. \n\nWhile this type of experimental study is reasonable and well motivated, it suffers from a huge problem. Namely it \"ignores\" 2 major recent works that are in direct contradictions with many claims of the paper ([a] \"End-to-end Learning of Deep Visual Representations for Image Retrieval\" by  Gordo et al. and [b] \"CNN Image Retrieval Learns from BoW: Unsupervised Fine-Tuning with Hard Examples\" by Radenovi\u0107 et al., both ECCV'16 papers). These works have shown that training for retrieval can be achieved with a siamese architectures and have demonstrated outstanding performance. As a result, many claims and findings of the paper are either outdated, questionable or just wrong.\n\nHere are some of the misleading claims: \n\n  - \"Features aggregated from these feature maps have been exploited for image retrieval tasks and achieved state-of-the-art performances in recent years.\"\n  Until [a] (not cited), the state-of-the-art was still largely dominated by sparse invariant features based methods (see last Table in [a]).\n  \n  - \"the proposed method [...] outperforms the state-of-the-art methods on four typical datasets\"\n  That is not true, for the same reasons than above, and also because the state-of-the-art is now dominated by [a] and [b].\n  \n  - \"Also in situations where a large numbers of training samples are not available, instance retrieval using unsupervised method is still preferable and may be the only option.\".\n  This is a questionable opinion. The method exposed in \"End-to-end Learning of Deep Visual Representations for Image Retrieval\" by Gordo et al. outperforms the state-of-the-art on the UKB dataset (3.84 without QE or DBA) whereas it was trained for landmarks retrieval and not objects, i.e. in a different retrieval context. This demonstrates that in spite of insufficient training data, training is still possible and beneficial.\n\n  - Finally, most findings are not even new or surprising (e.g. aggregate several regions in a multi-scale manner was already achieved by Tolias at al, etc.). So the interest of the paper is limited overall.\n\nIn addition, there are some problems in the experiments. For instance, the tuning experiments are only conducted on the Oxford dataset and using a single network (VGG-19), whereas it is not clear whether these conditions are well representative of all datasets and all networks (it is well known that the Oxford dataset behaves very differently than the Holidays dataset, for instance). In addition, tuning is performed very aggressively, making it look like the authors are tuning on the test set (e.g. see Table 3). \n\nTo conclude, the paper is one year too late with respect to recent developments in the state of the art.","label":0,"model":"human","source":"peerread","id":4924}
{"text":"The paper conducts a detailed evaluation of different CNN architectures applied to visual instance retrieval. The authors consider various deep neural network architectures, with a focus on architectures pre-trained for image classification. \n \n An important concern of the reviewers is the relevance of the evaluation given the recent impressive experimental results of deep neural networks trained end-to-end for visual instance retrieval by Gordo et al. \"End-to-end Learning of Deep Visual Representations for Image Retrieval\". Another concern is the novelty of the proposed evaluation given the evaluation of the performance for visual instance retrieval of deep neural network pre-trained for image classification performed in Paulin et al. \"Convolutional Patch Representations for Image Retrieval: An Unsupervised Approach\". \n \n A revision of the paper, following the reviewers' suggestions, will generate a stronger submission to a future venue.","label":0,"model":"human","source":"peerread","id":4925}
{"text":"The paper conducts a detailed evaluation of different CNN architectures applied to image retrieval. The authors focus on testing various architectural choices, but do not propose or compare to end-to-end learning frameworks.\n\nTechnically, the contribution is clear, particularly with the promised clarifications on how multiple scales are handled in the representation. However, I am still not entirely clear whether there would be a difference in the multi-scale settting for full and cropped queries.\n\nWhile the paper focuses on comparing different baseline architectures for CNN-based image retrieval, several recent papers have proposed to learn end-to-end representations specific for this task, with very good result (see for instance the recent work by Gordo et al. \"End-to-end Learning of Deep Visual Representations for Image Retrieval\"). The authors clarify that their work is orthogonal to papers such as Gordo et al. as they assess instead the performance of networks pre-trained from image classification. In fact, they also indicate that image retrieval is more difficult than image classification -- this is because it is performed by using features originally trained for classification. I can partially accept this argument. However, given the results in recent papers, it is clear than end-to-end training is far superior in practice and it is not clear the analysis developed by the authors in this work would transfer or be useful for that case as well.\n","label":0,"model":"human","source":"peerread","id":4926}
{"text":"Authors investigate how to use pretrained CNNs for retrieval and perform an extensive evaluation of the influence of various parameters. For detailed comments on everything see the questions I posted earlier. The summary is here:\n\nI don't think we learn much from this paper: we already knew that we should use the last conv layer, we knew we should use PCA with whitening, we knew we should use original size images (authors say Tolias didn't do this as they resized the images, but they did this exactly for the same reason as authors didn't evaluate on Holidays - the images are too big. So they basically used \"as large as possible\" image sizes, which is what this paper effectively suggests as well), etc. This paper essentially concatenates methods that people have already used, and performs some more parameter tweaking to achieve the state-of-the-art (while the tweaking is actually performed on the test set of some of the tests).\n\nThe setting of the state-of-the-art results is quite misleading as it doesn't really come from the good choice of parameters, but mainly due to the usage of the deeper VGG-19 network. \n\nFurthermore, I don't think it's sufficient to just try one network and claim these are the best practices for using CNNs for instance retrieval - what about ResNet, what about Inception, I don't know how to apply any of these conclusions for those networks, and would these conclusions even hold for them. Furthermore the parameter tweaking was done on Oxford, I really can't tell what conclusions would we get if we tuned on UKB for example. So a more appropriate paper title would be \"What are the best parameter values for VGG-19 on Oxford\/Paris benchmarks?\" - I don't think this is sufficiently novel nor interesting for the community.\n","label":0,"model":"human","source":"peerread","id":4927}
{"text":"\n5)\nI agree with other reviewers on the lack of comparison with Gordo et al and Radenovic et al, though I understand that authors' arguments are that they do not want to train their networks (though then comparing with Arandjelovic et al. also doesn't make sense). It's still worth citing these papers and commenting on them. Also, with such an extensive set of experiments, it's a bit arguable if authors don't really do training - they don't do the canonical SGD, but they essentially perform grid search for parameters on the test (see question 3).\n\n6)\nI'm not sure what did we actually learn from this paper. To use the last conv? We knew that before as all recent papers do this (Arandjelovic et al, Tolias et al, Gordo et al, Radenovic et al, Babenko and Lempitsky, ..). That using original image sizes is important? We knew this as well, early works (Babenko et al 2014, etc) used smaller images while all recent works apply the networks convolutionally over original size images (e.g. Tolias et al have this experiment in table 1). That one should use PCA with whitening (and if possible learn whitening on the test set)? We knew this already as well. So the only two things that haven't been done in exactly the same way as people did it before is the multi-scale pooling (though obviously various other similar versions exist), and the exploration of max\/sum pooling with l1 or l2 normalization (though the experiments in table 1 are basically ignored as sum-l1 works the best there, but authors then say that actually later they notice that for multiscale max-l2 works best). Actually the most interesting part for me, one that I can actually say I didn't know and don't think anyone knew, is figure 3.\n\n7)\nI think it's a bit of an overstatement to call this paper 'best practice for CNNs' when only a single CNN architecture, VGG-19, is considered. What is the best practice for other models, e.g. ResNet, Inception? Presumably the last conv is likely to be best though for ResNet it's not that clear, and I'm not sure if sum vs max pooling would change as those two networks were trained with sum pooling, and I'm not sure if any of the other conclusions hold either. This is more of a surgery of VGG-19 than best practices for CNNs in general.\n\n8)\nOn a more philosophical level, and not only aimed at authors but also at others who are potentially reading this - this conference is about learning representations, while no learning is being performed. Taking CNNs as black boxes and tweaking the inputs and outputs in different ways with different normalizations is much more like using hand-engineered features like SIFT (replace black-box SIFT extractor with black-box CNN) than actually doing Deep Learning. I'm not saying this type of paper shouldn't exist as it's good to know what works best, but my preference in terms of what papers I would like to see in the future is:\na) There have been too many papers for using CNNs as black-boxes, I hoped we are finally over with this\nb) For ICLR I think one should actually do some training, e.g. after we figure out the best image representation, now train the whole system end-to-end and see if you can improve the performance.\nc) Design architectures which are specifically aimed at image retrieval - maybe something different than CNNs for classification pops up?\nd) Figure out ways to train CNNs for retrieval, we know how to do it for classification by paying people to label millions of images, can we do something better for retrieval? (though this is to some extend addressed now by Arandjelovic et al, Gordo et al and Radenovic et al).\n\n\nOther minor comments:\n\n- I was also surprised by the \"harder than category retrieval\" statement, as reviewer 3. I wouldn't go as far as saying that the opposite is true either, the two just cannot be compared so easily.\n- Inconsistencies of references (e.g. \"Y. Lecun\" vs \"Ross Girshick\", \"CVPR\" versus \"Computer\nVision and Pattern Recognition\", ..\n","label":0,"model":"human","source":"peerread","id":4928}
{"text":"In this paper the authors propose various techniques to sample visualizations from generative models with high dimensional latent spaces like VAEs and GANs. For example, the authors highlight the well known but often not sufficiently appreciated fact that the probability mass of high dimensional Gaussian distributions concentrates near a thin hyper-shell with a certain radius. They therefore propose to use spherical interpolations (great arcs) instead of the commonly used linear interpolations. In a similar spirit they propose a visualisation for analogies and techniques to reinforce structure in VAE latent spaces.\n\nI find it hard to give clear recommendation for this paper: On the one hand I enjoyed reading it and I might want use some of the proposals (e.g. spherical interpolations; J-diagrams) in future work of mine. On the other hand, it\u2019s obvious that this paper is not a typical machine learning paper; it does not propose a new model, or training method, or provide (theoretical\/empirical) insight and it does not have the scientific quality and depth I\u2019ve seen in many other ICLR submissions. But it does more than just describing useful \u201ctricks\u201d. And all things considered I think this paper deserves a wider audience (but  I'm not convinced that ICLR is the right venue)","label":0,"model":"human","source":"peerread","id":4929}
{"text":"This paper proposes some interesting ideas about visualizing latent-variable models. The paper is nicely written and presented, but the originality and importance of the work isn't enough. Also, neither the reviewers nor I were convinced that spherical interpolation makes more sense than linear interpolation.","label":0,"model":"human","source":"peerread","id":4930}
{"text":"This paper proposes a variety of techniques for visualizing learned generative models, focussing specifically on VAE and GAN models. This paper is somewhat challenging to assess since it doesn't propose a new algorithm, model, application etc. On the one hand these techniques will be highly relevant to the generative modeling community and I think this paper deserves a wide audience. The techniques proposed are simple, well explained, and of immediate use to those working on generative models. However, I'm not sure the paper is appropriate for an ICLR conference track as it doesn't provide any greater theoretical insights into sampling generative models and there are no comparisons \/ quantitative evaluations of the techniques proposed. Overall, I'm very much on the fence since I think the techniques are useful and this paper should be read by those interested in generating modeling. I would be willing to increase my core if the author could present a case for why ICLR is an appropriate venue for this work.","label":0,"model":"human","source":"peerread","id":4931}
{"text":"This paper proposed a set of different things under the name of \"sampling generative models\", focusing on analyzing the learned latent space and synthesizing desirable output images with certain properties for GANs.  This paper does not have one single clear message or idea, but rather proposed a set of techniques that seem to produce visually good looking results.  While this paper has some interesting ideas, it also has a number of problems.\n\nThe spherical interpolation idea is interesting, but after a second thought this does not make much sense.  The proposed slerp interpolation equation (page 2) implicitly assumes that the two points q1 and q2 lie on the same sphere, in which case the parameter theta is the angle corresponding to the great arc connecting the two points on the sphere.  However, the latent space of a GAN, no matter trained with a uniform distribution or a Gaussian distribution, is not a distribution on a sphere, and many points have different distances to the origin.  The author's justification for this comes from the well known fact that in high dimensional space, even with a uniform distribution most points lie on a thin shell in the unit cube.  This is true because in high-dimensional space, the outer shell takes up most of the volume in space, and the inner part takes only a very small fraction of the space, in terms of volume.  This does not mean the density of data in the outer shell is greater than the inner part, though.  In a uniform distribution, the data density should be equal everywhere, a point on the outer shell is not more likely than a point in the inner part.  Under a Gaussian model, the data density is on the other hand higher in the center and much lower on the out side.  If we have a good model of data, then sampling the most likely points from the model should give us plausible looking samples.  In this sense, spherical interpolation should do no better than the normally used linear interpolation.  From the questions and answers it seems that the author does not recognize this distinction.  The results shown in this paper seem to indicate that spherical interpolation is better visually, but it is rather hard to make any concrete conclusions from three pairs of examples.  If this is really the case then there must be something else wrong about our understanding of the learned model.\n\nAside from these, the J-diagram and the nearest neighbor latent space traversal both seems to be good ways to explore the latent space of a learned model.  The attribute vector section on transforming images to new ones with desired attributes is also interesting, and it provides a few new ways to make the GAN latent space more interpretable.\n\nOverall I feel most of the techniques proposed in this paper are nice visualization tools.  The contributions however, are mostly on the design of the visualizations, and not much on the technical and model side.  The spherical interpolation provides the only mathematical equation in the paper, yet the correctness of the technique is arguable.  For the visualization tools, there are also no quantitative evaluation, maybe these results are more art than science.","label":0,"model":"human","source":"peerread","id":4932}
{"text":"In this paper the authors propose various techniques to sample visualizations from generative models with high dimensional latent spaces like VAEs and GANs. For example, the authors highlight the well known but often not sufficiently appreciated fact that the probability mass of high dimensional Gaussian distributions concentrates near a thin hyper-shell with a certain radius. They therefore propose to use spherical interpolations (great arcs) instead of the commonly used linear interpolations. In a similar spirit they propose a visualisation for analogies and techniques to reinforce structure in VAE latent spaces.\n\nI find it hard to give clear recommendation for this paper: On the one hand I enjoyed reading it and I might want use some of the proposals (e.g. spherical interpolations; J-diagrams) in future work of mine. On the other hand, it\u2019s obvious that this paper is not a typical machine learning paper; it does not propose a new model, or training method, or provide (theoretical\/empirical) insight and it does not have the scientific quality and depth I\u2019ve seen in many other ICLR submissions. But it does more than just describing useful \u201ctricks\u201d. And all things considered I think this paper deserves a wider audience (but  I'm not convinced that ICLR is the right venue)\n","label":0,"model":"human","source":"peerread","id":4933}
{"text":"Could you please explain why the spherical interpolation is better when the prior is uniform?","label":0,"model":"human","source":"peerread","id":4934}
{"text":"Dear Authors,\n\nPlease resubmit your paper in the ICLR 2017 format for your submissions to be considered. Thank you!","label":0,"model":"human","source":"peerread","id":4935}
{"text":"This work proposes to iteratively improve a sentence that has been generated from another MT system (in this case, a phrase-based system). The authors use a neural net that takes in the source sentence and a window of (gold) words around the current target word, and predicts the current target word. During testing, the gold words are replaced with the generated words. While this is an interesting area of research, I am not convinced by the proposed approach, and experimental evidence is lacking.\n\nUnder the current framework, it is all but impossible for the model to do anything more than a rudimentary word replacement (e.g. it cannot change \"I went to the fridge even though I was not hungry\" to \"Although I was not hungry, I went to the fridge\"). The fact that only 0.6 words are edited on average supports this. \n\nSpecific comments:\n- It would be interesting to see what the improvements are if the baseline model is a neural system.\n- It seems strange (to me at least) that T^i and L(y^{-i|k}) only look at a window of 2k words. It means that when making the decision to change the i-th word, the model does not know what was generated outside of the window? \n- Relatedly, the idea of changing individual words based on local (i.e. word-level) scores seems counterintuitive. Given that we have the full generated sentence, don't we want a global score? Scoring at the sentence-level could also make room for non-greedy search strategies, which could potentially facilitate richer edits.\n- How does the approach compare to a model that simply re-ranks the k-best output?\n- Instead of editing, did you consider learning an encoder-decoder that takes in x, y_g, and generates y_ref? When decoding you can attend to both x and y_g.\n\nMinor comments:\n- Iteratively improving a generated text was also explored in","label":0,"model":"human","source":"peerread","id":4936}
{"text":"The paper is an interesting first step, but the reviewers found flaws in the overall argument. Further, the method is not contextualized well enough in relation to prior work.","label":0,"model":"human","source":"peerread","id":4937}
{"text":"This paper proposes a method for iteratively improving the output of an existing machine translation by identifying potential mistakes and proposing a substitution, in this case using an attention-based model.  It is motivated by the method in which (it is assumed) human translators operate.\n\nThe paper is interesting and imaginative.  However, in general terms, I am somewhat sceptical of this kind of approach -- whereby a machine learning method is used to identify and correct the predictions of another method, or itself -- because in the first case, if the new method is better, why not use it from the outset in place of the other method?  And in the second case, since the method has no new information compared to previously, why is it more likely to identify more past mistakes and correct them, than identify past correct terms and turn them into new errors?  That is unless there is a specific reason that an iterative approach can be shown to converge to a better solution when run over several epochs.\n\nThis paper does not convince me on these points.  Indeed, unsurprisingly, the authors note that \"the probability of correctly labelling a word as a mistake remains low (62%)\" - this admittedly beats a random-chance baseline, but is not compared to something more meaningful, such as simply contrasting the existing system with a more powerful convolutional model and labelling all discrepancies as mistakes.  The oracle experiments are rather meaningless - they just serve to confirm that improving a translation is very easy when the existing mistakes have been identified, but much harder when they are not. \n\nAlthough I do like the paper on the whole, to really convince me that main objective -- ie. that **iterative** improvement is beneficial -- has been satifactorily demonstrated it would be necessary to include stronger baselines - and in particular, to show that an iterative refinement scheme can really improve over a system closely matched to the attention-based model, both when used in isolation and when used in system combination with a PBMT system, and to demonstrate that the PBMT system is not simply acting as a regulariser for the attention-based model.\n\nMinor comments:\n\nI find the notation excessively fiddly at times - eg F^i = (F^{i,1}, F^{i,|F^i|}) - why use |F^i| here when F is a matrix, so surely the length of the slice is not dependent on i?\n\nIn the discussion in section 4 - it seems that this still creates a mismatch between the training and test conditions - could anything be done about this?\n\n\n\n\n\n\n\n\n ","label":0,"model":"human","source":"peerread","id":4938}
{"text":"This paper proposes a model for iteratively refining translation hypotheses. This has several benefits, including enabling the translation model to condition not only on \u201cleft context\u201d, but also on \u201cright context\u201d, and potentially enabling more rapid and\/or accurate decoding. The motivation given is that often translators (and text generators generally) use a process of refinement in generating outputs.\n\nThis is an important idea that is not currently playing much of a role in neural net models, so this paper is a welcome contribution. However, while I think this is an important first step, I do feel that the lack of in depth analysis suggests this paper is not quite ready for a final publication version. For example, there are many possible connections to prior work in NLP, MT, and other parts of ML that could better contextualize this work (see specifics below). More substantively, the model in Section 3 could be interpreted as a globally normalized, undirected (~CRF) translation model trained using a pseudo-likelihood objective. In this analysis, the model squarely back in the context of traditional discriminative translation models which used \u201cundirected\u201d features, and the decoding algorithm then looks more like a standard greedy hill-climbing algorithm (albeit with an extra heuristic model for selecting which variable to update), which is also nothing unfamiliar.\n\nMy second criticism the limitations of the model are not well discussed. For example, the proposed editing procedure cannot obviously remove or insert a word from a translation. While I think this is a reasonable assumption than can be made for the sake of tractability, it is very unfortunate since missing or extra words (esp. function words) are a common problem in the baseline models that are being used. Second, the standard objections to absolute positional models (vs. relative positional models) seem particularly crucial to bring up in this work, especially since they might make some of the design decisions a bit more justifiable.\n\nOverall, this is an initial step in an interesting direction, but it needs more thorough analysis to demonstrate its value. A more thorough analysis will also likely suggest some important model variants (for example: is a global translation model really the goal? or is a post-editing model that fixes outputs with more complex operations more ideal?)\n\nRelated work:\nI think that more could be done to put this work in the context of what has come before and what is currently going on in other parts of ML. The idea of iterative refinement has been proposed in other problems that have complex output spaces, for example the DRAW model of Gregor et al. and the conditional adversarial network models used to refine images proposed recently by Isola et al. In NLP, there have been several (stochastic) hill climbing approaches that have been proposed, such as the work on parsing by Zhang and Lei et al. (2014) who use random initial guesses and then do greedy hill climbing using a series of local refinements, the structured prediction cascades of Weiss and Taskar (2009) (not to mention general coarse-to-fine modeling strategies). Finally, in MT, Arun et al. (2009) who use a Gibbs sampler to refine an initial guess to do decoding with a more complex model. The use of an explicit error model is rather novel in the context of correction, but I would point out that although the proposed architecture is different, the discriminative word lexicon models of Mauser et al. (2009) and the neural version of the same by Ha et al. (2014) are similar in spirit. There have also been a number of papers on \u201cautomatic post editing\u201d, including the shared task at WMT2016, and there are not only standard test sets and baselines, but also datasets that could actually be used to train a post editing model with human-generated data. Minimally using the techniques they described could be a useful foil for the models presented in this paper.\n\n\u201cthe target sentence is also embedded in distributional space via a lookup table\u201d I think \u201cdistributional space\u201d is a bit unclear. Maybe \u201cthe target sentence is represented in terms of distributed word representations via a lookup table\u201d or something like that. \u201cdistributional\u201d suggests that the representations are derived from how the words are distributed in the corpus, whereas you are learning these representations on this task which isn\u2019t modeling their distribution except only very indirectly.\n\nSection 3 Model: In Section 3, the model computes the distribution over target word types at an absolute position i in the output sentence, given the target language context and the source language context. It is introduced as the model that is used to refine an existing hypothesis, but it is not immediately clear that the training data for this model (at least in this section) are the gold standard translations- \u201ctraining set\u201d could be interpreted in variety of ways. This becomes clearer when reading later in the paper, but it\u2019s a bit less clear when reading from the beginning for the first time.\n\nThe use of a fixed sized window for representing the target word in context also seems to make something like a model 1 assumption since only the lexical features (and not any \u201calignment\u201d or \u201cpositional\u201d features) determine the attention. This should be clarified since it will make the assumptions of the model more transparent (and also suggest possible refinements to the model, e.g., including (representations) of i and j as components of S^j and T^i, which would allow model 2\/3-like responses to be learned- although by leaving them out, the model might behave a bit more like a relative positional model than an absolute positional model, which is probably attractive).\n\nFinally, some discussion for why a fixed window is used to represent the target sentence is worth including (since a global context is apparently used to represent the source sentence).\n\nThe relationship between this training objective and pseudo likelihood (PL; Besag, 1975) might be worth mentioning. Since I believe this is just a PL objective for a certain global model, this suggests alternative decoding algorithms, or certainly a different analysis of the proposed decoding objective.\n\nThe section 4 model conditions on the true context of a position in the true target, the current target guess, and the source. I don\u2019t completely understand the rationale for this model since at test time only two of these variables are available, and the replacement of y_ref with y_g seems hard to justify.","label":0,"model":"human","source":"peerread","id":4939}
{"text":"Disclosure: I am not an expert in machine translation algorithms.\n\nSummary: A human translator does not come up with the final translation right\naway. Instead, (s)he uses an iterative process, starting with a rough draft\nwhich is corrected little by little. The idea behind this paper is to\nimplement a similar framework for an automated system. \n\nThis paper is generally well written. \n\nIt is my opinion however that drawings illustrating the architectures would help\nunderstanding how the different algorithms relate to one another.\n\nI like a lot that you report on a preliminary experiment to give an\nintuition of how difficult the task is. You should highlight the links\nbetween the task of finding the errors in a guess translation and the task\nof iterative refinement. Could you use post-edited text to have a more\nsolid ground-truth?\n\nMy main concern with this paper is that in the experimental section the \niterative approach tries to improve upon only one type of machine translation. \nWhich immediately prompts these questions:\n- why did they choose that approach to improve on?\n- what is the part of the improvement that comes from the choice of the\n  initial draft (maybe it was a very bad draft)? \n\nHere are some minor typos:\n- p.2: ... a lookup table that replace*S* each word... ?\n- p.3: I might be mistanken but it seems to me that j is used for two\n  different things. It is confusing.\n- p.3: ...takes as input these representation*S* and outputs... ?\n","label":0,"model":"human","source":"peerread","id":4940}
{"text":"This work proposes to iteratively improve a sentence that has been generated from another MT system (in this case, a phrase-based system). The authors use a neural net that takes in the source sentence and a window of (gold) words around the current target word, and predicts the current target word. During testing, the gold words are replaced with the generated words. While this is an interesting area of research, I am not convinced by the proposed approach, and experimental evidence is lacking.\n\nUnder the current framework, it is all but impossible for the model to do anything more than a rudimentary word replacement (e.g. it cannot change \"I went to the fridge even though I was not hungry\" to \"Although I was not hungry, I went to the fridge\"). The fact that only 0.6 words are edited on average supports this. \n\nSpecific comments:\n- It would be interesting to see what the improvements are if the baseline model is a neural system.\n- It seems strange (to me at least) that T^i and L(y^{-i|k}) only look at a window of 2k words. It means that when making the decision to change the i-th word, the model does not know what was generated outside of the window? \n- Relatedly, the idea of changing individual words based on local (i.e. word-level) scores seems counterintuitive. Given that we have the full generated sentence, don't we want a global score? Scoring at the sentence-level could also make room for non-greedy search strategies, which could potentially facilitate richer edits.\n- How does the approach compare to a model that simply re-ranks the k-best output?\n- Instead of editing, did you consider learning an encoder-decoder that takes in x, y_g, and generates y_ref? When decoding you can attend to both x and y_g.\n\nMinor comments:\n- Iteratively improving a generated text was also explored in ","label":0,"model":"human","source":"peerread","id":4941}
{"text":"This paper introduces a maximum total correlation procedure, adds a target and then adds noise perturbations.\n\nTechnical issues:\n\nThe move from (1) to (2) is problematic. Yes it is a lower bound, but by igoring H(Z), equation (2) ignores the fact that H(Z) will potentially vary more significantly that H(Z|Y). As a result of removing H(Z), the objective (2) encourages Z that are low entropy as the H(Z) term is ignored, doubly so as low entropy Z results in low entropy Z|Y. Yes the -H(X|Z) mitigates against a complete entropy collapse for H(Z), but it still neglects critical terms. In fact one might wonder if this is the reason that semantic noise addition needs to be done anyway, just to push up the entropy of Z to stop it reducing too much.\n\nIn (3) arbitrary balancing paramters lamda_1 and lambda_2 are introduced ex-nihilo - they were not there in (2). This is not ever justified.\n\nThen in (5), a further choice is made by simply adding L_{NLL} to the objective. But in the supervised case, the targets are known and so turn up in H(Z|Y). Hence now H(Z|Y) should be conditioned on the targets. However instead another objective is added again without justification, and the conditional entropy of Z is left disconnected from the data it is to be conditioned on. One might argue the C(X,Y,Z) simply acts as a prior on the networks (and hence implicitly on the weights) that we consider, which is then combined with a likelihood term, but this case is not made. In fact there is no explicit probabilistic or information theoretic motivation for the chosen objective.\n\nGiven these issues, it is then not too surprising that some further things need to be done, such as semantic noise addition to actually get things working properly. It may be the form of noise addition is a good idea, but given the troublesome objective being used in the first place, it is very hard to draw conclusions.\n\nIn summary, substantially better theoretical justification of the chosen model is needed, before any reasonable conclusion on the semantic noise modelling can be made.","label":0,"model":"human","source":"peerread","id":4942}
{"text":"The reviewers all expressed concerns with the technical quality of this work. In particular, the reviewers are concerned that ignoring certain entropy terms in the objective is problematic and would require significantly more justification theoretically and empirically. The reviewers believe that the authors had to resort to unjustified tricks such as adding noise in order to compensate for the missing terms in the objective. Some of the reviewers also had concerns with the choice of experiments, expressing that the authors did not choose the right baseline comparisons to compare to (e.g. convolutional networks vs. fully connected networks on MNIST). Hopefully the thorough feedback and lengthly discussion, along with the authors' responses (both in the text and additions to the paper and appendix), will lead to a stronger submission to a future conference.","label":0,"model":"human","source":"peerread","id":4943}
{"text":"We have made overall revisions to the manuscript in order to clarify mathematical justification, explanations, and several empirical analyses. Among the revised contents, we especially focused on the methodology part (Section 2). Section 2 is divided into two subsections. Section 2.1 explains the base model as an extension of joint learning approaches. This subsection includes details of derivations for mathematical justification of the base model (See Appendix_A1). Section 2.2 shows the proposed latent space modeling method. This subsection focuses on explaining how to model the semantic-preserving perturbation on the latent space. We are sorry for the incompleteness of the early version of manuscript. We ask the reviewers to revisit the updated manuscript once again.\n","label":0,"model":"human","source":"peerread","id":4944}
{"text":"The paper introduces supervised deep learning with layer-wise reconstruction loss (in addition to the supervised loss) and class-conditional semantic additive noise for better representation learning. Total correlation measure and additional insights from auto-encoder are used to derive layer-wise reconstruction loss and is further combined with supervised loss. When combining with supervised loss the class-conditional additive noise model is proposed, which showed consistent improvement over the baseline model. Experiments on MNIST and CIFAR-10 datasets while changing the number of training examples per class are done extensively.\n\nThe derivation of Equation (3) from total correlation is hacky. Moreover, assuming graphical model between X, Y and Z, it should be more carefully derived to estimate H(X|Z) and H(Z|Y). The current proposal, encoding Z and Y from X and decoding from encoded representation is not really well justified.\n\nIs \\sigma in Equation 8 trainable parameter or hyperparameter? If it is trainable how it is trained? If it is not, how are they set? Does j correspond to one of the class? The proposed feature augmentation sounds like simply adding gaussian noise to the pre-softmax neurons. That being said, the proposed method is not different from gaussian dropout (Wang and Manning, ICML 2013) but applied on different layers. In addition, there is a missing reference (DisturbLabel: Regularizing CNN on the Loss Layer, CVPR 2016) that applied synthetic noise process on the loss layer.\n\nExperiments should be done for multiple times with different random subsets and authors should provide mean and standard error. Overall, I believe the proposed method is not very well justified and has limited novelty. ","label":0,"model":"human","source":"peerread","id":4945}
{"text":"The paper presents a new regularization technique for neural networks, which seeks to maximize correlation between input variables, latent variables and outputs. This is achieved by defining a measure of total correlation between these variables and decomposing it in terms of entropies and conditional entropies.\n\nAuthors explain that they do not actually maximize the total correlation, but a lower-bound of it that ignores simple entropy terms, and only considers conditional entropies. It is not clearly explained what is the rationale for discarding these entropy terms.\n\nEntropies measures are applying to probability distributions (i.e. this implies that the variables in the model should be random). The link between the conditional entropy formulation and the reconstruction error is not made explicit. In order to link these two views, I would have expected, for example, a noise model for the units of the network.\n\nLater in the paper, it is claimed that the original ladder network is not suitable for supervised learning with small samples, and some empirical results seek to demonstrate this. But a more theoretical explanation why it is the case would have been welcome.\n\nThe MNIST results are shown for a particular convolutional neural network architecture, however, most ladder network results for this dataset have been produced on standard fully-connected architectures. Results for such neural network architecture would have been desirable for more comparability with original ladder neural network results.","label":0,"model":"human","source":"peerread","id":4946}
{"text":"An updated version of the paper has been uploaded. This version includes more clear explanation of the base model, reflecting the issues raised by AnnoReviewer3.","label":0,"model":"human","source":"peerread","id":4947}
{"text":"This paper evaluates several strategies to reduce output vocabulary size in order to speed up NMT decoding and training. It could be quite useful to practitioners, although the main contributions of the paper seem somewhat orthogonal to representation learning and neural networks, and I am not sure ICLR is the ideal venue for this work.\n\n- Do the reported decoding times take into account the vocabulary reduction step?\n- Aside from machine translation, might there be applications to other settings such as language modeling, where large vocabulary is also a scalability challenge?\n- The proposed methods are helpful because of the difficulties induced by using a word-level model. But (at least in my opinion) starting from a character or even lower-level abstraction seems to be the obvious solution to the huge vocabulary problem.","label":0,"model":"human","source":"peerread","id":4948}
{"text":"The reviewers agree that the method is exciting as practical contributions go, but the case for originality is not strong enough.","label":0,"model":"human","source":"peerread","id":4949}
{"text":"This paper conducts a comprehensive series of experiments on vocabulary selection strategies to reduce the computational cost of neural machine translation.\n\nA range of techniques are investigated, ranging from very simple methods such as word co-occurences, to the relatively complex use of SVMs.\n\nThe experiments are solid, comprehensive and very useful in practical terms.  It is good to see that the best vocabulary selection method is very effective at achieving a very high proportion of the coverage of the full-vocabulary model (fig 3).  However, I feel that the experiments in section 4.3 (vocabulary selection during training) was rather limited in their scope - I would have liked to see more experiments here.\n\nA major criticism I have with this paper is that there is little novelty here.  The techniques are mostly standard methods and rather simple, and in particular, there it seems that there is not much additional material beyond the work of Mi et al (2016).  So although the work is solid, the lack of originality lets it down.\n\nMinor comments: in 2.1, the word co-occurence measure - was any smoothing used to make this measure more robust to low counts?\n\n\n\n\n\n\n\n\n","label":0,"model":"human","source":"peerread","id":4950}
{"text":"This paper compares several strategies for guessing a short list of vocabulary for the target language in neural machine translation. The primary findings are that word alignment dictionaries work better than a variety of other techniques.\n\nMy take on this paper is that to have a significant impact, it needs to make the case for why one might want vocabulary rather than characters or sub word units like BPE. I think there are likely many very good reasons to do this that could be argued for (synthesize morphology, deal with transliteration, etc), but most of these would suggest some particular models and experiments, which are of course not in this paper. As it is, I think this paper is a useful but minor contribution that shows that word alignment is a good way of getting short lists, but it does not strongly make the case that we should abandon work in other directions.\n\nMinor comments:\nIn addition to the SVM approach for modeling vocabulary, the discriminative word lexicon of Mauser et al. (2009) and the neural version of Ha et al. (2014) are also worth mentioning.\n\nIt would be useful to know what the coverage rate of the actual full vocabulary would be (rather than the 100k \u201cfull vocabulary\u201d). Since presumably this technique could be used to work with much larger vocabularies.\n\nWhen reducing the vocabulary size for training, the Mi et al. (2016) technique of taking the union of all the vocabularies in a mini batch seems like a rather strange objective. If the vocabulary of a single sentence is used, the probabilistic semantics of the translation model can still be preserved since p(e | f, vocab(f)) = p(e | f) if p(vocab(f) | f) = 1, i.e., is deterministic, which it is here. Whereas the objective is no longer a sensible probability model in the mini batch vocabulary case. Thus, while it may be a bit more difficult to implement, it seems like it would at least be a sensible comparison to make.","label":0,"model":"human","source":"peerread","id":4951}
{"text":"In this paper, the authors present several strategies to select a small subset of target vocabulary to work with per source sentence, which results in significant speedup. The results are convincing and I think this paper offers practical values to general seq2seq approaches to language tasks. However, there is little novelty in this work: the authors further mostly extend the work of (Mi et al., 2016) with more vocabulary selection strategies and thorough experiments. This paper will fit better in an NLP venue.","label":0,"model":"human","source":"peerread","id":4952}
{"text":"This paper evaluates several strategies to reduce output vocabulary size in order to speed up NMT decoding and training. It could be quite useful to practitioners, although the main contributions of the paper seem somewhat orthogonal to representation learning and neural networks, and I am not sure ICLR is the ideal venue for this work.\n\n- Do the reported decoding times take into account the vocabulary reduction step?\n- Aside from machine translation, might there be applications to other settings such as language modeling, where large vocabulary is also a scalability challenge?\n- The proposed methods are helpful because of the difficulties induced by using a word-level model. But (at least in my opinion) starting from a character or even lower-level abstraction seems to be the obvious solution to the huge vocabulary problem.\n","label":0,"model":"human","source":"peerread","id":4953}
{"text":"The primary point made by this paper is that given certain architectural characteristics of multi-GPU systems, namely the use of bi-directional PCI-E for communication and the integration of two independent DMA engines on recent GPU devices (providing support for simultaneous independent communications), and given the characteristics of the communications patterns required by synchronous SGD trainers for deep neural networks, namely that the messages are large, dense, and have a fixed length, it makes sense to design communication collectives such as broadcast, reduce, and allreduce specifically for the use case of synchronous SGD training on a multi-GPU system.  The paper describes the implementation of these three collectives (broadcast, reduce, and allreduce) using a linear pipelining (LP) scheme on a (logical) ring topology.  The paper compares the LP collectives to two alternatives:  collectives based on a minimal spanning tree (MST) topology and collectives based on bidirectional exchange (BE).  First, a theoretical comparison is made using a standard cost model used in the high performance computing community.  When assumptions based on multi-GPU system architecture (very low latency for messages) and on the communication characteristics of synchronous SGD training (very large messages) are integrated into the model, the paper finds that the LP collectives should be less costly than BE collectives by a factor of 2 and less costly than MST collectives by a factor of log(p), where p is the number of GPUs being used.  Second, an empirical comparison is performed in which (1) the time required to perform each of the different collectives on a 4-device (k40m) system is measured as a function of message size and (2) the time required to perform each of the different collectives with a 200 MB message length is measured as a function of the number of devices in the system.  These measurements show that the LP-based collectives are consistently the fastest.  Third, DNN training experiments with AlexNet and GoogLeNet are performed on a 4-device system using three different synchronous SGD algorithms with the different implementations of the collectives (a total of 6 different algorithms in all).  Measurements of the communication and computation costs show that the LP collectives reduce communication costs without affecting computation costs (as expected).  Measurements of the convergence of the training loss as a function of time for the two DNN architectures show that use of the LP collectives leads to faster training.\n\nWhile the theory says that the costs of LP collectives should be invariant to the number of devices in a multi-GPU system, the empirical work shows that in practice this does not hold going from 4 to 5 devices (in the tested configuration) because in a 5-device system messages must traverse the QPI.  Are there other practical considerations that the authors are aware of that affect the scaling of the LP collectives?  If so, these should be mentioned in the paper.\n\nIn the sentence \"Worringen (2003) proposed a pipeline collective model in shared memory environment for CPU data, but communications of different MPI processes sharing the same CPU memory bus within the same CPU socket.\" I really can't figure out what the words after \"but communications of different MPI processes\" are trying to convey.  This sentence is not comprehensible.\n\n\"Please note the latency term is log p\u03b1, which is the smallest among algorithms in Table.1. Therefore, MST only suits for high frequent short messages.\"  The claim that MST collectives are only suitable for high-frequency, short messages does not follow from the statement that MST collectives have the smallest latency term.  You also need to consider the way the cost scales with message size (the bandwidth term).  If the MST collectives had a better bandwidth term than the other collectives, then they would also be superior for large messages.\n\n\"Let\u2019s take an appropriate block size b to ensure n\/b \u226a \u03b1.\"  This looks wrong, since n > b.  Should it be b\/n \u226a \u03b1?\n\n\"However, the parameters are not consistent after several iterations due to the precision issues of float multiplications in Gradient Update.\"  Are you sure the inconsistency in weight estimates across devices is due to multiplication?  I would expect that it would be due to gradients being accumulated in different orders; that is, because floating point addition is not commutative.\n\nI recommend replacing the term \"sub-gradients\" in this paper with \"partial gradients.\"  In the optimization literature, the term \"sub-gradient\" has a very specific meaning that differs from this paper's use of the term (see","label":0,"model":"human","source":"peerread","id":4954}
{"text":"The authors propose improvements for the utilization of modern hardware when training using stochastic gradient. However, the reviewers bring up several issues with the paper, including major clarity issues as well as notational issues and some comments about the theory vs. practice.","label":0,"model":"human","source":"peerread","id":4955}
{"text":"To Reviewers,\n\nWe have improved the readability according to the feedback from Reviewer 3. Please check at the revision.","label":0,"model":"human","source":"peerread","id":4956}
{"text":"This paper presents a linear pipeline All-reduce approach for parallel neural networks on multiple GPU. The paper provides both theoretical analysis and experiments. Overall, the results presented in the paper are interesting, but the writing can be improved. \n\nComments:\n\n- The authors compare their proposed approach with several alternative approaches and demonstrate strong performance of the proposed approaches. But it is unclear if the improvement is from the proposed approach or from the implementation.  \n\n- The paper is not easy to follow and the writing can be improved in many place (aside from typos and missing references). Specifically, the authors should provide more intuitions of the proposed approach in the introduction and in Section 3. \n\n- The proposition and the analysis in Section 3.2 do not suggest the communication cost of linear pipeline is approximately 2x and log p faster than BE and MST, respectively, as claimed in many places in the paper. Instead, it suggests LP *cannot* be faster than these methods by 2x and log p  times. More specifically, Eq (2) shows T_broadcase_BE\/ T_broadcase_LP < 2. This does not provide an upper-bound of T_broadcase_LP and it can be arbitrary worse when comparing with T_broadcase_BE from this inequality. Therefore, instead of showing T_broadcase_BE\/ T_broadcase_LP < 2, the authors should state T_broadcase_BE\/ T_broadcase_LP > 1 when n approaches infinity. \n\n- It would be interesting to emphasize more on the differences between designing parallel algorithms on CPU v.s. on GPU to motivate the paper. \n","label":0,"model":"human","source":"peerread","id":4957}
{"text":"This paper analyzes the ring-based AllReduce approach for multi-GPU data parallel training of deep net.\nComments\n1) The name linear pipeline is somewhat confusing to the readers, as the technique is usually referred as ring based approach in Allreduce literature. The author should use the standard name to make the connection easier. \n2) The cost analysis of ring-based Allreduce is already provided in the existing literature. This paper applied the analysis to the case of multi-GPU deep net training, and concluded that the scaling is invariant of number of GPUs.\n3) The ring-based allreduce approach is already supported by NVidia\u2019s NCCL library, although the authors claim that their implementation comes earlier than the NCCL implementation.\n4) The overlap of communication of computation is an already applied technique in systems such as TensorFlow and MXNet. The schedule proposed by the authors exploits the overlap partially, doing backprop of t-1 while doing reduce.  Note that the dependency pattern can be further exploited; with the forward of layer t depend on update of parameter of layer t in last iteration. This can be done by a dependency scheduler.\t\n5) Since this paper is about analysis of Allreduce, it would be nice to include detailed analysis of tree-shape reduction, ring-based approach and all-to-all approach. The discussion of all-to-all approach is missing in the current paper. \nIn summary, this is a paper discussed existing Allreduce techniques for data parallel multi-GPU training of deep net, with cost analysis based on existing results. While I personally find the claimed result not surprising as it follows from existing analysis of Allreduce, the analysis might help some other readers. I view this as a baseline paper. The analysis of Allreduce could also been improved (see comment 5).\n\n\n\n\n\n\n\n\n\n","label":0,"model":"human","source":"peerread","id":4958}
{"text":"The primary point made by this paper is that given certain architectural characteristics of multi-GPU systems, namely the use of bi-directional PCI-E for communication and the integration of two independent DMA engines on recent GPU devices (providing support for simultaneous independent communications), and given the characteristics of the communications patterns required by synchronous SGD trainers for deep neural networks, namely that the messages are large, dense, and have a fixed length, it makes sense to design communication collectives such as broadcast, reduce, and allreduce specifically for the use case of synchronous SGD training on a multi-GPU system.  The paper describes the implementation of these three collectives (broadcast, reduce, and allreduce) using a linear pipelining (LP) scheme on a (logical) ring topology.  The paper compares the LP collectives to two alternatives:  collectives based on a minimal spanning tree (MST) topology and collectives based on bidirectional exchange (BE).  First, a theoretical comparison is made using a standard cost model used in the high performance computing community.  When assumptions based on multi-GPU system architecture (very low latency for messages) and on the communication characteristics of synchronous SGD training (very large messages) are integrated into the model, the paper finds that the LP collectives should be less costly than BE collectives by a factor of 2 and less costly than MST collectives by a factor of log(p), where p is the number of GPUs being used.  Second, an empirical comparison is performed in which (1) the time required to perform each of the different collectives on a 4-device (k40m) system is measured as a function of message size and (2) the time required to perform each of the different collectives with a 200 MB message length is measured as a function of the number of devices in the system.  These measurements show that the LP-based collectives are consistently the fastest.  Third, DNN training experiments with AlexNet and GoogLeNet are performed on a 4-device system using three different synchronous SGD algorithms with the different implementations of the collectives (a total of 6 different algorithms in all).  Measurements of the communication and computation costs show that the LP collectives reduce communication costs without affecting computation costs (as expected).  Measurements of the convergence of the training loss as a function of time for the two DNN architectures show that use of the LP collectives leads to faster training.\n\nWhile the theory says that the costs of LP collectives should be invariant to the number of devices in a multi-GPU system, the empirical work shows that in practice this does not hold going from 4 to 5 devices (in the tested configuration) because in a 5-device system messages must traverse the QPI.  Are there other practical considerations that the authors are aware of that affect the scaling of the LP collectives?  If so, these should be mentioned in the paper.\n\nIn the sentence \"Worringen (2003) proposed a pipeline collective model in shared memory environment for CPU data, but communications of different MPI processes sharing the same CPU memory bus within the same CPU socket.\" I really can't figure out what the words after \"but communications of different MPI processes\" are trying to convey.  This sentence is not comprehensible.\n\n\"Please note the latency term is log p\u03b1, which is the smallest among algorithms in Table.1. Therefore, MST only suits for high frequent short messages.\"  The claim that MST collectives are only suitable for high-frequency, short messages does not follow from the statement that MST collectives have the smallest latency term.  You also need to consider the way the cost scales with message size (the bandwidth term).  If the MST collectives had a better bandwidth term than the other collectives, then they would also be superior for large messages.\n\n\"Let\u2019s take an appropriate block size b to ensure n\/b \u226a \u03b1.\"  This looks wrong, since n > b.  Should it be b\/n \u226a \u03b1?\n\n\"However, the parameters are not consistent after several iterations due to the precision issues of float multiplications in Gradient Update.\"  Are you sure the inconsistency in weight estimates across devices is due to multiplication?  I would expect that it would be due to gradients being accumulated in different orders; that is, because floating point addition is not commutative.\n\nI recommend replacing the term \"sub-gradients\" in this paper with \"partial gradients.\"  In the optimization literature, the term \"sub-gradient\" has a very specific meaning that differs from this paper's use of the term (see ","label":0,"model":"human","source":"peerread","id":4959}
{"text":"This paper proposes a new kind of expert model where a sparse subset of most reliable experts is chosen instead of the usual logarithmic opinion pool of a PoE.\nI find the paper very unclear. I tried to find a proper definition of the joint model p(x,z) but could not extract this from the text. The proposed \u201cEM-like\u201d algorithm should then also follow directly from this definition. At this point I do not see if such as definition even exists. In other words, is there is an objective function on which the iterates of the proposed algorithm are guaranteed to improve on the train data?\nWe also note that the \u201cproduct of unifac models\u201d from Hinton tries to do something very similar where only a subset of the experts will get activated to generate the input:","label":0,"model":"human","source":"peerread","id":4960}
{"text":"This paper is about learning distributed representations. All reviewers agreed that the first draft was not clear enough for acceptance.\n \n Reviewer time is limited and a paper that needed a complete overhaul after the reviews were written is not going to get the same consideration as a paper that was well-drafted from the beginning.\n \n It's still the case that it's unclear from the paper how the learning updates or derived. The results are not visually impressive in themselves. It's also still the case that more is needed to demonstrate that this direction is promising compared to other approaches to representation learning.","label":0,"model":"human","source":"peerread","id":4961}
{"text":"The paper addresses the problem of learning compact binary data representations. I have a hard time understanding the setting and the writing of the paper is not making it any easier. For example I can't find a simple explanation of the problem and I am not familiar with these line of research. I read all the responses provided by authors to reviewer's questions and re-read the paper again and I still do not fully understand the setting and thus can't really evaluate the contributions of these work. The related work section does not exist and instead the analysis of the literature is somehow scattered across the paper. There are no derivations provided. Statements often miss references, e.g. the ones in the fourth paragraph of Section 3. This makes me conclude that the paper still requires significant work before it can be published.","label":0,"model":"human","source":"peerread","id":4962}
{"text":"The goal of this paper is to learn \u201c a collection of experts that are individually\nmeaningful and that have disjoint responsibilities.\u201d Unlike a standard mixture model, they \u201cuse a different mixture for each dimension d.\u201d While the results seem promising, the paper exposition needs significant improvement.\n\nComments:\n\nThe paper jumps in with no motivation at all. What is the application, or even the algorithm, or architecture that this is used for? This should be addressed at the beginning.\n\nThe subsequent exposition is not very clear. There are assertions made with no justification, e.g. \u201cthe experts only have a small variance for some subset of the variables while the variance of the other variables is large.\u201d \n\nSince you\u2019re learning both the experts and the weights, can this be rephrased in terms of dictionary learning? Please discuss the relevant related literature.\n\nThe horse data set is quite small with respect to the feature dimension, and so the conclusions may not necessarily generalize.\n\n","label":0,"model":"human","source":"peerread","id":4963}
{"text":"This paper proposes a new kind of expert model where a sparse subset of most reliable experts is chosen instead of the usual logarithmic opinion pool of a PoE.\nI find the paper very unclear. I tried to find a proper definition of the joint model p(x,z) but could not extract this from the text. The proposed \u201cEM-like\u201d algorithm should then also follow directly from this definition. At this point I do not see if such as definition even exists. In other words, is there is an objective function on which the iterates of the proposed algorithm are guaranteed to improve on the train data?\nWe also note that the \u201cproduct of unifac models\u201d from Hinton tries to do something very similar where only a subset of the experts will get activated to generate the input: ","label":0,"model":"human","source":"peerread","id":4964}
{"text":"The authors introduce a new memory model which allows memory access in O(log n) time.\n\nPros:\n* The paper is well written and everything is clear.\n* It's a new model and I'm not aware of a similar model.\n* It's clear that memory access time is an issue for longer sequences and it is clear how this model solves this problem.\n\nCons:\n* The motivation for O(log n) access time is to be able to use the model on very long sequences. While it is clear from the definition that the computation time is low because of its design, it is not clear that the model will really generalize well to very long sequences.\n* The model was also not tested on any real-world task.\n\nI think such experiments should be added to show whether the model really works on long sequences and real-world tasks, otherwise it is not clear if this is a useful model.","label":0,"model":"human","source":"peerread","id":4965}
{"text":"This paper introduces a novel hierarchical memory architecture for neural networks, based on a binary tree with leaves corresponding to memory cells.  This allows for O(log n) memory access, and experiments additionally demonstrate ability to solve more challenging tasks such as sorting from pure input-output examples and dealing with longer sequences.\n\nThe idea of the paper is novel and well-presented, and the memory structure seems reasonable to have advantages in practice. However, the main weakness of the paper is the experiments. There is no experimental comparison with other external memory-based approaches (e.g. those discussed in Related Work), or experimental analysis of computational efficiency given overhead costs (beyond just computational complexity) despite that being one of the main advantages. Furthermore, the experimental setups are relatively weak, all on artificial tasks with moderate increases in sequence length.  Improving on these would greatly strengthen the paper, as the core idea is interesting.","label":0,"model":"human","source":"peerread","id":4966}
{"text":"This paper proposes to use a hierarchical softmax to speed up attention based memory addressing in memory augmented network (e.g. NTM, memNN\u2026).\n\nThe model build a hierarchical softmax on top of the input sequence then at each time step SEARCH for the most relevant input to predict the next output (this search is discrete), and use its corresponding embedding to update the state of an LSTM that will then produce the output. Finally the embedding of the used input is update by a WRITE function (an LSTM working that takes hidden state of the other LSTM as an input). The model has a discrete component (the SEARCH) and is thus trained with REINFORCE. In the experimental section they test their approach on several algorithmic tasks such as search, sort...\n\nThe main advantage of replacing the full softmax by a hierarchical softmax is that during inference, the complexity goes from O(N) to O(log(N)). It would be great to see if the gain in complexity allows to tackle problem which are a few orders of magnitude bigger than the one addressed with full softmax. However the authors only test on toy sequences up to 32 tokens, which is quite small. \n\nThe model requires a relatively complex search mechanism that can only be trained with REINFORCE. While this seems to work on problems with relatively small and simple sequences, it would be great to see how performance changes with the size of the problem. \n\nOverall, while the idea of replacing the softmax in the attention mechanism by a hierachical softmax is appealing, this work is not quite convincing yet. Their approach is not very natural, may be hard to train and may not be that simple to scale. The experiment section is very weak.\n","label":0,"model":"human","source":"peerread","id":4967}
{"text":"The authors introduce a new memory model which allows memory access in O(log n) time.\n\nPros:\n* The paper is well written and everything is clear.\n* It's a new model and I'm not aware of a similar model.\n* It's clear that memory access time is an issue for longer sequences and it is clear how this model solves this problem.\n\nCons:\n* The motivation for O(log n) access time is to be able to use the model on very long sequences. While it is clear from the definition that the computation time is low because of its design, it is not clear that the model will really generalize well to very long sequences.\n* The model was also not tested on any real-world task.\n\nI think such experiments should be added to show whether the model really works on long sequences and real-world tasks, otherwise it is not clear if this is a useful model.\n","label":0,"model":"human","source":"peerread","id":4968}
{"text":"This paper was easy to read, the main idea was presented very clearly.\n\nThe main points of the paper (and my concerns are below) can be summarized as follows:\n1. synchronous algoriths suffer from some struggeling nodes, for which the algorithm has to wait. From my own experience, this has never happend for me on e.g. Amazon EC2 cloud, however, it happens on our own cluster at my university, if the cluster is shared and some users make some nodes very busy. So maybe if the nodes would be dedicated to just user's job, it wouldn't be such a big concer (I am not sure what kind of cluster was used to produce Figure 3 and 4). Also how many experiments have you run? In my own experience, most of the time I get the gradient on time from all nodes equality fast, but maybe just in less than 0.1% of iterations I observe that it took maybe twice as long for some node. Also the increasing shape of the curve is somehow implying some weird implementation of communication. Isn't it only because you are somehow serialize the communication? And it would be maybe much faster if a \"MPI_Reduce\" would be used (even if we wait for the slowest guy)?\n2. asynchronous algorithms are cutting the waiting time, however, the convergence speed may be slower. Moreover, those algorithms can be divergence it special care is not given to stale gradients. Also they have a nice guarantees for convex functions, but the non-convex DNN may cause pain.\n3.they propose to take gradient from the first \"N\" workers out of \"N+b\" \nworkers available. My concern here is that they focused only on the \nworkers, but what if the \"parameter server\" will became to slow? What \nif the parameter server would be the bottleneck? How would you address \nthis situation? But still if the number of nodes (N) is not large, and \nthe deep DNN is used, I can imagine that the communciation will not \ntake more than 30% of the run-time.\n\n\nMy largest concern is with the experiments. Different batch size \nimplies that different learning rate should be chosen, right? How did \nyou tune the learning rates and other parameters for e.g. Figure 5 you \nprovide some formulas in (A2) but clearly this can bias your Figures, \nright? meaning, that if you tune \"\\gamma, \\beta\" for each N, it could \nbe somehow more representative? also it would be nicer if you run the \nexperiment many times and then report average, best and worst case \nbehaviour. because now it can be just coinsidence, right?","label":0,"model":"human","source":"peerread","id":4969}
{"text":"Counter to the current wisdom, this work proposes that synchronous training may be advantageous over asynchronous training (provided that \"backup workers\" are available). This is shown empirical and without theoretical results. The contribution is somewhat straightforward and designed for a specific large-scale hardware scenario, but this is often an important bottleneck in the learning process. However, there is some concern about the long-term impact of this work, due to its dependence on the hardware.","label":0,"model":"human","source":"peerread","id":4970}
{"text":"This paper was easy to read, the main idea was presented very clearly.\n\nThe main points of the paper (and my concerns are below) can be summarized as follows:\n1. synchronous algoriths suffer from some struggeling nodes, for which the algorithm has to wait. From my own experience, this has never happend for me on e.g. Amazon EC2 cloud, however, it happens on our own cluster at my university, if the cluster is shared and some users make some nodes very busy. So maybe if the nodes would be dedicated to just user's job, it wouldn't be such a big concer (I am not sure what kind of cluster was used to produce Figure 3 and 4). Also how many experiments have you run? In my own experience, most of the time I get the gradient on time from all nodes equality fast, but maybe just in less than 0.1% of iterations I observe that it took maybe twice as long for some node. Also the increasing shape of the curve is somehow implying some weird implementation of communication. Isn't it only because you are somehow serialize the communication? And it would be maybe much faster if a \"MPI_Reduce\" would be used (even if we wait for the slowest guy)?\n2. asynchronous algorithms are cutting the waiting time, however, the convergence speed may be slower. Moreover, those algorithms can be divergence it special care is not given to stale gradients. Also they have a nice guarantees for convex functions, but the non-convex DNN may cause pain.\n3.they propose to take gradient from the first \"N\" workers out of \"N+b\" \nworkers available. My concern here is that they focused only on the \nworkers, but what if the \"parameter server\" will became to slow? What \nif the parameter server would be the bottleneck? How would you address \nthis situation? But still if the number of nodes (N) is not large, and \nthe deep DNN is used, I can imagine that the communciation will not \ntake more than 30% of the run-time.\n\n\nMy largest concern is with the experiments. Different batch size \nimplies that different learning rate should be chosen, right? How did \nyou tune the learning rates and other parameters for e.g. Figure 5 you \nprovide some formulas in (A2) but clearly this can bias your Figures, \nright? meaning, that if you tune \"\\gamma, \\beta\" for each N, it could \nbe somehow more representative? also it would be nicer if you run the \nexperiment many times and then report average, best and worst case \nbehaviour. because now it can be just coinsidence, right? \n","label":0,"model":"human","source":"peerread","id":4971}
{"text":"This paper proposed a synchronous parallel SGD by employing several backup machines. The parameter server does not have to wait for the return from all machines to perform the update on the model, which reduce the synchronization overhead. It sounds like a reasonable and straightforward idea. \n\nMy main concern is that this approach is only suitable for some very specific scenario, that is, most learners (except a small number of learners) are at the same pace to return the results. If the efficiency of learners does not follow such distribution, I do not think that the proposed algorithm will work. So I suggest two revisions:\n\n- provide more experiments to show the performance with different efficiency distributions of learners.\n- assuming that all learners follow the same distribution of efficiency and show the expected idle time is minor by using the proposed algorithm.","label":0,"model":"human","source":"peerread","id":4972}
{"text":"The paper claim that, when supported by a number of backup workers, synchronized-SGD \nactually works better than async-SGD. The paper first analyze the problem of staled updates\nin async-SGDs, and proposed the sync-SGD with backup workers. In the experiments, the \nauthors shows the effectiveness of the proposed method in applications to Inception Net\nand PixelCNN.\n\nThe idea is very simple, but in practice it can be quite useful in industry settings where \nadding some backup workders is not a big problem in cost. Nevertheless, I think the \nproposed solution is quite straightforward to come up with when we assume that \neach worker contains the full dataset and we have budge to add more workers. So, \nunder this setting, it seems quite natural to have a better performance with the additional \nbackup workers that avoid the staggering worker problem. And, with this assumtion I'm not \nsure if the proposed solution is solving difficult enough problem with novel enough idea. \n\nIn the experiments, for fair comparison, I think the Async-SGD should also have a mechanism \nto cut off updates of too much staledness just as the proposed method ignores all the remaining \nupdates after having N updates. For example, one can measure the average time spent to \nobtain N updates in sync-SGD setting and use that time as the cut-off threashold in Async-SGD \nso that Async-SGD does not perform so poorly.","label":0,"model":"human","source":"peerread","id":4973}
{"text":"Is this technique efficient? If you are using 5-10% extra resources (backup workers), do you get at least 5-10% speedup over sync SGD? Figure 8(a) does not have any sync results. It is well understood that async may sometimes never converge to the correct results.","label":0,"model":"human","source":"peerread","id":4974}
{"text":"The proposed technique of over-provisioning a few workers to improve the hardware efficiency of synchronous training gives a very welcome boost, as evidenced beautifully by Figure 6.\n\nHowever, as I have discussed in private communication with the authors, the comparison with asynchronous methods leaves a few questions regarding tuning.\u00a0\n\nLearning rate:\u00a0\n\nThe authors carefully describe the process of tuning the synchronous implementation. However no tuning is reported for the asynchronous implementation: the value is set to 0.045 for all configurations. Figure 7(a) shows that tuning the learning rate can cause a difference of almost a whole percentage point in test precision.\n1. What other values of LR have the authors tried for asynchronous training?\n2. Is 0.045 the best one for all configurations they report?\n3. Is there any chance that the 0.5% difference in precision between sync and async is due to insufficient LR tuning (as suggested by Figure 7(a))?\u00a0\n\n\nMomentum:\n\nOur results (","label":0,"model":"human","source":"peerread","id":4975}
{"text":"Authors propose a competitive learning architecture that learn different RNN predictors independently, akin to a committee of experts which are chosen with a hard switch at run-time. This work is applied to the task of predictive different driving behaviors from human drivers, and combines behaviors at test time, often switching behaviors within seconds. Prediction loss is lower than the similar but non-competitive architecture used as a baseline.\nIt is not very clear how to interpret the results, what is the real impact of the model. If behaviors switch very often, can this really be seen as choosing the best driving mode for a given situation? Maybe the motivation needs to be rephrased a little to be more convincing?\nThe competitive approach presented is interesting but not really novel, thus the impact of this paper for a conference such as ICLR may be limited.","label":0,"model":"human","source":"peerread","id":4976}
{"text":"The authors present a prediction framework that involves multiple 'competitive' RNNs, and they claim that they are predicting human intention. It is unclear if this method, which seems quite ad-hoc, is any different from a simple ensemble approach, and it is unclear that the model is predicting human intention. The experiments do not adequately demonstrate either.","label":0,"model":"human","source":"peerread","id":4977}
{"text":"\nThis paper introduces a neural network architecture and training procedure for predicting the speed of a vehicle several seconds into the future based on video and vehicle state input. The architecture allows several RNNs to compete to make the best predictions, with only the best prediction receiving back propagation training at each time step. Preliminary experimental results show that this scheme can yield reduced prediction error.\n\n It is not clear how the best-performing RNN is chosen for each time point at test time. That is, how is the \u201cintegrated prediction\u201d obtained in Fig. 7? Is the prediction the one with minimum error over all of the output layers? If so, this means the prediction cannot be made until you already know the value to be predicted.\n\nIt seems possible that a larger generic RNN might be able to generate accurate predictions. If I understand correctly, the competitive architectures have many more parameters than the baseline. Is the improved performance here due to the competitive scheme, or just a larger model? \n\nA large amount of additional work is required to sustain the claim that this scheme is successfully extracting driver \u2018intentions\u2019. It would be interesting to see if the scheme, suitably extended, can automatically infer the intention to stop at a stop sign vs slowing but not stopping due to a car in front, say, or to pass a car vs simply changing lanes. Adding labels to the dataset may enable this comparison more clearly.\n\nMore generally, the intention of the driver seems more related to the goals they are pursuing at the moment; there is a fair amount of work in inverse reinforcement learning that examines this problem (some of it in the context of driving style as well).\n","label":0,"model":"human","source":"peerread","id":4978}
{"text":"This paper proposes a neural network architecture for car state prediction while driving based on competitive learning. Competitive learning creates several duplicates of the baseline neural architecture and during training only updates the architecture with minimum loss. The experiments compare the competitive learning approach to a single baseline architecture on a driving benchmark task. The paper is understandable but could benefit from some copy editing. \n\nThe competitive learning approach seems rather adhoc and this paper feels quite incomplete without significant discussion and comparisons to ensembling. Much recent work has shown that duplicating and ensembling neural architectures can produce gains, and it\u2019s not clear why competitive learning is better than ensembling, it seems less theoretically sound to me.\n\nThere is a huge confound in the experiments due to the competitive learning architecture having many more free parameters than the baseline architecture. Again I think comparing to ensembling with the same number of architectures duplicated and perhaps comparing to a single baseline with larger hidden layers to make the total number of free parameters comparable is critical to validating the proposed approach.\n\nThe graphical model of the driving process depicted in figure 1 seems nonsensical. If e is observed then all variables are known given the dependencies shown. Further, it is at best very poor notation to say that the driving action d decided at time t affects the vehicle state s at that same time. It should be that s_t depends on d_(t-1). Also, according to this figure the driving decision d does not depend on the observed vehicle state x which also seems invalid.\n\nOdd to have a paragraph break in abstract\n\nFigure 1 caption should include a brief explanation of the variables shown\n","label":0,"model":"human","source":"peerread","id":4979}
{"text":"This paper proposes SEM, a simple large-size multilabel learning algorithm which models the probability of each label as softmax(sigmoid(W^T X) + b), so a one-layer hidden network. This in and of itself is not novel, nor is the idea of optimizing this by adagrad. Though it's weird that the paper explicitly derives the gradient and suggests doing alternating adagrad steps instead of the more standard adagrad steps; it's unclear whether this matters at all for performance. The main trick responsible for increasing the efficiency of this model is the candidate label sampling, which is done in a relatively standard way by sampling labels proportionally to their frequency in the dataset.\n\nGiven that neither the model nor the training strategy is novel, it's surprising that the results are better than the state-of-the-art in quality and efficiency (though non-asymptotic efficiency claims are always questionable since implementation effort trades off fairly well against performance). I feel like this paper doesn't quite meet the bar.","label":0,"model":"human","source":"peerread","id":4980}
{"text":"This is largely a well written paper proposing a sensible approach for multilabel learning that is shown to be effective in practice. However, the main technical elements of this work: the model used and its connections to basic MLPs and related methods in the literature, the optimization strategy, and the speedup tricks are all familiar from prior work. Hence the reviewers are somewhat unanimous in their view that the novelty aspect of this paper is its main shortcomings. The authors are encouraged to revise the paper and clarify the precise contributions.","label":0,"model":"human","source":"peerread","id":4981}
{"text":"The paper proposes a semantic embedding based approach to multilabel classification. \nConversely to previous proposals, SEM considers the underlying parameters determining the\nobserved labels are low-rank rather than that the observed label matrix is itself low-rank. \nHowever, It is not clear to what extent the difference between the two assumptions is significant\n\nSEM models the labels for an instance as draws from a multinomial distribution\nparametrized by nonlinear functions of the instance features. As such, it is a neural network.\nThe proposed training algorithm is slightly more complicated than vanilla backprop.  The significance of the results compared to NNML (in particular on large datasets Delicious and EUrlex) is not very clear. \n\nThe paper is well written and the main idea is clearly presented. However, the experimental results are not significant enough to compensate the lack of conceptual novelty. \n\n\n","label":0,"model":"human","source":"peerread","id":4982}
{"text":"The paper presents the semantic embedding model for multi-label prediction.\nIn my questions, I pointed that the proposed approach assumes the number of labels to predict is known, and the authors said this was an orthogonal question, although I don't think it is!\nI was trying to understand how different is SEM from a basic MLP with softmax output which would be trained with a two step approach instead of stochastic gradient descent. It seems reasonable given their similarity to compare to this very basic baseline.\nRegarding the sampling strategy to estimate the posterior distribution, and the difference with Jean et al, I agree it is slightly different but I think you should definitely refer to it and point to the differences.\nOne last question: why is it called \"semantic\" embeddings? usually this term is used to show some semantic meaning between trained embeddings, but this doesn't seem to appear in this paper.\n","label":0,"model":"human","source":"peerread","id":4983}
{"text":"The authors propose to sample from VAEs through a Markov chain [z_t ~ q(z|x=x_{t-1}), x_t ~ p(x|z=z_t)]. The paper uses confusing notation, oversells the novelty, ignoring some relevant previous results. The qualitative difference between regular sampling and this Gibbs chain is not very convincing, judging from the figures. It would be a great workshop paper (perhaps more), if the authors fix the notation, fix the discussion to related work, and produce more convincing (perhaps simply upscaled?) figures.\n\nComments: \n - Rezende et al's (2014) original VAE paper already discusses the Markov chain, which is ignored in this paper\n - Notation is nonstandard \/ confusing. At page 1, it\u2019s unclear what the authors mean with \u201cp(x|z) which is approximated as q(x|z)\u201d.\n- It\u2019s also not clear what\u2019s meant with q(z). At page 2, q(z) is called the learned distribution, while p(z) can in general also be a learned distribution.\n- It\u2019s not true that it\u2019s impossible to draw samples from q(z): one can sample x ~ q(x) from the dataset, then draw z ~ q(z|x).\n- It's not explained whether the analysis only applies to continuous observed spaces, or also discrete observed spaces\n- Figures 3 and 4 are not very convincing.","label":0,"model":"human","source":"peerread","id":4984}
{"text":"This approach taken in this paper is topical, especially since the importance of sampling and generating diverse samples is increasingly discussed in work on generative models. There were several concerns from reviewers, in three areas particularly: connection and comparison to related work; lack of clarity and understanding of the paper; experiments that are not sufficiently convincing. These have been addressed to some extent by the authors, discussing in more detail the related work, especially in connection to Rezende et al., and GSN of Bengio et al., and with improved figures. But these points are still of concern especially in terms of assessing sample diversity in relation to much of the recent work on richer variational posterior methods and other techniques. For these reasons, the paper is not yet ready for acceptance at this years conference.","label":0,"model":"human","source":"peerread","id":4985}
{"text":"We have updated our paper to include changes that reflect comments made by the reviewers. A PDF document detailing our changes  may be found at: ","label":0,"model":"human","source":"peerread","id":4986}
{"text":"This paper attempts to learn a Markov chain to estimate a probability distribution over latent variables Z, such that P(X | Z) can be eased to generate samples from a data distribution.\n\nThe paper in its current form is not acceptable due to the following reasons:\n1. No quantitative evaluation. The authors do include samples from the generative model, which however are insufficient to judge performance of the model. See comment 2.\n2. The description of the model is very unclear. I had to indulge in a lot of charity to interpret what the authors \"must be doing\". What does Q(Z) mean? Does it mean the true posterior P(Z | X) ? What is the generative model here? Typically, it's P(Z)P(X|Z). VAEs use a variational approximation Q(Z | X) to the true posterior P(Z | X). Are you trying to say that your model can sample from the true posterior P(Z | X)?\n\nComments:\n1. Using additive noise in the input does not seem like a reasonable idea. Any justification of why this is being done?\n2. Approaches which learn transition operators are usually very amenable to data augmentation-based semi-supervised learning. I encourage the authors to improve their paper by testing their model on semi-supervised learning benchmarks.","label":0,"model":"human","source":"peerread","id":4987}
{"text":"The authors propose to sample from VAEs through a Markov chain [z_t ~ q(z|x=x_{t-1}), x_t ~ p(x|z=z_t)]. The paper uses confusing notation, oversells the novelty, ignoring some relevant previous results. The qualitative difference between regular sampling and this Gibbs chain is not very convincing, judging from the figures. It would be a great workshop paper (perhaps more), if the authors fix the notation, fix the discussion to related work, and produce more convincing (perhaps simply upscaled?) figures.\n\nComments: \n - Rezende et al's (2014) original VAE paper already discusses the Markov chain, which is ignored in this paper\n - Notation is nonstandard \/ confusing. At page 1, it\u2019s unclear what the authors mean with \u201cp(x|z) which is approximated as q(x|z)\u201d.\n- It\u2019s also not clear what\u2019s meant with q(z). At page 2, q(z) is called the learned distribution, while p(z) can in general also be a learned distribution.\n- It\u2019s not true that it\u2019s impossible to draw samples from q(z): one can sample x ~ q(x) from the dataset, then draw z ~ q(z|x).\n- It's not explained whether the analysis only applies to continuous observed spaces, or also discrete observed spaces\n- Figures 3 and 4 are not very convincing.\n","label":0,"model":"human","source":"peerread","id":4988}
{"text":"The authors argues that the standard ancestral sampling from stochastic autoencoders (such as the Variational Autoencoder and the Adversarial\nAutoencoder) imposes the overly-restrictive constraint that the encoder distribution must marginally match the latent variable prior. They propose, as an alternative, a Markov Chain Monte Carlo approach that avoids the need to specify a simple parametric form for the prior.\n\nThe paper is not clearly written. Most critically, the notation the authors use is either deeply flawed, or there are simple misunderstanding with respect to the manipulations of probability distributions. For example, the authors seem to suggest that both distributions Q(Z|X) and Q(X|Z) are parametrized. For this to be true the model must either be trivially simple, or an energy-based model. There is no indication that they are speaking of an energy-based model. Another example of possible confusion is the statement that the ratio of distributions Q(Z|X)\/P(Z) = 1. I believe this is supposed to be a ratio of marginals: Q(Z)\/P(X) = 1. Overall, it seems like there is a confusion of what Q and P represent. The standard notation used in VAEs is to use P to represent the decoder distribution and\nQ to represent the encoder distribution. This seems not to be how the authors are using these terms. Nor does it seem like there is a single consistent interpretation. \n\nThe empirical results consist entirely of qualitative results (samples and reconstructions) from a single dataset (CelebA). The samples are also not at all up to the quality of the SOTA models. The interpolations shown in Figures 1 and 3 both seems to look like interpolation in pixel space for both the VAE model and the proposed DVAE. \n","label":0,"model":"human","source":"peerread","id":4989}
{"text":"While you cite Rezende et al. 2014 when referring to VAEs, you claim the main contribution of your work is the generation of posterior samples from a Markov Chain. However, Rezende et al. 2014 presented a very similar idea. Let me quote them directly:\n\nWe do not integrate over the missing\nvalues, but use a procedure that simulates a Markov\nchain that we show converges to the true marginal distribution\nof missing given observed pixels.\n\n-- Section 5.4\n\nImage completion can be approximatively achieved by\na simple iterative procedure which consists of (i) initializing\nthe non-observed pixels with random values;\n(ii) sampling from the recognition distribution given\nthe resulting image; (iii) reconstruct the image given\nthe sample from the recognition model; (iv) iterate the\nprocedure.\n\n-- Appendix F\n\nHow is this procedure different from your main contribution? I mean, they motivate with missing data imputation and start in a different way, but the main loop seems the same. Even if there is a significant difference between the procedures, I believe this should be addressed in the paper. I also suggest taking a closer look at appendix F, they provide some proofs which seem relevant to your work.\n","label":0,"model":"human","source":"peerread","id":4990}
{"text":"How are the novelty of samples affected as more samples are generated from the Markov Chain? I played around with a similar idea and found that after about 6-7 Monte Carlo samples, the images looked identical to the training data. Do you observe something similar in your experiments?\n\nAlso, the interpolation experiments are interesting. It seems that the proposed method implicitly interpolates along the Fischer metric rather than the Euclidean metric. Some discussion on this might be illuminating. ","label":0,"model":"human","source":"peerread","id":4991}
{"text":"This paper proposed to use GAN for encrypted communications.\n\nIn section 2, the authors proposed a 3 part neural network trained to encode and decode data. This model does not have any practical value except paving the way for describing the next model in section 3: it is strictly worse than any provable cryptography system.\n\nIn section 3, the authors designed a task where they want to hide part of the data, which has correlated fields, while publishing the rest. However, I'm having trouble thinking of an application where this system is better than simply decorrelating the data and encrypting the fields one wants to hide with a provable cryptography system while publishing the rest in plain text.","label":0,"model":"human","source":"peerread","id":4992}
{"text":"Interesting paper but not over the accept bar.","label":0,"model":"human","source":"peerread","id":4993}
{"text":"The submission proposes to modify the typical GAN architecture slightly to include \"encrypt\" (Alice) and \"decrypt\" (Bob) modules as well as a module trying to decrypt the signal without a key (Eve).  Through repeated transmission of signals, the adversarial game is intended to converge to a system in which Alice and Bob can communicate securely (or at least a designated part of the signal should be secure), while a sophisticated Eve cannot break their code.  Examples are given on toy data:\n\"As a proof-of-concept, we implemented Alice, Bob, and Eve networks that take N-bit random plain-text and key values, and produce N-entry floating-point ciphertexts, for N = 16, 32, and 64.  Both plaintext and key values are uniformly distributed.\"\n\nThe idea considered here is cute.  If some, but not necessarily all of the signal is meant to be secure, the modules can learn to encrypt and decrypt a signal, while an adversary is simultaneously learned that tries to break the encryption.  In this way, some of the data can remain unencrypted, while the portion that is e.g. correlated with the encrypted signal will have to be encrypted in order for Eve to not be able to predict the encrypted part.\n\nWhile this is a nice thought experiment, there are significant barriers to this submission having a practical impact:\n1) GANs, and from the convergence figures also the objective considered here, are quite unstable to optimize.  The only guarantees of privacy are for an Eve that is converged to a very strong adversary (stronger than a dedicated attack over time).  I do not see how one can have any sort of reliable guarantee of the safety of the data transmission from the proposed approach, at least the paper does not outline such a guarantee.\n2) Public key encryption systems are readily available, computationally feasible, and successfully applied almost anywhere.  The toy examples given in the paper do not at all convince me that this is solving a real-world problem at this point.  Perhaps a good example will come up in the near future, and this work will be shown to be justified, but until such an example is shown, the approach is more of an interesting thought experiment.","label":0,"model":"human","source":"peerread","id":4994}
{"text":"The paper deals with an interesting application of adversarial training to encryption. It considers the standard scenario of Alice, Eve and Bob, where A and B aim to exchange messages conditioned on a shared key, while Eve should be unable to encrypt the message. Experiments are performed in a simple symmetric 16 bit encryption task, and an application on privacy. The concepts, ideas and previous literature are quite nicely and carefully presented.\n\nThe only major concern I have - and I apologize to the authors for not raising this earlier - are the experiments in section 3. In particular, I don't quite get the scenario. The reasoning here seems to be as follows: given information < A, B, C, D >, I want to give the public the value of D (e.g. movies watched) without releasing information about C (e.g. gender). In this scenario, Eve would need to be able to reconstruct D as good as possible without gaining information about C. What is described in section 3, however, is that D and D-public are both reconstructed by Bob, but why would Bob reconstruct the latter (he is not public, in particular because he is allowed to reconstruct C, which is not tested here)? Also, Eve only tries to estimate C, thus rendering the scenario not different in any way to the scenario considered in section 2.\n\nI have two more minor concerns:\n\n1) As raised in the pre-review, Eve should actually be stronger then Alice and Bob in order to be able to compensate for the missing key. The authors noted they have been doing these experiments and are going to add the results.\n\n2) In any natural encryption case I would expect the length of the key to be much shorter then the length of the message. This, however, could potentially make the scenario much easier for Eve (although I doubt any of the results will change if the key is long enough).\n\nI like the creative application of adversarial training to a completely different domain, and I believe it could be the starting point of a very interesting direction in cryptographic systems or in privacy applications (although it is unclear whether the weak guarantees of neural network based approaches can ever be overcome). At the same time the application in the privacy setting leaves me quite confused, and the symmetric encryption example is not particularly strong either. I'd appreciate if the authors could address the major concern I raised above, and I will be quite happy to raise the score in case this confusion can be resolved.","label":0,"model":"human","source":"peerread","id":4995}
{"text":"This manuscript tries to tackle neural network regularization by blending the target distribution with predictions of the model itself. In this sense it is similar in spirit to scheduled sampling (Bengio et al) and SEARN (Daume et al) DAgger (Ross et al) which consider a \"roll-in\" mixture of the target and model distributions during training. It was clarified in the pre-review questions that these targets are generated on-line rather than from a lagged distribution, which I think makes the algorithm pseudocode somewhat misleading if I understand it correctly.\n\nThis is an incremental improvement on the idea of label softening\/smoothing that has recently been revived, and so the novelty is not that high. The author points out that co-label similarity is better preserved by this method but it doesn't follow that this is causal re: regularization; a natural baseline would be a fixed, soft label distribution, as well as one where the softening\/temperature of the label distribution is gradually reduced (as one would expect for this method to do as the model gets closer and closer to reproducing the target distribution).\n\nIt's an interesting and somewhat appealing idea but the case is not clearly made that this is all that useful. The dropout baselines for MNIST seem quite far from results already in the literature (Srivastava et al 2014 achieves 1.06% with a 3x1024 MLP with dropout and a simple max norm constraint; the dropout baselines here fail to break 1.3% which is rather high by contemporary standards on the permutation-invariant task), and results for CIFAR10 are quite far from the current state of the art, making it difficult to judge the contribution in light of other innovations. The largest benchmark considered is SVHN where the reported accuracies are quite bad indeed; SOTA for single net performance has been less than half the reported error rates for 3-4 years now. It's unclear what conclusions can be drawn about how this would help (or even hurt) in a better-tuned setting.\n\nI have remaining reservations about data hygiene, namely reporting minimum test loss\/maximum test accuracy rather than an unbiased method for model selection (minimum validation set error, for example). Relatedly, the regularization potential of early stopping on a validation set is not considered. See, e.g. the protocol in Goodfellow et al (2013).","label":0,"model":"human","source":"peerread","id":4996}
{"text":"The paper introduced a regularization scheme through soft-target that are produced by mixing between the true hard label and the current model prediction. Very similar method was proposed in Section 6 from (Hinton et al. 2016, Distilling the Knowledge in a Neural Network). \n\nPros: \n+ Comprehensive analysis on the co-label similarity.\n\nCons:\n- Weak baselines. I am not sure the authors have found the best hyper-parameters in their experiments. I just trained a 5 layer fully connected MNIST model with 512 hidden units without any regularizer and achieved 0.986 acc. using Adam and He initialization, where the paper reported 0.981 for such architecture. \n- The authors failed to bring the novel idea. It is very similar to (Hinton et al. 2016). This is probably not enough for ICLR.","label":0,"model":"human","source":"peerread","id":4997}
{"text":"Inspired by the analysis on the effect of the co-label similarity (Hinton et al., 2015), this paper proposes a soft-target regularization that iteratively trains the network using weighted average of the exponential moving average of past labels and hard labels as target argument of loss. They claim that this prevents the disappearing of co-label similarity after early training and  yields a competitive regularization to dropout without sacrificing network capacity.\n\nIn order to make a fair comparison to dropout,  the dropout should be tuned carefully. Showing that it performs better than dropout regularization for some particular values of dropout (Table 2) does not demonstrate a convincing advantage. It is possible that dropout performs better after a reasonable tuning with cross-validation.\n\nThe baseline architectures used in the experiments do not belong the recent state of art methods thus yielding significantly lower accuracy. It seems also that experiment setup does not involve any data augmentation, the results can also change with augmentation. It is not clear why number of epochs are set to a small number like 100 without putting some convergence tests.. Therefore the significance of the method is not convincingly demonstrated in empirical study.\n\nCo-label similarities could be calculated using softmax results at final layer rather than using predicted labels.  The advantage over dropout is not clear in Figure 4, the dropout is set to 0.2 without any cross-validation.  \n\n\nRegularizing by enforcing the training steps to keep co-label similarities is interesting idea but not very novel and the results are not significant.\n\nPros : \n- provides an investigation of regularization on co-label similarity during training\n\nCons:\n-The empirical results do not support the intuitive claims regarding proposed procedure\nIterative version can be unstable in practice\n\n\n","label":0,"model":"human","source":"peerread","id":4998}
{"text":"\n- You definitely need to report misclassification error results on test data for obvious reasons related to losses and final test misclassification error. Currently comparisons are not conclusive.\n\n-  Can you explain better the reason for using the particular updates in (3) and (4) better? Why don't you do for example totally corrective update, e.g. take convex combination of all \\cal{F}'s (or some portion) up to current iteration in (3)? Therefore \\beta and \\gamma should be tuned reasonably well to see whether (3) and (4) is really helping or not and the range for cross validation should be reported.\n\n- The reason to set n_t n_b is not satisfactory.  It is crucial to cross-validate such parameters. Isn't  n_t = {1,2} unreasonably small number that can cause unstable results? why all n_b and n_t are equal?Are there results on other n_b and n_t's that were tried?\n\n- It is stated that colabel similarities disappear when network starts to overfit. However distillation ( Hinton et.al. ,2015 ) captures colabel similarities after training a model and using distillation. This method seems an iterative extension of distillation without using a bigger teacher model. Does proposed method gives better results then a two step version of distillation ?  \n\n- How do you tune \\lambda for weight decay? \n\n- From paper: \"We considered a frozen set of hyper-parameters for the SoftTarget regularization to show that SoftTarget regularization can still work without a having to conduct a large grid search\". This argument is not valid in ML, maybe if you did a reasonable search, you would get worse results (since you should not look test error until you finish the cross-validation).   Why a common hyper parameter tuning procedure is not used e.g. random search (Bergstra and Bengio, JMLR 2012) or Bayesian optimization (Snoek et al ,NIPS 2012) ?  Setting the hyper parameters to some numbers without searching a range or set can dramatically ruin fair comparison. ","label":0,"model":"human","source":"peerread","id":4999}
